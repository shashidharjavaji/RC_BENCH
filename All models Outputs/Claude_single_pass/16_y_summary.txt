Claim 1:
Type: result
Statement: LLMs exhibit significant deficiencies in expertise in financial domain knowledge and mathematical capabilities when analyzing XBRL reports
Location: Section 3.2
Exact Quote: Even the best-performing model, Qwen2-7B, only achieves an 81% score in XBRL Term and a mere 51% in Domain Query to XBRL Reports...Even the best-performing model, Llama3-8B, only achieves 38% accuracy in Financial Formula Calculation and 24% in Numeric Query to XBRL Reports task

Evidence:
- Evidence Text: Performance metrics for XBRL domain knowledge tasks
  Strength: strong
  Location: Section 3.2
  Limitations: Limited to three specific LLM models
  Exact Quote: Qwen2-7B achieves 81% in XBRL Term and 51% in Domain Query to XBRL Reports

- Evidence Text: Performance metrics for numerical calculations
  Strength: strong
  Location: Section 3.2
  Limitations: Tests limited to specific financial formulas and queries
  Exact Quote: Llama3-8B achieves 38% accuracy in Financial Formula Calculation and 24% in Numeric Query to XBRL Reports task

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Multiple performance metrics across different models consistently show poor performance in both domain knowledge and mathematical tasks
Key Limitations: Limited model selection, specific test cases may not represent full range of real-world scenarios

--------------------------------------------------

Claim 2:
Type: result
Statement: The proposed enhancement methods using retriever and calculator tools substantially improve LLM performance in XBRL analysis
Location: Section 5.2
Exact Quote: For XBRL Term, Qwen2-7B achieves 89% accuracy, followed by Llama3-8B (84%) and Gemma2-9B (83%)...For Financial Math, Llama3-8B achieves an accuracy of 63%, followed by Qwen2-7B (58%) and Gemma2-9B (52%), showing 25-35 percentage point improvements

Evidence:
- Evidence Text: Improvement in domain knowledge tasks with retriever
  Strength: strong
  Location: Section 5.2
  Limitations: Improvements vary across different models
  Exact Quote: These results represent increases of 14 to 17 percentage points

- Evidence Text: Improvement in mathematical tasks with calculator
  Strength: strong
  Location: Section 5.2
  Limitations: Modest gains in some numeric queries
  Exact Quote: showing 25-35 percentage point improvements

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: high
Justification: Clear performance improvements demonstrated across multiple tasks and models
Key Limitations: Varying levels of improvement across different tasks, some modest gains in certain areas

--------------------------------------------------

Claim 3:
Type: result
Statement: Combining both retriever and calculator tools yields better results than single tool implementation
Location: Section 5.2
Exact Quote: Numeric Query to XBRL Reports task exhibits profound improvements: Llama3-8B (53%), Gemma2-9B (49%), and Qwen2-7B (46%), representing increases of 25 to 30 percentage points compared to the single tool approach

Evidence:
- Evidence Text: Combined tool performance comparison
  Strength: strong
  Location: Section 5.2
  Limitations: Limited to specific query types
  Exact Quote: increases of 25 to 30 percentage points compared to the single tool approach

- Evidence Text: Ablation study results
  Strength: moderate
  Location: Section 5.3
  Limitations: Only tested retriever in isolation
  Exact Quote: results shown in Fig. 8 demonstrate notable improvements over the baseline (without tool) but are inferior to the combined retriever-and-calculator approach

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Consistent improvement patterns across models when using combined tools
Key Limitations: Complete ablation study for calculator alone not presented

--------------------------------------------------

