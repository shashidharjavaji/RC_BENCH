Claim 1:
Type: methodology
Statement: The modality-augmented training strategy enables effective joint training with video data across different modalities
Location: Section 3.2
Exact Quote: We hereby propose a novel training paradigm, termed Modality-Augmented Training (MAT), to jointly train three modal types of samples (i.e., visual-only, audio-only, and audio-visual joint samples) within a single batch.

Evidence:
- Evidence Text: Performance improvements shown in Table 4 comparing MAT to plain training approaches
  Strength: strong
  Location: Section 4.3
  Limitations: Limited to specific video QA tasks tested
  Exact Quote: Tab. 4 shows the results, where our MAT brings a +1.4% on MSVD-QA, + 2.2% MSRVTT-QA, and +1.6% ActivityNet-QA than PT.

- Evidence Text: Consistent performance improvements across different model architectures
  Strength: strong
  Location: Section 4.3
  Limitations: Tested only on a subset of possible architectures
  Exact Quote: Modality-Augmented Training (MAT) consistently outperforms the non-end-to-end single-modality Plain Training (PT) across various model architectures

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Multiple experimental results consistently show performance improvements across different tasks and architectures
Key Limitations: Limited to specific video understanding tasks and architectures tested

--------------------------------------------------

Claim 2:
Type: performance
Statement: Audio-Visual LLM achieves state-of-the-art performance on video question-answering tasks
Location: Section 4.2
Exact Quote: The results demonstrate that our method surpasses both prior non-LLM-based works and LLM-based works across all the datasets by a large margin.

Evidence:
- Evidence Text: Specific performance improvements on MSRVTT-QA
  Strength: strong
  Location: Section 4.2
  Limitations: Single dataset comparison
  Exact Quote: +6.6% accuracy on MSRVTT-QA, a +10.4% accuracy on MSVD-QA, and a +2.4% accuracy on ActivityNet-QA

- Evidence Text: Detailed performance comparisons in Table 1
  Strength: strong
  Location: Table 1
  Limitations: Limited to specific benchmarks
  Exact Quote: Ours CC, WV, VS, WC, ACAV, COCO 1.6M V, A 53.7+4.4 67.3+1.9 47.2+2.4

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Comprehensive benchmark comparisons showing consistent improvements across multiple datasets
Key Limitations: Limited to specific video QA tasks, may not generalize to all video understanding scenarios

--------------------------------------------------

Claim 3:
Type: performance
Statement: The method achieves competitive performance on audio tasks despite focusing primarily on video understanding
Location: Section 4.2
Exact Quote: our method performs consistent advantages over prior works by a +3.5% CIDEr on ClothoV1 and a +2.1% CIDEr on AudioCaps

Evidence:
- Evidence Text: Performance comparisons on audio captioning benchmarks
  Strength: moderate
  Location: Table 3
  Limitations: Limited to two audio benchmarks
  Exact Quote: Table 3. Comparison with existing LLM-based methods on 2 audio captioning benchmarks.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Shows improvements on audio tasks but with limited scope of evaluation
Key Limitations: Tested on only two audio benchmarks, may not generalize to all audio tasks

--------------------------------------------------

