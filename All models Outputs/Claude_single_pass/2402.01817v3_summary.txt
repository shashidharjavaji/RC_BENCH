Claim 1:
Type: result
Statement: LLMs cannot generate executable plans in autonomous mode
Location: Section 2.1
Exact Quote: On average, only about 12% of the plans that the best LLM (GPT-4) generates are actually executable without errors and goal-reaching.

Evidence:
- Evidence Text: Experimental results on planning benchmark showing poor performance across multiple LLMs
  Strength: strong
  Location: Section 2.1, Table 1
  Limitations: Limited to specific planning domains tested
  Exact Quote: Results of state-of-the-art LLMs GPT-4o, GPT-4-Turbo, Claude-3-Opus, Gemini Pro and LLaMA-3 70B for Plan Generation with prompts in natural language show dismal performance

- Evidence Text: Performance deteriorates further with obfuscated names
  Strength: moderate
  Location: Section 2.1
  Limitations: Specific to naming modifications
  Exact Quote: We demonstrate that the performance deteriorates further if the names of the actions and objects in the domain are obfuscated

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Multiple experiments across different LLMs and conditions consistently show poor performance
Key Limitations: Testing limited to specific planning domains and benchmarks

--------------------------------------------------

Claim 2:
Type: result
Statement: LLMs cannot verify plans and thus cannot improve through self-critiquing
Location: Section 2.2
Exact Quote: Our results indicate that in direct mode, LLMs are, perhaps not surprisingly, pretty bad at solving graph coloring instances. More interestingly, they are no better at verifying solutions.

Evidence:
- Evidence Text: Experiments on graph coloring tasks showing poor verification ability
  Strength: strong
  Location: Section 2.2
  Limitations: Focused primarily on graph coloring domain
  Exact Quote: We report that the performance is in fact worse because the system can't recognize a correct coloring and thus merrily passes over fortuitously correct colorings it has generated

- Evidence Text: Similar results in planning domains
  Strength: moderate
  Location: Section 2.2
  Limitations: Reference to other work without detailed results
  Exact Quote: Similar results have also been reported for planning problems in (Valmeekam et al., 2023c)

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Consistent findings across multiple domains, though primary evidence from graph coloring
Key Limitations: Could benefit from more diverse problem domains

--------------------------------------------------

Claim 3:
Type: result
Statement: LLM-Modulo Framework improves planning performance
Location: Section 4
Exact Quote: with back prompting from VAL acting as the external verifier and critic, LLM performance in Blocks World improves to 82% within 15 back prompting rounds, while in Logistics, it improves to 70%

Evidence:
- Evidence Text: Significant improvement in Blocks World and Logistics domains
  Strength: strong
  Location: Section 4
  Limitations: Limited to specific planning domains
  Exact Quote: LLM performance in Blocks World improves to 82% within 15 back prompting rounds, while in Logistics, it improves to 70%

- Evidence Text: Improvement in travel planning benchmark
  Strength: moderate
  Location: Section 4
  Limitations: Preliminary results
  Exact Quote: Our preliminary results show that LLM-Modulo based agentification with automated critics in the loop significantly improves the performance (6x of baselines)

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Consistent improvements shown across multiple domains, though some results are preliminary
Key Limitations: Limited domain coverage, some preliminary results

--------------------------------------------------

