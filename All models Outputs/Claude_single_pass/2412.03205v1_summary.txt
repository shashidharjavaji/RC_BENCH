Claim 1:
Type: contribution
Statement: U-MATH is a novel benchmark containing 1,100 unpublished university-level math problems with 20% being multimodal
Location: Introduction
Exact Quote: we introduce U-MATH, a novel benchmark of 1,100 unpublished open-ended university-level problems sourced from teaching materials. It is balanced across six core subjects, with 20% of multimodal problems.

Evidence:
- Evidence Text: Detailed breakdown of problem distribution across subjects
  Strength: strong
  Location: Dataset Statistics section
  Limitations: Sample selection process may introduce biases
  Exact Quote: The U-MATH benchmark comprises 1,100 carefully curated and validated mathematical problems... distributed across 6 core subjects with about 20% of the tasks incorporating visual elements

- Evidence Text: Problems sourced from Gradarius platform
  Strength: moderate
  Location: Dataset Collection section
  Limitations: May not represent full diversity of university math curricula
  Exact Quote: we collaborate with Gradarius, a platform providing learning content and software for top US universities specialized in mathematics

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Clear documentation of dataset composition and sourcing
Key Limitations: Potential selection bias in problem curation

--------------------------------------------------

Claim 2:
Type: result
Statement: The best LLM achieves only 63.4% accuracy on text problems and 45% on visual problems
Location: Results section
Exact Quote: The highest accuracy achieved was 63.4% on text-based tasks and 45.0% on visual problems (Gemini-1.5-pro-002)

Evidence:
- Evidence Text: Detailed performance comparison across models
  Strength: strong
  Location: Table 4
  Limitations: Results depend on chosen evaluation metrics
  Exact Quote: Gemini 1.5 Pro 60.1 63.4 45.0 91.3 60.0 50.7 47.1 27.3 24.1 60.7 57.1 87.3 70.0 63.3 50.0

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Comprehensive evaluation across multiple models with clear metrics
Key Limitations: Evaluation depends on GPT-4 as judge which has its own limitations

--------------------------------------------------

Claim 3:
Type: result
Statement: Solution assessment capabilities of LLMs are limited with best F1-score of 80% on µ-MATH
Location: Meta-evaluation Results section
Exact Quote: Solution assessment remains difficult, with Gemini hiy top µ-MATH F1-score of 80%, showing room for improvement

Evidence:
- Evidence Text: Detailed meta-evaluation results
  Strength: strong
  Location: Table 5
  Limitations: Limited to subset of problems
  Exact Quote: Gemini 1.5 Pro 63.4 80.7 / 69.1 77.5 84.5 85.2 76.4

- Evidence Text: Analysis of judge behavior patterns
  Strength: moderate
  Location: Meta-evaluation section
  Limitations: May not generalize to all types of math problems
  Exact Quote: proprietary models tend to be more conservative — having relatively high TNR compared to their TPR

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Comprehensive analysis of judgment capabilities with multiple metrics
Key Limitations: µ-MATH only covers 25% of U-MATH problems

--------------------------------------------------

