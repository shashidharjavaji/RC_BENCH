Claim 1:
Type: result
Statement: Small fractions (10%) of poor quality judgments from apathetic or adversarial voting can substantially impact model rankings on Chatbot Arena
Location: Section 3.1 and 3.2
Exact Quote: We find that only 10% of apathetic votes in the dataset can change the leaderboard rankings of 2/3 models by 5 places

Evidence:
- Evidence Text: Experimental results showing rank changes with 10% apathetic votes
  Strength: strong
  Location: Table 1
  Limitations: Limited to 3 test models only
  Exact Quote: 10% poor quality annotations can change the rank of 2/3 systems by 5 places

- Evidence Text: Experimental results showing rank changes with adversarial votes
  Strength: strong
  Location: Table 2
  Limitations: Assumes adversaries can get 10% of total votes
  Exact Quote: Across all models, we show that adversarial attacks can substantially change leaderboard rankings if adversaries get to contribute 10% votes for their model

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Clear experimental evidence showing impact on rankings, though limited to small set of test models
Key Limitations: Small test set of models, assumes ability to get 10% vote share

--------------------------------------------------

Claim 2:
Type: methodology
Statement: Simple model attribution algorithms can effectively detect target model outputs to enable adversarial voting
Location: Section 3.2
Exact Quote: We find that our detector algorithm reports very high performance (e.g. TPR=91.13%, and TNR=88.46% for Llama-2-7b-chat)

Evidence:
- Evidence Text: Detection performance metrics across 3 models
  Strength: strong
  Location: Table 3
  Limitations: Only tested on 3 models, requires access to model logits
  Exact Quote: TPR=91.13%, and TNR=88.46% for Llama-2-7b-chat

- Evidence Text: Proof-of-concept attack on live platform
  Strength: moderate
  Location: Section 3.2
  Limitations: Only demonstrated vote submission, not manipulation
  Exact Quote: We programmatically launched 100 queries into Chatbot Arena, extracted the two model responses, and successfully submitted a preference vote

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: High detection performance demonstrated but with important limitations
Key Limitations: Requires model access, limited test set, proof-of-concept only demonstrated submission not manipulation

--------------------------------------------------

Claim 3:
Type: result
Statement: Even well-incentivized annotators show low agreement on subjective prompts
Location: Section 3.3
Exact Quote: Overall, we find very low agreement between these well-intentioned annotators with clear guidelines, irrespective of the performance difference between the model pairs

Evidence:
- Evidence Text: Inter-annotator agreement scores across evaluation dimensions
  Strength: moderate
  Location: Table 4
  Limitations: Small study with only 4 annotators
  Exact Quote: Fleiss' Kappa between four annotators on different evaluation axis

Evaluation:
Conclusion Justified: Yes
Robustness: low
Confidence Level: low
Justification: Shows low agreement but from very limited annotator pool
Key Limitations: Very small annotator sample size, limited prompt types tested

--------------------------------------------------

