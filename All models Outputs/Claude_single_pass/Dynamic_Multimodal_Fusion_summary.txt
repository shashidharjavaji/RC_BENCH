Claim 1:
Type: performance
Statement: DynMM can reduce computation costs while maintaining accuracy on CMU-MOSEI sentiment analysis
Location: Section 4.3
Exact Quote: Compared with the best performing static network (i.e., Late Fusion), DynMM-a can reduce computations by 46.5% with a slightly decreased accuracy (i.e., -0.47%)

Evidence:
- Evidence Text: DynMM-a achieves 79.07% accuracy with 165.5M MAdds vs Late Fusion's 79.54% accuracy with 309.6M MAdds
  Strength: strong
  Location: Table 2
  Limitations: Only tested on one dataset
  Exact Quote: DynMM-a 79.07 0.62 165.5

- Evidence Text: DynMM-b improves both efficiency and accuracy
  Strength: strong
  Location: Section 4.3
  Limitations: Trade-off between computation and accuracy not fully explored
  Exact Quote: DynMM-b improves both inference efficiency (i.e., reduce MAdds by 17.8%) and prediction accuracy

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Multiple variants of DynMM demonstrate consistent computation-accuracy trade-offs with detailed metrics
Key Limitations: Results limited to one sentiment analysis dataset

--------------------------------------------------

Claim 2:
Type: performance
Statement: DynMM improves robustness against noisy multimodal data compared to static fusion
Location: Section 4.4
Exact Quote: the performance gap between DynMM and ESANet becomes larger when the noise level of depth images increases

Evidence:
- Evidence Text: Experimental results with injected Gaussian noise showing better performance
  Strength: moderate
  Location: Section 4.4
  Limitations: Only tested with synthetic noise
  Exact Quote: We consider three settings by injecting random Gaussian noise with probability 1/3 to (1) RGB modality; (2) depth modality and (3) both modalities

- Evidence Text: Visual evidence of better segmentation under noise
  Strength: moderate
  Location: Figure 7
  Limitations: Limited qualitative examples
  Exact Quote: DynMM is more robust to noisy multimodal data compared with the static ESANet

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Both quantitative and qualitative evidence support improved robustness, though testing conditions are somewhat limited
Key Limitations: Only tested with synthetic noise, limited test scenarios

--------------------------------------------------

Claim 3:
Type: performance
Statement: DynMM achieves better balance between performance and efficiency compared to SOTA methods on semantic segmentation
Location: Section 4.4
Exact Quote: These results clearly show that our proposed method achieves the best balance between performance and efficiency

Evidence:
- Evidence Text: Comparative results on NYU Depth V2 dataset
  Strength: strong
  Location: Table 4
  Limitations: Single dataset evaluation
  Exact Quote: DynMM-b ResNet-50 51.0 52.2

- Evidence Text: Computation cost comparison
  Strength: strong
  Location: Section 4.4
  Limitations: Limited architecture variations tested
  Exact Quote: The computation cost of DynMM is similar to a unimodal lightweight RefineNet, yet its performance can match methods that use ResNet-101 as the backbone

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Comprehensive comparison with multiple SOTA methods showing better efficiency-performance trade-off
Key Limitations: Limited to one dataset and specific architectural choices

--------------------------------------------------

