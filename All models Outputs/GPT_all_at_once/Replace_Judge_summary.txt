Claim 1:
Type: methodology
Statement: Panel of LLm evaluators (PoLL) drawn from different model families outperforms a single large judge
Location: Abstract
Exact Quote: using a Panel of LLm evaluators (PoLL). Across three distinct judge settings and spanning six different datasets, we find that using a PoLL composed of a larger number of smaller models outperforms a single large judge

Evidence:
- Evidence Text: PoLL has the highest correlation with human judgements across multiple datasets, exhibiting less intra-model bias and being over seven times cheaper.
  Strength: strong
  Location: Abstract/Section 5
  Limitations: Evaluation limited to three settings and a select number of datasets.
  Exact Quote: using a PoLL composed of a larger number of smaller models outperforms a single large judge, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive

- Evidence Text: Experimental results show PoLL led to higher kappa values in judge model performance, indicating better agreement with human judgement.
  Strength: strong
  Location: Section 4.1/4.2/Table 1 & 2
  Limitations: Results may not generalize to all types of evaluations or datasets not included in the study.
  Exact Quote: In Table 1...PoLL 0.917...In Table 2...PoLL is best correlated with the gold rankings

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: PoLL's advantages are consistently demonstrated across multiple datasets and experimental settings, with quantifiable improvements in correlation to human judgements and reduced costs.
Key Limitations: Limited evaluation settings and datasets may constrain generalizability of findings.

--------------------------------------------------

Claim 2:
Type: result
Statement: GPT-4 can be a weak judge with high variance in performance based on minor prompt changes.
Location: Section 4.3
Exact Quote: Based on the observation that GPT-4 was the weakest judge model on our KILT evaluations, we investigated how the model reacts to modifications to its prompt

Evidence:
- Evidence Text: GPT-4's agreement level with human annotators changes significantly based on prompt alterations.
  Strength: moderate
  Location: Section 4.3/Table 3
  Limitations: The study focuses on GPT-4's performance without exploring the behaviors of other models under similar conditions.
  Exact Quote: These changes bring the agreement level for GPT-4 up to the level of GPT-3.5 when using our few-shot standard prompt, though still below Command-R, Haiku, and PoLL

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Evidence shows clear variability in GPT-4's performance with different prompts, supporting the claim of its variance and relative weakness as a judge under certain conditions.
Key Limitations: The limited focus on GPT-4 without comparative analysis across models restricts understanding of whether this is a unique or common issue.

--------------------------------------------------

Claim 3:
Type: contribution
Statement: PoLL reduces intra-model scoring bias by pooling judgements across a panel of heterogeneous evaluator models.
Location: Section 4.4
Exact Quote: One of the biggest motivators for replacing a single large judge with a panel of heterogeneous models is to reduce bias in evaluation

Evidence:
- Evidence Text: Figures 3 and 4 demonstrate smaller spread in PoLL's scores and lower bias compared to single judges
  Strength: strong
  Location: Section 4.4
  Limitations: Analysis focused solely on variance and bias, without direct measurement of accuracy or other performance metrics.
  Exact Quote: We can see how the different judges score different models and how far those predictions deviate from human annotator decisions

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Evidence from specified figures and comparative analysis strongly supports the claim of reduced bias and variance via PoLL.
Key Limitations: The assessment of reduced bias is primarily comparative, not absolute, and does not account for potential external factors influencing model judgment.

--------------------------------------------------

