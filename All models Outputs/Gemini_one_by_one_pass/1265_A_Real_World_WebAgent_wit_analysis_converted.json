{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "We propose a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA) designed to identify query-relevant neurons in LLMs.",
                "type": "",
                "location": "1 Introduction",
                "exact_quote": "We propose a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA) designed to identify query-relevant neurons in LLMs."
            },
            "evidence": [
                {
                    "evidence_text": "We propose QRNCA to detect query-relevant neurons for each input query.",
                    "strength": "strong",
                    "limitations": "There is no direct evidence to support this claim; however, this paper's main contribution is the development of QRNCA and it is defined here as such.",
                    "location": "Introduction",
                    "exact_quote": "We propose a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA) designed to identify query-relevant neurons in LLMs."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "QRNCA aims to extract Query-Relevant (QR) neurons for each input query.",
                "type": "",
                "location": "1 Introduction",
                "exact_quote": "QRNCA aims to extract Query-Relevant (QR) neurons for each input query."
            },
            "evidence": [
                {
                    "evidence_text": "QRNCA, the proposed framework, aims to extract QueryRelevant (QR) neurons for each input query.",
                    "strength": "strong",
                    "limitations": "none stated",
                    "location": "Section 4.1",
                    "exact_quote": "QRNCA aims to extract Query-Relevant (QR) neurons for each input query."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "The principal advantages of our framework are its architecture-agnostic nature and its capability of handling long-form text generation effectively, as shown in Table 1.",
                "type": "",
                "location": "1 Introduction",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "QRNCA, just like Knowledge Attribution, can be used to locate knowledge neurons in a variety of architectures.",
                    "strength": "strong",
                    "limitations": "None specified",
                    "location": "Section 4.2 Neuron Attribution",
                    "exact_quote": "In order to measure the attribution score (or contribution) of a neuron, they gradually change the wi[l] from 0 to its original value computed during the forward pass and integrate the gradients (Sundararajan, Taly, and Yan 2017):"
                },
                {
                    "evidence_text": "Unlike Knowledge Attribution, which is only applicable to fill-in-the-blank tasks, QRNCA is architecture-agnostic and can be applied to other natural language tasks.",
                    "strength": "strong",
                    "limitations": "None specified",
                    "location": "Section 4 Locating Query-Relevant (QR) Neurons in Autoregressive LLMs",
                    "exact_quote": "While Knowledge Attribution (Dai et al. 2022) effectively identifies neurons linked to factual queries, its applicability is limited to encoder-only architectures, and it mandates the output to be a single-token word."
                },
                {
                    "evidence_text": "QRNCA outperforms Knowledge Attribution in terms of its ability to identify knowledge neurons in long-form text generation tasks.",
                    "strength": "strong",
                    "limitations": "None specified",
                    "location": "Section 5.3 QR Neurons Can Impact the Knowledge Expression",
                    "exact_quote": "Table 3 presents the overall performance of various methods. Our QRNCA method consistently outperforms the other baselines, evidenced by its higher PCR. This indicates that our identified QR neurons significantly affect the probability of correct answers while exerting a relatively low impact on unrelated queries."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "QRNCA outperforms baseline approaches.",
                "type": "",
                "location": "1 Introduction",
                "exact_quote": "Table 3 presents the overall performance of various methods. Our QRNCA method consistently outperforms other baseline approaches, evidenced by its higher PCR."
            },
            "evidence": [
                {
                    "evidence_text": "Our QRNCA method consistently outperforms other baseline approaches, evidenced by its higher PCR.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Section 5.1, Paragraph 2",
                    "exact_quote": "Table 3 presents the overall performance of various methods. Our QRNCA method consistently outperforms other baseline approaches, evidenced by its higher PCR."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "In summary, our main contribution is four-fold: (1) A scalable method: we propose QRNCA to detect query-relevant neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations.",
                "type": "",
                "location": "1 Introduction",
                "exact_quote": "In summary, our main contribution is four-fold: (1) A scalable method: ...; the QRNCA method is architecture-agnostic and can deal with long-form generations."
            },
            "evidence": [
                {
                    "evidence_text": "We propose QRNCA to detect query-relevant neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Section 4: Locating Query-Relevant (QR) Neurons in Autoregressive LLMs",
                    "exact_quote": "In summary, our main contribution is four-fold: (1) A scalable method: ...; the QRNCA method is architecture-agnostic and can deal with long-form generations."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "(2) Two new datasets: we curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge.",
                "type": "",
                "location": "1 Introduction",
                "exact_quote": "We construct two datasets to locate knowledge neurons that cover two different categories: subject domains and languages."
            },
            "evidence": [
                {
                    "evidence_text": "We construct two datasets to locate knowledge neurons that cover two different categories: subject domains and languages.",
                    "strength": "strong",
                    "limitations": "None.",
                    "location": "Section 5.1",
                    "exact_quote": "We construct two datasets to locate knowledge neurons that cover two different categories: subject domains and languages."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 7,
            "claim": {
                "text": "(3) In-depth studies: we visualize distributions of detected neurons and we are the first to show that there are visible localized regions in Llama.",
                "type": "",
                "location": "1 Introduction",
                "exact_quote": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages."
            },
            "evidence": [
                {
                    "evidence_text": "We visualize domain- or language-specific neurons on a 2D geographical heatmap. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notedly, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns.",
                    "strength": "strong",
                    "limitations": "None specified",
                    "location": "Section 5.4",
                    "exact_quote": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages."
                },
                {
                    "evidence_text": "Regarding language-specific neurons, their role in accessing linguistic knowledge across different layers likely accounts for their more sparse and distributed locations.",
                    "strength": "moderate",
                    "limitations": "None specified",
                    "location": "Section 5.4",
                    "exact_quote": "On the other hand, language neurons are more sparsely distributed and languages like Arabic and Russian exhibit less localized properties."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 8,
            "claim": {
                "text": "(4) Potential applications: we show that QRNCA might be useful for knowledge editing and neuron-based prediction.",
                "type": "",
                "location": "1 Introduction",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "We provide two usage examples to showcase the potential applications of our detected QR neurons: Knowledge Editing and Neuron-Based Prediction.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Section 6",
                    "exact_quote": "We provide two usage examples to showcase the potential applications of our detected QR neurons: Knowledge Editing and Neuron-Based Prediction."
                },
                {
                    "evidence_text": "Table 5 presents the successful rates of knowledge editing on our constructed language datasets.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Section 6.1",
                    "exact_quote": "Table 5 presents the successful rates of knowledge editing on our constructed language datasets."
                },
                {
                    "evidence_text": "The results are summarised in Table 6.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Section 6.2",
                    "exact_quote": "The results are summarised in Table 6."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        }
    ],
    "execution_times": {
        "single_pass_analysis_time": "105.96 seconds",
        "total_execution_time": "1684.48 seconds"
    }
}