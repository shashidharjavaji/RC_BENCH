{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "We propose QRNCA to detect query-relevant neurons in LLMs.",
                "type": "",
                "location": "Introduction",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "We propose a novel gradient-based attribution method aimed at locating input-output knowledge within LLMs.",
                    "strength": "strong",
                    "limitations": "the limitation is that current gradient-based techniques have primarily focused on single-token triplet facts",
                    "location": "section 3 Background, paragraph 2",
                    "exact_quote": ""
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "QRNCA is architecture-agnostic and can handle long-form text generation.",
                "type": "",
                "location": "Introduction",
                "exact_quote": "\"Our framework is architecture-agnostic and capable of handling long-form generation."
            },
            "evidence": [
                {
                    "evidence_text": "\"Our framework is architecture-agnostic and capable of handling long-form generation. To clarify the main concepts in our framework, we provide the following key notions: ... Our primary objective is to identify a set of QR neurons for a given input.\" Our framework first resorts to the proxy task of Multi-Choice QA to deal with long-form texts.",
                    "strength": "strong",
                    "limitations": "None specified",
                    "location": "Section 4. Locating Query-Relevant (QR) Neurons in Autoregressive LLMs",
                    "exact_quote": "\"Our framework is architecture-agnostic and capable of handling long-form generation."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "Distinct localized regions emerge in the middle layers for domain-specific neurons.",
                "type": "",
                "location": "Results: Are There Localized Regions in LLMs?",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Our findings indicate that LLMs tend to complete the formation of domain-specific concepts within these middle layers.",
                    "strength": "strong",
                    "limitations": null,
                    "location": "Results: Are There Localized Regions in LLMs? Paragraph 5",
                    "exact_quote": "Our findings indicate that LLMs tend to complete the formation of domain-specific concepts within these middle layers."
                },
                {
                    "evidence_text": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains.",
                    "strength": "moderate",
                    "limitations": null,
                    "location": "Results: Are There Localized Regions in LLMs? Paragraph 5",
                    "exact_quote": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "Language-specific neurons are more sparsely distributed, indicating that LLMs likely draw on linguistic knowledge at all processing levels.",
                "type": "",
                "location": "Results: Are There Localized Regions in LLMs?",
                "exact_quote": "Regarding language-specific neurons, their role in accessing linguistic knowledge across different layers likely accounts for their more sparse and distributed locations. This distribution reflects the necessity of engaging with language-specific neurons at multiple stages of information processing."
            },
            "evidence": [
                {
                    "evidence_text": "Regarding language-specific neurons, their role in accessing linguistic knowledge across different layers likely accounts for their more sparse and distributed locations. This distribution reflects the necessity of engaging with language-specific neurons at multiple stages of information processing.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "5.4 Are There Localized Regions in LLMs?",
                    "exact_quote": "Regarding language-specific neurons, their role in accessing linguistic knowledge across different layers likely accounts for their more sparse and distributed locations. This distribution reflects the necessity of engaging with language-specific neurons at multiple stages of information processing."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "QRNCA may be useful for knowledge editing and neuron-based prediction.",
                "type": "",
                "location": "Potential Applications",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "apart from the regular prompt-based prediction, we also provide two usage examples to showcase the potential applications of our detected QR neurons: knowledge editing and neuron-based prediction.",
                    "strength": "strong",
                    "limitations": "no limitations stated",
                    "location": "6. Potential Applications",
                    "exact_quote": "Apart from using the metric of PCR in Section 5.3, we are also interested in whether the detected QR neurons can be used for knowledge editing and neuron-based prediction."
                },
                {
                    "evidence_text": "For this goal, we adjust the values of QR neurons by either boosting or suppressing them to determine if we can change the prediction of a query from incorrect to correctr vice versa.",
                    "strength": "moderate",
                    "limitations": "no limitations stated",
                    "location": "6.1 Knowledge Editing",
                    "exact_quote": "For this goal, we adjust the values of QR neurons by either boosting or suppressing them to determine if we can change the prediction of a query from incorrect to correct or vice versa."
                },
                {
                    "evidence_text": "Table 5 presents the successful rates of knowledge editing on our constructed language datasets. Our observations indicate that QRNCA achieves higher success rates than other baselines.",
                    "strength": "strong",
                    "limitations": "only tested on language datasets",
                    "location": "6.1 Knowledge Editing",
                    "exact_quote": "Table 5 presents the successful rates of knowledge editing on our constructed language datasets. Our observations indicate that QRNCA achieves higher success rates than other baselines."
                },
                {
                    "evidence_text": "The intuition behind neuron-based prediction is that for a domain-specific question, if the corresponding localized regions are properly activated, the LLM is more likely to generate truthful answers. Otherwise, the LLM may produce hallucinated answers.",
                    "strength": "moderate",
                    "limitations": "no limitations stated",
                    "location": "6.2 Neuron-Based Prediction",
                    "exact_quote": "The intuition behind neuron-based prediction is that for a domainspecific question, if the corresponding localized regions are properly activated, the LLM is more likely to generate truthful answers. Otherwise, the LLM may produce hallucinated answers."
                },
                {
                    "evidence_text": "We experiment on a specifically constructed MMLU (Hendrycks et al. 2020) validation set with a different set of questions compared to those used to determine the QR neurons (See Section B in the SM for details on our experimental strategy). The results are summarised in Table 6.",
                    "strength": "strong",
                    "limitations": "no limitations stated",
                    "location": "6.2 Neuron-Based Prediction",
                    "exact_quote": "We experiment on a specifically constructed MMLU (Hendrycks et al. 2020) validation set with a different set of questions compared to those used to determine the QR neurons (See Section B in the SM for details on our experimental strategy). The results are summarised in Table 6."
                },
                {
                    "evidence_text": "We observe that the accuracy of the neuron-based prediction is very close to the accuracy of the standard prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM).",
                    "strength": "strong",
                    "limitations": "only tested on a specific MMLU validation set",
                    "location": "6.2 Neuron-Based Prediction",
                    "exact_quote": "We observe that the accuracy of the neuron-based prediction is very close to the accuracy of the standard prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM)."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        }
    ],
    "execution_times": {
        "single_pass_analysis_time": "102.69 seconds + 498.58 seconds + 500.55 seconds",
        "total_execution_time": "1101.82 seconds"
    }
}