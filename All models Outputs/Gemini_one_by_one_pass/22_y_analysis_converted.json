{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "We propose a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA) designed to identify query-relevant neurons in LLMs.",
                "type": "primary",
                "location": "Introduction",
                "exact_quote": "We propose a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA) designed to identify query-relevant neurons in LLMs."
            },
            "evidence": [
                {
                    "evidence_text": "We propose a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA) designed to identify query-relevant neurons in LLMs.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Methods",
                    "exact_quote": "We propose a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA) designed to identify query-relevant neurons in LLMs."
                },
                {
                    "evidence_text": "The framework is architecture-agnostic and capable of handling long-form generation.",
                    "strength": "moderate",
                    "limitations": "Only tested on Llama-2-7B and Mistral-7B.",
                    "location": "Methods",
                    "exact_quote": "The framework is architecture-agnostic and capable of handling long-form generation."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "QRNCA aims to extract Query-Relevant (QR) neurons for each input query.",
                "type": "primary",
                "location": "Introduction",
                "exact_quote": "QRNCA aims to extract Query-Relevant (QR) neurons for each input query."
            },
            "evidence": [
                {
                    "evidence_text": "QRNCA aims to extract Query-Relevant (QR) neurons for each input query.",
                    "strength": "strong",
                    "limitations": "None.",
                    "location": "Introduction, Paragraph 1",
                    "exact_quote": "QRNCA aims to extract Query-Relevant (QR) neurons for each input query."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "The principal advantages of our framework are its architecture-agnostic nature and its capability of handling long-form text generation effectively, as shown in Table 1.",
                "type": "primary",
                "location": "Introduction",
                "exact_quote": "The principal advantages of our framework are its architecture-agnostic nature and its capability of handling long-form text generation effectively, as shown in Table 1."
            },
            "evidence": [
                {
                    "evidence_text": "QRNCA aims to extract Query-Relevant (QR) neurons for each input query. The process begins by transforming an open-ended generation task into a multiple-choice question-answering format. By employing prompt engineering, we constrain LLMs to generate only the option letter rather than the complete answer. This approach allows for the examination of long-form generation beyond single tokens.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Section 3.3",
                    "exact_quote": "QRNCA aims to extract Query-Relevant (QR) neurons for each input query. The process begins by transforming an open-ended generation task into a multiple-choice question-answering format. By employing prompt engineering, we constrain LLMs to generate only the option letter rather than the complete answer. This approach allows for the examination of long-form generation beyond single tokens."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "Empirical evaluations demonstrate that our proposed method outperforms baseline approaches.",
                "type": "primary",
                "location": "Introduction",
                "exact_quote": "Empirical evaluations demonstrate that our proposed method outperforms baseline approaches."
            },
            "evidence": [
                {
                    "evidence_text": "Table 3 presents the overall performance of various methods. Our QRNCA method consistently outperforms other baseline methods, evidenced by its higher PCR.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Section 5.1, Table 3",
                    "exact_quote": "Table 3 presents the comparisons of different knowledge locating methods for Llama-2-7B. The metric here is the Probability Change Ratio (PCR) described in Section 5.1. Details are shown in Table A2 in the appendix."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge.",
                "type": "primary",
                "location": "Related Work",
                "exact_quote": "We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge."
            },
            "evidence": [
                {
                    "evidence_text": "To investigate the existence of localized knowledge regions, we construct two multi-choice QA datasets encompassing various domains and languages.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Section 5.1 Experimental Settings",
                    "exact_quote": "\n\nTo investigate the existence of localized knowledge regions, we construct two multi-choice QA datasets encompassing various domains and languages."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "Our findings indicate that distinct localized regions emerge in the middle layers, particularly for domain-specific neurons.",
                "type": "primary",
                "location": "Results",
                "exact_quote": "Our findings indicate that distinct localized regions emerge in the middle layers, particularly for domain-specific neurons."
            },
            "evidence": [
                {
                    "evidence_text": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns.",
                    "strength": "strong",
                    "limitations": "The study was conducted on a specific LLM (Llama-2-7B) and may not generalize to other models.",
                    "location": "Section 5.4, paragraph 2",
                    "exact_quote": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 7,
            "claim": {
                "text": "Conversely, language-specific neurons are more sparsely distributed, indicating that LLMs likely draw on linguistic knowledge at all processing levels.",
                "type": "primary",
                "location": "Results",
                "exact_quote": "Conversely, language-specific neurons are more sparsely distributed, indicating that LLMs likely draw on linguistic knowledge at all processing levels."
            },
            "evidence": [
                {
                    "evidence_text": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns. In contrast, language neurons are more sparsely distributed, indicating that LLMs likely draw on linguistic knowledge at all processing levels.",
                    "strength": "strong",
                    "limitations": "This assumes that the distribution of QR neurons is indicative of the distribution and usage of linguistic knowledge",
                    "location": "5.4 Are There Localized Regions in LLMs?",
                    "exact_quote": "Conversely, language-specific neurons are more sparsely distributed, indicating that LLMs likely draw on linguistic knowledge at all processing levels."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 8,
            "claim": {
                "text": "In summary, our main contribution is four-fold: (1) We propose QRNCA to detect query-relevant neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations.",
                "type": "primary",
                "location": "Conclusion",
                "exact_quote": "In summary, our main contribution is four-fold: (1) We propose QRNCA to detect query-relevant neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations."
            },
            "evidence": [
                {
                    "evidence_text": "Our proposed method outperforms baseline approaches, as shown in Table 3.",
                    "strength": "strong",
                    "limitations": "Results may vary depending on the specific LLM and dataset used.",
                    "location": "Section 5.1",
                    "exact_quote": "Table 3 presents the overall performance of various methods. Our QRNCA method consistently outperforms other baselines, evidenced by its higher PCR."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 9,
            "claim": {
                "text": "Two new datasets: we curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge.",
                "type": "primary",
                "location": "Conclusion",
                "exact_quote": "Two new datasets: we curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge."
            },
            "evidence": [
                {
                    "evidence_text": "We construct two datasets to locate knowledge neurons that cover two different categories: subject domains and languages.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Section 5.1",
                    "exact_quote": "We construct two datasets to locate knowledge neurons that cover two different categories: subject domains and languages."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 10,
            "claim": {
                "text": "In-depth studies: we visualize distributions of detected neurons and we are the first to show that there are visible localized regions in Llama.",
                "type": "primary",
                "location": "Conclusion",
                "exact_quote": "In-depth studies: we visualize distributions of detected neurons and we are the first to show that there are visible localized regions in Llama."
            },
            "evidence": [
                {
                    "evidence_text": "Our findings indicate that distinct localized regions emerge in the middle layers, particularly for domain-specific neurons. This suggests that LLMs tend to complete the formation of domainspecific concepts within these middle layers.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Section 5.4",
                    "exact_quote": "Our findings indicate that distinct localized regions emerge in the middle layers, particularly for domain-specific neurons. This suggests that LLMs tend to complete the formation of domainspecific concepts within these middle layers."
                },
                {
                    "evidence_text": "Conversely, language-specific neurons are more sparsely distributed, indicating that LLMs likely draw on linguistic knowledge at all processing levels.",
                    "strength": "moderate",
                    "limitations": "None stated.",
                    "location": "Section 5.4",
                    "exact_quote": "Conversely, language-specific neurons are more sparsely distributed, indicating that LLMs likely draw on linguistic knowledge at all processing levels."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 11,
            "claim": {
                "text": "Potential applications: we show that QRNCA might be useful for knowledge editing and neuron-based prediction.",
                "type": "primary",
                "location": "Conclusion",
                "exact_quote": "Potential applications: we show that QRNCA might be useful for knowledge editing and neuron-based prediction."
            },
            "evidence": [
                {
                    "evidence_text": "In Section 6.1, we adjusted the values of QR neurons by either boosting or suppressing them to determine if we can change the prediction of a query from incorrect to correct or vice versa.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Section 6.1",
                    "exact_quote": "In Section 6.1, we adjusted the values of QR neurons by either boosting or suppressing them to determine if we can change the prediction of a query from incorrect to correct or vice versa."
                },
                {
                    "evidence_text": "Table 5 presents the successful rates of knowledge editing on our constructed language datasets. Our observations indicate that QRNCA achieves higher success rates than other baselines.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Section 6.1",
                    "exact_quote": "Table 5 presents the successful rates of knowledge editing on our constructed language datasets. Our observations indicate that QRNCA achieves higher success rates than other baselines."
                },
                {
                    "evidence_text": "The results are summarised in Table 6. We observe that the accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM). This suggests that the activity of identified neurons can reflect the model\u2019s reasoning process to some extent.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Section 6.2",
                    "exact_quote": "The results are summarised in Table 6. We observe that the accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM). This suggests that the activity of identified neurons can reflect the model\u2019s reasoning process to some extent."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        }
    ],
    "execution_times": {
        "single_pass_analysis_time": "109.34 seconds",
        "total_execution_time": "2282.96 seconds"
    }
}