{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "Query-Relevant Neuron Cluster Attribution (QRNCA) can identify query-relevant neurons in contemporary autoregressive LLMs, such as Llama and Mistral.",
                "type": "",
                "location": "Abstract",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Experimental evaluations demonstrate that our proposed method outperforms baseline approaches.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Section 2 Related Work",
                    "exact_quote": "Experimental evaluations demonstrate that our proposed method outperforms baseline approaches."
                },
                {
                    "evidence_text": "To investigate the existence of localized knowledge regions, we construct two multi-choice QA datasets encompassing various domains and languages. Then, we visualize the geographical locations of the detected neurons in Llama.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Section 3",
                    "exact_quote": "To investigate the existence of localized knowledge regions, we construct two multi-choice QA datasets encompassing various domains and languages. Then, we visualize the geographical locations of the detected neurons in Llama."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "QRNCA outperforms baseline methods, such as Knowledge Attribution, ROME, and Knowledge Subnetwork, in locating query-relevant neurons.",
                "type": "",
                "location": "Abstract",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Table 3 presents the overall performance of various methods. Our QRNCA method consistently outperforms other baselines, evidenced by its higher PCR.",
                    "strength": "strong",
                    "limitations": "PCR is used as a metric of performance.",
                    "location": "Section 5.2, paragraph 1",
                    "exact_quote": "Table 3 presents the overall performance of various methods. Our QRNCA method consistently outperforms other baselines, evidenced by its higher PCR."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "QRNCA can reveal localized knowledge regions in LLMs, particularly within different domains.",
                "type": "",
                "location": "Abstract",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "In contrast, language neurons are more sparsely distributed, indicating that LLMs tend to allow the storage of multiple domain-specific concepts in a single neuron (polysemantic). Although language-specific neurons are not monosemantic (Chen et al. 2024b), they prefer to encode one specific language concepts, which is also consistent with recent findings (Tang et al. 2024).",
                    "strength": "moderate",
                    "limitations": "The evidence is based on previous studies.",
                    "location": "Section 5.2, paragraph 4",
                    "exact_quote": null
                },
                {
                    "evidence_text": "Regarding layer distribution, the QR neurons are predominantly located in the middle layers (15-18) and the top layers (around 30), as depicted in Figure 2b. This finding indicates knowledge concepts are mainly stored in the middle and top layers, and we may only modify these neurons for efficient knowledge updating (Ding et al. 2023).",
                    "strength": "strong",
                    "limitations": null,
                    "location": "Section 5.2, paragraph 5",
                    "exact_quote": null
                },
                {
                    "evidence_text": "Based on prior studies, LLMs process and represent information in a hierarchical manner (Geva et al. 2022; Wendler et al. 2024; Tang et al. 2024). The early layers are primarily responsible for extracting low-level features, while the middle layers begin to integrate this information, forming more complex semantic representations. The late layers are typically dedicated to generating the final output. Therefore, we suppose that domain-specific knowledge representation is built in the middle layer and the top layers are then mainly responsible for next-token prediction, which may explain the visible regions for different subject domains.",
                    "strength": "moderate",
                    "limitations": "The evidence is based on previous studies.",
                    "location": "Section 5.4, paragraph 2",
                    "exact_quote": null
                },
                {
                    "evidence_text": "To investigate this, we visualize domain-or language-specific neurons on a 2D geographical heatmap. The width of the heatmap corresponds to the dimension of FFNs in Llama-2-7B (11008), and the length represents the layer depth (32). We accumulate the value of naica(n[l]i[)][ to populate the heatmap. Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages.",
                    "strength": "strong",
                    "limitations": null,
                    "location": "Section 5.4, paragraph 3",
                    "exact_quote": null
                },
                {
                    "evidence_text": "The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns.",
                    "strength": "strong",
                    "limitations": null,
                    "location": "Section 5.4, paragraph 4",
                    "exact_quote": null
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "QRNCA advances the state-of-the-art in locating query-relevant neurons in LLMs by addressing the challenges of long-form text generation and the existence of localized knowledge regions.",
                "type": "",
                "location": "Introduction",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Our proposed method outperforms baseline approaches across a range of tasks.",
                    "strength": "strong",
                    "limitations": "The method may not work well with all types of LLMs.",
                    "location": "Section 5.3",
                    "exact_quote": "Our QRNCA method consistently outperforms other baseline methods, evidenced by its higher PCR."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "QRNCA is a scalable method for detecting query-relevant neurons in LLMs.",
                "type": "",
                "location": "Related Work",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Our QRNCA method consistently outperforms other baseline approaches, evidenced by its higher PCR. This indicates that our identified QR neurons significantly affect the probability of correct answers while exerting a relatively low impact on unrelated queries.",
                    "strength": "strong",
                    "limitations": "The results may vary depending on the specific LLM and dataset used.",
                    "location": "Section 5.3, paragraph 2",
                    "exact_quote": "Our QRNCA method consistently outperforms other baseline approaches, evidenced by its higher PCR. This indicates that our identified QR neurons significantly affect the probability of correct answers while exerting a relatively low impact on unrelated queries."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "Previous research has focused on identifying neurons responsible for specific tasks, such as linguistic, privacy-related, and bias-related neurons.",
                "type": "",
                "location": "Related Work",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Recent studies have focused on identifying neurons responsible for specific tasks, such as linguistic (Chen et al. 2024b; Tang et al. 2024; Kojima et al. 2024), privacy-related (Wu et al. 2023; Chen et al. 2024a) and bias-related neurons (Yang, Kang, and Jung 2023).",
                    "strength": "strong",
                    "limitations": null,
                    "location": "Paragraph 2, Section 2.2 Analyzing Knowledge Distribution in LLMs",
                    "exact_quote": "Recent studies have focused on identifying neurons responsible for specific tasks, such as linguistic (Chen et al. 2024b; Tang et al. 2024; Kojima et al. 2024), privacy-related (Wu et al. 2023; Chen et al. 2024a) and bias-related neurons (Yang, Kang, and Jung 2023)."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 7,
            "claim": {
                "text": "QRNCA provides evidence for the existence of localized knowledge regions in LLMs, particularly in the middle layers for domain-specific neurons and across different layers for language-specific neurons.",
                "type": "",
                "location": "Analyzing Detected QR Neurons",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Our experimental results show that our method outperforms existing baselines in identifying associated neurons. Additionally, this study pioneers the exploration of localized knowledge regions in LLMs and demonstrates Llama contains knowledge-specific regions in the middle layers while language-specific neurons tend to be distributed across different layers.",
                    "strength": "strong",
                    "limitations": "The results are based on a specific dataset and model size.",
                    "location": "Section 5.3",
                    "exact_quote": "Our experimental results show that our method outperforms existing baselines in identifying associated neurons. Additionally, this study pioneers the exploration of localized knowledge regions in LLMs and demonstrates Llama contains knowledge-specific regions in the middle layers while language-specific neurons tend to be distributed across different layers."
                },
                {
                    "evidence_text": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns.",
                    "strength": "strong",
                    "limitations": "The results are based on a specific dataset and model size.",
                    "location": "Section 5.4",
                    "exact_quote": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns."
                },
                {
                    "evidence_text": "In contrast, language neurons are more sparsely distributed across different domains and languages, and languages like Arabic and Russian exhibit less localized properties.",
                    "strength": "moderate",
                    "limitations": "The results are based on a specific dataset and model size.",
                    "location": "Section 5.4",
                    "exact_quote": "In contrast, language neurons are more sparsely distributed across different domains and languages, and languages like Arabic and Russian exhibit less localized properties."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 8,
            "claim": {
                "text": "Common neurons, which are consistently activated by a wide range of inputs, represent general knowledge or concepts.",
                "type": "",
                "location": "Analyzing Detected QR Neurons",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Some neurons with a relatively high attribution score are still shared across clusters. Through case studies, we demonstrate that they express commonly used concepts such as option letters (\u2018\u2018A\u2019\u2019 and \u2018\u2018B\u2019\u2019) or stop words (\u2018\u2018and\u2019\u2019 and \u2018\u2018the\u2019\u2019).",
                    "strength": "strong",
                    "limitations": "The case studies are not exhaustive and may not represent all common neurons.",
                    "location": "4.4 Common Neurons",
                    "exact_quote": null
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 9,
            "claim": {
                "text": "QR neurons can significantly impact the prediction of correct answers in multi-choice QA tasks.",
                "type": "",
                "location": "Analyzing Detected QR Neurons",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Our QRNCA method outperforms other baselines, evidenced by its higher PCR.",
                    "strength": "strong",
                    "limitations": "The results may vary depending on the specific language model and dataset used.",
                    "location": "Section 5.3, paragraph 2",
                    "exact_quote": "Our QRNCA method consistently outperforms other baseline methods, evidenced by its higher PCR."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 10,
            "claim": {
                "text": "QRNCA can be used for knowledge editing, as demonstrated by its success in changing the prediction of a query from incorrect to correct or vice versa.",
                "type": "",
                "location": "Potential Applications",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Our observations indicate that QRNCA achieves higher success rates than other baselines.",
                    "strength": "strong",
                    "limitations": "Only tested on a constructed language dataset, may not generalize to other domains or languages.",
                    "location": "Section 6.1, Table 5",
                    "exact_quote": null
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 11,
            "claim": {
                "text": "QRNCA can be used for neuron-based prediction, where the activity of identified neurons can reflect the model\u2019s reasoning process to some extent.",
                "type": "",
                "location": "Potential Applications",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "We experiment on a specifically constructed MMLU (Hendrycks et al. 2020) validation set with a different set of questions than those used to determine the QR neurons (see Section B in the SM for details on our experimental strategy). The results are summarised in Table 6. We observe that the accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM). This suggests that the activity of identified neurons can reflect the model\u2019s reasoning process to some extent.",
                    "strength": "strong",
                    "limitations": "The experiment is conducted on a specific dataset and may not generalize to other datasets.",
                    "location": "Section 6.2",
                    "exact_quote": "We experiment on a specifically constructed MMLU (Hendrycks et al. 2020) validation set with a different set of questions than those used to determine the QR neurons (see Section B in the SM for details on our experimental strategy). The results are summarised in Table 6. We observe that the accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM). This suggests that the activity of identified neurons can reflect the model\u2019s reasoning process to some extent."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "No robustness analysis available",
                "justification": "The experiment is designed to evaluate the accuracy of neuron-based predictions, but it does not investigate the underlying mechanism of how identified neurons contribute to the model's reasoning process. Therefore, the evidence is not sufficient to support the claim.",
                "key_limitations": "The experiment only evaluates the accuracy of neuron-based predictions, but it does not investigate the underlying mechanism of how identified neurons contribute to the model's reasoning process.",
                "confidence_level": ""
            }
        }
    ],
    "execution_times": {
        "single_pass_analysis_time": "2295.50 seconds",
        "total_execution_time": "2295.50 seconds"
    }
}