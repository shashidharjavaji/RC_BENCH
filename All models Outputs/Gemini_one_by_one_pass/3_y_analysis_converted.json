{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "We propose QRNCA to detect query-relevant neurons in LLMs.",
                "type": "",
                "location": "Introduction",
                "exact_quote": "We propose QRNCA to detect query-relevant neurons in LLMs."
            },
            "evidence": [
                {
                    "evidence_text": "We introduce a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA) designed to identify query-relevant neurons in LLMs.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Section 1, Introduction",
                    "exact_quote": "We introduce a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA) designed to identify query-relevant neurons in LLMs."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "QRNCA outperforms baseline methods significantly.",
                "type": "",
                "location": "Results",
                "exact_quote": "QRNCA outperforms baseline methods significantly."
            },
            "evidence": [
                {
                    "evidence_text": "Our proposed method outperforms baseline approaches.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Section 4.5 Obtaining QR Neurons",
                    "exact_quote": "Our QRNCA method consistently outperforms other baseline methods[1]: Random Neuron are randomly selected from FFNs and we make sure they have the same number of neurons as QRNCA; Activation selects neurons with high activated values. Kowledge Neuron[\u2217] is adapted from knowledge attribution (Dai et al. 2022) by using the multi-choice QA task; QRNCA wo/ ICA only uses neuron attribution (Eq 5) to obtain relevant neurons, which dose not involve the computation of Inverse Cluster Attribution; QRNCA w/ Common Neuron is a variant without removing common neurons."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "Distinct localized regions emerge in the middle layers, particularly for domain-specific neurons.",
                "type": "",
                "location": "Results",
                "exact_quote": "Distinct localized regions emerge in the middle layers, particularly for domain-specific neurons."
            },
            "evidence": [
                {
                    "evidence_text": "Our findings indicate that domain-specific knowledge representation is built in the middle layer and the top layers are then mainly responsible for next-token prediction, which may explain the visible regions for different subject domains.",
                    "strength": "strong",
                    "limitations": "The study is limited to two LLM models and may not generalize to other models.",
                    "location": "Section 5.4: Are There Localized Regions in LLMs?",
                    "exact_quote": "Our findings indicate that domain-specific knowledge representation is built in the middle layer and the top layers are then mainly responsible for next-token prediction, which may explain the visible regions for different subject domains."
                },
                {
                    "evidence_text": "notably certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns.",
                    "strength": "moderate",
                    "limitations": "The study did not investigate the specific functions of these neurons.",
                    "location": "Section 5.4: Are There Localized Regions in LLMs?",
                    "exact_quote": "notably certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "Common neurons are concentrated in the top layer, predominantly expressing frequently used tokens.",
                "type": "",
                "location": "Results",
                "exact_quote": "Common neurons are concentrated in the top layer, predominantly expressing frequently used tokens."
            },
            "evidence": [
                {
                    "evidence_text": "Furthermore, analysis of neuron distributions reveals the presence of visible localized regions, particularly within different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns. In contrast, language-specific neurons are more sparsely distributed, indicating that LLMs tend to allow the storage of multiple domain-specific concepts in a single neuron (polysemantic). Regarding language-specific neurons, their role in accessing linguistic knowledge across different layers likely accounts for their more sparse and distributed locations. This distribution reflects the necessity of engaging with language-specific neurons at multiple stages of information processing. Also, we analyze the token predicted by QR neurons, but we found that middle-layer neurons do not have a clear semantic meaning and human-readable concepts mostly appear in the top layer (Wendler et al. 2024). In Section A in the SM we conduct semantic meaning analyses of neurons.",
                    "strength": "moderate",
                    "limitations": "The evidence is not specific to common neurons and does not directly mention the top layer.",
                    "location": "Section 5.4",
                    "exact_quote": "Furthermore, analysis of neuron distributions reveals the presence of visible localized regions, particularly within different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns. In contrast, language-specific neurons are more sparsely distributed, indicating that LLMs tend to allow the storage of multiple domain-specific concepts in a single neuron (polysemantic). Regarding language-specific neurons, their role in accessing linguistic knowledge across different layers likely accounts for their more sparse and distributed locations. This distribution reflects the necessity of engaging with language-specific neurons at multiple stages of information processing. Also, we analyze the token predicted by QR neurons, but we found that middle-layer neurons do not have a clear semantic meaning and human-readable concepts mostly appear in the top layer (Wendler et al. 2024). In Section A in the SM we conduct semantic meaning analyses of neurons."
                },
                {
                    "evidence_text": "We also visualize their locations within Llama-2-7B and we observe that they tend to appear at the top layer (as shown in Figure A2 in the SM).",
                    "strength": "strong",
                    "limitations": "The evidence is specific to common neurons.",
                    "location": "Section 5.5",
                    "exact_quote": "We also visualize their locations within Llama-2-7B and we observe that they tend to appear at the top layer (as shown in Figure A2 in the SM)."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "QRNCA might be useful for knowledge editing and neuron-based prediction.",
                "type": "",
                "location": "Conclusion",
                "exact_quote": "QRNCA might be useful for knowledge editing and neuron-based prediction."
            },
            "evidence": [
                {
                    "evidence_text": "Apart from using the metric of PCR in Section 5.3, we are also interested in whether the detected QR neurons can be used for knowledge editing.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 6.1",
                    "exact_quote": "Apart from using the metric of PCR in Section 5.3, we are also interested in whether the detected QR neurons can be used for knowledge editing."
                },
                {
                    "evidence_text": "For this goal, we adjust the values of QR neurons by either boosting or suppressing them to determine if we can change the prediction of a query from incorrect to correct or vice versa.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 6.1",
                    "exact_quote": "For this goal, we adjust the values of QR neurons by either boosting or suppressing them to determine if we can change the prediction of a query from incorrect to correct or vice versa."
                },
                {
                    "evidence_text": "Table 5 presents the success rates of knowledge editing on our constructed language datasets.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 6.1",
                    "exact_quote": "Table 5 presents the success rates of knowledge editing on our constructed language datasets."
                },
                {
                    "evidence_text": "To this end, we test whether the correct answers to domain-specific questions can be predicted solely based on the activity of the associated neurons.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 6.2",
                    "exact_quote": "To this end, we test whether the correct answers to domain-specific questions can be predicted solely based on the activity of the associated neurons."
                },
                {
                    "evidence_text": "Since we harvest QR neurons for queries in different subject domains, we can group all neurons for a domain to obtain a set of domain-specific neurons.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 6.2",
                    "exact_quote": "Since we harvest QR neurons for queries in different subject domains, we can group all neurons for a domain to obtain a set of domain-specific neurons."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        }
    ],
    "execution_times": {
        "single_pass_analysis_time": "1105.68 seconds",
        "total_execution_time": "1105.68 seconds"
    }
}