Claim 1:
Type: contribution
Statement: LLMs are amazing giant external non-veridical memories that can serve as powerful cognitive orthotics for human or machine agents, if rightly used.
Location: Introduction
Exact Quote: The LLM-Modulo Framework (a name loosely inspired by SAT Modulo Theories (Nieuwenhuis & Oliveras, 2006)); see Figure 3. LLMs play a spectrum of roles in this architecture, from guessing candidate plans, to translating those plans into syntactic forms that are more accessible to external critics, to helping end users flesh out incomplete specifications, to helping expert users acquire domain models (that in turn drive model-based critics).

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The paper provides a compelling argument for the potential of LLMs to enhance human and machine intelligence when used appropriately.
Key Limitations: The paper does not provide empirical evidence to support this claim.

--------------------------------------------------

Claim 2:
Type: contribution
Statement: LLMs cannot plan themselves but can play a variety of constructive roles in solving planning tasks–especially as approximate knowledge sources and candidate plan generators in so-called LLM-Modulo Frameworks, where they are used in conjunction with external sound model-based verifiers.
Location: Introduction
Exact Quote: We support this position by first reviewing literature, including our own works, that establishes that LLMs cannot be used as planners or plan verifiers themselves (Section 2).

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The paper provides a comprehensive review of the literature on LLM planning capabilities, which supports the claim that LLMs cannot plan independently.
Key Limitations: The paper does not provide empirical evidence to support this claim.

--------------------------------------------------

Claim 3:
Type: result
Statement: On average, only about 12% of the plans that the best LLM (GPT-4) generates are actually executable without errors and goal-reaching.
Location: 2.1. LLMs cannot generate executable plans in autonomous mode
Exact Quote: On average, only about 12% of the plans that the best LLM (GPT-4) generates are actually executable without errors and goal-reaching.

Evidence:
- Evidence Text: We show that results in the autonomous mode are pretty bleak. On average, only about 12% of the plans that the best LLM (GPT-4) generates are actually executable without errors and goal-reaching.
  Strength: strong
  Location: 2.1. LLMs cannot generate executable plans in autonomous mode
  Limitations: The results are based on a specific set of planning problems and may not generalize to other domains.
  Exact Quote: We show that results in the autonomous mode are pretty bleak. On average, only about 12% of the plans that the best LLM (GPT-4) generates are actually executable without errors and goal-reaching.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by strong evidence from a study that evaluated the performance of LLMs on a suite of planning problems.
Key Limitations: The study did not evaluate the performance of LLMs on other types of reasoning tasks, such as natural language understanding or question answering.

--------------------------------------------------

Claim 4:
Type: result
Statement: We demonstrate that the performance deteriorates further if the names of the actions and objects in the domain are obfuscated–a change that doesn’t in any way affect the performance of the standard AI planners. This further suggests that LLMs are more likely doing approximate retrieval of plans than actual planning.
Location: 2.1. LLMs cannot generate executable plans in autonomous mode
Exact Quote: We demonstrate that the performance deteriorates further if the names of the actions and objects in the domain are obfuscated–a change that doesn’t in any way affect the performance of the standard AI planners. This further suggests that LLMs are more likely doing approximate retrieval of plans than actual planning.

Evidence:
- Evidence Text: We demonstrate that the performance deteriorates further if the names of the actions and objects in the domain are obfuscated–a change that doesn’t in any way affect the performance of the standard AI planners.
  Strength: strong
  Location: 2.1. LLMs cannot generate executable plans in autonomous mode
  Limitations: The results are based on a specific set of planning problems and may not generalize to other domains.
  Exact Quote: We demonstrate that the performance deteriorates further if the names of the actions and objects in the domain are obfuscated–a change that doesn’t in any way affect the performance of the standard AI planners.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by strong evidence from a study that evaluated the performance of LLMs on a suite of planning problems.
Key Limitations: The study did not evaluate the performance of LLMs on other types of reasoning tasks, such as natural language understanding or question answering.

--------------------------------------------------

Claim 5:
Type: finding
Statement: LLMs cannot verify solutions to planning problems.
Location: Part 2, Section 2.2
Exact Quote: Our results indicate that in direct mode, LLMs are, perhaps not surprisingly, pretty bad at solving graph coloring instances. More interestingly, they are no better at verifying solutions.

Evidence:
- Evidence Text: Experiments on graph coloring and CSP verification tasks showed that LLMs were not able to effectively verify solutions.
  Strength: strong
  Location: Part 2, Section 2.2
  Limitations: Limited to graph coloring and CSP verification tasks.
  Exact Quote: None

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: high
Justification: The experiments provide direct evidence to support the claim.
Key Limitations: May not generalize to other types of planning problems.

--------------------------------------------------

Claim 6:
Type: finding
Statement: LLMs cannot self-critique their plans.
Location: Part 2, Section 2.3
Exact Quote: One important corollary of the fact that LLMs cannot self-critique their plans is that they also can’t self-improve by generating synthetic data, e.g. by generating plans themselves, critiquing the plans by themselves to improve them, and then using those to fine-tune themselves, as has been claimed in the literature.

Evidence:
- Evidence Text: Experiments with LLMs critiquing their own solutions did not show any improvement over the baseline.
  Strength: strong
  Location: Part 2, Section 2.2
  Limitations: Limited to the specific experimental setup used.
  Exact Quote: None

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: high
Justification: The experiments provide direct evidence to support the claim.
Key Limitations: May not generalize to other types of planning problems.

--------------------------------------------------

Claim 7:
Type: finding
Statement: Planning knowledge extracted from LLMs can be confused for executable plans.
Location: Part 2, Section 2.3
Exact Quote: On closer examination, many papers claiming LLMs have planning abilities wind up confusing general planning knowledge extracted from the LLMs for executable plans.

Evidence:
- Evidence Text: Examples are provided of papers that make this mistake.
  Strength: moderate
  Location: Part 2, Section 2.3
  Limitations: The examples may not be representative of all papers.
  Exact Quote: None

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The examples provide some evidence to support the claim, but it is not clear how generalizable this finding is.
Key Limitations: Limited to the specific examples provided.

--------------------------------------------------

Claim 8:
Type: contribution
Statement: LLMs are often good at extracting planning knowledge.
Location: Part 2, Section 2.3
Exact Quote: The fact that LLMs are often good at extracting planning knowledge can indeed be gainfully leveraged.

Evidence:
- Evidence Text: Examples are provided of LLMs being used to extract planning knowledge.
  Strength: moderate
  Location: Part 2, Section 2.3
  Limitations: The examples may not be representative of all LLMs.
  Exact Quote: None

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The examples provide some evidence to support the claim, but it is not clear how generalizable this finding is.
Key Limitations: Limited to the specific examples provided.

--------------------------------------------------

Claim 9:
Type: contribution
Statement: LLMs can be used as idea generators in a generate-test-critique architecture.
Location: Part 2, Section 3
Exact Quote: While Section 2 questions the claims that LLMs are capable of planning/reasoning by themselves, it is certainly not meant to imply that LLMs don’t have any constructive roles to play in solving planning/reasoning tasks.

Evidence:
- Evidence Text: The generate-test-critique architecture is proposed, which uses LLMs to generate ideas and external critics to evaluate them.
  Strength: strong
  Location: Part 2, Section 3
  Limitations: The architecture has not been empirically evaluated.
  Exact Quote: None

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The architecture is a sound theoretical proposal, but its effectiveness has not been empirically demonstrated.
Key Limitations: Lack of empirical evaluation.

--------------------------------------------------

Claim 10:
Type: result
Statement: LLM performance in Block World improves to 82% within 15 back prompting rounds, while in Logistics, it improves to 70% when using back prompting from VAL (Howey et al., 2004).
Location: Section 4, Paragraph 1
Exact Quote: In the former case, the results (presented in Section 5.2 and Table 4 of (Valmeekam et al., 2023c)) show that with back prompting from VAL (Howey et al., 2004) acting as the external verifier and critic, LLM performance in Blocks World improves to 82% within 15 back prompting rounds, while in Logistics, it improves to 70%.

Evidence:
- Evidence Text: In the former case, the results (presented in Section 5.2 and Table 4 of (Valmeekam et al., 2023c)) show that with back prompting from VAL (Howey et al., 2004) acting as the external verifier and critic, LLM performance in Blocks World improves to 82% within 15 back prompting rounds, while in Logistics, it improves to 70%.
  Strength: strong
  Location: Section 4, Paragraph 1
  Limitations: The results are specific to the Blocks World and Logistics domains and may not generalize to other planning tasks.
  Exact Quote: In the former case, the results (presented in Section 5.2 and Table 4 of (Valmeekam et al., 2023c)) show that with back prompting from VAL (Howey et al., 2004) acting as the external verifier and critic, LLM performance in Blocks World improves to 82% within 15 back prompting rounds, while in Logistics, it improves to 70%.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: high
Justification: The evidence provided is specific and quantitative, menunjukkan the effectiveness of back prompting from VAL in improving LLM performance in planning tasks.
Key Limitations: The results may not generalize to other planning tasks.

--------------------------------------------------

Claim 11:
Type: result
Statement: The LLM-Modulo framework significantly improves performance in travel planning tasks, achieving 6x of baselines.
Location: Section 4, Paragraph 2
Exact Quote: Our preliminary results show (see Figure 5; additional results in (Gundawar et al., 2024)) that LLM-Modulo based agentification with automated critics in the loop significantly improves the performance (6x of baselines) even with a limit of 10 back prompting cycles, and weaker models such as GPT-3.5turbo.

Evidence:
- Evidence Text: Our preliminary results show (see Figure 5; additional results in (Gundawar et al., 2024)) that LLM-Modulo based agentification with automated critics in the loop significantly improves the performance (6x of baselines) even with a limit of 10 back prompting cycles, and weaker models such as GPT-3.5turbo.
  Strength: strong
  Location: Section 4, Paragraph 2
  Limitations: The results are based on preliminary experiments and may not generalize to all travel planning tasks.
  Exact Quote: Our preliminary results show (see Figure 5; additional results in (Gundawar et al., 2024)) that LLM-Modulo based agentification with automated critics in the loop significantly improves the performance (6x of baselines) even with a limit of 10 back prompting cycles, and weaker models such as GPT-3.5turbo.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: high
Justification: The evidence provided is specific and quantitative, menunjukkan the effectiveness of the LLM-Modulo framework in travel planning tasks.
Key Limitations: The results are based on preliminary experiments and may not generalize to all travel planning tasks.

--------------------------------------------------

Claim 12:
Type: contribution
Statement: LLMs can play various constructive roles in the LLM-Modulo framework, including knowledge provision and candidate plan generation.
Location: Section 4, Paragraph 3
Exact Quote: We also discussed how conflating approximate knowledge acquisition and generating executable plans of action is behind many of the claims about planning and verification abilities of LLMs. We then shared LLM-Modulo framework, our vision for a productive way to integrate the impressive idea generation/approximate knowledge provision capabilities of LLMs with external verifiers with correctness guarantees, for robust and expressive planning.

Evidence:
- Evidence Text: We also discussed how conflating approximate knowledge acquisition and generating executable plans of action is behind many of the claims about planning and verification abilities of LLMs.
  Strength: strong
  Location: Section 4, Paragraph 3
  Limitations: The evidence is qualitative and does not provide specific examples of how LLMs can play these roles.
  Exact Quote: We also discussed how conflating approximate knowledge acquisition and generating executable plans of action is behind many of the claims about planning and verification abilities of LLMs.

- Evidence Text: We then shared LLM-Modulo framework, our vision for a productive way to integrate the impressive idea generation/approximate knowledge provision capabilities of LLMs with external verifiers with correctness guarantees, for robust and expressive planning.
  Strength: strong
  Location: Section 4, Paragraph 3
  Limitations: The evidence is qualitative and does not provide specific examples of how the LLM-Modulo framework can be used.
  Exact Quote: We then shared LLM-Modulo framework, our vision for a productive way to integrate the impressive idea generation/approximate knowledge provision capabilities of LLMs with external verifiers with correctness guarantees, for robust and expressive planning.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence provided is qualitative but convincing, arguing that LLMs can play valuable roles in planning and reasoning tasks when integrated with external verifiers.
Key Limitations: The evidence does not provide specific examples of how LLMs can play these roles or how the LLM-Modulo framework can be used.

--------------------------------------------------

Claim 13:
Type: result
Statement: GPT4 often makes confident yet incorrect responses.
Location: Introduction
Exact Quote: GPT-4 often makes confident yet incorrect responses.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: low
Confidence Level: low
Justification: The claim is made without any supporting evidence.
Key Limitations: Lack of supporting evidence.

--------------------------------------------------

Claim 14:
Type: result
Statement: GPT4 is not able to reason logically.
Location: Introduction
Exact Quote: GPT-4 is not able to reason logically.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: low
Confidence Level: low
Justification: The claim is made without any supporting evidence.
Key Limitations: Lack of supporting evidence.

--------------------------------------------------

Claim 15:
Type: result
Statement: GPT4 has difficulty handling simple instructions.
Location: Introduction
Exact Quote: GPT-4 has difficulty handling simple instructions.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: low
Confidence Level: low
Justification: The claim is made without any supporting evidence.
Key Limitations: Lack of supporting evidence.

--------------------------------------------------

