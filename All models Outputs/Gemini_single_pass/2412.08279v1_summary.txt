Claim 1:
Type: contribution
Statement: Y-NQ is a newly released dataset that enables to compare generative open-book reading comprehension between English and Yorùbá.
Location: Conclusions
Exact Quote: Y-NQ is a newly released dataset that enables to compare generative open-book reading comprehension between English and Yorùbá.

Evidence:
- Evidence Text: Y-NQ is a newly released dataset that contains 6,482 question/answer pairs in English and 5,568 question/answer pairs in Yorùbá, covering a variety of topics and document lengths.
  Strength: weak
  Location: Introduction
  Limitations: Evidence speaks to the availability of the dataset but not the direct ability to compare open-book reading comprehension.
  Exact Quote: Y-NQ is a newly released dataset that contains 6,482 question/answer pairs in English and 5,568 question/answer pairs in Yorùbá, covering a variety of topics and document lengths.

Evaluation:
Conclusion Justified: Yes
Robustness: low
Confidence Level: low
Justification: The evidence provided does not directly support the claim that Y-NQ enables the comparison open-book reading comprehension, only that the dataset is available.
Key Limitations: Lack of evidence to support the claim.

--------------------------------------------------

Claim 2:
Type: contribution
Statement: The main contributions of our data set are to allow for the comparison of LLM results in a reading comprehension task across a high- and a low-resource language, showing what are the generalization capabilities of LLMs in this particular case.
Location: Conclusions
Exact Quote: The main contributions of our data set are to allow for the comparison of LLM results in a reading comprehension task across a high- and a low-resource language, showing what are the generalization capabilities of LLMs in this particular case.

Evidence:
- Evidence Text: We evaluate our dataset with GPT-4o, o1-mini, and LlaMA-3.1-8b, thereby covering both open and closed models, as well as models of different sizes.
  Strength: moderate
  Location: Experiments
  Limitations: The evaluation was limited to only 3 LLM models and may not generalize to other models.
  Exact Quote: We evaluate our dataset with GPT-4o, o1-mini, and LlaMA-3.1-8b, thereby covering both open and closed models, as well as models of different sizes.

- Evidence Text: Our experiments show that the reading comprehension capabilities of current English LLMs do not extend to Yorùbá.
  Strength: moderate
  Location: Conclusions
  Limitations: It is unclear if this is due to the lower-resource nature of Yorùbá or other factors.
  Exact Quote: Our experiments show that the reading comprehension capabilities of current English LLMs do not extend to Yorùbá.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence provides some support for the claim that Y-NQ allows for the comparison of LLM results and demonstrates the generalization capabilities of LLMs to some extent. However, the evaluation was limited and more research is needed to fully evaluate the claim.
Key Limitations: Limited evaluation, lack of comparison to other datasets.

--------------------------------------------------

Claim 3:
Type: result
Statement: Yorùbá consistently performs worse than English.
Location: Experiments
Exact Quote: Yorùbá consistently performs worse than English (e.g., losing 0.4 in Rouge-1).

Evidence:
- Evidence Text: Model performance changes with the length of the document, as shown in Figure 1. The dataset was split into equal size of documents in each length bucket. We can see a drop in performance when the Yorùbá documents reach 1,500 words, which shows the challenges that current models face in long-context understanding of low-resource languages.
  Strength: moderate
  Location: Experiments
  Limitations: The drop in performance may not be solely due to the low-resource nature of Yorùbá and could be influenced by other factors.
  Exact Quote: Model performance changes with the length of the document, as shown in Figure 1. The dataset was split into equal size of documents in each length bucket. We can see a drop in performance when the Yorùbá documents reach 1,500 words, which shows the challenges that current models face in long-context understanding of low-resource languages.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence provides some support for the claim that Yorùbá consistently performs worse than English, particularly in longer documents. However, the reasons for this difference are not fully explored.
Key Limitations: Lack of exploration of potential causes for the performance difference.

--------------------------------------------------

Claim 4:
Type: contribution
Statement: Y-NQ is freely available on HuggingFace.
Location: Conclusions
Exact Quote: Y-NQ is freely available on HuggingFace.

Evidence:
- Evidence Text: Y-NQ is a newly released dataset that enables to compare generative open-book reading comprehension between English and Yorùbá.
  Strength: weak
  Location: Conclusions
  Limitations: The evidence does not directly address the availability of Y-NQ on HuggingFace.
  Exact Quote: Y-NQ is a newly released dataset that enables to compare generative open-book reading comprehension between English and Yorùbá.

Evaluation:
Conclusion Justified: Yes
Robustness: low
Confidence Level: low
Justification: The evidence provided does not directly support the claim that Y-NQ is freely available on HuggingFace.
Key Limitations: Lack of evidence to support the claim.

--------------------------------------------------

Claim 5:
Type: result
Statement: Current English LLMs do not extend to Yorùbá.
Location: Conclusions
Exact Quote: Our experiments show that the reading comprehension capabilities of current English LLMs do not extend to Yorùbá.

Evidence:
- Evidence Text: We evaluate our dataset with GPT-4o, o1-mini, and LlaMA-3.1-8b, thereby covering both open and closed models, as well as models of different sizes.
  Strength: moderate
  Location: Experiments
  Limitations: The evaluation was limited to only 3 LLM models and may not generalize to other models.
  Exact Quote: We evaluate our dataset with GPT-4o, o1-mini, and LlaMA-3.1-8b, thereby covering both open and closed models, as well as models of different sizes.

- Evidence Text: Yorùbá consistently performs worse than English (e.g., losing 0.4 in Rouge-1).
  Strength: moderate
  Location: Experiments
  Limitations: The difference in performance may not be solely due to the inability of LLMs to extend to Yorùbá.
  Exact Quote: Yorùbá consistently performs worse than English (e.g., losing 0.4 in Rouge-1).

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence provides some support for the claim that current English LLMs do not extend to Yorùbá. However, the evaluation was limited and more research is needed to fully evaluate the claim.
Key Limitations: Limited evaluation, lack of exploration of potential causes for the performance difference.

--------------------------------------------------

