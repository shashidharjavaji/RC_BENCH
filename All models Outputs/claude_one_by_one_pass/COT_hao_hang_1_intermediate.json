{
    "claims": {
        "claims": [
            {
                "claim_id": 1,
                "claim_text": "Chain-of-thought prompting significantly improves large language models' ability to perform complex reasoning",
                "location": "Abstract",
                "claim_type": "Main finding",
                "exact_quote": "We explore how generating a chain of thought\u2014a series of intermediate reasoning steps\u2014significantly improves the ability of large language models to perform complex reasoning."
            },
            {
                "claim_id": 2,
                "claim_text": "Reasoning abilities emerge naturally in sufficiently large language models via chain-of-thought prompting",
                "location": "Abstract",
                "claim_type": "Key finding",
                "exact_quote": "In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain-of-thought prompting"
            },
            {
                "claim_id": 3,
                "claim_text": "PaLM 540B with chain-of-thought prompting achieves state-of-the-art accuracy on GSM8K benchmark",
                "location": "Abstract",
                "claim_type": "Performance result",
                "exact_quote": "prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier."
            },
            {
                "claim_id": 4,
                "claim_text": "Chain-of-thought prompting allows models to decompose multi-step problems",
                "location": "Section 2",
                "claim_type": "Method capability",
                "exact_quote": "First, chain of thought, in principle, allows models to decompose multi-step problems into intermediate steps, which means that additional computation can be allocated to problems that require more reasoning steps."
            },
            {
                "claim_id": 5,
                "claim_text": "Chain-of-thought provides interpretable insight into model reasoning",
                "location": "Section 2",
                "claim_type": "Method benefit",
                "exact_quote": "Second, a chain of thought provides an interpretable window into the behavior of the model, suggesting how it might have arrived at a particular answer"
            },
            {
                "claim_id": 6,
                "claim_text": "Chain-of-thought prompting is broadly applicable across different reasoning tasks",
                "location": "Section 2",
                "claim_type": "Method capability",
                "exact_quote": "Third, chain-of-thought reasoning can be used for tasks such as math word problems, commonsense reasoning, and symbolic manipulation, and is potentially applicable (at least in principle) to any task that humans can solve via language."
            },
            {
                "claim_id": 7,
                "claim_text": "Chain-of-thought prompting works with off-the-shelf language models without finetuning",
                "location": "Section 2",
                "claim_type": "Method advantage",
                "exact_quote": "Finally, chain-of-thought reasoning can be readily elicited in sufficiently large off-the-shelf language models simply by including examples of chain of thought sequences into the exemplars of few-shot prompting."
            },
            {
                "claim_id": 8,
                "claim_text": "Chain-of-thought prompting is an emergent ability requiring sufficient model scale",
                "location": "Results sections",
                "claim_type": "Key finding",
                "exact_quote": "chain-of-thought prompting does not positively impact performance for small models, and only yields performance gains when used with models of 100B parameters."
            },
            {
                "claim_id": 9,
                "claim_text": "Performance gains are larger for more complex problems",
                "location": "Results",
                "claim_type": "Finding",
                "exact_quote": "chain-of-thought prompting has larger performance gains for more-complicated problems."
            },
            {
                "claim_id": 10,
                "claim_text": "Chain-of-thought prompting enables length generalization beyond seen examples",
                "location": "Symbolic Reasoning section",
                "claim_type": "Capability",
                "exact_quote": "chain-of-thought prompting facilitates length generalization to inference-time inputs longer than those seen in the few-shot exemplars."
            }
        ]
    },
    "evidence": [
        {
            "claim_id": 1,
            "evidence": [
                {
                    "evidence_id": 1,
                    "evidence_text": "On GSM8K benchmark of math word problems, chain-of-thought prompting with PaLM 540B achieved state-of-the-art accuracy of 56.9%, significantly outperforming standard prompting (17.9%)",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Only tested on specific model sizes and specific reasoning tasks",
                    "location": "Section 3.2 Results",
                    "exact_quote": "For instance, for GSM8K (the dataset with the lowest baseline performance), performance more than doubled for the largest GPT and PaLM models."
                },
                {
                    "evidence_id": 2,
                    "evidence_text": "On commonsense reasoning tasks, PaLM 540B with chain-of-thought achieved 77.8% on StrategyQA (vs 68.6% baseline) and 95.4% on sports understanding (vs 80.5% baseline)",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Limited to specific commonsense reasoning datasets",
                    "location": "Section 4 Results",
                    "exact_quote": "With chain-of-thought prompting, PaLM 540B achieved strong performance relative to baselines, outperforming the prior state of the art on StrategyQA (75.6% vs 69.4%) and outperforming an unaided sports enthusiast on sports understanding (95.4% vs 84%)."
                },
                {
                    "evidence_id": 3,
                    "evidence_text": "On symbolic reasoning tasks, chain-of-thought enabled length generalization beyond training examples",
                    "evidence_type": "primary",
                    "strength": "moderate",
                    "limitations": "Only tested on two synthetic tasks",
                    "location": "Section 5 Results",
                    "exact_quote": "With chain-of-thought prompting, language models achieve upward scaling curves (though performance is lower than in the in-domain setting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of thought for language models of sufficient scale."
                }
            ]
        },
        {
            "claim_id": 2,
            "evidence": [
                {
                    "evidence_id": 1,
                    "evidence_text": "Chain-of-thought prompting shows strong performance gains only with large models (100B+ parameters), while hurting performance in smaller models",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Limited to specific arithmetic, commonsense and symbolic reasoning tasks tested",
                    "location": "Section 3.2 Results",
                    "exact_quote": "chain-of-thought prompting is an emergent ability of model scale (Wei et al., 2022b). That is, chain-of-thought prompting does not positively impact performance for small models, and only yields performance gains when used with models of 100B parameters."
                },
                {
                    "evidence_id": 2,
                    "evidence_text": "Results across multiple model scales show emergence pattern for arithmetic reasoning",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Tested only on GSM8K, SVAMP and MAWPS benchmarks",
                    "location": "Figure 4",
                    "exact_quote": "Chain-of-thought prompting enables large language models to solve challenging math problems. Notably, chain-of-thought reasoning is an emergent ability of increasing model scale."
                },
                {
                    "evidence_id": 3,
                    "evidence_text": "Similar emergence pattern observed for symbolic manipulation tasks",
                    "evidence_type": "primary",
                    "strength": "moderate",
                    "limitations": "Limited to two synthetic tasks - letter concatenation and coin flip",
                    "location": "Section 5 Results",
                    "exact_quote": "Note that these in-domain evaluations are 'toy tasks' in the sense that perfect solution structures are already provided by the chains of thought in the few-shot exemplars... And yet, small models still fail\u2014the ability to perform abstract manipulations on unseen symbols for these three tasks only arises at the scale of 100B model parameters."
                }
            ]
        },
        {
            "claim_id": 3,
            "evidence": [
                {
                    "evidence_id": 1,
                    "evidence_text": "PaLM 540B achieves 58.6% accuracy on GSM8K with chain-of-thought prompting and external calculator, surpassing prior state-of-the-art of 55%",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Required external calculator for best performance; Results based on one model size/configuration",
                    "location": "Section 3.2 Results & Table 1",
                    "exact_quote": "Third, chain-of-thought prompting via GPT-3 175B and PaLM 540B compares favorably to prior state of the art, which typically finetunes a task-specific model on a labeled training dataset. Figure 4 shows how PaLM 540B uses chain-of-thought prompting to achieve new state of the art on GSM8K"
                },
                {
                    "evidence_id": 2,
                    "evidence_text": "Experimental results showing PaLM 540B performance compared to baseline and prior SOTA",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Limited to one benchmark dataset",
                    "location": "Table 1 & Table 2",
                    "exact_quote": "PaLM 540B Standard 17.9 Chain of thought 56.9 (+39.0) + ext. calc 58.6"
                }
            ]
        },
        {
            "claim_id": 4,
            "evidence": [
                {
                    "evidence_id": 1,
                    "evidence_text": "Analysis of model outputs shows successful decomposition of multi-step math problems into intermediate steps with correct reasoning",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Analysis was done manually on a limited sample of 50 correct answers",
                    "location": "Section 3.2 Results",
                    "exact_quote": "Of 50 random examples where the model returned the correct final answer, all of the generated chains of thought were also logically and mathematically correct except two that coincidentally arrived at the correct answer"
                },
                {
                    "evidence_id": 2,
                    "evidence_text": "Example showing multi-step problem decomposition for date calculation",
                    "evidence_type": "primary",
                    "strength": "moderate",
                    "limitations": "Single example, may not be representative",
                    "location": "Table 17 in Appendix D",
                    "exact_quote": "May 6, 1992 is ten years ago, so today is May 6, 2002. So a month ago will be April 6, 2002. So the answer is 04/06/2002."
                },
                {
                    "evidence_id": 3,
                    "evidence_text": "Chain-of-thought prompting enables length generalization by breaking down longer problems",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Only tested on two symbolic reasoning tasks",
                    "location": "Section 5 Symbolic Reasoning",
                    "exact_quote": "With chain-of-thought prompting, language models achieve upward scaling curves (though performance is lower than in the in-domain setting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of thought for language models of sufficient scale."
                }
            ]
        },
        {
            "claim_id": 5,
            "evidence": [
                {
                    "evidence_id": 1,
                    "evidence_text": "Manual analysis of 50 correct answers showed that 48 of 50 chains of thought were logically and mathematically correct, providing insight into the model's reasoning process",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Only examined cases where final answer was correct; limited to math problems",
                    "location": "Section 3.2, Paragraph 4",
                    "exact_quote": "Of 50 random examples where the model returned the correct final answer, all of the generated chains of thought were also logically and mathematically correct except two that coincidentally arrived at the correct answer"
                },
                {
                    "evidence_id": 2,
                    "evidence_text": "Error analysis of incorrect answers categorized different types of reasoning failures",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Analysis was subjective/manual; focused only on math problems",
                    "location": "Section 3.2, Paragraph 4-5",
                    "exact_quote": "We also randomly examined 50 random samples for which the model gave the wrong answer. The summary of this analysis is that 46% of the chains of thought were almost correct, barring minor mistakes (calculator error, symbol mapping error, or one reasoning step missing), and that the other 54% of the chains of thought had major errors in semantic understanding or coherence"
                }
            ]
        },
        {
            "claim_id": 6,
            "evidence": [
                {
                    "evidence_id": 1,
                    "evidence_text": "Chain-of-thought prompting improves performance across arithmetic, commonsense, and symbolic reasoning tasks when using sufficiently large language models",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Only works with large language models (>100B parameters)",
                    "location": "Sections 3-5, Results sections",
                    "exact_quote": "The empirical evaluations on arithmetic, commonsense, and symbolic reasoning benchmarks show that chain-of-thought prompting outperforms standard prompting, sometimes to a striking degree."
                },
                {
                    "evidence_id": 2,
                    "evidence_text": "Demonstrated effectiveness on multiple commonsense reasoning datasets including CSQA, StrategyQA, Date Understanding, Sports Understanding and SayCan",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Performance varies across tasks and models",
                    "location": "Section 4, Commonsense Reasoning",
                    "exact_quote": "For all tasks, scaling up model size improved the performance of standard prompting; chain-of-thought prompting led to further gains, with improvements appearing to be largest for PaLM 540B."
                },
                {
                    "evidence_id": 3,
                    "evidence_text": "Improves performance on symbolic reasoning tasks and enables length generalization",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Only tested on two symbolic tasks (letter concatenation and coin flip)",
                    "location": "Section 5, Symbolic Reasoning",
                    "exact_quote": "With chain-of-thought prompting, language models achieve upward scaling curves (though performance is lower than in the in-domain setting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of thought for language models of sufficient scale."
                }
            ]
        },
        {
            "claim_id": 7,
            "evidence": [
                {
                    "evidence_id": 1,
                    "evidence_text": "Experiments show chain-of-thought prompting improves performance across multiple benchmarks using unmodified LaMDA, GPT-3, and PaLM models",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Only works with sufficiently large models (around 100B parameters)",
                    "location": "Section 3.2 Results",
                    "exact_quote": "chain-of-thought prompting via GPT-3 175B and PaLM 540B compares favorably to prior state of the art, which typically finetunes a task-specific model on a labeled training dataset."
                },
                {
                    "evidence_id": 2,
                    "evidence_text": "The study uses only prompting with no model finetuning",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Limited to specific types of reasoning tasks tested",
                    "location": "Section E.2 Computational Resources",
                    "exact_quote": "For all three language models we evaluated, we did prompting-based inference only. No finetuning was done for this paper."
                }
            ]
        },
        {
            "claim_id": 8,
            "evidence": [
                {
                    "evidence_id": 1,
                    "evidence_text": "Experiments across multiple models (LaMDA, GPT, PaLM) show chain-of-thought prompting only improves performance for models above ~100B parameters",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Limited to specific arithmetic, commonsense and symbolic reasoning tasks tested",
                    "location": "Section 3.2 Results",
                    "exact_quote": "chain-of-thought prompting is an emergent ability of model scale (Wei et al., 2022b). That is, chain-of-thought prompting does not positively impact performance for small models, and only yields performance gains when used with models of 100B parameters."
                },
                {
                    "evidence_id": 2,
                    "evidence_text": "Quantitative results showing chain-of-thought prompting hurts performance for smaller models but helps larger ones",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Results specific to math word problems tested",
                    "location": "Section 3.2 Results - Table 2",
                    "exact_quote": "Note that chain of thought prompting is an emergent ability of model scale\u2014it does not positively impact performance until used with a model of sufficient scale."
                },
                {
                    "evidence_id": 3,
                    "evidence_text": "Figure 4 shows scaling curves demonstrating emergence of chain-of-thought abilities",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Limited to specific models and tasks tested",
                    "location": "Section 3.2 Results - Figure 4",
                    "exact_quote": "Chain-of-thought prompting enables large language models to solve challenging math problems. Notably, chain-of-thought reasoning is an emergent ability of increasing model scale."
                }
            ]
        },
        {
            "claim_id": 9,
            "evidence": [
                {
                    "evidence_id": 1,
                    "evidence_text": "Chain-of-thought prompting shows larger performance gains on GSM8K (the dataset with lowest baseline performance) compared to simpler problems",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Limited to arithmetic reasoning tasks",
                    "location": "Section 3.2 Results",
                    "exact_quote": "For instance, for GSM8K (the dataset with the lowest baseline performance), performance more than doubled for the largest GPT and PaLM models. On the other hand, for SingleOp, the easiest subset of MAWPS which only requires a single step to solve, performance improvements were either negative or very small"
                },
                {
                    "evidence_id": 2,
                    "evidence_text": "Detailed performance comparison on MAWPS subsets of varying complexity",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Limited to one benchmark dataset",
                    "location": "Table 3",
                    "exact_quote": "The point of stratifying the MAWPS benchmark is to show that performance gains are minimal on easy one-step or two-step problems where large language models already achieve high performance (e.g., SingleOp, SingleEq, and AddSub)."
                }
            ]
        },
        {
            "claim_id": 10,
            "evidence": [
                {
                    "evidence_id": 1,
                    "evidence_text": "On symbolic reasoning tasks (last letter concatenation and coin flip), chain-of-thought prompting enabled models to generalize to longer sequences than seen in training. For example, PaLM 540B achieved 94.8% accuracy on 3-word names and 63% on 4-word names despite only being trained on 2-word examples.",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Only demonstrated on two specific symbolic tasks; effectiveness depends on model scale",
                    "location": "Section 5 Results",
                    "exact_quote": "As for the OOD evaluations, standard prompting fails for both tasks. With chain-of-thought prompting, language models achieve upward scaling curves (though performance is lower than in the in-domain setting). Hence, chain-of-thought prompting facilitates length generalization beyond seen chains of thought for language models of sufficient scale."
                },
                {
                    "evidence_id": 2,
                    "evidence_text": "Results from experimental data table show specific performance numbers on out-of-domain (OOD) longer sequence tests",
                    "evidence_type": "primary",
                    "strength": "strong",
                    "limitations": "Limited to specific model architectures tested",
                    "location": "Table 5",
                    "exact_quote": "PaLM 540B with chain-of-thought prompting achieved: Last Letter Concatenation - OOD 3: 94.8%, OOD 4: 63.0%; Coin Flip - OOD 3: 98.6%, OOD 4: 90.2%"
                }
            ]
        }
    ],
    "conclusions": {
        "conclusions": []
    },
    "execution_times": {
        "claims_analysis_time": "21.71 seconds",
        "evidence_analysis_time": "303.21 seconds",
        "conclusions_analysis_time": "0.00 seconds",
        "total_execution_time": "400.86 seconds"
    }
}