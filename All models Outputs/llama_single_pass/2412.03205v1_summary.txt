Claim 1:
Type: contribution
Statement: U-MATH is a novel multimodal benchmark for evaluating the university-level mathematical reasoning of LLMs.
Location: Abstract
Exact Quote: U-MATH: A UNIVERSITY-LEVEL BENCHMARK FOR EVALUATING MATHEMATICAL SKILLS IN LLMS

Evidence:
- Evidence Text: 1,100 unpublished free-form problems from real teaching materials, covering 6 core mathematical subjects, with 20% involving image-based reasoning.
  Strength: strong
  Location: Section 3
  Limitations: Limited to university-level problems, may introduce biases by favoring certain problem types.
  Exact Quote: U-MATH includes 1,100 unpublished free-form problems from real teaching materials, covering 6 core mathematical subjects, with 20% involving image-based reasoning.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence supports the claim by providing a comprehensive and diverse set of problems for evaluating LLMs' mathematical reasoning capabilities.
Key Limitations: Limited scope and potential biases in problem selection.

--------------------------------------------------

Claim 2:
Type: performance
Statement: The highest accuracy achieved on U-MATH was 63.4% on text-based tasks and 45.0% on visual problems.
Location: Section 4.2
Exact Quote: Among text-only models, the math-specific model Qwen2.5-Math-72B achieves the highest overall accuracy at 50.2%,... In the multi-modal model group, Gemini-1.5-pro-002 leads with an overall accuracy of 60.1%...

Evidence:
- Evidence Text: Qwen2.5-Math-72B achieved 50.2% accuracy on text-based tasks, and Gemini-1.5-pro-002 achieved 60.1% accuracy on overall tasks.
  Strength: strong
  Location: Table 4
  Limitations: Results may not generalize to other models or tasks.
  Exact Quote: Qwen2.5-Math-72B **50.2** **59.0**... Gemini 1.5 Pro **60.1** **63.4**

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence supports the claim by providing concrete accuracy results for top-performing models on U-MATH.
Key Limitations: Results may be model-specific.

--------------------------------------------------

Claim 3:
Type: performance
Statement: The best model on µ-MATH achieved a macro F1-score of 80%.
Location: Section 4.3
Exact Quote: Our results show that the best model achieving the macro F1-score of 80%, showing room for improvement and underscoring the limitations of widely used models like GPT-4o in evaluation tasks.

Evidence:
- Evidence Text: Qwen2.5-Math-72B achieved a macro F1-score of 80.9% on µ-MATH.
  Strength: strong
  Location: Table 5
  Limitations: Results may not generalize to other evaluation tasks or models.
  Exact Quote: Qwen2.5-Math 72B **80.9**

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence supports the claim by providing a concrete F1-score result for the top-performing model on µ-MATH.
Key Limitations: Results may be model-specific.

--------------------------------------------------

