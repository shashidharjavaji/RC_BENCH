Claim 1:
Type: performance
Statement: FAME-ViL achieves superior performance across a set of diverse fashion tasks with much fewer parameters.
Location: Section 1. Introduction
Exact Quote: Compared with existing approaches, FAME-ViL applies a single model for multiple heterogeneous fashion tasks, therefore being much more parameter-efficient.

Evidence:
- Evidence Text: Table 1: Cross-Modal Retrieval (XMR) results on FashionGen [61].
  Strength: strong
  Location: Section 4.1. Comparisons with prior art methods
  Limitations: Results are based on a specific dataset (FashionGen) and protocol (random 100).
  Exact Quote: Our FAME-ViL outperforms all prior art fashion models often by a large margin, validating the performance advantages of our method over alternatives in addition to better parameter efficiency.

- Evidence Text: Table 2: Text-Guided Image Retrieval (TGIR) results on FashionIQ [83].
  Strength: strong
  Location: Section 4.1. Comparisons with prior art methods
  Limitations: Results are based on a specific dataset (FashionIQ) and protocol (original protocol used by FashionIQ).
  Exact Quote: Our single-task variant already achieves a new art performance. With a simple addition-based fusion mechanism, FAME-ViL can even outperform significantly [2] with the same CLIP pre-training and a complex fusion module.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence from Tables 1 and 2 strongly supports the claim, demonstrating FAME-ViL's superior performance across diverse fashion tasks with fewer parameters.
Key Limitations: Dataset and protocol specificity

--------------------------------------------------

Claim 2:
Type: methodology
Statement: FAME-ViL's task-versatile architecture with cross-attention adapters and task-specific adapters is effective in exploiting inter-task relatedness.
Location: Section 3.1. Model architecture
Exact Quote: Our proposed adapters also serve as the key component for task-versatile architecture design and enabling stable MTL.

Evidence:
- Evidence Text: Group (I) of Table 4: Ablation study on architecture
  Strength: moderate
  Location: Section 4.2. Ablation study on architecture
  Limitations: Results are based on a specific setting (single-task learning) and may not generalize to multi-task learning.
  Exact Quote: TSA and XAA can bring in 3%-6% relative improvements for XMR and TGIR.

- Evidence Text: Group (II) of Table 4: Ablation study on architecture
  Strength: strong
  Location: Section 4.2. Ablation study on architecture
  Limitations: Results are based on a specific setting (multi-task learning) and may not generalize to other architectures.
  Exact Quote: When TSA and XAA work together, the model can achieve better relative performance than the sum of their gains.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence from Groups (I) and (II) of Table 4 supports the claim, demonstrating the effectiveness of FAME-ViL's architecture in exploiting inter-task relatedness, although with some limitations in generalizability.
Key Limitations: Specificity of settings (single-task vs. multi-task learning)

--------------------------------------------------

Claim 3:
Type: methodology
Statement: FAME-ViL's multi-teacher distillation (MTD) based training strategy is effective in alleviating negative transfer.
Location: Section 3.2. Heterogeneous multi-task learning
Exact Quote: Guided by the soft ground truth of each teacher, overfitting can be well avoided in an elegant manner.

Evidence:
- Evidence Text: Group (III) of Table 4: Ablation study on multi-task training strategy
  Strength: strong
  Location: Section 4.3. Ablation study on multi-task training strategy
  Limitations: Results are based on a specific comparison with other methods (IAS and IMTLG).
  Exact Quote: Our MTD can improve over both IAS and IMTLG by a large margin, demonstrating its effectiveness in alleviating negative transfer.

- Evidence Text: Figure 5: Training dynamics of our multi-teacher distillation (MTD) and alternative multi-task learning methods
  Strength: strong
  Location: Section 4.4. Further analysis
  Limitations: Results are based on a specific visualization of training dynamics.
  Exact Quote: Our MTD yields consistent and significant performance boost per task, rendering it a more stable and effective learning strategy.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence from Group (III) of Table 4 and Figure 5 strongly supports the claim, demonstrating FAME-ViL's effectiveness in alleviating negative transfer through its MTD strategy.
Key Limitations: Specificity of comparison methods

--------------------------------------------------

