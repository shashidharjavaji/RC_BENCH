{
    "paper_analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "Fine-tuning LLMs on datasets containing problem contexts and reference plans does not lead to robust planning skills.",
                "location": "Abstract",
                "type": "Novel finding",
                "exact_quote": "We challenge the claim that fine-tuning LLMs simply on datasets containing problem contexts and reference plans acquire robust planning skills, by demonstrating their failure on OOD test sets."
            },
            "evidence": [
                {
                    "evidence_id": 1,
                    "evidence_text": "Fine-tuning LLMs on datasets containing problem contexts and reference plans does not lead to robust planning skills.",
                    "strength": "strong",
                    "limitations": "The model struggles to generalize to OOD scenarios, even when the core task logic is well comprehended.",
                    "location": "Section 4.1",
                    "exact_quote": "The model struggles to generalize to OOD scenarios, even when the core task logic is well comprehended."
                }
            ],
            "conclusion": {
                "claim_id": 1,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The claim is supported by the evidence that fine-tuning LLMs on datasets containing problem contexts and reference plans does not lead to robust planning skills.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "Strategies like CoT lead to incremental improvements in plan quality by enhancing plan executability, even if they do not directly increase validity rates.",
                "location": "Abstract",
                "type": "Novel finding",
                "exact_quote": "We show that strategies like CoT lead to incremental improvements in plan quality by enhancing plan executability, even if they do not directly increase validity rates."
            },
            "evidence": [
                {
                    "evidence_id": 2,
                    "evidence_text": "Strategies like CoT lead to incremental improvements in plan quality by enhancing plan executability, even if they do not directly increase validity rates.",
                    "strength": "strong",
                    "limitations": "Goal CoT hinders planning performance among OOD cases.",
                    "location": "Section 4.3",
                    "exact_quote": "Goal CoT is the only strategy that hinders planning performance among OOD cases, showing no improvement whatsoever."
                }
            ],
            "conclusion": {
                "claim_id": 2,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The claim is supported by the evidence that strategies like CoT lead to incremental improvements in plan quality by enhancing plan executability, even if they do not directly increase validity rates.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "Reinforcement learning with our proposed \u2018LCCS\u2019 reward emerges as the most effective strategy.",
                "location": "Abstract",
                "type": "Novel finding",
                "exact_quote": "We show that RL with our proposed \u2018LCCS\u2019 reward emerges as the most effective strategy."
            },
            "evidence": [
                {
                    "evidence_id": 3,
                    "evidence_text": "Reinforcement learning with our proposed \u2018LCCS\u2019 reward emerges as the most effective strategy.",
                    "strength": "strong",
                    "limitations": "The model achieves high performance across all domains in in-distribution tests, mirroring the positive result reported by PlanGPT.",
                    "location": "Section 4.7",
                    "exact_quote": "RL notably improves the performance under our end-to-end planning paradigm, especially on longer problems."
                }
            ],
            "conclusion": {
                "claim_id": 3,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The claim is supported by the evidence that reinforcement learning with our proposed \u2018LCCS\u2019 reward emerges as the most effective strategy.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "Fine-tuning LLMs on datasets containing problem contexts and reference plans struggles to foster robust planning skills beyond in-distribution instances.",
                "location": "Abstract",
                "type": "Novel finding",
                "exact_quote": "Nonetheless, our research reveals that RL stands out as the most effective strategy in this end-to-end paradigm, enhancing both the validity and executability rates on longer problems."
            },
            "evidence": [
                {
                    "evidence_id": 4,
                    "evidence_text": "Fine-tuning LLMs on datasets containing problem contexts and reference plans struggles to foster robust planning skills beyond in-distribution instances.",
                    "strength": "strong",
                    "limitations": "The model struggles to generalize to OOD scenarios, even when the core task logic is well comprehended.",
                    "location": "Section 4.1",
                    "exact_quote": "The model struggles to generalize to OOD scenarios, even when the core task logic is well comprehended."
                }
            ],
            "conclusion": {
                "claim_id": 4,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The claim is supported by the evidence that fine-tuning LLMs on datasets containing problem contexts and reference plans struggles to foster robust planning skills beyond in-distribution instances.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "The model achieves high performance across all domains in in-distribution tests, mirroring the positive result reported by PlanGPT.",
                "location": "4.1 LLMs Learn to Plan in Natural Language, but Struggle in OOD Scenarios",
                "type": "Novel finding",
                "exact_quote": "The model indeed achieved high performance across all domains in in-distribution tests, mirroring the positive result reported by PlanGPT."
            },
            "evidence": [
                {
                    "evidence_id": 5,
                    "evidence_text": "The model achieves high performance across all domains in in-distribution tests, mirroring the positive result reported by PlanGPT.",
                    "strength": "strong",
                    "limitations": "The model struggles to generalize to OOD scenarios, even when the core task logic is well comprehended.",
                    "location": "Section 4.1",
                    "exact_quote": "The model achieves high performance across all domains in in-distribution tests, mirroring the positive result reported by PlanGPT."
                }
            ],
            "conclusion": {
                "claim_id": 5,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The claim is supported by the evidence that the model achieves high performance across all domains in in-distribution tests, mirroring the positive result reported by PlanGPT.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "The model struggles to generalize to OOD scenarios, even when the core task logic is well comprehended.",
                "location": "4.1 LLMs Learn to Plan in Natural Language, but Struggle in OOD Scenarios",
                "type": "Novel finding",
                "exact_quote": "The model struggles to generalize to OOD scenarios, even when the core task logic is well comprehended."
            },
            "evidence": [
                {
                    "evidence_id": 6,
                    "evidence_text": "The model struggles to generalize to OOD scenarios, even when the core task logic is well comprehended.",
                    "strength": "strong",
                    "limitations": "The model struggles to generalize to OOD scenarios, even when the core task logic is well comprehended.",
                    "location": "Section 4.1",
                    "exact_quote": "The model struggles to generalize to OOD scenarios, even when the core task logic is well comprehended."
                }
            ],
            "conclusion": {
                "claim_id": 6,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The claim is supported by the evidence that the model struggles to generalize to OOD scenarios, even when the core task logic is well comprehended.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 7,
            "claim": {
                "text": "Permutation augmentation enhances the model's ability to parse unseen problem content.",
                "location": "4.2 The Secret Help of Permutation",
                "type": "Novel finding",
                "exact_quote": "While this technique does not significantly improve the validity rate, it largely enhances the executability rate."
            },
            "evidence": [
                {
                    "evidence_id": 7,
                    "evidence_text": "Permutation augmentation enhances the model's ability to parse unseen problem content.",
                    "strength": "strong",
                    "limitations": "The model struggles to generalize to OOD scenarios, even when the core task logic is well comprehended.",
                    "location": "Section 4.2",
                    "exact_quote": "Permutation augmentation enhances the model's ability to parse unseen problem content."
                }
            ],
            "conclusion": {
                "claim_id": 7,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The claim is supported by the evidence that permutation augmentation enhances the model's ability to parse unseen problem content.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 8,
            "claim": {
                "text": "Goal CoT hinders planning performance among OOD cases.",
                "location": "4.3 Goal CoT: The Complexity Paradox and Overfitting Issue",
                "type": "Novel finding",
                "exact_quote": "Goal CoT is the only strategy that hinders planning performance among OOD cases, showing no improvement whatsoever."
            },
            "evidence": [
                {
                    "evidence_id": 8,
                    "evidence_text": "Goal CoT hinders planning performance among OOD cases.",
                    "strength": "strong",
                    "limitations": "Goal CoT is the only strategy that hinders planning performance among OOD cases, showing no improvement whatsoever.",
                    "location": "Section 4.3",
                    "exact_quote": "Goal CoT is the only strategy that hinders planning performance among OOD cases, showing no improvement whatsoever."
                }
            ],
            "conclusion": {
                "claim_id": 8,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The claim is supported by the evidence that Goal CoT hinders planning performance among OOD cases.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 9,
            "claim": {
                "text": "Self-correction learning does not improve validity rates.",
                "location": "4.4 LLMs Recognize Mistakes But Fail to Correct Them",
                "type": "Novel finding",
                "exact_quote": "Despite high initial expectations for self-correction learning, this recently proposed strategy did not improve validity rates."
            },
            "evidence": [
                {
                    "evidence_id": 9,
                    "evidence_text": "Self-correction learning does not improve validity rates.",
                    "strength": "strong",
                    "limitations": "The model is able to identify errors in a high precision and recall rate, but fails to correct them effectively.",
                    "location": "Section 4.4",
                    "exact_quote": "The model is able to identify errors in a high precision and recall rate, but fails to correct them effectively."
                }
            ],
            "conclusion": {
                "claim_id": 9,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The claim is supported by the evidence that self-correction learning does not improve validity rates.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 10,
            "claim": {
                "text": "State CoT boosts executability with a caveat: efficacy limited to short problems.",
                "location": "4.5 State CoT Boosts Executability with a Caveat: Efficacy Limited to Short Problems",
                "type": "Novel finding",
                "exact_quote": "We observed that State CoT does not improve plan executability within the \u2018long\u2019 test set, yet it significantly enhances performance within the \u2018unseen\u2019 test set."
            },
            "evidence": [
                {
                    "evidence_id": 10,
                    "evidence_text": "State CoT boosts executability with a caveat: efficacy limited to short problems.",
                    "strength": "strong",
                    "limitations": "State CoT does not improve plan executability within the \u2018long\u2019 test set, yet it significantly enhances performance within the \u2018unseen\u2019 test set.",
                    "location": "Section 4.5",
                    "exact_quote": "State CoT does not improve plan executability within the \u2018long\u2019 test set, yet it significantly enhances performance within the \u2018unseen\u2019 test set."
                }
            ],
            "conclusion": {
                "claim_id": 10,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The claim is supported by the evidence that State CoT boosts executability with a caveat: efficacy limited to short problems.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 11,
            "claim": {
                "text": "RL enhances model performance under the end-to-end planning paradigm.",
                "location": "4.7 RL Enhances Model Performance",
                "type": "Novel finding",
                "exact_quote": "RL notably improves the performance under our end-to-end planning paradigm, especially on longer problems."
            },
            "evidence": [
                {
                    "evidence_id": 11,
                    "evidence_text": "RL enhances model performance under the end-to-end planning paradigm.",
                    "strength": "strong",
                    "limitations": "The model achieves high performance across all domains in in-distribution tests, mirroring the positive result reported by PlanGPT.",
                    "location": "Section 4.7",
                    "exact_quote": "RL notably improves the performance under our end-to-end planning paradigm, especially on longer problems."
                }
            ],
            "conclusion": {
                "claim_id": 11,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The claim is supported by the evidence that RL enhances model performance under the end-to-end planning paradigm.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 12,
            "claim": {
                "text": "Strategies that increase the response length, such as incorporating Chain of Thought prompts, cannot benefit from multiple sampling.",
                "location": "F Additional Results: pass@k Validity Rate",
                "type": "Novel finding",
                "exact_quote": "Strategies that increase the response length, such as incorporating Chain of Thought prompts, cannot benefit from multiple sampling."
            },
            "evidence": [
                {
                    "evidence_id": 12,
                    "evidence_text": "Strategies that increase the response length, such as incorporating Chain of Thought prompts, cannot benefit from multiple sampling.",
                    "strength": "strong",
                    "limitations": "The model struggles to generalize to OOD scenarios, even when the core task logic is well comprehended.",
                    "location": "Section 4.7",
                    "exact_quote": "Strategies that increase the response length, such as incorporating Chain of Thought prompts, cannot benefit from multiple sampling."
                }
            ],
            "conclusion": {
                "claim_id": 12,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The claim is supported by the evidence that strategies that increase the response length, such as incorporating Chain of Thought prompts, cannot benefit from multiple sampling.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 13,
            "claim": {
                "text": "The goal satisfiability metric fails to account for the characteristics of end-to-end plan generation in autoregressive language models.",
                "location": "G Additional Results: Goal Satisfiability Rate",
                "type": "Novel finding",
                "exact_quote": "The goal satisfiability metric fails to account for the characteristics of end-to-end plan generation in autoregressive language models."
            },
            "evidence": [
                {
                    "evidence_id": 13,
                    "evidence_text": "The goal satisfiability metric fails to account for the characteristics of end-to-end plan generation in autoregressive language models.",
                    "strength": "strong",
                    "limitations": "The goal satisfiability metric fail to account for the characteristics of end-to-end plan generation in autoregressive language models.",
                    "location": "Section 4.7",
                    "exact_quote": "The goal satisfiability metric fail to account for the characteristics of end-to-end plan generation in autoregressive language models."
                }
            ],
            "conclusion": {
                "claim_id": 13,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The claim is supported by the evidence that the goal satisfiability metric fails to account for the characteristics of end-to-end plan generation in autoregressive language models.",
                "confidence_level": "high"
            }
        }
    ],
    "execution_times": {
        "claims_analysis_time": "53.66 seconds",
        "evidence_analysis_time": "75.70 seconds",
        "conclusions_analysis_time": "41.88 seconds",
        "total_execution_time": "174.74 seconds"
    }
}