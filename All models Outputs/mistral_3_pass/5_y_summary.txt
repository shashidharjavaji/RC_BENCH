=== Paper Analysis Summary ===

Raw Claims:
```json
{
    "claims": [
        {
            "claim_id": 1,
            "claim_text": "The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets.",
            "location": "Abstract",
            "claim_type": "Observation",
            "exact_quote": "The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets."
        },
        {
            "claim_id": 2,
            "claim_text": "The proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols.",
            "location": "Abstract",
            "claim_type": "Observation",
            "exact_quote": "The proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols."
        },
        {
            "claim_id": 3,
            "claim_text": "It is unclear for both researchers and practitioners what models perform best.",
            "location": "Abstract",
            "claim_type": "Observation",
            "exact_quote": "It is unclear for both researchers and practitioners what models perform best."
        },
        {
            "claim_id": 4,
            "claim_text": "The field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems.",
            "location": "Abstract",
            "claim_type": "Observation",
            "exact_quote": "The field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems."
        },
        {
            "claim_id": 5,
            "claim_text": "We perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures.",
            "location": "Abstract",
            "claim_type": "Contribution",
            "exact_quote": "We perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures."
        },
        {
            "claim_id": 6,
            "claim_text": "The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works.",
            "location": "Abstract",
            "claim_type": "Contribution",
            "exact_quote": "The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works."
        },
        {
            "claim_id": 7,
            "claim_text": "The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks.",
            "location": "Abstract",
            "claim_type": "Contribution",
            "exact_quote": "The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks."
        },
        {
            "claim_id": 8,
            "claim_text": "Both models are compared to many existing architectures on a diverse set of tasks under the same training and tuning protocols.",
            "location": "Abstract",
            "claim_type": "Method",
            "exact_quote": "Both models are compared to many existing architectures on a diverse set of tasks under the same training and tuning protocols."
        },
        {
            "claim_id": 9,
            "claim_text": "We also compare the best DL models with Gradient Boosted Decision Trees and conclude that there is still no universally superior solution.",
            "location": "Abstract",
            "claim_type": "Conclusion",
            "exact_quote": "We also compare the best DL models with Gradient Boosted Decision Trees and conclude that there is still no universally superior solution."
        },
        {
            "claim_id": 10,
            "claim_text": "The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets.",
            "location": "Introduction",
            "claim_type": "Observation",
            "exact_quote": "The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets."
        },
        {
            "claim_id": 11,
            "claim_text": "The proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols.",
            "location": "Introduction",
            "claim_type": "Observation",
            "exact_quote": "The proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols."
        },
        {
            "claim_id": 12,
            "claim_text": "It is unclear for both researchers and practitioners what models perform best.",
            "location": "Introduction",
            "claim_type": "Observation",
            "exact_quote": "It is unclear for both researchers and practitioners what models perform best."
        },
        {
            "claim_id": 13,
            "claim_text": "The field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems.",
            "location": "Introduction",
            "claim_type": "Observation",
            "exact_quote": "The field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems."
        },
        {
            "claim_id": 14,
            "claim_text": "We perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures.",
            "location": "Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures."
        },
        {
            "claim_id": 15,
            "claim_text": "The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works.",
            "location": "Introduction",
            "claim_type": "Contribution",
            "exact_quote": "The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works."
        },
        {
            "claim_id": 16,
            "claim_text": "The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks.",
            "location": "Introduction",
            "claim_type": "Contribution",
            "exact_quote": "The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks."
        },
        {
            "claim_id": 17,
            "claim_text": "Both models are compared to many existing architectures on a diverse set of tasks under the same training and tuning protocols.",
            "location": "Introduction",
            "claim_type": "Method",
            "exact_quote": "Both models are compared to many existing architectures on a diverse set of tasks under the same training and tuning protocols."
        },
        {
            "claim_id": 18,
            "claim_text": "We also compare the best DL models with Gradient Boosted Decision Trees and conclude that there is still no universally superior solution.",
            "location": "Introduction",
            "claim_type": "Conclusion",
            "exact_quote": "We also compare the best DL models with Gradient Boosted Decision Trees and conclude that there is still no universally superior solution."
        },
        {
            "claim_id": 19,
            "claim_text": "We thoroughly evaluate the main models for tabular DL on a diverse set of tasks to investigate their relative performance.",
            "location": "Section 4.1",
            "claim_type": "Method",
            "exact_quote": "We thoroughly evaluate the main models for tabular DL on a diverse set of tasks to investigate their relative performance."
        },
        {
            "claim_id": 20,
            "claim_text": "We demonstrate that a simple ResNet-like architecture is an effective baseline for tabular DL, which was overlooked by existing literature.",
            "location": "Section 4.2",
            "claim_type": "Contribution",
            "exact_quote": "We demonstrate that a simple ResNet-like architecture is an effective baseline for tabular DL, which was overlooked by existing literature."
        },
        {
            "claim_id": 21,
            "claim_text": "Given its simplicity, we recommend this baseline for comparison in future tabular DL works.",
            "location": "Section 4.2",
            "claim_type": "Conclusion",
            "exact_quote": "Given its simplicity, we recommend this baseline for comparison in future tabular DL works."
        },
        {
            "claim_id": 22,
            "claim_text": "We introduce FT-Transformer — a simple adaptation of the Transformer architecture for tabular data that becomes a new powerful solution for the field.",
            "location": "Section 4.2",
            "claim_type": "Contribution",
            "exact_quote": "We introduce FT-Transformer — a simple adaptation of the Transformer architecture for tabular data that becomes a new powerful solution for the field."
        },
        {
            "claim_id": 23,
            "claim_text": "We observe that it is a more universal architecture: it performs well on a wider range of tasks than other DL models.",
            "location": "Section 4.2",
            "claim_type": "Observation",
            "exact_quote": "We observe that it is a more universal architecture: it performs well on a wider range of tasks than other DL models."
        },
        {
            "claim_id": 24,
            "claim_text": "We reveal that there is still no universally superior solution among GBDT and deep models.",
            "location": "Section 4.2",
            "claim_type": "Conclusion",
            "exact_quote": "We reveal that there is still no universally superior solution among GBDT and deep models."
        },
        {
            "claim_id": 25,
            "claim_text": "The main takeaways: MLP is still a good sanity check",
            "location": "Section 4.4",
            "claim_type": "Observation",
            "exact_quote": "The main takeaways: MLP is still a good sanity check"
        },
        {
            "claim_id": 26,
            "claim_text": "ResNet turns out to be an effective baseline that none of the competitors can consistently outperform.",
            "location": "Section 4.4",
            "claim_type": "Observation",
            "exact_quote": "ResNet turns out to be an effective baseline that none of the competitors can consistently outperform."
        },
        {
            "claim_id": 27,
            "claim_text": "FT-Transformer performs best on most tasks and becomes a new powerful solution for the field.",
            "location": "Section 4.4",
            "claim_type": "Observation",
            "exact_quote": "FT-Transformer performs best on most tasks and becomes a new powerful solution for the field."
        },
        {
            "claim_id": 28,
            "claim_text": "Tuning makes simple models such as MLP and ResNet competitive, so we recommend tuning baselines when possible.",
            "location": "Section 4.4",
            "claim_type": "Conclusion",
            "exact_quote": "Tuning makes simple models such as MLP and ResNet competitive, so we recommend tuning baselines when possible."
        },
        {
            "claim_id": 29,
            "claim_text": "Among other models, NODE (Popov et al., 2020) is the only one that demonstrates high performance on several tasks.",
            "location": "Section 4.4",
            "claim_type": "Observation",
            "exact_quote": "Among other models, NODE (Popov et al., 2020) is the only one that demonstrates high performance on several tasks."
        },
        {
            "claim_id": 30,
            "claim_text": "However, it is still inferior to ResNet on six datasets (Helena, Jannis, Higgs, ALOI, Epsilon, Covertype), while being a more complex solution.",
            "location": "Section 4.4",
            "claim_type": "Observation",
            "exact_quote": "However, it is still inferior to ResNet on six datasets (Helena, Jannis, Higgs, ALOI, Epsilon, Covertype), while being a more complex solution."
        },
        {
            "claim_id": 31,
            "claim_text": "Moreover, it is not a truly ‘single’ model; in fact, it often contains significantly more parameters than ResNet and FT-Transformer and has an ensemble-like structure.",
            "location": "Section 4.4",
            "claim_type": "Observation",
            "exact_quote": "Moreover, it is not a truly ‘single’ model; in fact, it often contains significantly more parameters than ResNet and FT-Transformer and has an ensemble-like structure."
        },
        {
            "claim_id": 32,
            "claim_text": "We illustrate that by comparing ensembles in Table 3. The results indicate that FT-Transformer and ResNet benefit more from ensembling; in this regime, FT-Transformer outperforms NODE and the gap between ResNet and NODE is significantly reduced.",
            "location": "Section 4.4",
            "claim_type": "Observation",
            "exact_quote": "We illustrate that by comparing ensembles in Table 3. The results indicate that FT-Transformer and ResNet benefit more from ensembling; in this regime, FT-Transformer outperforms NODE and the gap between ResNet and NODE is significantly reduced."
        },
        {
            "claim_id": 33,
            "claim_text": "Nevertheless, NODE remains a prominent solution among tree-based approaches.",
            "location": "Section 4.4",
            "claim_type": "Conclusion",
            "exact_quote": "Nevertheless, NODE remains a prominent solution among tree-based approaches."
        },
        {
            "claim_id": 34,
            "claim_text": "In this section, our goal is to check whether DL models are conceptually ready to outperform GBDT.",
            "location": "Section 4.5",
            "claim_type": "Method",
            "exact_quote": "In this section, our goal is to check whether DL models are conceptually ready to outperform GBDT."
        },
        {
            "claim_id": 35,
            "claim_text": "To this end, we compare the best possible metric values that one can achieve using GBDT or DL models, without taking speed and hardware requirements into account.",
            "location": "Section 4.5",
            "claim_type": "Method",
            "exact_quote": "To this end, we compare the best possible metric values that one can achieve using GBDT or DL models, without taking speed and hardware requirements into account."
        },
        {
            "claim_id": 36,
            "claim_text": "We accomplish that by comparing ensembles instead of single models since GBDT is essentially an ensembling technique and we expect that deep architectures will benefit more from ensembling.",
            "location": "Section 4.5",
            "claim_type": "Method",
            "exact_quote": "We accomplish that by comparing ensembles instead of single models since GBDT is essentially an ensembling technique and we expect that deep architectures will benefit more from ensembling."
        },
        {
            "claim_id": 37,
            "claim_text": "We report the results in Table 4.",
            "location": "Section 4.5",
            "claim_type": "Method",
            "exact_quote": "We report the results in Table 4."
        },
        {
            "claim_id": 38,
            "claim_text": "Table 4 demonstrates that the ensemble of FT-Transformers mostly outperforms the ensembles of GBDT, which is not the case for only two datasets (California Housing, Adult).",
            "location": "Section 4.5",
            "claim_type": "Observation",
            "exact_quote": "Table 4 demonstrates that the ensemble of FT-Transformers mostly outperforms the ensembles of GBDT, which is not the case for only two datasets (California Housing, Adult)."
        },
        {
            "claim_id": 39,
            "claim_text": "Interestingly, the ensemble of default FT-Transformers performs quite on par with the ensembles of tuned FT-Transformers.",
            "location": "Section 4.5",
            "claim_type": "Observation",
            "exact_quote": "Interestingly, the ensemble of default FT-Transformers performs quite on par with the ensembles of tuned FT-Transformers."
        },
        {
            "claim_id": 40,
            "claim_text": "The main takeaway: FT-Transformer allows building powerful ensembles out of the box.",
            "location": "Section 4.5",
            "claim_type": "Conclusion",
            "exact_quote": "The main takeaway: FT-Transformer allows building powerful ensembles out of the box."
        },
        {
            "claim_id": 41,
            "claim_text": "Once hyperparameters are properly tuned, GBDTs start dominating on some datasets (California Housing, Adult, Yahoo; see Table 4).",
            "location": "Section 4.5",
            "claim_type": "Observation",
            "exact_quote": "Once hyperparameters are properly tuned, GBDTs start dominating on some datasets (California Housing, Adult, Yahoo; see Table 4)."
        },
        {
            "claim_id": 42,
            "claim_text": "In those cases, the gaps are significant enough to conclude that DL models do not universally outperform GBDT.",
            "location": "Section 4.5",
            "claim_type": "Conclusion",
            "exact_quote": "In those cases, the gaps are significant enough to conclude that DL models do not universally outperform GBDT."
        },
        {
            "claim_id": 43,
            "claim_text": "Importantly, the fact that DL models outperform GBDT on most of the tasks does not mean that DL solutions are ‘better’ in any sense.",
            "location": "Section 4.5",
            "claim_type": "Conclusion",
            "exact_quote": "Importantly, the fact that DL models outperform GBDT on most of the tasks does not mean that DL solutions are ‘better’ in any sense."
        },
        {
            "claim_id": 44,
            "claim_text": "In fact, it only means that the constructed benchmark is slightly biased towards ‘DL-friendly’ problems.",
            "location": "Section 4.5",
            "claim_type": "Conclusion",
            "exact_quote": "In fact, it only means that the constructed benchmark is slightly biased towards ‘DL-friendly’ problems."
        },
        {
            "claim_id": 45,
            "claim_text": "Admittedly, GBDT remains an unsuitable solution to multiclass problems with a large number of classes.",
            "location": "Section 4.5",
            "claim_type": "Conclusion",
            "exact_quote": "Admittedly, GBDT remains an unsuitable solution to multiclass problems with a large number of classes."
        },
        {
            "claim_id": 46,
            "claim_text": "Depending on the number of classes, GBDT can demonstrate unsatisfactory performance (Helena) or even be untunable due to extremely slow training (ALOI).",
            "location": "Section 4.5",
            "claim_type": "Conclusion",
            "exact_quote": "Depending on the number of classes, GBDT can demonstrate unsatisfactory performance (Helena) or even be untunable due to extremely slow training (ALOI)."
        },
        {
            "claim_id": 47,
            "claim_text": "Table 4 tells one more important story. Namely, FT-Transformer delivers most of its advantage over the ‘conventional’ DL model in the form of ResNet exactly on those problems where GBDT is superior to ResNet (California Housing, Adult, Covertype, Yahoo, Microsoft) while performing on par with ResNet on the remaining problems.",
            "location": "Section 4.6",
            "claim_type": "Observation",
            "exact_quote": "Table 4 tells one more important story. Namely, FT-Transformer delivers most of its advantage over the ‘conventional’ DL model in the form of ResNet exactly on those problems where GBDT is superior to ResNet (California Housing, Adult, Covertype, Yahoo, Microsoft) while performing on par with ResNet on the remaining problems."
        },
        {
            "claim_id": 48,
            "claim_text": "In other words, FT-Transformer provides competitive performance on all tasks, while GBDT and ResNet perform well only on some subsets of the tasks.",
            "location": "Section 4.6",
            "claim_type": "Conclusion",
            "exact_quote": "In other words, FT-Transformer provides competitive performance on all tasks, while GBDT and ResNet perform well only on some subsets of the tasks."
        },
        {
            "claim_id": 49,
            "claim_text": "This observation may be the evidence that FT-Transformer is a more ‘universal’ model for tabular data problems.",
            "location": "Section 4.6",
            "claim_type": "Conclusion",
            "exact_quote": "This observation may be the evidence that FT-Transformer is a more ‘universal’ model for tabular data problems."
        },
        {
            "claim_id": 50,
            "claim_text": "We develop this intuition further in section 5.1.",
            "location": "Section 4.6",
            "claim_type": "Method",
            "exact_quote": "We develop this intuition further in section 5.1."
        },
        {
            "claim_id": 51,
            "claim_text": "In this section, we make the first step towards understanding the difference in behavior between FT-Transformer and ResNet, which was first observed in section 4.6.",
            "location": "Section 5.1",
            "claim_type": "Method",
            "exact_quote": "In this section, we make the first step towards understanding the difference in behavior between FT-Transformer and ResNet, which was first observed in section 4.6."
        },
        {
            "claim_id": 52,
            "claim_text": "To achieve that, we design a sequence of synthetic tasks where the difference in performance of the two models gradually changes from negligible to dramatic.",
            "location": "Section 5.1",
            "claim_type": "Method",
            "exact_quote": "To achieve that, we design a sequence of synthetic tasks where the difference in performance of the two models gradually changes from negligible to dramatic."
        },
        {
            "claim_id": 53,
            "claim_text": "Namely, we generate and fix objects {xi}i[n]=1[, perform the train-val-test] split once and interpolate between two regression targets: fGBDT, which is supposed to be easier for GBDT and fDL, which is expected to be easier for ResNet.",
            "location": "Section 5.1",
            "claim_type": "Method",
            "exact_quote": "Namely, we generate and fix objects {xi}i[n]=1[, perform the train-val-test] split once and interpolate between two regression targets: fGBDT, which is supposed to be easier for GBDT and fDL, which is expected to be easier for ResNet."
        },
        {
            "claim_id": 54,
            "claim_text": "Formally, for one object: _x ∼N_ (0, Ik), _y = α · fGBDT (x) + (1 −_ _α) · fDL(x)._",
            "location": "Section 5.1",
            "claim_type": "Method",
            "exact_quote": "Formally, for one object: _x ∼N_ (0, Ik), _y = α · fGBDT (x) + (1 −_ _α) · fDL(x)._"
        },
        {
            "claim_id": 55,
            "claim_text": "where fGBDT (x) is an average prediction of 30 randomly constructed decision trees, and _fDL(x) is an MLP with three randomly initialized hidden layers.",
            "location": "Section 5.1",
            "claim_type": "Method",
            "exact_quote": "where fGBDT (x) is an average prediction of 30 randomly constructed decision trees, and _fDL(x) is an MLP with three randomly initialized hidden layers."
        },
        {
            "claim_id": 56,
            "claim_text": "Both fGBDT and fDL are generated once, i.e. the same functions are applied to all objects (see supplementary for details).",
            "location": "Section 5.1",
            "claim_type": "Method",
            "exact_quote": "Both fGBDT and fDL are generated once, i.e. the same functions are applied to all objects (see supplementary for details)."
        },
        {
            "claim_id": 57,
            "claim_text": "The resulting targets are standardized before training.",
            "location": "Section 5.1",
            "claim_type": "Method",
            "exact_quote": "The resulting targets are standardized before training."
        },
        {
            "claim_id": 58,
            "claim_text": "The results are visualized in Figure 3.",
            "location": "Section 5.1",
            "claim_type": "Method",
            "exact_quote": "The results are visualized in Figure 3."
        },
        {
            "claim_id": 59,
            "claim_text": "ResNet and FT-Transformer perform similarly well on the ResNet-friendly tasks and outperform CatBoost on those tasks.",
            "location": "Section 5.1",
            "claim_type": "Observation",
            "exact_quote": "ResNet and FT-Transformer perform similarly well on the ResNet-friendly tasks and outperform CatBoost on those tasks."
        },
        {
            "claim_id": 60,
            "claim_text": "However, the ResNet’s relative performance drops significantly when the target becomes more GBDT friendly.",
            "location": "Section 5.1",
            "claim_type": "Observation",
            "exact_quote": "However, the ResNet’s relative performance drops significantly when the target becomes more GBDT friendly."
        },
        {
            "claim_id": 61,
            "claim_text": "By contrast, FT-Transformer yields competitive performance across the whole range of tasks.",
            "location": "Section 5.1",
            "claim_type": "Observation",
            "exact_quote": "By contrast, FT-Transformer yields competitive performance across the whole range of tasks."
        },
        {
            "claim_id": 62,
            "claim_text": "Additionally, the fact that these functions are based on decision trees correlates with the observations in section 4.6 and the results in Table 4, where FT-Transformer shows the most convincing improvements over ResNet exactly on those datasets where GBDT outperforms ResNet.",
            "location": "Section 5.1",
            "claim_type": "Observation",
            "exact_quote": "Additionally, the fact that these functions are based on decision trees correlates with the observations in section 4.6 and the results in Table 4, where FT-Transformer shows the most convincing improvements over ResNet exactly on those datasets where GBDT outperforms ResNet."
        },
        {
            "claim_id": 63,
            "claim_text": "In this section, we test some design choices of FT-Transformer.",
            "location": "Section 5.2",
            "claim_type": "Method",
            "exact_quote": "In this section, we test some design choices of FT-Transformer."
        },
        {
            "claim_id": 64,
            "claim_text": "First, we compare FT-Transformer with AutoInt (Song et al., 2019), since it is the closest competitor in its spirit.",
            "location": "Section 5.2",
            "claim_type": "Method",
            "exact_quote": "First, we compare FT-Transformer with AutoInt (Song et al., 2019), since it is the closest competitor in its spirit."
        },
        {
            "claim_id": 65,
            "claim_text": "AutoInt also converts all features to embeddings and applies self-attention on top of them.",
            "location": "Section 5.2",
            "claim_type": "Observation",
            "exact_quote": "AutoInt also converts all features to embeddings and applies self-attention on top of them."
        },
        {
            "claim_id": 66,
            "claim_text": "However, in its details, AutoInt significantly differs from FT-Transformer: its embedding layer does not include feature biases, its backbone significantly differs from the vanilla Transformer (Vaswani et al., 2017), and the inference mechanism does not use the [CLS] token.",
            "location": "Section 5.2",
            "claim_type": "Observation",
            "exact_quote": "However, in its details, AutoInt significantly differs from FT-Transformer: its embedding layer does not include feature biases, its backbone significantly differs from the vanilla Transformer (Vaswani et al., 2017), and the inference mechanism does not use the [CLS] token."
        },
        {
            "claim_id": 67,
            "claim_text": "Second, we check whether feature biases in Feature Tokenizer are essential for good performance.",
            "location": "Section 5.2",
            "claim_type": "Method",
            "exact_quote": "Second, we check whether feature biases in Feature Tokenizer are essential for good performance."
        },
        {
            "claim_id": 68,
            "claim_text": "We tune and evaluate FT-Transformer without feature biases following the same protocol as in section 4.3 and reuse the remaining numbers from Table 2.",
            "location": "Section 5.2",
            "claim_type": "Method",
            "exact_quote": "We tune and evaluate FT-Transformer without feature biases following the same protocol as in section 4.3 and reuse the remaining numbers from Table 2."
        },
        {
            "claim_id": 69,
            "claim_text": "The results averaged over 15 runs are reported in Table 5 and demonstrate both the superiority of the Transformer’s backbone to that of AutoInt and the necessity of feature biases.",
            "location": "Section 5.2",
            "claim_type": "Observation",
            "exact_quote": "The results averaged over 15 runs are reported in Table 5 and demonstrate both the superiority of the Transformer’s backbone to that of AutoInt and the necessity of feature biases."
        },
        {
            "claim_id": 70,
            "claim_text": "In this section, we evaluate attention maps as a source of information on feature importances for FT-Transformer for a given set of samples.",
            "location": "Section 5.3",
            "claim_type": "Method",
            "exact_quote": "In this section, we evaluate attention maps as a source of information on feature importances for FT-Transformer for a given set of samples."
        },
        {
            "claim_id": 71,
            "claim_text": "For the i-th sample, we calculate the average attention map _pi for the [CLS] token from Transformer’s forward pass.",
            "location": "Section 5.3",
            "claim_type": "Method",
            "exact_quote": "For the i-th sample, we calculate the average attention map _pi for the [CLS] token from Transformer’s forward pass."
        },
        {
            "claim_id": 72,
            "claim_text": "Then, the obtained individual distributions are averaged into one distribution p that represents the feature importances:",
            "location": "Section 5.3",
            "claim_type": "Method",
            "exact_quote": "Then, the obtained individual distributions are averaged into one distribution p that represents the feature importances:"
        },
        {
            "claim_id": 73,
            "claim_text": "1 _p = _nsamples_ _� 1 _p_i _pi = _nheads_ _L_ _×_ _� _pihl.",
            "location": "Section 5.3",
            "claim_type": "Method",
            "exact_quote": "1 _p = _nsamples_ _� 1 _p_i _pi = _nheads_ _L_ _×_ _� _pihl."
        },
        {
            "claim_id": 74,
            "claim_text": "where pihl is the h-th head’s attention map for the [CLS] token from the forward pass of the l-th layer on the i-th sample.",
            "location": "Section 5.3",
            "claim_type": "Method",
            "exact_quote": "where pihl is the h-th head’s attention map for the [CLS] token from the forward pass of the l-th layer on the i-th sample."
        },
        {
            "claim_id": 75,
            "claim_text": "The main advantage of the described heuristic technique is its efficiency: it requires a single forward for one sample.",
            "location": "Section 5.3",
            "claim_type": "Observation",
            "exact_quote": "The main advantage of the described heuristic technique is its efficiency: it requires a single forward for one sample."
        },
        {
            "claim_id": 76,
            "claim_text": "In order to evaluate our approach, we compare it with Integrated Gradients (IG, Sundararajan et al. (2017)), a general technique applicable to any differentiable model.",
            "location": "Section 5.3",
            "claim_type": "Method",
            "exact_quote": "In order to evaluate our approach, we compare it with Integrated Gradients (IG, Sundararajan et al. (2017)), a general technique applicable to any differentiable model."
        },
        {
            "claim_id": 77,
            "claim_text": "We use permutation test (PT, Breiman (2001)) as a reasonable interpretable method that allows us to establish a constructive metric, namely, rank correlation.",
            "location": "Section 5.3",
            "claim_type": "Method",
            "exact_quote": "We use permutation test (PT, Breiman (2001)) as a reasonable interpretable method that allows us to establish a constructive metric, namely, rank correlation."
        },
        {
            "claim_id": 78,
            "claim_text": "We run all the methods on the train set and summarize results in Table 6.",
            "location": "Section 5.3",
            "claim_type": "Method",
            "exact_quote": "We run all the methods on the train set and summarize results in Table 6."
        },
        {
            "claim_id": 79,
            "claim_text": "Interestingly, the proposed method yields reasonable feature importances and performs similarly to IG (note that this does not imply similarity to IG’s feature importances).",
            "location": "Section 5.3",
            "claim_type": "Observation",
            "exact_quote": "Interestingly, the proposed method yields reasonable feature importances and performs similarly to IG (note that this does not imply similarity to IG’s feature importances)."
        },
        {
            "claim_id": 80,
            "claim_text": "Given that IG can be orders of magnitude slower and the ‘baseline’ in the form of PT requires (nfeatures + 1) forward passes (versus one for the proposed method), we conclude that the simple averaging of attention maps can be a good choice in terms of cost-effectiveness.",
            "location": "Section 5.3",
            "claim_type": "Conclusion",
            "exact_quote": "Given that

Raw Evidence:
```json
{
    "evidence_sets": [
        {
            "claim_id": 1,
            "evidence": [
                {
                    "evidence_id": 1,
                    "evidence_text": "The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets."
                }
            ]
        },
        {
            "claim_id": 2,
            "evidence": [
                {
                    "evidence_id": 2,
                    "evidence_text": "The proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "The proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols."
                }
            ]
        },
        {
            "claim_id": 3,
            "evidence": [
                {
                    "evidence_id": 3,
                    "evidence_text": "It is unclear for both researchers and practitioners what models perform best.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "It is unclear for both researchers and practitioners what models perform best."
                }
            ]
        },
        {
            "claim_id": 4,
            "evidence": [
                {
                    "evidence_id": 4,
                    "evidence_text": "The field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "The field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems."
                }
            ]
        },
        {
            "claim_id": 5,
            "evidence": [
                {
                    "evidence_id": 5,
                    "evidence_text": "We perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures."
                }
            ]
        },
        {
            "claim_id": 6,
            "evidence": [
                {
                    "evidence_id": 6,
                    "evidence_text": "The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works."
                }
            ]
        },
        {
            "claim_id": 7,
            "evidence": [
                {
                    "evidence_id": 7,
                    "evidence_text": "The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks."
                }
            ]
        },
        {
            "claim_id": 8,
            "evidence": [
                {
                    "evidence_id": 8,
                    "evidence_text": "Both models are compared to many existing architectures on a diverse set of tasks under the same training and tuning protocols.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "Both models are compared to many existing architectures on a diverse set of tasks under the same training and tuning protocols."
                }
            ]
        },
        {
            "claim_id": 9,
            "evidence": [
                {
                    "evidence_id": 9,
                    "evidence_text": "We also compare the best DL models with Gradient Boosted Decision Trees and conclude that there is still no universally superior solution.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also compare the best DL models with Gradient Boosted Decision Trees and conclude that there is still no universally superior solution."
                }
            ]
        },
        {
            "claim_id": 10,
            "evidence": [
                {
                    "evidence_id": 10,
                    "evidence_text": "The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Introduction",
                    "exact_quote": "The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets."
                }
            ]
        },
        {
            "claim_id": 11,
            "evidence": [
                {
                    "evidence_id": 11,
                    "evidence_text": "The proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Introduction",
                    "exact_quote": "The proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols."
                }
            ]
        },
        {
            "claim_id": 12,
            "evidence": [
                {
                    "evidence_id": 12,
                    "evidence_text": "It is unclear for both researchers and practitioners what models perform best.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Introduction",
                    "exact_quote": "It is unclear for both researchers and practitioners what models perform best."
                }
            ]
        },
        {
            "claim_id": 13,
            "evidence": [
                {
                    "evidence_id": 13,
                    "evidence_text": "The field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Introduction",
                    "exact_quote": "The field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems."
                }
            ]
        },
        {
            "claim_id": 14,
            "evidence": [
                {
                    "evidence_id": 14,
                    "evidence_text": "We perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Introduction",
                    "exact_quote": "We perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures."
                }
            ]
        },
        {
            "claim_id": 15,
            "evidence": [
                {
                    "evidence_id": 15,
                    "evidence_text": "The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Introduction",
                    "exact_quote": "The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works."
                }
            ]
        },
        {
            "claim_id": 16,
            "evidence": [
                {
                    "evidence_id": 16,
                    "evidence_text": "The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Introduction",
                    "exact_quote": "The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks."
                }
            ]
        },
        {
            "claim_id": 17,
            "evidence": [
                {
                    "evidence_id": 17,
                    "evidence_text": "Both models are compared to many existing architectures on a diverse set of tasks under the same training and tuning protocols.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Introduction",
                    "exact_quote": "Both models are compared to many existing architectures on a diverse set of tasks under the same training and tuning protocols."
                }
            ]
        },
        {
            "claim_id": 18,
            "evidence": [
                {
                    "evidence_id": 18,
                    "evidence_text": "We also compare the best DL models with Gradient Boosted Decision Trees and conclude that there is still no universally superior solution.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Introduction",
                    "exact_quote": "We also compare the best DL models with Gradient Boosted Decision Trees and conclude that there is still no universally superior solution."
                }
            ]
        },
        {
            "claim_id": 19,
            "evidence": [
                {
                    "evidence_id": 19,
                    "evidence_text": "We thoroughly evaluate the main models for tabular DL on a diverse set of tasks to investigate their relative performance.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.1",
                    "exact_quote": "We thoroughly evaluate the main models for tabular DL on a diverse set of tasks to investigate their relative performance."
                }
            ]
        },
        {
            "claim_id": 20,
            "evidence": [
                {
                    "evidence_id": 20,
                    "evidence_text": "We demonstrate that a simple ResNet-like architecture is an effective baseline for tabular DL, which was overlooked by existing literature.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.2",
                    "exact_quote": "We demonstrate that a simple ResNet-like architecture is an effective baseline for tabular DL, which was overlooked by existing literature."
                }
            ]
        },
        {
            "claim_id": 21,
            "evidence": [
                {
                    "evidence_id": 21,
                    "evidence_text": "Given its simplicity, we recommend this baseline for comparison in future tabular DL works.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.2",
                    "exact_quote": "Given its simplicity, we recommend this baseline for comparison in future tabular DL works."
                }
            ]
        },
        {
            "claim_id": 22,
            "evidence": [
                {
                    "evidence_id": 22,
                    "evidence_text": "We introduce FT-Transformer — a simple adaptation of the Transformer architecture for tabular data that becomes a new powerful solution for the field.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.2",
                    "exact_quote": "We introduce FT-Transformer — a simple adaptation of the Transformer architecture for tabular data that becomes a new powerful solution for the field."
                }
            ]
        },
        {
            "claim_id": 23,
            "evidence": [
                {
                    "evidence_id": 23,
                    "evidence_text": "We observe that it is a more universal architecture: it performs well on a wider range of tasks than other DL models.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.2",
                    "exact_quote": "We observe that it is a more universal architecture: it performs well on a wider range of tasks than other DL models."
                }
            ]
        },
        {
            "claim_id": 24,
            "evidence": [
                {
                    "evidence_id": 24,
                    "evidence_text": "We reveal that there is still no universally superior solution among GBDT and deep models.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.2",
                    "exact_quote": "We reveal that there is still no universally superior solution among GBDT and deep models."
                }
            ]
        },
        {
            "claim_id": 25,
            "evidence": [
                {
                    "evidence_id": 25,
                    "evidence_text": "The main takeaways: MLP is still a good sanity check",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.4",
                    "exact_quote": "The main takeaways: MLP is still a good sanity check"
                }
            ]
        },
        {
            "claim_id": 26,
            "evidence": [
                {
                    "evidence_id": 26,
                    "evidence_text": "ResNet turns out to be an effective baseline that none of the competitors can consistently outperform.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.4",
                    "exact_quote": "ResNet turns out to be an effective baseline that none of the competitors can consistently outperform."
                }
            ]
        },
        {
            "claim_id": 27,
            "evidence": [
                {
                    "evidence_id": 27,
                    "evidence_text": "FT-Transformer performs best on most tasks and becomes a new powerful solution for the field.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.4",
                    "exact_quote": "FT-Transformer performs best on most tasks and becomes a new powerful solution for the field."
                }
            ]
        },
        {
            "claim_id": 28,
            "evidence": [
                {
                    "evidence_id": 28,
                    "evidence_text": "Tuning makes simple models such as MLP and ResNet competitive, so we recommend tuning baselines when possible.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.4",
                    "exact_quote": "Tuning makes simple models such as MLP and ResNet competitive, so we recommend tuning baselines when possible."
                }
            ]
        },
        {
            "claim_id": 29,
            "evidence": [
                {
                    "evidence_id": 29,
                    "evidence_text": "Among other models, NODE (Popov et al., 2020) is the only one that demonstrates high performance on several tasks.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.4",
                    "exact_quote": "Among other models, NODE (Popov et al., 2020) is the only one that demonstrates high performance on several tasks."
                }
            ]
        },
        {
            "claim_id": 30,
            "evidence": [
                {
                    "evidence_id": 30,
                    "evidence_text": "However, it is still inferior to ResNet on six datasets (Helena, Jannis, Higgs, ALOI, Epsilon, Covertype), while being a more complex solution.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.4",
                    "exact_quote": "However, it is still inferior to ResNet on six datasets (Helena, Jannis, Higgs, ALOI, Epsilon, Covertype), while being a more complex solution."
                }
            ]
        },
        {
            "claim_id": 31,
            "evidence": [
                {
                    "evidence_id": 31,
                    "evidence_text": "Moreover, it is not a truly ‘single’ model; in fact, it often contains significantly more parameters than ResNet and FT-Transformer and has an ensemble-like structure.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.4",
                    "exact_quote": "Moreover, it is not a truly ‘single’ model; in fact, it often contains significantly more parameters than ResNet and FT-Transformer and has an ensemble-like structure."
                }
            ]
        },
        {
            "claim_id": 32,
            "evidence": [
                {
                    "evidence_id": 32,
                    "evidence_text": "We illustrate that by comparing ensembles in Table 3. The results indicate that FT-Transformer and ResNet benefit more from ensembling; in this regime, FT-Transformer outperforms NODE and the gap between ResNet and NODE is significantly reduced.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.4",
                    "exact_quote": "We illustrate that by comparing ensembles in Table 3. The results indicate that FT-Transformer and ResNet benefit more from ensembling; in this regime, FT-Transformer outperforms NODE and the gap between ResNet and NODE is significantly reduced."
                }
            ]
        },
        {
            "claim_id": 33,
            "evidence": [
                {
                    "evidence_id": 33,
                    "evidence_text": "Nevertheless, NODE remains a prominent solution among tree-based approaches.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.4",
                    "exact_quote": "Nevertheless, NODE remains a prominent solution among tree-based approaches."
                }
            ]
        },
        {
            "claim_id": 34,
            "evidence": [
                {
                    "evidence_id": 34,
                    "evidence_text": "In this section, our goal is to check whether DL models are conceptually ready to outperform GBDT.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.5",
                    "exact_quote": "In this section, our goal is to check whether DL models are conceptually ready to outperform GBDT."
                }
            ]
        },
        {
            "claim_id": 35,
            "evidence": [
                {
                    "evidence_id": 35,
                    "evidence_text": "To this end, we compare the best possible metric values that one can achieve using GBDT or DL models, without taking speed and hardware requirements into account.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.5",
                    "exact_quote": "To this end, we compare the best possible metric values that one can achieve using GBDT or DL models, without taking speed and hardware requirements into account."
                }
            ]
        },
        {
            "claim_id": 36,
            "evidence": [
                {
                    "evidence_id": 36,
                    "evidence_text": "We accomplish that by comparing ensembles instead of single models since GBDT is essentially an ensembling technique and we expect that deep architectures will benefit more from ensembling.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.5",
                    "exact_quote": "We accomplish that by comparing ensembles instead of single models since GBDT is essentially an ensembling technique and we expect that deep architectures will benefit more from ensembling."
                }
            ]
        },
        {
            "claim_id": 37,
            "evidence": [
                {
                    "evidence_id": 37,
                    "evidence_text": "We report the results in Table 4.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.5",
                    "exact_quote": "We report the results in Table 4."
                }
            ]
        },
        {
            "claim_id": 38,
            "evidence": [
                {
                    "evidence_id": 38,
                    "evidence_text": "Table 4 demonstrates that the ensemble of FT-Transformers mostly outperforms the ensembles of GBDT, which is not the case for only two datasets (California Housing, Adult).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.5",
                    "exact_quote": "Table 4 demonstrates that the ensemble of FT-Transformers mostly outperforms the ensembles of GBDT, which is not the case for only two datasets (California Housing, Adult)."
                }
            ]
        },
        {
            "claim_id": 39,
            "evidence": [
                {
                    "evidence_id": 39,
                    "evidence_text": "Interestingly, the ensemble of default FT-Transformers performs quite on par with the ensembles of tuned FT-Transformers.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.5",
                    "exact_quote": "Interestingly, the ensemble of default FT-Transformers performs quite on par with the ensembles of tuned FT-Transformers."
                }
            ]
        },
        {
            "claim_id": 40,
            "evidence": [
                {
                    "evidence_id": 40,
                    "evidence_text": "The main takeaway: FT-Transformer allows building powerful ensembles out of the box.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.5",
                    "exact_quote": "The main takeaway: FT-Transformer allows building powerful ensembles out of the box."
                }
            ]
        },
        {
            "claim_id": 41,
            "evidence": [
                {
                    "evidence_id": 41,
                    "evidence_text": "Once hyperparameters are properly tuned, GBDTs start dominating on some datasets (California Housing, Adult, Yahoo; see Table 4).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.5",
                    "exact_quote": "Once hyperparameters are properly tuned, GBDTs start dominating on some datasets (California Housing, Adult, Yahoo; see Table 4)."
                }
            ]
        },
        {
            "claim_id": 42,
            "evidence": [
                {
                    "evidence_id": 42,
                    "evidence_text": "In those cases, the gaps are significant enough to conclude that DL models do not universally outperform GBDT.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.5",
                    "exact_quote": "In those cases, the gaps are significant enough to conclude that DL models do not universally outperform GBDT."
                }
            ]
        },
        {
            "claim_id": 43,
            "evidence": [
                {
                    "evidence_id": 43,
                    "evidence_text": "Importantly, the fact that DL models outperform GBDT on most of the tasks does not mean that DL solutions are ‘better’ in any sense.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.5",
                    "exact_quote": "Importantly, the fact that DL models outperform GBDT on most of the tasks does not mean that DL solutions are ‘better’ in any sense."
                }
            ]
        },
        {
            "claim_id": 44,
            "evidence": [
                {
                    "evidence_id": 44,
                    "evidence_text": "In fact, it only means that the constructed benchmark is slightly biased towards ‘DL-friendly’ problems.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.5",
                    "exact_quote": "In fact, it only means that the constructed benchmark is slightly biased towards ‘DL-friendly’ problems."
                }
            ]
        },
        {
            "claim_id": 45,
            "evidence": [
                {
                    "evidence_id": 45,
                    "evidence_text": "Admittedly, GBDT remains an unsuitable solution to multiclass problems with a large number of classes.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.5",
                    "exact_quote": "Admittedly, GBDT remains an unsuitable solution to multiclass problems with a large number of classes."
                }
            ]
        },
        {
            "claim_id": 46,
            "evidence": [
                {
                    "evidence_id": 46,
                    "evidence_text": "Depending on the number of classes, GBDT can demonstrate unsatisfactory performance (Helena) or even be untunable due to extremely slow training (ALOI).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.5",
                    "exact_quote": "Depending on the number of classes, GBDT can demonstrate unsatisfactory performance (Helena) or even be untunable due to extremely slow training (ALOI)."
                }
            ]
        },
        {
            "claim_id": 47,
            "evidence": [
                {
                    "evidence_id": 47,
                    "evidence_text": "Table 4 tells one more important story. Namely, FT-Transformer delivers most of its advantage over the ‘conventional’ DL model in the form of ResNet exactly on those problems where GBDT is superior to ResNet (California Housing, Adult, Covertype, Yahoo, Microsoft) while performing on par with ResNet on the remaining problems.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.6",
                    "exact_quote": "Table 4 tells one more important story. Namely, FT-Transformer delivers most of its advantage over the ‘conventional’ DL model in the form of ResNet exactly on those problems where GBDT is superior to ResNet (California Housing, Adult, Covertype, Yahoo, Microsoft) while performing on par with ResNet on the remaining problems."
                }
            ]
        },
        {
            "claim_id": 48,
            "evidence": [
                {
                    "evidence_id": 48,
                    "evidence_text": "In other words, FT-Transformer provides competitive performance on all tasks, while GBDT and ResNet perform well only on some subsets of the tasks.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.6",
                    "exact_quote": "In other words, FT-Transformer provides competitive performance on all tasks, while GBDT and ResNet perform well only on some subsets of the tasks."
                }
            ]
        },
        {
            "claim_id": 49,
            "evidence": [
                {
                    "evidence_id": 49,
                    "evidence_text": "This observation may be the evidence that FT-Transformer is a more ‘universal’ model for tabular data problems.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.6",
                    "exact_quote": "This observation may be the evidence that FT-Transformer is a more ‘universal’ model for tabular data problems."
                }
            ]
        },
        {
            "claim_id": 50,
            "evidence": [
                {
                    "evidence_id": 50,
                    "evidence_text": "We develop this intuition further in section 5.1.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 4.6",
                    "exact_quote": "We develop this intuition further in section 5.1."
                }
            ]
        },
        {
            "claim_id": 51,
            "evidence": [
                {
                    "evidence_id": 51,
                    "evidence_text": "In this section, we make the first step towards understanding the difference in behavior between FT-Transformer and ResNet, which was first observed in section 4.6.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 5.1",
                    "exact_quote": "In this section, we make the first step towards understanding the difference in behavior between FT-Transformer and ResNet, which was first observed in section 4.6."
                }
            ]
        },
        {
            "claim_id": 52,
            "evidence": [
                {
                    "evidence_id": 52,
                    "evidence_text": "To achieve that, we design a sequence of synthetic tasks where the difference in performance of the two models gradually changes from negligible to dramatic.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 5.1",
                    "exact_quote": "To achieve that, we design a sequence of synthetic tasks where the difference in performance of the two models gradually changes from negligible to dramatic."
                }
            ]
        },
        {
            "claim_id": 53,
            "evidence": [
                {
                    "evidence_id": 53,
                    "evidence_text": "Namely, we generate and fix objects {xi}i[n]=1[, perform the train-val-test] split once and interpolate between two regression targets: fGBDT, which is supposed to be easier for GBDT and fDL, which is expected to be easier for ResNet.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 5.1",
                    "exact_quote": "Namely, we generate and fix objects {xi}i[n]=1[, perform the train-val-test] split once and interpolate between two regression targets: fGBDT, which is supposed to be easier for GBDT and fDL, which is expected to be easier for ResNet."
                }
            ]
        },
        {
            "claim_id": 54,
            "evidence": [
                {
                    "evidence_id": 54,
                    "evidence_text": "Formally, for one object: _x ∼N_ (0, Ik), _y = α · fGBDT (x) + (1 −_ _α) · fDL(x)._",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 5.1",
                    "exact_quote": "Formally, for one object: _x ∼N_ (0, Ik), _y = α · fGBDT (x) + (1 −_ _α) · fDL(x)._"
                }
            ]
        },
        {
            "claim_id": 55,
            "evidence": [
                {
                    "evidence_id": 55,
                    "evidence_text": "where fGBDT (x) is an average prediction of 30 randomly constructed decision trees, and _fDL(x) is an MLP with three randomly initialized hidden layers.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 5.1",
                    "exact_quote": "where fGBDT (x) is an average prediction of 30 randomly constructed decision trees, and _fDL(x) is an MLP with three randomly initialized hidden layers."
                }
            ]
        },
        {
            "claim_id": 56,
            "evidence": [
                {
                    "evidence_id": 56,
                    "evidence_text": "Both fGBDT and fDL are generated once, i.e. the same functions are applied to all objects (see supplementary for details).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 5.1",
                    "exact_quote": "Both fGBDT and fDL are generated once, i.e. the same functions are applied to all objects (see supplementary for details)."
                }
            ]
        },
        {
            "claim_id": 57,
            "evidence": [
                {
                    "evidence_id": 57,
                    "evidence_text": "The resulting targets are standardized before training.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 5.1",
                    "exact_quote": "The resulting targets are standardized before training."
                }
            ]
        },
        {
            "claim_id": 58,
            "evidence": [
                {
                    "evidence_id": 58,
                    "evidence_text": "The results are visualized in Figure 3.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 5.1",
                    "exact_quote": "The results are visualized in Figure 3."
                }
            ]
        },
        {
            "claim_id": 59,
            "evidence": [
                {
                    "evidence_id": 59,
                    "evidence_text": "ResNet and FT-Transformer perform similarly well on the ResNet-friendly tasks and outperform CatBoost on those tasks.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 5.1",
                    "exact_quote": "ResNet and FT-Transformer perform similarly well on the ResNet-friendly tasks and outperform CatBoost on those tasks."
                }
            ]
        },
        {
            "claim_id": 60,
            "evidence": [
                {
                    "evidence_id": 60,
                    "evidence_text": "However, the ResNet’s relative performance drops significantly when the target becomes more GBDT friendly.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 5.1",
                    "exact_quote": "However, the ResNet’s relative performance drops significantly when the target becomes more GBDT friendly."
                }
            ]
        },
        {
            "claim_id": 61,
            "evidence": [
                {
                    "evidence_id": 61,
                    "evidence_text": "By contrast, FT-Transformer yields competitive performance across the whole range of tasks.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 5.1",
                    "exact_quote": "By contrast, FT-Transformer yields competitive performance across the whole range of tasks."
                }
            ]
        },
        {
            "claim_id": 62,
            "evidence": [
                {
                    "evidence_id": 62,
                    "evidence_text": "Additionally, the fact that these functions are based on decision trees correlates with the observations in section 4.6 and the results in Table 4, where FT-Transformer shows the most convincing improvements over ResNet exactly on those datasets where GBDT outperforms ResNet.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 5.1",
                    "exact_quote": "Additionally, the fact that these functions are based on decision trees correlates with the observations in section 4.6 and the results in Table 4, where FT-Transformer shows the most convincing improvements over ResNet exactly on those datasets where GBDT outperforms ResNet."
                }
            ]
        },
        {
            "

Raw Conclusions:
```json
{
    "conclusions": [
        {
            "claim_id": 1,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 2,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 3,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 4,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 5,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 6,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 7,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 8,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 9,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 10,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 11,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 12,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 13,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 14,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 15,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 16,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 17,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 18,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 19,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 20,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 21,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 22,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 23,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 24,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 25,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 26,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 27,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 28,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 29,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 30,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 31,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 32,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 33,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 34,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 35,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 36,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 37,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 38,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 39,
            "conclusion_justified": true,



Execution Times:
claims_analysis_time: 298.58 seconds
evidence_analysis_time: 354.57 seconds
conclusions_analysis_time: 97.85 seconds
total_execution_time: 757.28 seconds
