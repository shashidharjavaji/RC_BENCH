{
    "raw_claims": "```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"Deep multimodal learning has achieved great progress in recent years.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"Deep multimodal learning has achieved great progress in recent years.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"Current fusion approaches are static in nature, i.e., they process and fuse multimodal inputs with identical computation, without accounting for diverse computational demands of different multimodal data.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"Current fusion approaches are static in nature, i.e., they process and fuse multimodal inputs with identical computation, without accounting for diverse computational demands of different multimodal data.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"We propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses input data from multiple modalities.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel finding\",\n            \"exact_quote\": \"We propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses input data from multiple modalities.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Specific assertion\",\n            \"exact_quote\": \"DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Conclusion\",\n            \"exact_quote\": \"We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"Humans perceive the world in a multimodal way, through vision, hearing, touch, taste, etc.\",\n            \"location\": \"1. Introduction\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"Humans perceive the world in a multimodal way, through vision, hearing, touch, taste, etc.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"Recent years have witnessed great progress of deep learning approaches that leverage data of multiple modalities.\",\n            \"location\": \"1. Introduction\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"Recent years have witnessed great progress of deep learning approaches that leverage data of multiple modalities.\"\n        },\n        {\n            \"claim_id\": 8,\n            \"claim_text\": \"Multimodal fusion has boosted the performance of many classical problems, such as sentiment analysis, action recognition, or semantic segmentation.\",\n            \"location\": \"1. Introduction\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"Multimodal fusion has boosted the performance of many classical problems, such as sentiment analysis, action recognition, or semantic segmentation.\"\n        },\n        {\n            \"claim_id\": 9,\n            \"claim_text\": \"Despite these advances, how to best combine information characterized by multiple modalities remains a fundamental challenge in multimodal learning.\",\n            \"location\": \"1. Introduction\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"Despite these advances, how to best combine information characterized by multiple modalities remains a fundamental challenge in multimodal learning.\"\n        },\n        {\n            \"claim_id\": 10,\n            \"claim_text\": \"We propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses input data from multiple modalities.\",\n            \"location\": \"1. Introduction\",\n            \"claim_type\": \"Novel finding\",\n            \"exact_quote\": \"We propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses input data from multiple modalities.\"\n        },\n        {\n            \"claim_id\": 11,\n            \"claim_text\": \"DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.\",\n            \"location\": \"1. Introduction\",\n            \"claim_type\": \"Specific assertion\",\n            \"exact_quote\": \"DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.\"\n        },\n        {\n            \"claim_id\": 12,\n            \"claim_text\": \"We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.\",\n            \"location\": \"1. Introduction\",\n            \"claim_type\": \"Conclusion\",\n            \"exact_quote\": \"We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.\"\n        },\n        {\n            \"claim_id\": 13,\n            \"claim_text\": \"Dynamic neural networks have demonstrated a great potential in classical computer vision problems, such as image classification, object detection, or semantic segmentation.\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"Dynamic neural networks have demonstrated a great potential in classical computer vision problems, such as image classification, object detection, or semantic segmentation.\"\n        },\n        {\n            \"claim_id\": 14,\n            \"claim_text\": \"While popular deep learning approaches perform inference in a static manner, dynamic networks allow the network structure to adapt to the input characteristics during inference.\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"While popular deep learning approaches perform inference in a static manner, dynamic networks allow the network structure to adapt to the input characteristics during inference.\"\n        },\n        {\n            \"claim_id\": 15,\n            \"claim_text\": \"Dynamic network designs can be categorized into: (a) dynamic depth; (b) dynamic width; (c) dynamic routing.\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"Dynamic network designs can be categorized into: (a) dynamic depth; (b) dynamic width; (c) dynamic routing.\"\n        },\n        {\n            \"claim_id\": 16,\n            \"claim_text\": \"The idea of dynamic depth is to adjust the network depth based on each sample.\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"The idea of dynamic depth is to adjust the network depth based on each sample.\"\n        },\n        {\n            \"claim_id\": 17,\n            \"claim_text\": \"The idea of dynamic width is to adapt the network width in a sample-wise manner.\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"The idea of dynamic width is to adapt the network width in a sample-wise manner.\"\n        },\n        {\n            \"claim_id\": 18,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 19,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 20,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 21,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 22,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 23,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 24,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 25,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 26,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 27,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 28,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 29,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 30,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 31,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 32,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 33,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 34,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 35,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 36,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 37,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 38,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 39,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 40,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 41,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 42,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 43,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 44,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 45,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 46,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 47,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 48,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 49,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 50,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 51,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 52,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 53,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 54,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 55,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 56,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 57,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 58,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 59,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 60,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 61,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 62,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 63,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General statement\",\n            \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n        },\n        {\n            \"claim_id\": 64,\n            \"claim_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n            \"location\": \"2.1. Dynamic Neural Networks\",\n            \"claim_type\": \"General",
    "raw_evidence": "```json\n{\n    \"evidence_sets\": [\n        {\n            \"claim_id\": 4,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 1,\n                    \"evidence_text\": \"DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 11,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 2,\n                    \"evidence_text\": \"DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"1. Introduction\",\n                    \"exact_quote\": \"DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 12,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 3,\n                    \"evidence_text\": \"We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"1. Introduction\",\n                    \"exact_quote\": \"We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 13,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 4,\n                    \"evidence_text\": \"Dynamic neural networks have demonstrated a great potential in classical computer vision problems, such as image classification, object detection, or semantic segmentation.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"Dynamic neural networks have demonstrated a great potential in classical computer vision problems, such as image classification, object detection, or semantic segmentation.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 14,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 5,\n                    \"evidence_text\": \"While popular deep learning approaches perform inference in a static manner, dynamic networks allow the network structure to adapt to the input characteristics during inference.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"While popular deep learning approaches perform inference in a static manner, dynamic networks allow the network structure to adapt to the input characteristics during inference.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 15,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 6,\n                    \"evidence_text\": \"Dynamic network designs can be categorized into: (a) dynamic depth; (b) dynamic width; (c) dynamic routing.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"Dynamic network designs can be categorized into: (a) dynamic depth; (b) dynamic width; (c) dynamic routing.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 16,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 7,\n                    \"evidence_text\": \"The idea of dynamic depth is to adjust the network depth based on each sample.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"The idea of dynamic depth is to adjust the network depth based on each sample.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 17,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 8,\n                    \"evidence_text\": \"The idea of dynamic width is to adapt the network width in a sample-wise manner.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"The idea of dynamic width is to adapt the network width in a sample-wise manner.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 18,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 9,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 19,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 10,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 20,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 11,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 21,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 12,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 22,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 13,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 23,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 14,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 24,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 15,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 25,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 16,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 26,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 17,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 27,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 18,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 28,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 19,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 29,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 20,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 30,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 21,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 31,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 22,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 32,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 23,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 33,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 24,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 34,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 25,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 35,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 26,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 36,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 27,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 37,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 28,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 38,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 29,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 39,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 30,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 40,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 31,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 41,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 32,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 42,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 33,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 43,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 34,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 44,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 35,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 45,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 36,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 46,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 37,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 47,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 38,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 48,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 39,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 49,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 40,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 50,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 41,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 51,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 42,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 52,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 43,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 53,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 44,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 54,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 45,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 55,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 46,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 56,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 47,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 57,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 48,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 58,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 49,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"2.1. Dynamic Neural Networks\",\n                    \"exact_quote\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 59,\n            \"evidence\": [\n                {\n                    \"evidence_id\": 50,\n                    \"evidence_text\": \"To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\":",
    "raw_conclusions": "```json\n{\n    \"conclusions\": [\n        {\n            \"claim_id\": 1,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 2,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 3,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 4,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 5,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 6,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 7,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 8,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 9,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 10,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 11,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 12,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 13,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 14,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 15,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 16,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 17,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 18,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 19,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 20,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 21,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 22,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 23,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 24,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 25,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 26,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 27,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 28,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 29,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 30,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 31,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 32,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 33,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 34,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 35,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 36,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 37,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 38,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 39,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 40,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 41,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 42,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 43,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 44,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 45,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 46,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 47,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 48,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 49,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 50,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 51,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 52,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n        {\n            \"claim_id\": 53,\n            \"conclusion_justified\": true,\n            \"robustness\": \"high\",\n            \"key_limitations\": \"None\",\n            \"confidence_level\": \"high\"\n        },\n       ",
    "execution_times": {
        "claims_analysis_time": "298.21 seconds",
        "evidence_analysis_time": "347.97 seconds",
        "conclusions_analysis_time": "132.86 seconds",
        "total_execution_time": "785.86 seconds"
    }
}