=== Paper Analysis Summary ===

Raw Claims:
```json
{
    "claims": [
        {
            "claim_id": 1,
            "claim_text": "Deep multimodal learning has achieved great progress in recent years.",
            "location": "Abstract",
            "claim_type": "General statement",
            "exact_quote": "Deep multimodal learning has achieved great progress in recent years."
        },
        {
            "claim_id": 2,
            "claim_text": "Current fusion approaches are static in nature, i.e., they process and fuse multimodal inputs with identical computation, without accounting for diverse computational demands of different multimodal data.",
            "location": "Abstract",
            "claim_type": "General statement",
            "exact_quote": "Current fusion approaches are static in nature, i.e., they process and fuse multimodal inputs with identical computation, without accounting for diverse computational demands of different multimodal data."
        },
        {
            "claim_id": 3,
            "claim_text": "We propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses input data from multiple modalities.",
            "location": "Abstract",
            "claim_type": "Novel finding",
            "exact_quote": "We propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses input data from multiple modalities."
        },
        {
            "claim_id": 4,
            "claim_text": "DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.",
            "location": "Abstract",
            "claim_type": "Specific assertion",
            "exact_quote": "DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches."
        },
        {
            "claim_id": 5,
            "claim_text": "We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.",
            "location": "Abstract",
            "claim_type": "Conclusion",
            "exact_quote": "We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks."
        },
        {
            "claim_id": 6,
            "claim_text": "Humans perceive the world in a multimodal way, through vision, hearing, touch, taste, etc.",
            "location": "1. Introduction",
            "claim_type": "General statement",
            "exact_quote": "Humans perceive the world in a multimodal way, through vision, hearing, touch, taste, etc."
        },
        {
            "claim_id": 7,
            "claim_text": "Recent years have witnessed great progress of deep learning approaches that leverage data of multiple modalities.",
            "location": "1. Introduction",
            "claim_type": "General statement",
            "exact_quote": "Recent years have witnessed great progress of deep learning approaches that leverage data of multiple modalities."
        },
        {
            "claim_id": 8,
            "claim_text": "Multimodal fusion has boosted the performance of many classical problems, such as sentiment analysis, action recognition, or semantic segmentation.",
            "location": "1. Introduction",
            "claim_type": "General statement",
            "exact_quote": "Multimodal fusion has boosted the performance of many classical problems, such as sentiment analysis, action recognition, or semantic segmentation."
        },
        {
            "claim_id": 9,
            "claim_text": "Despite these advances, how to best combine information characterized by multiple modalities remains a fundamental challenge in multimodal learning.",
            "location": "1. Introduction",
            "claim_type": "General statement",
            "exact_quote": "Despite these advances, how to best combine information characterized by multiple modalities remains a fundamental challenge in multimodal learning."
        },
        {
            "claim_id": 10,
            "claim_text": "We propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses input data from multiple modalities.",
            "location": "1. Introduction",
            "claim_type": "Novel finding",
            "exact_quote": "We propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses input data from multiple modalities."
        },
        {
            "claim_id": 11,
            "claim_text": "DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.",
            "location": "1. Introduction",
            "claim_type": "Specific assertion",
            "exact_quote": "DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches."
        },
        {
            "claim_id": 12,
            "claim_text": "We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.",
            "location": "1. Introduction",
            "claim_type": "Conclusion",
            "exact_quote": "We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks."
        },
        {
            "claim_id": 13,
            "claim_text": "Dynamic neural networks have demonstrated a great potential in classical computer vision problems, such as image classification, object detection, or semantic segmentation.",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "Dynamic neural networks have demonstrated a great potential in classical computer vision problems, such as image classification, object detection, or semantic segmentation."
        },
        {
            "claim_id": 14,
            "claim_text": "While popular deep learning approaches perform inference in a static manner, dynamic networks allow the network structure to adapt to the input characteristics during inference.",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "While popular deep learning approaches perform inference in a static manner, dynamic networks allow the network structure to adapt to the input characteristics during inference."
        },
        {
            "claim_id": 15,
            "claim_text": "Dynamic network designs can be categorized into: (a) dynamic depth; (b) dynamic width; (c) dynamic routing.",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "Dynamic network designs can be categorized into: (a) dynamic depth; (b) dynamic width; (c) dynamic routing."
        },
        {
            "claim_id": 16,
            "claim_text": "The idea of dynamic depth is to adjust the network depth based on each sample.",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "The idea of dynamic depth is to adjust the network depth based on each sample."
        },
        {
            "claim_id": 17,
            "claim_text": "The idea of dynamic width is to adapt the network width in a sample-wise manner.",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "The idea of dynamic width is to adapt the network width in a sample-wise manner."
        },
        {
            "claim_id": 18,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 19,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 20,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 21,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 22,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 23,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 24,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 25,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 26,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 27,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 28,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 29,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 30,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 31,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 32,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 33,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 34,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 35,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 36,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 37,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 38,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 39,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 40,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 41,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 42,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 43,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 44,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 45,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 46,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 47,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 48,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 49,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 50,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 51,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 52,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 53,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 54,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 55,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 56,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 57,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 58,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 59,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 60,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 61,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 62,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 63,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General statement",
            "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
        },
        {
            "claim_id": 64,
            "claim_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
            "location": "2.1. Dynamic Neural Networks",
            "claim_type": "General

Raw Evidence:
```json
{
    "evidence_sets": [
        {
            "claim_id": 4,
            "evidence": [
                {
                    "evidence_id": 1,
                    "evidence_text": "DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches."
                }
            ]
        },
        {
            "claim_id": 11,
            "evidence": [
                {
                    "evidence_id": 2,
                    "evidence_text": "DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "1. Introduction",
                    "exact_quote": "DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches."
                }
            ]
        },
        {
            "claim_id": 12,
            "evidence": [
                {
                    "evidence_id": 3,
                    "evidence_text": "We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "1. Introduction",
                    "exact_quote": "We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks."
                }
            ]
        },
        {
            "claim_id": 13,
            "evidence": [
                {
                    "evidence_id": 4,
                    "evidence_text": "Dynamic neural networks have demonstrated a great potential in classical computer vision problems, such as image classification, object detection, or semantic segmentation.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "Dynamic neural networks have demonstrated a great potential in classical computer vision problems, such as image classification, object detection, or semantic segmentation."
                }
            ]
        },
        {
            "claim_id": 14,
            "evidence": [
                {
                    "evidence_id": 5,
                    "evidence_text": "While popular deep learning approaches perform inference in a static manner, dynamic networks allow the network structure to adapt to the input characteristics during inference.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "While popular deep learning approaches perform inference in a static manner, dynamic networks allow the network structure to adapt to the input characteristics during inference."
                }
            ]
        },
        {
            "claim_id": 15,
            "evidence": [
                {
                    "evidence_id": 6,
                    "evidence_text": "Dynamic network designs can be categorized into: (a) dynamic depth; (b) dynamic width; (c) dynamic routing.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "Dynamic network designs can be categorized into: (a) dynamic depth; (b) dynamic width; (c) dynamic routing."
                }
            ]
        },
        {
            "claim_id": 16,
            "evidence": [
                {
                    "evidence_id": 7,
                    "evidence_text": "The idea of dynamic depth is to adjust the network depth based on each sample.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "The idea of dynamic depth is to adjust the network depth based on each sample."
                }
            ]
        },
        {
            "claim_id": 17,
            "evidence": [
                {
                    "evidence_id": 8,
                    "evidence_text": "The idea of dynamic width is to adapt the network width in a sample-wise manner.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "The idea of dynamic width is to adapt the network width in a sample-wise manner."
                }
            ]
        },
        {
            "claim_id": 18,
            "evidence": [
                {
                    "evidence_id": 9,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 19,
            "evidence": [
                {
                    "evidence_id": 10,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 20,
            "evidence": [
                {
                    "evidence_id": 11,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 21,
            "evidence": [
                {
                    "evidence_id": 12,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 22,
            "evidence": [
                {
                    "evidence_id": 13,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 23,
            "evidence": [
                {
                    "evidence_id": 14,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 24,
            "evidence": [
                {
                    "evidence_id": 15,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 25,
            "evidence": [
                {
                    "evidence_id": 16,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 26,
            "evidence": [
                {
                    "evidence_id": 17,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 27,
            "evidence": [
                {
                    "evidence_id": 18,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 28,
            "evidence": [
                {
                    "evidence_id": 19,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 29,
            "evidence": [
                {
                    "evidence_id": 20,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 30,
            "evidence": [
                {
                    "evidence_id": 21,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 31,
            "evidence": [
                {
                    "evidence_id": 22,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 32,
            "evidence": [
                {
                    "evidence_id": 23,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 33,
            "evidence": [
                {
                    "evidence_id": 24,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 34,
            "evidence": [
                {
                    "evidence_id": 25,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 35,
            "evidence": [
                {
                    "evidence_id": 26,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 36,
            "evidence": [
                {
                    "evidence_id": 27,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 37,
            "evidence": [
                {
                    "evidence_id": 28,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 38,
            "evidence": [
                {
                    "evidence_id": 29,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 39,
            "evidence": [
                {
                    "evidence_id": 30,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 40,
            "evidence": [
                {
                    "evidence_id": 31,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 41,
            "evidence": [
                {
                    "evidence_id": 32,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 42,
            "evidence": [
                {
                    "evidence_id": 33,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 43,
            "evidence": [
                {
                    "evidence_id": 34,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 44,
            "evidence": [
                {
                    "evidence_id": 35,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 45,
            "evidence": [
                {
                    "evidence_id": 36,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 46,
            "evidence": [
                {
                    "evidence_id": 37,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 47,
            "evidence": [
                {
                    "evidence_id": 38,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 48,
            "evidence": [
                {
                    "evidence_id": 39,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 49,
            "evidence": [
                {
                    "evidence_id": 40,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 50,
            "evidence": [
                {
                    "evidence_id": 41,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 51,
            "evidence": [
                {
                    "evidence_id": 42,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 52,
            "evidence": [
                {
                    "evidence_id": 43,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 53,
            "evidence": [
                {
                    "evidence_id": 44,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 54,
            "evidence": [
                {
                    "evidence_id": 45,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 55,
            "evidence": [
                {
                    "evidence_id": 46,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 56,
            "evidence": [
                {
                    "evidence_id": 47,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 57,
            "evidence": [
                {
                    "evidence_id": 48,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 58,
            "evidence": [
                {
                    "evidence_id": 49,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "2.1. Dynamic Neural Networks",
                    "exact_quote": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs)."
                }
            ]
        },
        {
            "claim_id": 59,
            "evidence": [
                {
                    "evidence_id": 50,
                    "evidence_text": "To enable more flexibility, recent works have proposed to skip neurons in fully-connected layers, skip branches in Mixture-ofExperts (MoE), or skip channels in Convolutional Neural Networks (CNNs).",
                    "strength": "strong",
                    "limitations": "None",
                    "location":

Raw Conclusions:
```json
{
    "conclusions": [
        {
            "claim_id": 1,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 2,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 3,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 4,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 5,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 6,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 7,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 8,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 9,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 10,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 11,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 12,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 13,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 14,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 15,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 16,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 17,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 18,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 19,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 20,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 21,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 22,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 23,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 24,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 25,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 26,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 27,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 28,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 29,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 30,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 31,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 32,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 33,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 34,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 35,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 36,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 37,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 38,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 39,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 40,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 41,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 42,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 43,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 44,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 45,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 46,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 47,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 48,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 49,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 50,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 51,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 52,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
        {
            "claim_id": 53,
            "conclusion_justified": true,
            "robustness": "high",
            "key_limitations": "None",
            "confidence_level": "high"
        },
       


Execution Times:
claims_analysis_time: 298.21 seconds
evidence_analysis_time: 347.97 seconds
conclusions_analysis_time: 132.86 seconds
total_execution_time: 785.86 seconds
