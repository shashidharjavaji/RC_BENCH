=== Paper Analysis Summary ===

Raw Claims:
 For example:
        ```json
        {
            "claims": [
                {
                    "claim_id": 1,
                    "claim_text": "Example claim text",
                    "location": "Section X",
                    "claim_type": "Example type",
                    "exact_quote": "Exact quote from the paper"
                }
            ]
        }
        ```

        Analyze the research paper and extract ALL possible claims made by the authors.

        Paper text: ## DocPrompting: GENERATING CODE BY RETRIEVING THE DOCS

        **Shuyan Zhou[†], Uri Alon[†]**

        **Frank F. Xu[†], Zhiruo Wang[†], Zhengbao Jiang[†], Graham Neubig[†‡]**

        †Language Technologies Institute, Carnegie Mellon University,

        ‡Inspired Cognition

        shuyanzh,ualon,fangzhex,zhiruow,zhengbaj,gneubig @cs.cmu.edu

        _ __

        ABSTRACT

        Publicly available source-code libraries are continuously growing and changing. This makes it impossible for models of code to keep current with all available APIs by simply training these models on existing code repositories. Thus, existing models inherently cannot generalize to using unseen functions and libraries, because these would never appear in their training data. In contrast, when human programmers use functions and libraries for the first time, they frequently refer to textual resources such as code manuals and documentation, to explore and understand the available functionality. Inspired by this observation, we introduce DocPrompting: a natural-language-to-code generation approach that explicitly leverages code documentation by (1) retrieving the relevant documentation pieces given a natural language (NL) intent, and (2) generating code based on the NL intent and the retrieved documentation. DocPrompting is general: it can be applied to any programming language, and is agnostic to the underlying neural model. We demonstrate that DocPrompting consistently improves NL-to-code models: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo-1.3B by up to absolute 6.9% exact match. [1]

        1 INTRODUCTION

        We address the task of natural language to code generation (NL code): generating a code snippet, written in a general-purpose programming language such as Python or Bash, given a natural language intent. This task has seen sharply growing popularity recently due to the emergence of large language models trained on vast amounts of natural language and code (Chen et al., 2021; Xu et al., 2022; Fried et al., 2022). NL code models facilitate programming for both professional and inexperienced programmers, by allowing programmers to write code by only expressing their higher-level intent.

        Many existing code generation models either learn directly from input-output pairs provided as training data (Allamanis et al., 2015; Yin and Neubig, 2017; Iyer et al., 2018; Brockschmidt et al., 2019; Xu et al., 2020; Alon et al., 2020; Wang et al., 2021), or learn the mapping between input and output implicitly from naturally occurring corpora of intertwined natural language and code (Austin et al., 2021; Nijkamp et al., 2022). Nevertheless, all these works assume that all libraries and function calls were seen in the training data; and that at test time, the trained model will need to generate only seen libraries and function calls. However, new functions and libraries are introduced all the time, and even a seen function call can have unseen arguments. Thus, these existing models inherently cannot generalize to generate such unseen usages.

        In contrast to these existing models, human programmers frequently refer to manuals and documentation when writing code (Nykaza et al., 2002; Lethbridge et al., 2003). This allows humans to easily use functions and libraries they have never seen nor used before. Inspired by this ability, we propose DocPrompting: a code generation approach that learns to retrieve code documentation before generating the code. An overview of our approach is illustrated in Figure 1: First, a document retriever uses the NL intent n⃝ to retrieve relevant code documentation {⃝[d]1 _,⃝d2_ _, d⃝3_ } from a documentation pool _⃝[D]_. Then, a code generator uses these docs in its prompt to generate the corresponding code c⃝. The documentation pool serves as an external data store that can be updated frequently with new contents (e.g., documentation of newly released libraries), without re-training any model component. This way, DocPrompting can leverage newly added documentation, and it can generate code containing unseen and unused functions and libraries. DocPrompting is general and applicable to any programming language and underlying base architecture. To the best of our knowledge, this is the first demonstration of leveraging documentation in models of code explicitly and effectively.

        We demonstrate the effectiveness of DocPrompting on two NL code benchmarks and tasks, across two programming languages, and using several base models: GPT-Neo (Black et al., 2021), T5 (Raffel et al., 2020), CodeT5 (Wang et al., 2021), Fusion-in-Decoder (Izacard and Grave, 2021)), and Codex (Chen et al., 2021). Further, we experiment with both sparse retrievers such as BM25 (Robertson and Jones, 1976) and dense retrieval models such as SimCSE (Gao et al., 2021). Finally, we introduce two new benchmarks for retrieval-based code generation: (a) in Bash, we curate a new benchmark by crawling the tldr repository, and constructing the training/development/test splits without overlapping commands; (b) in Python, we re-split the popular CoNaLa benchmark (Yin et al., 2018) by making every test example contain at least one Python function that is not seen in the training data. Models that use DocPrompting consistently outperform their base models that generate code solely based on the NL intents. Using DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation in CoNaLa; on the new tldr dataset, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match. We release our new benchmarks, including annotation of oracle documents for each example and pools of documentation, to serve as a test-bed for future retrieval-based code generation models.

        2 CODE GENERATION BY READING THE DOCS

        Our underlying assumption is that code documentation is the most exhaustive yet succinct resource for most libraries and programming languages (Roehm et al., 2012), and that documentation allows to effectively generalize to unseen libraries and functions (Forward and Lethbridge, 2002). We follow the retrieve-then-generate paradigm (Lewis et al., 2020; Guu et al., 2020), focusing on retrieving documentation. In this section, we describe the general approach of DocPrompting; in §3 and §6.2, we elaborate and experiment with practical implementations of DocPrompting.

        Formulation Given NL intent n, our goal is to generate a corresponding code snippet c written in some programming language (PL) such as Python. We assume that a model has access to a collection of code documentation D. Each document di ∈D describes the usage of a library, a function, or an argument in that PL. The construction of is flexible: it can either be a comprehensive set of all available libraries and functions in a PL, or a customized subset for the scope of a specific project.

        2.1 BACKGROUND: RETRIEVAL-CONDITIONED GENERATION

        Although a model may use the entire collection of documents, only a few documents in are relevant for any particular intent. Further, it is usually computationally infeasible to directly condition on the entire, unbounded, collection of documents while making predictions. Thus, we first let the model select a subset of documents Dn = {d1,d2,..,dk} ⊆D that are potentially relevant given n, and refer to this subset while generating c.

        Overall, we decompose the probability of generating c into the probability of choosing a particular subset of documents P (Dn ∣D,n), and the probability of generating the code conditioned on the intent and the selected documents P (c ∣Dn,n); finally, we marginalizing over all Dn ⊆D:

        _P (c ∣D, n) = ∑Dn⊆D P (c ∣Dn, n) ⋅_ _P (Dn ∣D, n)_ (1)

        assuming that c is independent of D given Dn (that is, (c �D ∣Dn)). Since enumerating all possible subsets Dn is computationally infeasible, we follow the common practice and approximate the marginalization over Dn in Equation (1) by taking the most probable subset of retrieved documents Dˆn, and then conditioning the prediction of c on these most likely documents:

        Dˆn ∶= argmaxDn⊆DP (Dn ∣D, n) _P_ (c ∣D, n) ≈ _P_ (c ∣ D[ˆ]n, n) ⋅ _P_ (D[ˆ]n ∣D, n) (2)

        2.2 DocPrompting: GENERATING CODE BY RETRIEVING THE DOCS

        Equation 2 implies that DocPrompting relies of two main components: A retriever retrieves relevant documents D[ˆ]n given the intent n; and a generator G generates the code snippet c conditioned on the retrieved documents D[ˆ]n and the intent n, which compose a new prompt. Specifically, R computes a similarity score s (di,n) between a intent n and every document di ∈D. Thus, the subset D[ˆ]n ⊆D is the top-k documents with the highest similarity scores: D[ˆ]n = top-kdi∈D (s (di,n)).

        An overview of our approach is illustrated in Figure 1: given the intent Generate HTML with python syntax highlighting for “print(’reading docs’)”, the retriever retrieves three relevant documents: R _d1 describes the syntax highlighting library pygments, d2 describes the class PythonLexer, and d3 describes the HtmlFormatter class. Given these docs and the intent, the generator G generates the code snippet c, which uses PythonLexer and HtmlFormatter from the pygment library.

        3 PRACTICAL INSTANTIATIONS OF DocPrompting

        DocPrompting is a general approach that is not bound to any specific model choices, and it can be instantiated with any base retriever and generator. This section presents the concrete instantiations of and that we found to provide the best performance in our experiments.

        R G

        3.1 RETRIEVER INSTANTIATION

        We experiment with two main types of retrievers: sparse retrievers and dense retrievers. As our sparse retriever, we use Elasticsearch[2] with the standard BM25 (Robertson and Jones, 1976). This retriever represents documents using sparse features that rely on word frequencies, such as BM25 and TF-IDF.

        As our dense retriever, we follow prior work (Chen et al., 2020; Karpukhin et al., 2020; Gao et al., 2021): given a triplet (n,c, Dn[∗][)][, where][ D]n[∗] [are the oracle docs for][ n][, each][ d]i[+] [∈D]n[∗] [and][ n][ form a positive pair (n,d[+]i [)][, while each][ d]j[−] [∉D]n[∗] [and][ n][ form a][ negative][ pair][ (][n][i][,d]j[−][)][. We train the retriever in a contrastive fashion where the similarity score of a positive pair is maximized while that of in-batch negative pairs is minimized. For a pair (ni,d[+]i [)][, the loss function is defined as:]

        exp (sim(hn, hd+i
        log [))] (3)
        L[r] = −

        exp (sim(hn, hd+i [)) + ∑][d]j[−][∈B/D][n][∗] [exp][ (][sim][(][h][n][,][ h][d]j[−] [))]

        [2https://github.com/elastic/elasticsearch](https://github.com/elastic/elasticsearch)

        where hx is the representation of x computed by a neural encoder, and B are positive docs for other examples in the batch. We define sim(hx, **_hy) as the cosine similarity between hx and hy._**

        We use all (ni,d[+]i [)][ in the training set as our supervised training dataset. Additionally, we use all sentences in the documentation pool for weak supervision: Following Chen et al. (2020) and Gao et al. (2021), representations of the same sentence with different dropout masks are treated as a positive example. Instead of using either supervised or weakly supervised training as in Gao et al. (2021), we simply mix the two resulting supervision signals, and examples are randomly distributed into batches. This mixture of tasks not only facilitates the learning process (§6.2), but also reduces the engineering effort required to store and reload models for separate supervised and unsupervised training phases. We initialize the retriever encoder with either the best model of Gao et al. (2021) or the encoder of CodeT5-base (Wang et al., 2021). Additional training details are provided in Appendix C

        3.2 GENERATOR INSTANTIATION

        We experimented with a variety of generator models. We used GPT-Neo-125M, GPT-Neo-1.3B (Black et al., 2021) and Codex (Chen et al., 2021), where we concatenate the retrieved documents and the NL intent as a single, long, prompt. T5-base (Raffel et al., 2019) and CodeT5-base (Wang et al., 2021) have a shorter input size of 512 tokens, which is sometimes too short for the concatenation of multiple docs. Thus, for T5 and CodeT5 we apply the fusion-in-decoder approach (FiD; Izacard and Grave, 2021): we first concatenate the intent n with each retrieved di ∈ D[ˆ]n and encode each (n,di) pair independently. Then, the decoder attends to all encoded NL-document pairs. We finetune the generator to maximize the log-likelihood of the reference code c given n and D[ˆ]n.

        With Codex (Chen et al., 2021), we performed few-shot learning rather than finetuning because the model parameters are not publicly available. We constructed the prompt with three static examples, each of which is a concatenation of retrieved documentation, an NL intent and the reference code snippet. We then appended the test example and its retrieved documentation to the few-shot examples. We used the code-davinci-001 version because we suspect potential leakage of the test set into the training set of code-davinci-002. See more details in Appendix H. Training details, hyper-parameter settings and example prompts can be found in Appendices E and D.

        4 EXPERIMENTAL SETUP

        We evaluate DocPrompting on two NL code tasks: shell scripting (§4.1), in which we generate complex shell commands given an intent, and Python programming (§4.2), where we generate answers in Python for NL questions. In this section, we first introduce a newly curated benchmark tldr; we then describe our re-split of the popular CoNaLa benchmark (Yin et al., 2018). For each benchmark, we provide a global documentation pool that is shared for all examples and oracle D documents Dn[∗] [which we use to train the retriever. We release our newly curated benchmarks to serve as test-bed for future retrieval-based code generation models.

        4.1 SHELL SCRIPTING

        tldr is a community-driven project that maintains easilyreadable help pages with examples for over 2.5k Bash commands in over 25 natural languages[3]. We collected pairs of English intents and Bash command lines. The NL intents are written by human users, and the Bash commands range from popular ones like cat and tar, to uncommon commands such as toilet and faketime. Our resulting tldr benchmark contains 1,879 unique Bash commands and 9,187 NL Bash pairs. We constructed the training, development and the test set with completely disjoint commands to test the generalizability of a code generation model. The shared documentation pool is made up of the 400k paragraphs from the 1,879 Bash manuals. Each paragraph describes a single concept such as an argument flag. We further curated the oracle documents Dn[∗] [for each example using simple string matching. An example from tldr is shown in Figure 2. To the best of our knowledge, this is the first work to leverage tldr as an NL code benchmark. Detailed statistics and additional details are provided in Appendix A. In tldr, each NL intent results in a single Bash command with a combination of argument flags. We therefore first retrieve an entire Bash manual; then, we take the top manual and retrieve the top-10 paragraphs from that manual.

        Evaluation metrics We measure: (a) command name accuracy (CMD Acc) – whether the command name (e.g., cat) is an exact match; (b) exact match (EM) – exact match between the reference and the generation; (c) token-level F1; and (d) character-level BLEU (charBLEU; Lin et al., 2018; Shi et al., 2022). In all metrics, we disregard user-specific variable names in the references and the models outputs. For example, “mycli -u [user] -h [host] [database]” is evaluated as “mycli -u $1 -h $2 $3”.

        4.2 PYTHON PROGRAMMING

        CoNaLa (Yin et al., 2018) is a popular benchmark for NL Python generation. NL intents are StackOverflow questions, and code snippets are their answers. Both intents and code snippets are rewritten by human annotators. We re-split the dataset to test models’ generalization to unseen Python functions. In our re-split, we verified that every example in the development or the test set uses at least one Python function (e.g., plt.plot) that was not seen in the training data. In addition, we make sure that the examples from the same StackOverflow posts are in the same set to prevent leakage. This re-split results in 2,135/201/543 examples in the training/development/test sets, respectively.

        The CoNaLa documentation pool contains 35,763 documents, each describing a single function, D from all Python libraries available on DevDocs (https://devdocs.io). These include built-in libraries and other popular libraries such as numpy. We constructed the oracle docs Dn[∗] [for each example by matching all function names in the target code c with docs. More details in Appendix B.

        Evaluation metrics We follow Yin et al. (2018) and measure BLEU-4. Since we focus on general-ization to unseen functions, we additionally report function name recall (recall) and unseen function recall (recallunseen), which measures recall among function calls that do not appear in the training set. Finally, following Chen et al. (2021); Austin et al. (2021), we used the manually written unit tests from Wang et al. (2022) for 100 examples from CoNaLa’s test set and measure pass@k. We followed Chen et al. (2021) and performed nucleus sampling (Holtzman et al., 2019) with p 0.95. = For each k, we searched for the best temperature for each model from 0.2, 0.4, 0.6, 0.8, 1.0. On average, each example has 2.03 tests. The concatenation of multiple Python docs often exceeded the length limit of GPT-Neo, we hence experimented in this dataset with FiD, which allows longer inputs. Additional details are provided in Appendix B.

        5 RESULTS

        In all following results, all models with DocPrompting use the top-10 retrieved docs from the best retriever on that dataset (Table 4). Every baseline uses the exact same setup as its “+DocPrompting” version, except for not using the documentation.

        5.1 SHELL SCRIPTING RESULTS

        Results for tldr are shown in Table 1. DocPrompting consistently improves the base models. For example, T5+DocPrompting achieves more than twice higher accuracy in predicting the command name, more than 16 charBLEU points on the entire prediction, and almost 9% of absolute exact match gain, compared to the vanilla T5. In the few-shot learning setting with Codex, DocPrompting brings gains of 6.7 charBLEU points, and consistent improvement across all metrics over the baseline that observes only NL-code pairs in its prompt. These results show that retrieving documentation also benefits strong models such as Codex, and with only few examples in the context.

        Code generation with oracle command names In realistic settings, a human programmer may know the command name they need to use (e.g., awk), but not know the exact usage and flags. In fact, better understanding of the usage of known commands is the purpose of Unix man pages and the tldr project. We conducted an oracle experiment where we provided T5 (which was the strongest model using DocPrompting) and Codex with the oracle command name (e.g., awk). This oracle information is provided to both the baseline and the model that uses DocPrompting. The results are shown on the bottom part of Table 1. When the oracle command is given, DocPrompting further improves over the base models. For example, when providing Codex with the ground truth command name, DocPrompting improves its exact match from 22.44% to 32.43%.

        Should we retrieve documentation or examples? All existing retrieval-based models of code retrieve NL-code pairs or code snippets, rather than documentation. To simulate this scenario, we followed Parvez et al. (2021) and Pasupat et al. (2021) to retrieve NL-code pairs from the training set of tldr, and refer to this baseline as ExPrompting. We finetuned the best retriever RoBERTa and two generators, and retrieved the top-30 NL-code pairs for every example. As shown in Table 2, retrieving documentation (DocPrompting) provides much higher gains than retrieving examples (ExPrompting). Theoretically, adding examples of unseen commands can help ExPrompting generalize to them as well. However, new libraries and functions may not have available examples on the web yet, while documentation often does becomes available when the library is released.

        5.2 PYTHON PROGRAMMING RESULTS

        Table 3 shows the results on CoNaLa. CodeT5+DocPrompting yields a 1.65 BLEU improvement over the state-of-the-art baseline that was initialized with CodeT5.[4] When measuring the recall of the generated function names, the benefit of DocPrompting is especially higher for unseen functions (recallunseen). For example, DocPrompting achieves 18.30 compared to only 9.03 of the base CodeT5 in unseen functions. Additionally, DocPrompting improves in-context learning setting with Codex.

        4In a separate experiment on the original split of CoNaLa, this baseline achieved a BLEU score of 39.12, which outperforms the previous state-of-the-art (Beau and Crabb´e, 2022) by 4.92 BLEU points.

        Execution-based evaluation The results are shown in Figure 3. Using DocPrompting consistently outperforms the baseline CodeT5 for all values of pass@k. For example, DocPrompting yields 2.85% improvement on pass@1 and 4.45% improvement on pass@5, which are realistic numbers of completions that can be suggested in an IDE. When k 200, DocPrompting widens the gap to 8.38%. These results demonstrate that DocPrompting does not only improve the quality of the generated code in its surface form, but also increase its functional correctness. Additional details and results are provided in Appendix G.

        6 ANALYSIS

        6.1 WHY DOES READING THE DOCUMENTATION HELP GENERATING MORE ACCURATE CODE?

        We believe that one of the major reasons is that documentation eases the mapping between NL intents and code, since the documentation contains both NL descriptions and function signatures. We calculated the n-gram overlap between the NL intents and their corresponding code snippets (NL code), and the overlap between the NL intents with their top-10 retrieved documents and their code snippets ((NL+docs) code). As shown in Figure 4, adding documentation significantly increases the overlap across n-grams, and increases, for example, the unigram overlap from 12% to 24% in tldr. That is, one of the reasons that retrieving documentation helps generating accurate code is that documentation bridges the gap between the “intent terminology” and the “code terminology”.

        6.2 ABLATION STUDY

        We compared different configurations of the retriever, to gather more insights for effective DocPrompting. Table 4 shows a comparison between different retrievers and their setups. First, the performance of BM25 varies among datasets: In tldr, BM25 matches the recall of trained dense retrievers; however in CoNaLa, BM25 achieves only recall@10 of 9.73%, and strong dense retrievers such as the encoder of CodeT5 achieve recall@10 of 55.81. We hypothesize that this difference between datasets stems from the ways these datasets were created: tldr intents were written based on existing Bash commands and manuals; while CoNaLa examples were mined from StackOverflow posts, where users ask questions with limited or no context. Thus, NL intents in CoNaLa require a better semantic alignment with the documents, and thus benefit from dense retrievers. The gap resulting from different data curation processes was also observed by Rodriguez and Boyd-Graber (2021) in open-domain question answering (QA).

        Second, retrievers that were pretrained on the target programming language are generally stronger. For example in CoNaLa, CodeT5 which was pretrained on Python, is both a better off-the-shelf retriever and a better finetuned-retriever than RoBERTa, which was pretrained mainly on text. In contrast, tldr is based on Bash, which neither CodeT5 nor RoBERTa were explicitly pretrained on. Thus, tldr benefits mostly from BM25 and RoBERTa rather than CodeT5 as retrievers.

        Finally, training the retriever using weak supervision on the documentation pool (Section 3.1) dramatically improves the retriever. The recall of the best retrievers of each dataset without this corpus is shown in the last column of Table 4 (“Best w/o weak sup.”). On CoNaLa, removing this corpus results in severe performance degradation. One possible explanation is that this weak supervision helps the retriever perform domain adaptation more effectively.

        6.3 CASE STUDY

        We examine the models’ outputs and show two representative examples in Table 5. In the first example, Image.open was not seen in the training set, and the baseline CodeT5 incorrectly predicts os.open. In contrast, using DocPrompting allows to retrieve the docs and to correctly predict Image.open. In the second example, df.to csv was not seen in training, and the baseline CodeT5 fails to correctly predict it. In contrast, DocPrompting does predict most of the df.to csv call correctly, thanks to the retrieved docs. Nevertheless, DocPrompting generates an incorrect argument skiprows=1, instead of header=False. The reason is that along with the retrieved documentation of df.to csv, the retriever also retrieved the documentation of df.read csv, which has a skiprows argument. That is, the generator uses an argument of df.read csv with the function df.to csv. Further improving the retrievers and the generators, and post-filtering based on the validity of argument names, may mitigate such mistakes.

        7 RELATED WORK

        Code generation The most common practice in NL code generation is training a model on a dataset of NL-code pairs (Allamanis et al., 2015; Yin and Neubig, 2017; Rabinovich et al., 2017; Iyer et al., 2018). Nevertheless, all these works assume that their training corpus covers all required libraries and functions, and their models are inherently incapable of generating libraries and functions that were not seen in the training data. On the contrary, DocPrompting allows models to generate calls to unseen function, by retrieving these functions’ documentation and reading them at test time. Hayati et al. (2018); Parvez et al. (2021); Hashimoto et al. (2018) and Lu et al. (2017) learn to retrieve examples at test time; Pasupat et al. (2021) also considered settings where the test data has a distribution shift from the training data. However, when new libraries are released they often come with documentation, and thus we assume that documentation for new libraries is much more likely to be available than concrete natural language intent and code snippet pairs _n,c_ that use these libraries already. The ( ) models of Shrivastava et al. and Wu et al. (2021) retrieve code snippets from relevant files in the same project; contrarily, when predicting new libraries and functions that are external to the user’s project, documentation is the source that is the most likely to be available.

        Retrieval augmented generation The paradigm of retrieve-then-generate has gained popularity in the field of open-domain question answering (Guu et al., 2020; Lewis et al., 2020; Karpukhin et al., 2020), where the answer for an open-domain question exists in only few documents out of a much larger pool. Although DocPrompting takes a similar approach, documentation retrieval in code generation is even more valuable, since code libraries are updated constantly, and new libraries are introduced daily. Thus, DocPrompting allows updating the documentation pool frequently with new contents, without re-training any model components.

        Documentation conditioned generation The model of Zhong et al. (2019) reads documents to understand environment dynamics in a grid-world game, and Branavan et al. (2011) controls situated agents in a game (Civilization II) by reading the game’s manual. However, all their models were tailored to specific games; in contrast, DocPrompting is general and is applicable for a variety of programming languages and datasets.

        8 CONCLUSION

        We propose DocPrompting, a simple and effective approach for code generation by retrieving the relevant documentation. DocPrompting consistently improves NL code models in two tasks, in two PLs, and across multiple strong base models. DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo-1.3B by up to 6.9% exact match, and Codex by 6.78 charBLEU score.

        These results open a promising direction for NL code generation. We believe that our results can be further improved using more clever encoding of the structured nature of long documents, and using joint training of the retriever and the generator, which hopefully will avoid cascading errors. Further, we believe that the principles and the methods presented in this paper are applicable to additional code-related tasks, and other documentation-like resources such as tutorials and blog posts. To these ends, we make all our code, data, and models publicly available.

        9 ACKNOWLEDGEMENT

        We thanks the anonymous reviewers for their useful comments and suggestions. This work is supported by a gift from Amazon AI and a contract from the Air Force Research Laboratory under agreement number FA8750-19-2-0200. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the Air Force Research Laboratory or the U.S. Government.

        REFERENCES

        Miltiadis Allamanis, Daniel Tarlow, Andrew D. Gordon, and Yi Wei. Bimodal modelling of source code and natural language. In Francis R. Bach and David M. Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, volume 37 of JMLR Workshop [and Conference Proceedings, pages 2123–2132. JMLR.org, 2015. URL http://proceedings.mlr.press/v37/allamanis15.html](http://proceedings.mlr.press/v37/allamanis15.html)

        Uri Alon, Roy Sadaka, Omer Levy, and Eran Yahav. Structural language models of code. In International conference on machine learning, pages 245–256. PMLR, 2020.

        Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. ArXiv preprint, [abs/2108.07732, 2021. URL https://arxiv.org/abs/2108.07732.](https://arxiv.org/abs/2108.07732)

        Nathanael Beau and Beno¨ ˆıt Crabbe. The impact of lexical and grammatical processing on generating code´ from natural language. ArXiv preprint, abs/2202.13972, 2022. URL https://arxiv.org/abs/2202.13972](https://arxiv.org/abs/2202.13972)

        Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregres[sive Language Modeling with Mesh-Tensorflow, 2021. URL https://doi.org/10.5281/zenodo.](https://doi.org/10.5281/zenodo.5297715)

        S.R.K. Branavan, David Silver, and Regina Barzilay. Learning to win by reading manuals in a Monte-Carlo framework. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 268–277, Portland, Oregon, USA, 2011. Association for Computational Linguistics. URL https://aclanthology.org/P11-1028.](https://aclanthology.org/P11-1028)

        Marc Brockschmidt, Miltiadis Allamanis, Alexander L. Gaunt, and Oleksandr Polozov. Generative code modeling with graphs. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=Bke4KsA5FX](https://openreview.net/forum?id=Bke4KsA5FX)

        Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harri Edwards, Yura Burda, Nicholas

Raw Evidence:


Raw Conclusions:


Execution Times:
claims_analysis_time: 350.28 seconds
evidence_analysis_time: 2.04 seconds
conclusions_analysis_time: 2.03 seconds
total_execution_time: 358.79 seconds
