{
    "paper_analysis": [],
    "raw_claims": " For example:\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"The authors demonstrate that nonsense prompts composed of random tokens can also elicit the LLMs to respond with hallucinations.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Method\",\n            \"exact_quote\": \"Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be knowledgeable and able to adapt to many tasks. However, we still can not completely trust their answer, since LLMs suffer from hallucination\u2014fabricating non-existent facts to cheat users without perception. And the reasons for their existence and pervasiveness remain unclear. In this paper, we demonstrate that nonsense prompts composed of random tokens can also elicit the LLMs to respond with hallucinations.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"The authors propose an automatic triggering method called hallucination attack, which includes two modes: weak semantic and OoD attacks.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Method\",\n            \"exact_quote\": \"To achieve it, we propose an automatic triggering method called hallucination attack, which includes two modes: weak semantic and OoD attacks.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"The authors found that some non-sense Out-of-Distribution(OoD) prompts composed of random tokens can also elicit the LLMs responding hallucinations.\",\n            \"location\": \"Section 2.2\",\n            \"claim_type\": \"Finding\",\n            \"exact_quote\": \"We found that some non-sense Out-of-Distribution(OoD) prompts composed of random tokens can also elicit the LLMs responding hallucinations.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"The authors demonstrate that both mainstream open-source models failed to resist the hallucination attacks.\",\n            \"location\": \"Section 4.1\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"We surprisingly find that both mainstream open-source models failed to resist the hallucination attacks.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 8,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 9,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 10,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 11,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 12,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 13,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 14,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 15,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 16,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 17,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 18,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 19,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 20,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 21,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 22,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 23,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 24,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 25,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 26,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 27,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 28,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 29,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 30,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 31,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 32,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 33,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 34,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 35,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 36,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 37,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 38,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 39,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 40,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 41,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 42,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 43,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 44,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 45,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 46,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 47,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 48,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 49,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 50,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 51,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 52,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 53,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 54,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense from some basic aspect of LLMs to explore whether there could be other feasible approaches.\"\n        },\n        {\n            \"claim_id\": 55,\n            \"claim_text\": \"The authors propose a simple yet effective defense strategy to avoid this hazardous adversarial attack.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To avoid hazard adversarial attack in LLMs, we conduct experiments further explore defence method. LLMs are quite different from conventional deep learning models that their training cost and period are much more and longer than the conventional small models. Therefore, direct adversarial training could not be a feasible solution, although it is the most effective so far. We investigate the defense",
    "raw_evidence": "",
    "raw_conclusions": "",
    "execution_times": {
        "claims_analysis_time": "317.11 seconds",
        "evidence_analysis_time": "1.63 seconds",
        "conclusions_analysis_time": "1.64 seconds",
        "total_execution_time": "329.37 seconds"
    }
}