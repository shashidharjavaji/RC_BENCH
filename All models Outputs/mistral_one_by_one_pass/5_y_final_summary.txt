=== Paper Analysis Summary ===

Raw Claims:
 For example:
```json
{
    "claims": [
        {
            "claim_id": 1,
            "claim_text": "The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets.",
            "location": "Abstract",
            "claim_type": "Method",
            "exact_quote": "The existing literature on deep learning for tabular data proposes a wide range of novel architectures and reports competitive results on various datasets."
        },
        {
            "claim_id": 2,
            "claim_text": "The proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols.",
            "location": "Abstract",
            "claim_type": "Method",
            "exact_quote": "The proposed models are usually not properly compared to each other and existing works often use different benchmarks and experiment protocols."
        },
        {
            "claim_id": 3,
            "claim_text": "It is unclear for both researchers and practitioners what models perform best.",
            "location": "Abstract",
            "claim_type": "Method",
            "exact_quote": "It is unclear for both researchers and practitioners what models perform best."
        },
        {
            "claim_id": 4,
            "claim_text": "The field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems.",
            "location": "Abstract",
            "claim_type": "Method",
            "exact_quote": "The field still lacks effective baselines, that is, the easy-to-use models that provide competitive performance across different problems."
        },
        {
            "claim_id": 5,
            "claim_text": "In this work, we perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures.",
            "location": "Abstract",
            "claim_type": "Method",
            "exact_quote": "In this work, we perform an overview of the main families of DL architectures for tabular data and raise the bar of baselines in tabular DL by identifying two simple and powerful deep architectures."
        },
        {
            "claim_id": 6,
            "claim_text": "The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works.",
            "location": "Abstract",
            "claim_type": "Method",
            "exact_quote": "The first one is a ResNet-like architecture which turns out to be a strong baseline that is often missing in prior works."
        },
        {
            "claim_id": 7,
            "claim_text": "The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks.",
            "location": "Abstract",
            "claim_type": "Method",
            "exact_quote": "The second model is our simple adaptation of the Transformer architecture for tabular data, which outperforms other solutions on most tasks."
        },
        {
            "claim_id": 8,
            "claim_text": "Both models are compared to many existing architectures on a diverse set of tasks under the same training and tuning protocols.",
            "location": "Abstract",
            "claim_type": "Method",
            "exact_quote": "Both models are compared to many existing architectures on a diverse set of tasks under the same training and tuning protocols."
        },
        {
            "claim_id": 9,
            "claim_text": "We also compare the best DL models with Gradient Boosted Decision Trees and conclude that there is still no universally superior solution.",
            "location": "Abstract",
            "claim_type": "Method",
            "exact_quote": "We also compare the best DL models with Gradient Boosted Decision Trees and conclude that there is still no universally superior solution."
        },
        {
            "claim_id": 10,
            "claim_text": "We summarize the contributions of our paper as follows:",
            "location": "Abstract",
            "claim_type": "Method",
            "exact_quote": "We summarize the contributions of our paper as follows:"
        },
        {
            "claim_id": 11,
            "claim_text": "We thoroughly evaluate the main models for tabular DL on a diverse set of tasks to investigate their relative performance.",
            "location": "Abstract",
            "claim_type": "Method",
            "exact_quote": "We thoroughly evaluate the main models for tabular DL on a diverse set of tasks to investigate their relative performance."
        },
        {
            "claim_id": 12,
            "claim_text": "We demonstrate that a simple ResNet-like architecture is an effective baseline for tabular DL, which was overlooked by existing literature.",
            "location": "Abstract",
            "claim_type": "Method",
            "exact_quote": "We demonstrate that a simple ResNet-like architecture is an effective baseline for tabular DL, which was overlooked by existing literature."
        },
        {
            "claim_id": 13,
            "claim_text": "Given its simplicity, we recommend this baseline for comparison in future tabular DL works.",
            "location": "Abstract",
            "claim_type": "Method",
            "exact_quote": "Given its simplicity, we recommend this baseline for comparison in future tabular DL works."
        },
        {
            "claim_id": 14,
            "claim_text": "We introduce FT-Transformer — a simple adaptation of the Transformer architecture for tabular data that becomes a new powerful solution for the field.",
            "location": "Abstract",
            "claim_type": "Method",
            "exact_quote": "We introduce FT-Transformer — a simple adaptation of the Transformer architecture for tabular data that becomes a new powerful solution for the field."
        },
        {
            "claim_id": 15,
            "claim_text": "We observe that it is a more universal architecture: it performs well on a wider range of tasks than other DL models.",
            "location": "Abstract",
            "claim_type": "Method",
            "exact_quote": "We observe that it is a more universal architecture: it performs well on a wider range of tasks than other DL models."
        },
        {
            "claim_id": 16,
            "claim_text": "Finally, we compare the best DL models to GBDT and conclude that there is still no universally superior solution.",
            "location": "Abstract",
            "claim_type": "Method",
            "exact_quote": "Finally, we compare the best DL models to GBDT and conclude that there is still no universally superior solution."
        },
        {
            "claim_id": 17,
            "claim_text": "The “shallow” state-of-the-art for problems with tabular data is currently ensembles of decision trees, such as GBDT.",
            "location": "Section 2",
            "claim_type": "Method",
            "exact_quote": "The “shallow” state-of-the-art for problems with tabular data is currently ensembles of decision trees, such as GBDT."
        },
        {
            "claim_id": 18,
            "claim_text": "At the moment, there are several established GBDT libraries, such as XGBoost, LightGBM, CatBoost, which are widely used by both ML researchers and practitioners.",
            "location": "Section 2",
            "claim_type": "Method",
            "exact_quote": "At the moment, there are several established GBDT libraries, such as XGBoost, LightGBM, CatBoost, which are widely used by both ML researchers and practitioners."
        },
        {
            "claim_id": 19,
            "claim_text": "While these implementations vary in detail, on most of the tasks, their performances do not differ much.",
            "location": "Section 2",
            "claim_type": "Method",
            "exact_quote": "While these implementations vary in detail, on most of the tasks, their performances do not differ much."
        },
        {
            "claim_id": 20,
            "claim_text": "During several recent years, a large number of deep learning models for tabular data have been developed.",
            "location": "Section 2",
            "claim_type": "Method",
            "exact_quote": "During several recent years, a large number of deep learning models for tabular data have been developed."
        },
        {
            "claim_id": 21,
            "claim_text": "Most of these models can be roughly categorized into three groups, which we briefly describe below.",
            "location": "Section 2",
            "claim_type": "Method",
            "exact_quote": "Most of these models can be roughly categorized into three groups, which we briefly describe below."
        },
        {
            "claim_id": 22,
            "claim_text": "The first group of models is motivated by the strong performance of decision trees.",
            "location": "Section 2",
            "claim_type": "Method",
            "exact_quote": "The first group of models is motivated by the strong performance of decision trees."
        },
        {
            "claim_id": 23,
            "claim_text": "Since decision trees are not differentiable and do not allow gradient optimization, they cannot be used as a component for pipelines trained in the end-to-end fashion.",
            "location": "Section 2",
            "claim_type": "Method",
            "exact_quote": "Since decision trees are not differentiable and do not allow gradient optimization, they cannot be used as a component for pipelines trained in the end-to-end fashion."
        },
        {
            "claim_id": 24,
            "claim_text": "To address this issue, several works propose to “smooth” decision functions in the internal tree nodes to make the overall tree function and tree routing differentiable.",
            "location": "Section 2",
            "claim_type": "Method",
            "exact_quote": "To address this issue, several works propose to “smooth” decision functions in the internal tree nodes to make the overall tree function and tree routing differentiable."
        },
        {
            "claim_id": 25,
            "claim_text": "While the methods of this family can outperform GBDT on some tasks, in our experiments, they do not consistently outperform ResNet.",
            "location": "Section 2",
            "claim_type": "Method",
            "exact_quote": "While the methods of this family can outperform GBDT on some tasks, in our experiments, they do not consistently outperform ResNet."
        },
        {
            "claim_id": 26,
            "claim_text": "Due to the ubiquitous success of attention-based architectures for different domains, several authors propose to employ attention-like modules for tabular DL as well.",
            "location": "Section 2",
            "claim_type": "Method",
            "exact_quote": "Due to the ubiquitous success of attention-based architectures for different domains, several authors propose to employ attention-like modules for tabular DL as well."
        },
        {
            "claim_id": 27,
            "claim_text": "In our experiments, we show that the properly tuned ResNet outperforms the existing attention-based models.",
            "location": "Section 2",
            "claim_type": "Method",
            "exact_quote": "In our experiments, we show that the properly tuned ResNet outperforms the existing attention-based models."
        },
        {
            "claim_id": 28,
            "claim_text": "Nevertheless, we identify an effective way to apply the Transformer architecture to tabular data: the resulting architecture outperforms ResNet on most of the tasks.",
            "location": "Section 2",
            "claim_type": "Method",
            "exact_quote": "Nevertheless, we identify an effective way to apply the Transformer architecture to tabular data: the resulting architecture outperforms ResNet on most of the tasks."
        },
        {
            "claim_id": 29,
            "claim_text": "In the literature on recommender systems and click-through-rate prediction, several works criticize MLP since it is unsuitable for modeling multiplicative interactions between features.",
            "location": "Section 2",
            "claim_type": "Method",
            "exact_quote": "In the literature on recommender systems and click-through-rate prediction, several works criticize MLP since it is unsuitable for modeling multiplicative interactions between features."
        },
        {
            "claim_id": 30,
            "claim_text": "Inspired by this motivation, some works have proposed different ways to incorporate feature products into MLP.",
            "location": "Section 2",
            "claim_type": "Method",
            "exact_quote": "Inspired by this motivation, some works have proposed different ways to incorporate feature products into MLP."
        },
        {
            "claim_id": 31,
            "claim_text": "In our experiments, however, we do not find such methods to be superior to properly tuned baselines.",
            "location": "Section 2",
            "claim_type": "Method",
            "exact_quote": "In our experiments, however, we do not find such methods to be superior to properly tuned baselines."
        },
        {
            "claim_id": 32,
            "claim_text": "The literature also proposes some other architectural designs that cannot be explicitly assigned to any of the groups above.",
            "location": "Section 2",
            "claim_type": "Method",
            "exact_quote": "The literature also proposes some other architectural designs that cannot be explicitly assigned to any of the groups above."
        },
        {
            "claim_id": 33,
            "claim_text": "Overall, the community has developed a variety of models that are evaluated on different benchmarks and are rarely compared to each other.",
            "location": "Section 2",
            "claim_type": "Method",
            "exact_quote": "Overall, the community has developed a variety of models that are evaluated on different benchmarks and are rarely compared to each other."
        },
        {
            "claim_id": 34,
            "claim_text": "Our work aims to establish a fair comparison of them and identify the solutions that consistently provide high performance.",
            "location": "Section 2",
            "claim_type": "Method",
            "exact_quote": "Our work aims to establish a fair comparison of them and identify the solutions that consistently provide high performance."
        },
        {
            "claim_id": 35,
            "claim_text": "In this section, we describe the main deep architectures that we highlight in our work, as well as the existing solutions included in the comparison.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "In this section, we describe the main deep architectures that we highlight in our work, as well as the existing solutions included in the comparison."
        },
        {
            "claim_id": 36,
            "claim_text": "Since we argue that the field needs strong easy-to-use baselines, we try to reuse well-established DL building blocks as much as possible when designing ResNet and FT-Transformer.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "Since we argue that the field needs strong easy-to-use baselines, we try to reuse well-established DL building blocks as much as possible when designing ResNet and FT-Transformer."
        },
        {
            "claim_id": 37,
            "claim_text": "We hope this approach will result in conceptually familiar models that require less effort to achieve good performance.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "We hope this approach will result in conceptually familiar models that require less effort to achieve good performance."
        },
        {
            "claim_id": 38,
            "claim_text": "Additional discussion and technical details for all the models are provided in supplementary.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "Additional discussion and technical details for all the models are provided in supplementary."
        },
        {
            "claim_id": 39,
            "claim_text": "In this work, we consider supervised learning problems.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "In this work, we consider supervised learning problems."
        },
        {
            "claim_id": 40,
            "claim_text": "D={(xi, yi)}i[n]=1 denotes a dataset, where xi=(x[(]i[num][)], x[(]i[cat][)]) ∈ X represents numerical x[(]ij[num][)] and categorical x[(]ij[cat][)] features of an object and yi ∈ Y denotes the corresponding object label.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "D={(xi, yi)}i[n]=1 denotes a dataset, where xi=(x[(]i[num][)], x[(]i[cat][)]) ∈ X represents numerical x[(]ij[num][)] and categorical x[(]ij[cat][)] features of an object and yi ∈ Y denotes the corresponding object label."
        },
        {
            "claim_id": 41,
            "claim_text": "The total number of features is denoted as k.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "The total number of features is denoted as k."
        },
        {
            "claim_id": 42,
            "claim_text": "The dataset is split into three disjoint subsets: D = Dtrain ∪ _Dval ∪_ _Dtest, where _Dtrain is used for training, Dval is used for early stopping and hyperparameter tuning, and Dtest is used for the final evaluation.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "The dataset is split into three disjoint subsets: D = Dtrain ∪ _Dval ∪_ _Dtest, where _Dtrain is used for training, Dval is used for early stopping and hyperparameter tuning, and Dtest is used for the final evaluation."
        },
        {
            "claim_id": 43,
            "claim_text": "We consider three types of tasks: binary classification Y = {0, 1}, multiclass classification Y = {1,..., C} and regression Y = R.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "We consider three types of tasks: binary classification Y = {0, 1}, multiclass classification Y = {1,..., C} and regression Y = R."
        },
        {
            "claim_id": 44,
            "claim_text": "We formalize the “MLP” architecture in Equation 1.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "We formalize the “MLP” architecture in Equation 1."
        },
        {
            "claim_id": 45,
            "claim_text": "We are aware of one attempt to design a ResNet-like baseline where the reported results were not competitive.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "We are aware of one attempt to design a ResNet-like baseline where the reported results were not competitive."
        },
        {
            "claim_id": 46,
            "claim_text": "However, given ResNet’s success story in computer vision and its recent achievements on NLP tasks, we give it a second try and construct a simple variation of ResNet as described in Equation 2.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "However, given ResNet’s success story in computer vision and its recent achievements on NLP tasks, we give it a second try and construct a simple variation of ResNet as described in Equation 2."
        },
        {
            "claim_id": 47,
            "claim_text": "The main building block is simplified compared to the original architecture, and there is an almost clear path from the input to output which we find to be beneficial for the optimization.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "The main building block is simplified compared to the original architecture, and there is an almost clear path from the input to output which we find to be beneficial for the optimization."
        },
        {
            "claim_id": 48,
            "claim_text": "Overall, we expect this architecture to outperform MLP on tasks where deeper representations can be helpful.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "Overall, we expect this architecture to outperform MLP on tasks where deeper representations can be helpful."
        },
        {
            "claim_id": 49,
            "claim_text": "In this section, we introduce FT-Transformer — a simple adaptation of the Transformer architecture for the tabular domain.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "In this section, we introduce FT-Transformer — a simple adaptation of the Transformer architecture for the tabular domain."
        },
        {
            "claim_id": 50,
            "claim_text": "In a nutshell, our model transforms all features (categorical and numerical) to embeddings and applies a stack of Transformer layers to the embeddings.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "In a nutshell, our model transforms all features (categorical and numerical) to embeddings and applies a stack of Transformer layers to the embeddings."
        },
        {
            "claim_id": 51,
            "claim_text": "Thus, every Transformer layer operates on the feature level of one object.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "Thus, every Transformer layer operates on the feature level of one object."
        },
        {
            "claim_id": 52,
            "claim_text": "We compare FT-Transformer to conceptually similar AutoInt in section 5.2.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "We compare FT-Transformer to conceptually similar AutoInt in section 5.2."
        },
        {
            "claim_id": 53,
            "claim_text": "In this section, we list the existing models designed specifically for tabular data that we include in the comparison.",
            "location": "Section 3",
            "claim_type": "Method",
            "exact_quote": "In this section, we list the existing models designed specifically for tabular data that we include in the comparison."
        },
        {
            "claim_id": 54,
            "claim_text": "We use a diverse set of eleven public datasets.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "We use a diverse set of eleven public datasets."
        },
        {
            "claim_id": 55,
            "claim_text": "For each dataset, there is exactly one train-validation-test split, so all algorithms use the same splits.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "For each dataset, there is exactly one train-validation-test split, so all algorithms use the same splits."
        },
        {
            "claim_id": 56,
            "claim_text": "The datasets include: California Housing (CA, real estate data, Kelley Pace and Barry (1997)), Adult (AD, income estimation, Kohavi (1996)), Helena (HE, anonymized dataset, Guyon et al. (2019)), Jannis (JA, anonymized dataset, Guyon et al. (2019)), Higgs (HI, simulated physical particles, Baldi et al. (2014); we use the version with 98K samples available at the OpenML repository (Vanschoren et al., 2014)), ALOI (AL, images, Geusebroek et al. (2005)), Epsilon (EP, simulated physics experiments), Year (YE, audio features, Bertin-Mahieux et al. (2011)), Covertype (CO, forest characteristics, Blackard and Dean. (2000)), Yahoo (YA, search queries, Chapelle and Chang (2011)), Microsoft (MI, search queries, Qin and Liu (2013)).",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "The datasets include: California Housing (CA, real estate data, Kelley Pace and Barry (1997)), Adult (AD, income estimation, Kohavi (1996)), Helena (HE, anonymized dataset, Guyon et al. (2019)), Jannis (JA, anonymized dataset, Guyon et al. (2019)), Higgs (HI, simulated physical particles, Baldi et al. (2014); we use the version with 98K samples available at the OpenML repository (Vanschoren et al., 2014)), ALOI (AL, images, Geusebroek et al. (2005)), Epsilon (EP, simulated physics experiments), Year (YE, audio features, Bertin-Mahieux et al. (2011)), Covertype (CO, forest characteristics, Blackard and Dean. (2000)), Yahoo (YA, search queries, Chapelle and Chang (2011)), Microsoft (MI, search queries, Qin and Liu (2013))."
        },
        {
            "claim_id": 57,
            "claim_text": "We follow the pointwise approach to learning-to-rank and treat ranking problems (Microsoft, Yahoo) as regression problems.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "We follow the pointwise approach to learning-to-rank and treat ranking problems (Microsoft, Yahoo) as regression problems."
        },
        {
            "claim_id": 58,
            "claim_text": "The dataset properties are summarized in Table 1.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "The dataset properties are summarized in Table 1."
        },
        {
            "claim_id": 59,
            "claim_text": "Data preprocessing is known to be vital for DL models.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "Data preprocessing is known to be vital for DL models."
        },
        {
            "claim_id": 60,
            "claim_text": "For each dataset, the same preprocessing was used for all deep models for a fair comparison.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "For each dataset, the same preprocessing was used for all deep models for a fair comparison."
        },
        {
            "claim_id": 61,
            "claim_text": "By default, we used the quantile transformation from the Scikit-learn library (Pedregosa et al., 2011).",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "By default, we used the quantile transformation from the Scikit-learn library (Pedregosa et al., 2011)."
        },
        {
            "claim_id": 62,
            "claim_text": "We apply standardization (mean subtraction and scaling) to Helena and ALOI.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "We apply standardization (mean subtraction and scaling) to Helena and ALOI."
        },
        {
            "claim_id": 63,
            "claim_text": "The latter one represents image data, and standardization is a common practice in computer vision.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "The latter one represents image data, and standardization is a common practice in computer vision."
        },
        {
            "claim_id": 64,
            "claim_text": "On the Epsilon dataset, we observed preprocessing to be detrimental to deep models’ performance, so we use the raw features on this dataset.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "On the Epsilon dataset, we observed preprocessing to be detrimental to deep models’ performance, so we use the raw features on this dataset."
        },
        {
            "claim_id": 65,
            "claim_text": "We apply standardization to regression targets for all algorithms.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "We apply standardization to regression targets for all algorithms."
        },
        {
            "claim_id": 66,
            "claim_text": "For every dataset, we carefully tune each model’s hyperparameters.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "For every dataset, we carefully tune each model’s hyperparameters."
        },
        {
            "claim_id": 67,
            "claim_text": "The best hyperparameters are the ones that perform best on the validation set, so the test set is never used for tuning.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "The best hyperparameters are the ones that perform best on the validation set, so the test set is never used for tuning."
        },
        {
            "claim_id": 68,
            "claim_text": "For most algorithms, we use the Optuna library (Akiba et al., 2019) to run Bayesian optimization (the Tree-Structured Parzen Estimator algorithm), which is reported to be superior to random search (Turner et al., 2021).",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "For most algorithms, we use the Optuna library (Akiba et al., 2019) to run Bayesian optimization (the Tree-Structured Parzen Estimator algorithm), which is reported to be superior to random search (Turner et al., 2021)."
        },
        {
            "claim_id": 69,
            "claim_text": "For the rest, we iterate over predefined sets of configurations recommended by corresponding papers.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "For the rest, we iterate over predefined sets of configurations recommended by corresponding papers."
        },
        {
            "claim_id": 70,
            "claim_text": "We set the budget for Optuna-based tuning in terms of iterations and provide additional analysis on setting the budget in terms of time in supplementary.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "We set the budget for Optuna-based tuning in terms of iterations and provide additional analysis on setting the budget in terms of time in supplementary."
        },
        {
            "claim_id": 71,
            "claim_text": "For each tuned configuration, we run 15 experiments with different random seeds and report the performance on the test set.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "For each tuned configuration, we run 15 experiments with different random seeds and report the performance on the test set."
        },
        {
            "claim_id": 72,
            "claim_text": "For some algorithms, we also report the performance of default configurations without hyperparameter tuning.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "For some algorithms, we also report the performance of default configurations without hyperparameter tuning."
        },
        {
            "claim_id": 73,
            "claim_text": "For each model, on each dataset, we obtain three ensembles by splitting the 15 single models into three disjoint groups of equal size and averaging predictions of single models within each group.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "For each model, on each dataset, we obtain three ensembles by splitting the 15 single models into three disjoint groups of equal size and averaging predictions of single models within each group."
        },
        {
            "claim_id": 74,
            "claim_text": "We minimize cross-entropy for classification problems and mean squared error for regression problems.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "We minimize cross-entropy for classification problems and mean squared error for regression problems."
        },
        {
            "claim_id": 75,
            "claim_text": "For TabNet and GrowNet, we follow the original implementations and use the Adam optimizer (Kingma and Ba, 2017).",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "For TabNet and GrowNet, we follow the original implementations and use the Adam optimizer (Kingma and Ba, 2017)."
        },
        {
            "claim_id": 76,
            "claim_text": "For all other algorithms, we use the AdamW optimizer (Loshchilov and Hutter, 2019).",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "For all other algorithms, we use the AdamW optimizer (Loshchilov and Hutter, 2019)."
        },
        {
            "claim_id": 77,
            "claim_text": "We do not apply learning rate schedules.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "We do not apply learning rate schedules."
        },
        {
            "claim_id": 78,
            "claim_text": "For each dataset, we use a predefined batch size for all algorithms unless special instructions on batch sizes are given in the corresponding papers.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "For each dataset, we use a predefined batch size for all algorithms unless special instructions on batch sizes are given in the corresponding papers."
        },
        {
            "claim_id": 79,
            "claim_text": "We continue training until there are patience + 1 consecutive epochs without improvements on the validation set; we set patience = 16 for all algorithms.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "We continue training until there are patience + 1 consecutive epochs without improvements on the validation set; we set patience = 16 for all algorithms."
        },
        {
            "claim_id": 80,
            "claim_text": "For XGBoost, we use one-hot encoding.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "For XGBoost, we use one-hot encoding."
        },
        {
            "claim_id": 81,
            "claim_text": "For CatBoost, we employ the built-in support for categorical features.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "For CatBoost, we employ the built-in support for categorical features."
        },
        {
            "claim_id": 82,
            "claim_text": "For Neural Networks, we use embeddings of the same dimensionality for all categorical features.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "For Neural Networks, we use embeddings of the same dimensionality for all categorical features."
        },
        {
            "claim_id": 83,
            "claim_text": "We report the results for deep architectures in Table 2.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "We report the results for deep architectures in Table 2."
        },
        {
            "claim_id": 84,
            "claim_text": "The main takeaways:",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "The main takeaways:"
        },
        {
            "claim_id": 85,
            "claim_text": "MLP is still a good sanity check",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "MLP is still a good sanity check"
        },
        {
            "claim_id": 86,
            "claim_text": "ResNet turns out to be an effective baseline that none of the competitors can consistently outperform.",
            "location": "Section 4",
            "claim_type": "Method",
            "exact_quote": "ResNet turns out to be an effective baseline that none of the competitors can consistently outperform."
        },
        {
            "claim_id

Raw Evidence:


Raw Conclusions:


Execution Times:
claims_analysis_time: 311.30 seconds
evidence_analysis_time: 1.49 seconds
conclusions_analysis_time: 1.49 seconds
total_execution_time: 320.50 seconds
