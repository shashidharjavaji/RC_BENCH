Claim 1:
Type: contribution
Statement: Fine-tuning LLMs on datasets containing problem contexts and reference plans does not lead to robust planning skills.
Location: Introduction
Exact Quote: We challenge the claim that fine-tuning LLMs simply on datasets containing problem contexts and reference plans acquire robust planning skills, by demonstrating their failure on OOD test sets.

Evidence:
- Evidence Text: The model achieved high performance across all domains in in-distribution tests but struggled to generalize to OOD cases.
  Strength: strong
  Location: 4.1 LLMs Learn to Plan in Natural Language, but Struggle in OOD Scenarios
  Limitations: The model's performance drop in OOD scenarios suggests that the planning capabilities acquired through end-to-end fine-tuning are not robust.
  Exact Quote: The model achieved high performance across all domains in in-distribution tests but struggled to generalize to OOD cases.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The model's performance drop in OOD scenarios supports the claim that fine-tuning LLMs on datasets containing problem contexts and reference plans does not lead to robust planning skills.
Key Limitations: The model's performance drop in OOD scenarios suggests that the planning capabilities acquired through end-to-end fine-tuning are not robust.

--------------------------------------------------

Claim 2:
Type: contribution
Statement: Strategies like CoT lead to incremental improvements in plan quality by enhancing plan executability, even if they do not directly increase validity rates.
Location: Introduction
Exact Quote: We show that strategies like CoT lead to incremental improvements in plan quality by enhancing plan executability, even if they do not directly increase validity rates.

Evidence:
- Evidence Text: The model's executability rate improved significantly in the 'long' test set when using CoT.
  Strength: strong
  Location: 4.3 Goal CoT: The Complexity Paradox and Overfitting Issue
  Limitations: The model's executability rate did not improve in the 'unseen' test set.
  Exact Quote: The model's executability rate improved significantly in the 'long' test set when using CoT.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The model's executability rate improvement in the 'long' test set supports the claim that strategies like CoT lead to incremental improvements in plan quality by enhancing plan executability.
Key Limitations: The model's executability rate did not improve in the 'unseen' test set.

--------------------------------------------------

Claim 3:
Type: contribution
Statement: Reinforcement Learning (RL) with the proposed 'Longest Contiguous Common Subsequence' (LCCS) reward emerges as the most effective strategy.
Location: Introduction
Exact Quote: We show that RL with our proposed ‘LCCS’ reward emerges as the most effective strategy.

Evidence:
- Evidence Text: RL improved the validity rate on the 'long' test set from 34.8% to 41.5% and the executability rate from 42.3% to 53.6%.
  Strength: strong
  Location: 4.7 RL Enhances Model Performance
  Limitations: The model's performance improvement in the 'long' test set suggests that RL is effective but may not generalize to other OOD scenarios.
  Exact Quote: RL improved the validity rate on the 'long' test set from 34.8% to 41.5% and the executability rate from 42.3% to 53.6%.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The model's performance improvement in the 'long' test set supports the claim that RL with the proposed 'LCCS' reward emerges as the most effective strategy.
Key Limitations: The model's performance improvement in the 'long' test set suggests that RL is effective but may not generalize to other OOD scenarios.

--------------------------------------------------

