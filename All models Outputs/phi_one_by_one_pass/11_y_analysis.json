{
    "paper_analysis": [],
    "raw_claims": "\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"The proposed MGN significantly reduces the number of false positives for audio and visual events.\",\n            \"location\": \"Section 4.3\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.\",\n            \"location\": \"Section 1\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB).\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\",\n            \"location\": \"Section 3.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"The proposed MGN introduces a modality-aware cross-modal grouping module to match the video-level label.\",\n            \"location\": \"Section 3.3\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We introduce a modality-aware cross-modal grouping module to match the video-level target, although the given target does not indicate modalities.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"The proposed MGN is expected to parse a video into events with different categories and modalities.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Conclusion\",\n            \"exact_quote\": \"The potential future work is to add more grouping stages with learned class-tokens as supervision for each one.\"\n        }\n    ]\n}\n```\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"The proposed MGN significantly reduces the number of false positives for audio and visual events.\",\n            \"location\": \"Section 4.3\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.\",\n            \"location\": \"Section 1\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB).\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\",\n            \"location\": \"Section 3.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"The proposed MGN introduces a modality-aware cross-modal grouping module to match the video-level label.\",\n            \"location\": \"Section 3.3\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We introduce a modality-aware cross-modal grouping module to match the video-level target, although the given target does not indicate modalities.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"The proposed MGN is expected to parse a video into events with different categories and modalities.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Conclusion\",\n            \"exact_quote\": \"The potential future work is to add more grouping stages with learned class-tokens as supervision for each one.\"\n        }\n    ]\n}\n```\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"The proposed MGN significantly reduces the number of false positives for audio and visual events.\",\n            \"location\": \"Section 4.3\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.\",\n            \"location\": \"Section 1\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB).\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\",\n            \"location\": \"Section 3.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"The proposed MGN introduces a modality-aware cross-modal grouping module to match the video-level label.\",\n            \"location\": \"Section 3.3\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We introduce a modality-aware cross-modal grouping module to match the video-level target, although the given target does not indicate modalities.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"The proposed MGN is expected to parse a video into events with different categories and modalities.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Conclusion\",\n            \"exact_quote\": \"The potential future work is to add more grouping stages with learned class-tokens as supervision for each one.\"\n        }\n    ]\n}\n```\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"The proposed MGN significantly reduces the number of false positives for audio and visual events.\",\n            \"location\": \"Section 4.3\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.\",\n            \"location\": \"Section 1\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB).\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\",\n            \"location\": \"Section 3.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"The proposed MGN introduces a modality-aware cross-modal grouping module to match the video-level label.\",\n            \"location\": \"Section 3.3\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We introduce a modality-aware cross-modal grouping module to match the video-level target, although the given target does not indicate modalities.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"The proposed MGN is expected to parse a video into events with different categories and modalities.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Conclusion\",\n            \"exact_quote\": \"The potential future work is to add more grouping stages with learned class-tokens as supervision for each one.\"\n        }\n    ]\n}\n```\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"The proposed MGN significantly reduces the number of false positives for audio and visual events.\",\n            \"location\": \"Section 4.3\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.\",\n            \"location\": \"Section 1\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB).\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\",\n            \"location\": \"Section 3.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"The proposed MGN introduces a modality-aware cross-modal grouping module to match the video-level label.\",\n            \"location\": \"Section 3.3\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We introduce a modality-aware cross-modal grouping module to match the video-level target, although the given target does not indicate modalities.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"The proposed MGN is expected to parse a video into events with different categories and modalities.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Conclusion\",\n            \"exact_quote\": \"The potential future work is to add more grouping stages with learned class-tokens as supervision for each one.\"\n        }\n    ]\n}\n```\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"The proposed MGN significantly reduces the number of false positives for audio and visual events.\",\n            \"location\": \"Section 4.3\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.\",\n            \"location\": \"Section 1\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB).\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\",\n            \"location\": \"Section 3.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"The proposed MGN introduces a modality-aware cross-modal grouping module to match the video-level label.\",\n            \"location\": \"Section 3.3\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We introduce a modality-aware cross-modal grouping module to match the video-level target, although the given target does not indicate modalities.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"The proposed MGN is expected to parse a video into events with different categories and modalities.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Conclusion\",\n            \"exact_quote\": \"The potential future work is to add more grouping stages with learned class-tokens as supervision for each one.\"\n        }\n    ]\n}\n```\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"The proposed MGN significantly reduces the number of false positives for audio and visual events.\",\n            \"location\": \"Section 4.3\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.\",\n            \"location\": \"Section 1\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB).\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\",\n            \"location\": \"Section 3.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"The proposed MGN introduces a modality-aware cross-modal grouping module to match the video-level label.\",\n            \"location\": \"Section 3.3\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We introduce a modality-aware cross-modal grouping module to match the video-level target, although the given target does not indicate modalities.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"The proposed MGN is expected to parse a video into events with different categories and modalities.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Conclusion\",\n            \"exact_quote\": \"The potential future work is to add more grouping stages with learned class-tokens as supervision for each one.\"\n        }\n    ]\n}\n```\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"The proposed MGN significantly reduces the number of false positives for audio and visual events.\",\n            \"location\": \"Section 4.3\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.\",\n            \"location\": \"Section 1\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB).\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\",\n            \"location\": \"Section 3.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"The proposed MGN introduces a modality-aware cross-modal grouping module to match the video-level label.\",\n            \"location\": \"Section 3.3\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We introduce a modality-aware cross-modal grouping module to match the video-level target, although the given target does not indicate modalities.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"The proposed MGN is expected to parse a video into events with different categories and modalities.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Conclusion\",\n            \"exact_quote\": \"The potential future work is to add more grouping stages with learned class-tokens as supervision for each one.\"\n        }\n    ]\n}\n```\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"The proposed MGN significantly reduces the number of false positives for audio and visual events.\",\n            \"location\": \"Section 4.3\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.\",\n            \"location\": \"Section 1\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB).\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\",\n            \"location\": \"Section 3.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"The proposed MGN introduces a modality-aware cross-modal grouping module to match the video-level label.\",\n            \"location\": \"Section 3.3\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We introduce a modality-aware cross-modal grouping module to match the video-level target, although the given target does not indicate modalities.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"The proposed MGN is expected to parse a video into events with different categories and modalities.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Conclusion\",\n            \"exact_quote\": \"The potential future work is to add more grouping stages with learned class-tokens as supervision for each one.\"\n        }\n    ]\n}\n```\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"The proposed MGN significantly reduces the number of false positives for audio and visual events.\",\n            \"location\": \"Section 4.3\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"Section 4.2\",\n            \"claim_type\": \"Results\",\n            \"exact_quote\": \"Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.\",\n            \"location\": \"Section 1\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB).\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\",\n            \"location\": \"Section 3.2\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"The proposed MGN introduces a mod",
    "raw_evidence": "\n```",
    "raw_conclusions": "",
    "execution_times": {
        "claims_analysis_time": "704.84 seconds",
        "evidence_analysis_time": "2.11 seconds",
        "conclusions_analysis_time": "765.75 seconds",
        "total_execution_time": "1482.45 seconds"
    }
}