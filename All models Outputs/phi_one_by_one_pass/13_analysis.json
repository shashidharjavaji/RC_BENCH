{
    "paper_analysis": [],
    "raw_claims": "\n\n        ```json\n        {\n            \"claims\": [\n                {\n                    \"claim_id\": 1,\n                    \"claim_text\": \"The dense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy.\",\n                    \"location\": \"Abstract\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"our dense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy\"\n                },\n                {\n                    \"claim_id\": 2,\n                    \"claim_text\": \"Dense Passage Retriever (DPR) outperforms BM25 by a large margin (65.2% vs. 42.9% in Top-5 accuracy).\",\n                    \"location\": \"Section 3.2\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"65.2% vs. 42.9% in Top-5 accuracy\"\n                },\n                {\n                    \"claim_id\": 3,\n                    \"claim_text\": \"DPR results in a substantial improvement on the end-to-end QA accuracy compared to ORQA (41.5% vs. 33.3%) in the open Natural Questions setting.\",\n                    \"location\": \"Section 3.2\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"41.5% vs. 33.3%\"\n                },\n                {\n                    \"claim_id\": 4,\n                    \"claim_text\": \"DPR trained using only question-passage pairs is sufficient to greatly outperform BM25.\",\n                    \"location\": \"Section 3.2\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"our dense retriever outperforms BM25 by a large margin (65.2% vs. 42.9% in Top-5 accuracy)\"\n                },\n                {\n                    \"claim_id\": 5,\n                    \"claim_text\": \"Additional pretraining may not be needed for dense retrieval.\",\n                    \"location\": \"Section 3.2\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"our empirical results also suggest that additional pretraining may not be needed\"\n                },\n                {\n                    \"claim_id\": 6,\n                    \"claim_text\": \"Higher retrieval precision translates to a higher end-to-end QA accuracy.\",\n                    \"location\": \"Section 3.2\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"a higher retrieval precision indeed translates to a higher end-to-end QA accuracy\"\n                },\n                {\n                    \"claim_id\": 7,\n                    \"claim_text\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets.\",\n                    \"location\": \"Section 6\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets\"\n                },\n                {\n                    \"claim_id\": 8,\n                    \"claim_text\": \"DPR trained using multiple datasets performs comparably to DPR trained using individual datasets.\",\n                    \"location\": \"Section 6\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"models trained using multiple datasets (Multi) perform comparably to those trained using the individual training set (Single)\"\n                },\n                {\n                    \"claim_id\": 9,\n                    \"claim_text\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.\",\n                    \"location\": \"Section 6\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"achieving the new state of the art\"\n                },\n                {\n                    \"claim_id\": 10,\n                    \"claim_text\": \"DPR-based models outperform ORQA on NQ and TriviaQA.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR manages to outperform them on both NQ and TriviaQA\"\n                },\n                {\n                    \"claim_id\": 11,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"1% to 12% absolute differences in exact match accuracy\"\n                },\n                {\n                    \"claim_id\": 12,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets\"\n                },\n                {\n                    \"claim_id\": 13,\n                    \"claim_text\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"achieving the new state of the art\"\n                },\n                {\n                    \"claim_id\": 14,\n                    \"claim_text\": \"DPR-based models outperform ORQA on NQ and TriviaQA.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR manages to outperform them on both NQ and TriviaQA\"\n                },\n                {\n                    \"claim_id\": 15,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"1% to 12% absolute differences in exact match accuracy\"\n                },\n                {\n                    \"claim_id\": 16,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets\"\n                },\n                {\n                    \"claim_id\": 17,\n                    \"claim_text\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"achieving the new state of the art\"\n                },\n                {\n                    \"claim_id\": 18,\n                    \"claim_text\": \"DPR-based models outperform ORQA on NQ and TriviaQA.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR manages to outperform them on both NQ and TriviaQA\"\n                },\n                {\n                    \"claim_id\": 19,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"1% to 12% absolute differences in exact match accuracy\"\n                },\n                {\n                    \"claim_id\": 20,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets\"\n                },\n                {\n                    \"claim_id\": 21,\n                    \"claim_text\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"achieving the new state of the art\"\n                },\n                {\n                    \"claim_id\": 22,\n                    \"claim_text\": \"DPR-based models outperform ORQA on NQ and TriviaQA.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR manages to outperform them on both NQ and TriviaQA\"\n                },\n                {\n                    \"claim_id\": 23,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"1% to 12% absolute differences in exact match accuracy\"\n                },\n                {\n                    \"claim_id\": 24,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets\"\n                },\n                {\n                    \"claim_id\": 25,\n                    \"claim_text\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"achieving the new state of the art\"\n                },\n                {\n                    \"claim_id\": 26,\n                    \"claim_text\": \"DPR-based models outperform ORQA on NQ and TriviaQA.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR manages to outperform them on both NQ and TriviaQA\"\n                },\n                {\n                    \"claim_id\": 27,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"1% to 12% absolute differences in exact match accuracy\"\n                },\n                {\n                    \"claim_id\": 28,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets\"\n                },\n                {\n                    \"claim_id\": 29,\n                    \"claim_text\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"achieving the new state of the art\"\n                },\n                {\n                    \"claim_id\": 30,\n                    \"claim_text\": \"DPR-based models outperform ORQA on NQ and TriviaQA.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR manages to outperform them on both NQ and TriviaQA\"\n                },\n                {\n                    \"claim_id\": 31,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"1% to 12% absolute differences in exact match accuracy\"\n                },\n                {\n                    \"claim_id\": 32,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets\"\n                },\n                {\n                    \"claim_id\": 33,\n                    \"claim_text\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"achieving the new state of the art\"\n                },\n                {\n                    \"claim_id\": 34,\n                    \"claim_text\": \"DPR-based models outperform ORQA on NQ and TriviaQA.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR manages to outperform them on both NQ and TriviaQA\"\n                },\n                {\n                    \"claim_id\": 35,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"1% to 12% absolute differences in exact match accuracy\"\n                },\n                {\n                    \"claim_id\": 36,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets\"\n                },\n                {\n                    \"claim_id\": 37,\n                    \"claim_text\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"achieving the new state of the art\"\n                },\n                {\n                    \"claim_id\": 38,\n                    \"claim_text\": \"DPR-based models outperform ORQA on NQ and TriviaQA.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR manages to outperform them on both NQ and TriviaQA\"\n                },\n                {\n                    \"claim_id\": 39,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"1% to 12% absolute differences in exact match accuracy\"\n                },\n                {\n                    \"claim_id\": 40,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets\"\n                },\n                {\n                    \"claim_id\": 41,\n                    \"claim_text\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"achieving the new state of the art\"\n                },\n                {\n                    \"claim_id\": 42,\n                    \"claim_text\": \"DPR-based models outperform ORQA on NQ and TriviaQA.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR manages to outperform them on both NQ and TriviaQA\"\n                },\n                {\n                    \"claim_id\": 43,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"1% to 12% absolute differences in exact match accuracy\"\n                },\n                {\n                    \"claim_id\": 44,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets\"\n                },\n                {\n                    \"claim_id\": 45,\n                    \"claim_text\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"achieving the new state of the art\"\n                },\n                {\n                    \"claim_id\": 46,\n                    \"claim_text\": \"DPR-based models outperform ORQA on NQ and TriviaQA.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR manages to outperform them on both NQ and TriviaQA\"\n                },\n                {\n                    \"claim_id\": 47,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"1% to 12% absolute differences in exact match accuracy\"\n                },\n                {\n                    \"claim_id\": 48,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets\"\n                },\n                {\n                    \"claim_id\": 49,\n                    \"claim_text\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"achieving the new state of the art\"\n                },\n                {\n                    \"claim_id\": 50,\n                    \"claim_text\": \"DPR-based models outperform ORQA on NQ and TriviaQA.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR manages to outperform them on both NQ and TriviaQA\"\n                },\n                {\n                    \"claim_id\": 51,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"1% to 12% absolute differences in exact match accuracy\"\n                },\n                {\n                    \"claim_id\": 52,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets\"\n                },\n                {\n                    \"claim_id\": 53,\n                    \"claim_text\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"achieving the new state of the art\"\n                },\n                {\n                    \"claim_id\": 54,\n                    \"claim_text\": \"DPR-based models outperform ORQA on NQ and TriviaQA.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR manages to outperform them on both NQ and TriviaQA\"\n                },\n                {\n                    \"claim_id\": 55,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"1% to 12% absolute differences in exact match accuracy\"\n                },\n                {\n                    \"claim_id\": 56,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets\"\n                },\n                {\n                    \"claim_id\": 57,\n                    \"claim_text\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"achieving the new state of the art\"\n                },\n                {\n                    \"claim_id\": 58,\n                    \"claim_text\": \"DPR-based models outperform ORQA on NQ and TriviaQA.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR manages to outperform them on both NQ and TriviaQA\"\n                },\n                {\n                    \"claim_id\": 59,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"1% to 12% absolute differences in exact match accuracy\"\n                },\n                {\n                    \"claim_id\": 60,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets\"\n                },\n                {\n                    \"claim_id\": 61,\n                    \"claim_text\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"achieving the new state of the art\"\n                },\n                {\n                    \"claim_id\": 62,\n                    \"claim_text\": \"DPR-based models outperform ORQA on NQ and TriviaQA.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR manages to outperform them on both NQ and TriviaQA\"\n                },\n                {\n                    \"claim_id\": 63,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"1% to 12% absolute differences in exact match accuracy\"\n                },\n                {\n                    \"claim_id\": 64,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets\"\n                },\n                {\n                    \"claim_id\": 65,\n                    \"claim_text\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"achieving the new state of the art\"\n                },\n                {\n                    \"claim_id\": 66,\n                    \"claim_text\": \"DPR-based models outperform ORQA on NQ and TriviaQA.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR manages to outperform them on both NQ and TriviaQA\"\n                },\n                {\n                    \"claim_id\": 67,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"1% to 12% absolute differences in exact match accuracy\"\n                },\n                {\n                    \"claim_id\": 68,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets\"\n                },\n                {\n                    \"claim_id\": 69,\n                    \"claim_text\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"achieving the new state of the art\"\n                },\n                {\n                    \"claim_id\": 70,\n                    \"claim_text\": \"DPR-based models outperform ORQA on NQ and TriviaQA.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR manages to outperform them on both NQ and TriviaQA\"\n                },\n                {\n                    \"claim_id\": 71,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"1% to 12% absolute differences in exact match accuracy\"\n                },\n                {\n                    \"claim_id\": 72,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets\"\n                },\n                {\n                    \"claim_id\": 73,\n                    \"claim_text\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"achieving the new state of the art\"\n                },\n                {\n                    \"claim_id\": 74,\n                    \"claim_text\": \"DPR-based models outperform ORQA on NQ and TriviaQA.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                    \"exact_quote\": \"DPR manages to outperform them on both NQ and TriviaQA\"\n                },\n                {\n                    \"claim_id\": 75,\n                    \"claim_text\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.\",\n                    \"location\": \"Section 7\",\n                    \"claim_type\": \"Research finding\",\n                   ",
    "raw_evidence": "\n            ```json\n            [\n                {\n                    \"claim_id\": 1,\n                    \"evidence\": \"In this work, our dense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, as shown in Table 2.\"\n                },\n                {\n                    \"claim_id\": 2,\n                    \"evidence\": \"DPR outperforms BM25 by a large margin (65.2% vs. 42.9% in Top-5 accuracy), as shown in Table 2.\"\n                },\n                {\n                    \"claim_id\": 3,\n                    \"evidence\": \"DPR results in a substantial improvement on the end-to-end QA accuracy compared to ORQA (41.5% vs. 33.3%) in the open Natural Questions setting, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 4,\n                    \"evidence\": \"DPR trained using only question-passage pairs is sufficient to greatly outperform BM25, as shown in Table 2.\"\n                },\n                {\n                    \"claim_id\": 5,\n                    \"evidence\": \"Our empirical results also suggest that additional pretraining may not be needed, as shown in the ablation study in Section 5.2.\"\n                },\n                {\n                    \"claim_id\": 6,\n                    \"evidence\": \"A higher retrieval precision indeed translates to a higher end-to-end QA accuracy, as shown in the end-to-end QA results in Table 4.\"\n                },\n                {\n                    \"claim_id\": 7,\n                    \"evidence\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 8,\n                    \"evidence\": \"DPR-based models trained using multiple datasets (Multi) perform comparably to those trained using the individual training set (Single), as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 9,\n                    \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 10,\n                    \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 11,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 12,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 13,\n                    \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 14,\n                    \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 15,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 16,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 17,\n                    \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 18,\n                    \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 19,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 20,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 21,\n                    \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 22,\n                    \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 23,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 24,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 25,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 26,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 27,\n                    \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 28,\n                    \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 29,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 30,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 31,\n                    \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 32,\n                    \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 33,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 34,\n                    \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 35,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 36,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 37,\n                    \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 38,\n                    \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 39,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 40,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 41,\n                    \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 42,\n                    \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 43,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 44,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 45,\n                    \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 46,\n                    \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 47,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 48,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 49,\n                    \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 50,\n                    \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 51,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 52,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 53,\n                    \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 54,\n                    \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 55,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 56,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 57,\n                    \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 58,\n                    \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 59,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 60,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 61,\n                    \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 62,\n                    \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 63,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 64,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 65,\n                    \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 66,\n                    \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 67,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 68,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 69,\n                    \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 70,\n                    \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 71,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 72,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 73,\n                    \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 74,\n                    \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n                },\n                {\n                    \"claim_id\": 75,\n                    \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n                }\n            ]\n```\n\n### ASSISTANT:\n```json\n[\n    {\n        \"claim_id\": 1,\n        \"evidence\": \"In this work, our dense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, as shown in Table 2.\"\n    },\n    {\n        \"claim_id\": 2,\n        \"evidence\": \"DPR outperforms BM25 by a large margin (65.2% vs. 42.9% in Top-5 accuracy), as shown in Table 2.\"\n    },\n    {\n        \"claim_id\": 3,\n        \"evidence\": \"DPR results in a substantial improvement on the end-to-end QA accuracy compared to ORQA (41.5% vs. 33.3%) in the open Natural Questions setting, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 4,\n        \"evidence\": \"DPR trained using only question-passage pairs is sufficient to greatly outperform BM25, as shown in Table 2.\"\n    },\n    {\n        \"claim_id\": 5,\n        \"evidence\": \"Our empirical results also suggest that additional pretraining may not be needed, as shown in the ablation study in Section 5.2.\"\n    },\n    {\n        \"claim_id\": 6,\n        \"evidence\": \"A higher retrieval precision indeed translates to a higher end-to-end QA accuracy, as shown in the end-to-end QA results in Table 4.\"\n    },\n    {\n        \"claim_id\": 7,\n        \"evidence\": \"DPR-based models outperform previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 8,\n        \"evidence\": \"DPR-based models trained using multiple datasets (Multi) perform comparably to those trained using the individual training set (Single), as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 9,\n        \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 10,\n        \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 11,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 12,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 13,\n        \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 14,\n        \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 15,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 16,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 17,\n        \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 18,\n        \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 19,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 20,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 21,\n        \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 22,\n        \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 23,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 24,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 25,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 26,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 27,\n        \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 28,\n        \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 29,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 30,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 31,\n        \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 32,\n        \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 33,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 34,\n        \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 35,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 36,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 37,\n        \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 38,\n        \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 39,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 40,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 41,\n        \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 42,\n        \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 43,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 44,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 45,\n        \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 46,\n        \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 47,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 48,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 49,\n        \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 50,\n        \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 51,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 52,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 53,\n        \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 54,\n        \"evidence\": \"DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 55,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 56,\n        \"evidence\": \"DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4.\"\n    },\n    {\n        \"claim_id\": 57,\n        \"evidence\": \"DPR-based models achieve new state-of-the-art results on WebQu",
    "raw_conclusions": "",
    "execution_times": {
        "claims_analysis_time": "723.18 seconds",
        "evidence_analysis_time": "785.86 seconds",
        "conclusions_analysis_time": "63.93 seconds",
        "total_execution_time": "1575.21 seconds"
    }
}