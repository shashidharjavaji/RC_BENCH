=== Paper Analysis Summary ===

Raw Claims:


```json
{
    "claims": [
        {
            "claim_id": 1,
            "claim_text": "Intermediate layers often yield more informative representations for downstream tasks than the final layers.",
            "location": "Abstract",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers often yield more informative representations for downstream tasks than the final layers."
        },
        {
            "claim_id": 2,
            "claim_text": "Intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "location": "Introduction",
            "claim_type": "Major claim",
            "exact_quote": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers."
        },
        {
            "claim_id": 3,
            "claim_text": "Intermediate layers in Transformers exhibit greater representational variability and information compression.",
            "location": "Section 4.3.1",
            "claim_type": "Minor claim",
            "exact_quote": "Transformers exhibited greater representational variability and information compression within intermediate layers."
        },
        {
            "claim_id": 4,
            "claim_text": "State Space Models (SSMs) display more stable and consistent representations across layers.",
            "location": "Section 4.3.1",
            "claim_type": "Minor claim",
            "exact_quote": "SSMs displayed more stable and consistent representations across layers."
        },
        {
            "claim_id": 5,
            "claim_text": "The most significant changes in representation quality occur in the intermediate layers during training.",
            "location": "Section 4.3.2",
            "claim_type": "Major claim",
            "exact_quote": "The most substantial changes occur in the intermediate layers."
        },
        {
            "claim_id": 6,
            "claim_text": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "location": "Section 5",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers play a pivotal role in adapting to diverse input scenarios."
        },
        {
            "claim_id": 7,
            "claim_text": "The bimodal distribution of prompt entropies in Transformer models is an open question.",
            "location": "Section 5",
            "claim_type": "Major claim",
            "exact_quote": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question."
        },
        {
            "claim_id": 8,
            "claim_text": "The study applies a diverse set of evaluation metrics to highlight distinct behaviors in Transformer-based architectures and SSMs.",
            "location": "Section 5",
            "claim_type": "Minor claim",
            "exact_quote": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs."
        },
        {
            "claim_id": 9,
            "claim_text": "The study provides practical guidance for optimizing model design, training, and application.",
            "location": "Conclusion",
            "claim_type": "Minor claim",
            "exact_quote": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application."
        }
    ]
}
```


```json
{
    "claims": [
        {
            "claim_id": "1",
            "claim_text": "Intermediate layers often yield more informative representations for downstream tasks than the final layers.",
            "location": "Abstract",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers often yield more informative representations for downstream tasks than the final layers."
        },
        {
            "claim_id": "2",
            "claim_text": "Intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "location": "Introduction",
            "claim_type": "Major claim",
            "exact_quote": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers."
        },
        {
            "claim_id": "3",
            "claim_text": "Intermediate layers in Transformers exhibit greater representational variability and information compression.",
            "location": "Section 4.3.1",
            "claim_type": "Minor claim",
            "exact_quote": "Transformers exhibited greater representational variability and information compression within intermediate layers."
        },
        {
            "claim_id": "4",
            "claim_text": "State Space Models (SSMs) display more stable and consistent representations across layers.",
            "location": "Section 4.3.1",
            "claim_type": "Minor claim",
            "exact_quote": "SSMs displayed more stable and consistent representations across layers."
        },
        {
            "claim_id": "5",
            "claim_text": "The most significant changes in representation quality occur in the intermediate layers during training.",
            "location": "Section 4.3.2",
            "claim_type": "Major claim",
            "exact_quote": "The most substantial changes occur in the intermediate layers."
        },
        {
            "claim_id": "6",
            "claim_text": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "location": "Section 5",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers play a pivotal role in adapting to diverse input scenarios."
        },
        {
            "claim_id": "7",
            "claim_text": "The bimodal distribution of prompt entropies in Transformer models is an open question.",
            "location": "Section 5",
            "claim_type": "Major claim",
            "exact_quote": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question."
        },
        {
            "claim_id": "8",
            "claim_text": "The study applies a diverse set of evaluation metrics to highlight distinct behaviors in Transformer-based architectures and SSMs.",
            "location": "Section 5",
            "claim_type": "Minor claim",
            "exact_quote": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs."
        },
        {
            "claim_id": "9",
            "claim_text": "The study provides practical guidance for optimizing model design, training, and application.",
            "location": "Conclusion",
            "claim_type": "Minor claim",
            "exact_quote": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application."
        }
    ]
}
```


```json
{
    "claims": [
        {
            "claim_id": "1",
            "claim_text": "Intermediate layers often yield more informative representations for downstream tasks than the final layers.",
            "location": "Abstract",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers often yield more informative representations for downstream tasks than the final layers."
        },
        {
            "claim_id": "2",
            "claim_text": "Intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "location": "Introduction",
            "claim_type": "Major claim",
            "exact_quote": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers."
        },
        {
            "claim_id": "3",
            "claim_text": "Intermediate layers in Transformers exhibit greater representational variability and information compression.",
            "location": "Section 4.3.1",
            "claim_type": "Minor claim",
            "exact_quote": "Transformers exhibited greater representational variability and information compression within intermediate layers."
        },
        {
            "claim_id": "4",
            "claim_text": "State Space Models (SSMs) display more stable and consistent representations across layers.",
            "location": "Section 4.3.1",
            "claim_type": "Minor claim",
            "exact_quote": "SSMs displayed more stable and consistent representations across layers."
        },
        {
            "claim_id": "5",
            "claim_text": "The most significant changes in representation quality occur in the intermediate layers during training.",
            "location": "Section 4.3.2",
            "claim_type": "Major claim",
            "exact_quote": "The most substantial changes occur in the intermediate layers."
        },
        {
            "claim_id": "6",
            "claim_text": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "location": "Section 5",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers play a pivotal role in adapting to diverse input scenarios."
        },
        {
            "claim_id": "7",
            "claim_text": "The bimodal distribution of prompt entropies in Transformer models is an open question.",
            "location": "Section 5",
            "claim_type": "Major claim",
            "exact_quote": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question."
        },
        {
            "claim_id": "8",
            "claim_text": "The study applies a diverse set of evaluation metrics to highlight distinct behaviors in Transformer-based architectures and SSMs.",
            "location": "Section 5",
            "claim_type": "Minor claim",
            "exact_quote": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs."
        },
        {
            "claim_id": "9",
            "claim_text": "The study provides practical guidance for optimizing model design, training, and application.",
            "location": "Conclusion",
            "claim_type": "Minor claim",
            "exact_quote": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application."
        }
    ]
}
```


```json
{
    "claims": [
        {
            "claim_id": "1",
            "claim_text": "Intermediate layers often yield more informative representations for downstream tasks than the final layers.",
            "location": "Abstract",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers often yield more informative representations for downstream tasks than the final layers."
        },
        {
            "claim_id": "2",
            "claim_text": "Intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "location": "Introduction",
            "claim_type": "Major claim",
            "exact_quote": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers."
        },
        {
            "claim_id": "3",
            "claim_text": "Intermediate layers in Transformers exhibit greater representational variability and information compression.",
            "location": "Section 4.3.1",
            "claim_type": "Minor claim",
            "exact_quote": "Transformers exhibited greater representational variability and information compression within intermediate layers."
        },
        {
            "claim_id": "4",
            "claim_text": "State Space Models (SSMs) display more stable and consistent representations across layers.",
            "location": "Section 4.3.1",
            "claim_type": "Minor claim",
            "exact_quote": "SSMs displayed more stable and consistent representations across layers."
        },
        {
            "claim_id": "5",
            "claim_text": "The most significant changes in representation quality occur in the intermediate layers during training.",
            "location": "Section 4.3.2",
            "claim_type": "Major claim",
            "exact_quote": "The most substantial changes occur in the intermediate layers."
        },
        {
            "claim_id": "6",
            "claim_text": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "location": "Section 5",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers play a pivotal role in adapting to diverse input scenarios."
        },
        {
            "claim_id": "7",
            "claim_text": "The bimodal distribution of prompt entropies in Transformer models is an open question.",
            "location": "Section 5",
            "claim_type": "Major claim",
            "exact_quote": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question."
        },
        {
            "claim_id": "8",
            "claim_text": "The study applies a diverse set of evaluation metrics to highlight distinct behaviors in Transformer-based architectures and SSMs.",
            "location": "Section 5",
            "claim_type": "Minor claim",
            "exact_quote": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs."
        },
        {
            "claim_id": "9",
            "claim_text": "The study provides practical guidance for optimizing model design, training, and application.",
            "location": "Conclusion",
            "claim_type": "Minor claim",
            "exact_quote": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application."
        }
    ]
}
```


```json
{
    "claims": [
        {
            "claim_id": "1",
            "claim_text": "Intermediate layers often yield more informative representations for downstream tasks than the final layers.",
            "location": "Abstract",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers often yield more informative representations for downstream tasks than the final layers."
        },
        {
            "claim_id": "2",
            "claim_text": "Intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "location": "Introduction",
            "claim_type": "Major claim",
            "exact_quote": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers."
        },
        {
            "claim_id": "3",
            "claim_text": "Intermediate layers in Transformers exhibit greater representational variability and information compression.",
            "location": "Section 4.3.1",
            "claim_type": "Minor claim",
            "exact_quote": "Transformers exhibited greater representational variability and information compression within intermediate layers."
        },
        {
            "claim_id": "4",
            "claim_text": "State Space Models (SSMs) display more stable and consistent representations across layers.",
            "location": "Section 4.3.1",
            "claim_type": "Minor claim",
            "exact_quote": "SSMs displayed more stable and consistent representations across layers."
        },
        {
            "claim_id": "5",
            "claim_text": "The most significant changes in representation quality occur in the intermediate layers during training.",
            "location": "Section 4.3.2",
            "claim_type": "Major claim",
            "exact_quote": "The most substantial changes occur in the intermediate layers."
        },
        {
            "claim_id": "6",
            "claim_text": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "location": "Section 5",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers play a pivotal role in adapting to diverse input scenarios."
        },
        {
            "claim_id": "7",
            "claim_text": "The bimodal distribution of prompt entropies in Transformer models is an open question.",
            "location": "Section 5",
            "claim_type": "Major claim",
            "exact_quote": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question."
        },
        {
            "claim_id": "8",
            "claim_text": "The study applies a diverse set of evaluation metrics to highlight distinct behaviors in Transformer-based architectures and SSMs.",
            "location": "Section 5",
            "claim_type": "Minor claim",
            "exact_quote": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs."
        },
        {
            "claim_id": "9",
            "claim_text": "The study provides practical guidance for optimizing model design, training, and application.",
            "location": "Conclusion",
            "claim_type": "Minor claim",
            "exact_quote": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application."
        }
    ]
}
```


```json
{
    "claims": [
        {
            "claim_id": "1",
            "claim_text": "Intermediate layers often yield more informative representations for downstream tasks than the final layers.",
            "location": "Abstract",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers often yield more informative representations for downstream tasks than the final layers."
        },
        {
            "claim_id": "2",
            "claim_text": "Intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "location": "Introduction",
            "claim_type": "Major claim",
            "exact_quote": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers."
        },
        {
            "claim_id": "3",
            "claim_text": "Intermediate layers in Transformers exhibit greater representational variability and information compression.",
            "location": "Section 4.3.1",
            "claim_type": "Minor claim",
            "exact_quote": "Transformers exhibited greater representational variability and information compression within intermediate layers."
        },
        {
            "claim_id": "4",
            "claim_text": "State Space Models (SSMs) display more stable and consistent representations across layers.",
            "location": "Section 4.3.1",
            "claim_type": "Minor claim",
            "exact_quote": "SSMs displayed more stable and consistent representations across layers."
        },
        {
            "claim_id": "5",
            "claim_text": "The most significant changes in representation quality occur in the intermediate layers during training.",
            "location": "Section 4.3.2",
            "claim_type": "Major claim",
            "exact_quote": "The most substantial changes occur in the intermediate layers."
        },
        {
            "claim_id": "6",
            "claim_text": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "location": "Section 5",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers play a pivotal role in adapting to diverse input scenarios."
        },
        {
            "claim_id": "7",
            "claim_text": "The bimodal distribution of prompt entropies in Transformer models is an open question.",
            "location": "Section 5",
            "claim_type": "Major claim",
            "exact_quote": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question."
        },
        {
            "claim_id": "8",
            "claim_text": "The study applies a diverse set of evaluation metrics to highlight distinct behaviors in Transformer-based architectures and SSMs.",
            "location": "Section 5",
            "claim_type": "Minor claim",
            "exact_quote": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs."
        },
        {
            "claim_id": "9",
            "claim_text": "The study provides practical guidance for optimizing model design, training, and application.",
            "location": "Conclusion",
            "claim_type": "Minor claim",
            "exact_quote": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application."
        }
    ]
}
```


```json
{
    "claims": [
        {
            "claim_id": "1",
            "claim_text": "Intermediate layers often yield more informative representations for downstream tasks than the final layers.",
            "location": "Abstract",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers often yield more informative representations for downstream tasks than the final layers."
        },
        {
            "claim_id": "2",
            "claim_text": "Intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "location": "Introduction",
            "claim_type": "Major claim",
            "exact_quote": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers."
        },
        {
            "claim_id": "3",
            "claim_text": "Intermediate layers in Transformers exhibit greater representational variability and information compression.",
            "location": "Section 4.3.1",
            "claim_type": "Minor claim",
            "exact_quote": "Transformers exhibited greater representational variability and information compression within intermediate layers."
        },
        {
            "claim_id": "4",
            "claim_text": "State Space Models (SSMs) display more stable and consistent representations across layers.",
            "location": "Section 4.3.1",
            "claim_type": "Minor claim",
            "exact_quote": "SSMs displayed more stable and consistent representations across layers."
        },
        {
            "claim_id": "5",
            "claim_text": "The most significant changes in representation quality occur in the intermediate layers during training.",
            "location": "Section 4.3.2",
            "claim_type": "Major claim",
            "exact_quote": "The most substantial changes occur in the intermediate layers."
        },
        {
            "claim_id": "6",
            "claim_text": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "location": "Section 5",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers play a pivotal role in adapting to diverse input scenarios."
        },
        {
            "claim_id": "7",
            "claim_text": "The bimodal distribution of prompt entropies in Transformer models is an open question.",
            "location": "Section 5",
            "claim_type": "Major claim",
            "exact_quote": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question."
        },
        {
            "claim_id": "8",
            "claim_text": "The study applies a diverse set of evaluation metrics to highlight distinct behaviors in Transformer-based architectures and SSMs.",
            "location": "Section 5",
            "claim_type": "Minor claim",
            "exact_quote": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs."
        },
        {
            "claim_id": "9",
            "claim_text": "The study provides practical guidance for optimizing model design, training, and application.",
            "location": "Conclusion",
            "claim_type": "Minor claim",
            "exact_quote": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application."
        }
    ]
}
```


```json
{
    "claims": [
        {
            "claim_id": "1",
            "claim_text": "Intermediate layers often yield more informative representations for downstream tasks than the final layers.",
            "location": "Abstract",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers often yield more informative representations for downstream tasks than the final layers."
        },
        {
            "claim_id": "2",
            "claim_text": "Intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "location": "Introduction",
            "claim_type": "Major claim",
            "exact_quote": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers."
        },
        {
            "claim_id": "3",
            "claim_text": "Intermediate layers in Transformers exhibit greater representational variability and information compression.",
            "location": "Section 4.3.1",
            "claim_type": "Minor claim",
            "exact_quote": "Transformers exhibited greater representational variability and information compression within intermediate layers."
        },
        {
            "claim_id": "4",
            "claim_text": "State Space Models (SSMs) display more stable and consistent representations across layers.",
            "location": "Section 4.3.1",
            "claim_type": "Minor claim",
            "exact_quote": "SSMs displayed more stable and consistent representations across layers."
        },
        {
            "claim_id": "5",
            "claim_text": "The most significant changes in representation quality occur in the intermediate layers during training.",
            "location": "Section 4.3.2",
            "claim_type": "Major claim",
            "exact_quote": "The most substantial changes occur in the intermediate layers."
        },
        {
            "claim_id": "6",
            "claim_text": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "location": "Section 5",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers play a pivotal role in adapting to diverse input scenarios."
        },
        {
            "claim_id": "7",
            "claim_text": "The bimodal distribution of prompt entropies in Transformer models is an open question.",
            "location": "Section 5",
            "claim_type": "Major claim",
            "exact_quote": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question."
        },
        {
            "claim_id": "8",
            "claim_text": "The study applies a diverse set of evaluation metrics to highlight distinct behaviors in Transformer-based architectures and SSMs.",
            "location": "Section 5",
            "claim_type": "Minor claim",
            "exact_quote": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs."
        },
        {
            "claim_id": "9",
            "claim_text": "The study provides practical guidance for optimizing model design, training, and application.",
            "location": "Conclusion",
            "claim_type": "Minor claim",
            "exact_quote": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application."
        }
    ]
}
```


```json
{
    "claims": [
        {
            "claim_id": "1",
            "claim_text": "Intermediate layers often yield more informative representations for downstream tasks than the final layers.",
            "location": "Abstract",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers often yield more informative representations for downstream tasks than the final layers."
        },
        {
            "claim_id": "2",
            "claim_text": "Intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "location": "Introduction",
            "claim_type": "Major claim",
            "exact_quote": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers."
        },
        {
            "claim_id": "3",
            "claim_text": "Intermediate layers in Transformers exhibit greater representational variability and information compression.",
            "location": "Section 4.3.1",
            "claim_type": "Minor claim",
            "exact_quote": "Transformers exhibited greater representational variability and information compression within intermediate layers."
        },
        {
            "claim_id": "4",
            "claim_text": "State Space Models (SSMs) display more stable and consistent representations across layers.",
            "location": "Section 4.3.1",
            "claim_type": "Minor claim",
            "exact_quote": "SSMs displayed more stable and consistent representations across layers."
        },
        {
            "claim_id": "5",
            "claim_text": "The most significant changes in representation quality occur in the intermediate layers during training.",
            "location": "Section 4.3.2",
            "claim_type": "Major claim",
            "exact_quote": "The most substantial changes occur in the intermediate layers."
        },
        {
            "claim_id": "6",
            "claim_text": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "location": "Section 5",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers play a pivotal role in adapting to diverse input scenarios."
        },
        {
            "claim_id": "7",
            "claim_text": "The bimodal distribution of prompt entropies in Transformer models is an open question.",
            "location": "Section 5",
            "claim_type": "Major claim",
            "exact_quote": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question."
        },
        {
            "claim_id": "8",
            "claim_text": "The study applies a diverse set of evaluation metrics to highlight distinct behaviors in Transformer-based architectures and SSMs.",
            "location": "Section 5",
            "claim_type": "Minor claim",
            "exact_quote": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs."
        },
        {
            "claim_id": "9",
            "claim_text": "The study provides practical guidance for optimizing model design, training, and application.",
            "location": "Conclusion",
            "claim_type": "Minor claim",
            "exact_quote": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application."
        }
    ]
}
```


```json
{
    "claims": [
        {
            "claim_id": "1",
            "claim_text": "Intermediate layers often yield more informative representations for downstream tasks than the final layers.",
            "location": "Abstract",
            "claim_type": "Major claim",
            "exact_quote": "Intermediate layers often yield more informative representations for downstream tasks than the final layers."
        },
        {
            "claim_id": "2",
            "claim_text": "Intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "location": "Introduction",
            "claim_type": "Major claim",
            "ex

Raw Evidence:


```json
{
    "evidence": [
        {
            "claim_id": "1",
            "evidence": "We find that intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "section": "Introduction",
            "page": 2
        },
        {
            "claim_id": "2",
            "evidence": "Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer.",
            "section": "Abstract",
            "page": 1
        },
        {
            "claim_id": "3",
            "evidence": "Transformers exhibited greater representational variability and information compression within intermediate layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "4",
            "evidence": "SSMs displayed more stable and consistent representations across layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "5",
            "evidence": "The most substantial changes occur in the intermediate layers.",
            "section": "Section 4.3.2",
            "page": 11
        },
        {
            "claim_id": "6",
            "evidence": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "7",
            "evidence": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "8",
            "evidence": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "9",
            "evidence": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application.",
            "section": "Conclusion",
            "page": 13
        }
    ]
}
```


```json
{
    "evidence": [
        {
            "claim_id": "1",
            "evidence": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "section": "Introduction",
            "page": 2
        },
        {
            "claim_id": "2",
            "evidence": "Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer.",
            "section": "Abstract",
            "page": 1
        },
        {
            "claim_id": "3",
            "evidence": "Transformers exhibited greater representational variability and information compression within intermediate layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "4",
            "evidence": "SSMs displayed more stable and consistent representations across layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "5",
            "evidence": "The most substantial changes occur in the intermediate layers.",
            "section": "Section 4.3.2",
            "page": 11
        },
        {
            "claim_id": "6",
            "evidence": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "7",
            "evidence": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "8",
            "evidence": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "9",
            "evidence": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application.",
            "section": "Conclusion",
            "page": 13
        }
    ]
}
```


```json
{
    "evidence": [
        {
            "claim_id": "1",
            "evidence": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "section": "Introduction",
            "page": 2
        },
        {
            "claim_id": "2",
            "evidence": "Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer.",
            "section": "Abstract",
            "page": 1
        },
        {
            "claim_id": "3",
            "evidence": "Transformers exhibited greater representational variability and information compression within intermediate layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "4",
            "evidence": "SSMs displayed more stable and consistent representations across layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "5",
            "evidence": "The most substantial changes occur in the intermediate layers.",
            "section": "Section 4.3.2",
            "page": 11
        },
        {
            "claim_id": "6",
            "evidence": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "7",
            "evidence": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "8",
            "evidence": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "9",
            "evidence": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application.",
            "section": "Conclusion",
            "page": 13
        }
    ]
}
```


```json
{
    "evidence": [
        {
            "claim_id": "1",
            "evidence": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "section": "Introduction",
            "page": 2
        },
        {
            "claim_id": "2",
            "evidence": "Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer.",
            "section": "Abstract",
            "page": 1
        },
        {
            "claim_id": "3",
            "evidence": "Transformers exhibited greater representational variability and information compression within intermediate layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "4",
            "evidence": "SSMs displayed more stable and consistent representations across layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "5",
            "evidence": "The most substantial changes occur in the intermediate layers.",
            "section": "Section 4.3.2",
            "page": 11
        },
        {
            "claim_id": "6",
            "evidence": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "7",
            "evidence": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "8",
            "evidence": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "9",
            "evidence": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application.",
            "section": "Conclusion",
            "page": 13
        }
    ]
}
```


```json
{
    "evidence": [
        {
            "claim_id": "1",
            "evidence": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "section": "Introduction",
            "page": 2
        },
        {
            "claim_id": "2",
            "evidence": "Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer.",
            "section": "Abstract",
            "page": 1
        },
        {
            "claim_id": "3",
            "evidence": "Transformers exhibited greater representational variability and information compression within intermediate layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "4",
            "evidence": "SSMs displayed more stable and consistent representations across layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "5",
            "evidence": "The most substantial changes occur in the intermediate layers.",
            "section": "Section 4.3.2",
            "page": 11
        },
        {
            "claim_id": "6",
            "evidence": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "7",
            "evidence": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "8",
            "evidence": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "9",
            "evidence": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application.",
            "section": "Conclusion",
            "page": 13
        }
    ]
}
```


```json
{
    "evidence": [
        {
            "claim_id": "1",
            "evidence": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "section": "Introduction",
            "page": 2
        },
        {
            "claim_id": "2",
            "evidence": "Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer.",
            "section": "Abstract",
            "page": 1
        },
        {
            "claim_id": "3",
            "evidence": "Transformers exhibited greater representational variability and information compression within intermediate layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "4",
            "evidence": "SSMs displayed more stable and consistent representations across layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "5",
            "evidence": "The most substantial changes occur in the intermediate layers.",
            "section": "Section 4.3.2",
            "page": 11
        },
        {
            "claim_id": "6",
            "evidence": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "7",
            "evidence": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "8",
            "evidence": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "9",
            "evidence": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application.",
            "section": "Conclusion",
            "page": 13
        }
    ]
}
```


```json
{
    "evidence": [
        {
            "claim_id": "1",
            "evidence": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "section": "Introduction",
            "page": 2
        },
        {
            "claim_id": "2",
            "evidence": "Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer.",
            "section": "Abstract",
            "page": 1
        },
        {
            "claim_id": "3",
            "evidence": "Transformers exhibited greater representational variability and information compression within intermediate layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "4",
            "evidence": "SSMs displayed more stable and consistent representations across layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "5",
            "evidence": "The most substantial changes occur in the intermediate layers.",
            "section": "Section 4.3.2",
            "page": 11
        },
        {
            "claim_id": "6",
            "evidence": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "7",
            "evidence": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "8",
            "evidence": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "9",
            "evidence": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application.",
            "section": "Conclusion",
            "page": 13
        }
    ]
}
```


```json
{
    "evidence": [
        {
            "claim_id": "1",
            "evidence": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "section": "Introduction",
            "page": 2
        },
        {
            "claim_id": "2",
            "evidence": "Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer.",
            "section": "Abstract",
            "page": 1
        },
        {
            "claim_id": "3",
            "evidence": "Transformers exhibited greater representational variability and information compression within intermediate layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "4",
            "evidence": "SSMs displayed more stable and consistent representations across layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "5",
            "evidence": "The most substantial changes occur in the intermediate layers.",
            "section": "Section 4.3.2",
            "page": 11
        },
        {
            "claim_id": "6",
            "evidence": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "7",
            "evidence": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "8",
            "evidence": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "9",
            "evidence": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application.",
            "section": "Conclusion",
            "page": 13
        }
    ]
}
```


```json
{
    "evidence": [
        {
            "claim_id": "1",
            "evidence": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "section": "Introduction",
            "page": 2
        },
        {
            "claim_id": "2",
            "evidence": "Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer.",
            "section": "Abstract",
            "page": 1
        },
        {
            "claim_id": "3",
            "evidence": "Transformers exhibited greater representational variability and information compression within intermediate layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "4",
            "evidence": "SSMs displayed more stable and consistent representations across layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "5",
            "evidence": "The most substantial changes occur in the intermediate layers.",
            "section": "Section 4.3.2",
            "page": 11
        },
        {
            "claim_id": "6",
            "evidence": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "7",
            "evidence": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "8",
            "evidence": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "9",
            "evidence": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application.",
            "section": "Conclusion",
            "page": 13
        }
    ]
}
```


```json
{
    "evidence": [
        {
            "claim_id": "1",
            "evidence": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "section": "Introduction",
            "page": 2
        },
        {
            "claim_id": "2",
            "evidence": "Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer.",
            "section": "Abstract",
            "page": 1
        },
        {
            "claim_id": "3",
            "evidence": "Transformers exhibited greater representational variability and information compression within intermediate layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "4",
            "evidence": "SSMs displayed more stable and consistent representations across layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "5",
            "evidence": "The most substantial changes occur in the intermediate layers.",
            "section": "Section 4.3.2",
            "page": 11
        },
        {
            "claim_id": "6",
            "evidence": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "7",
            "evidence": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "8",
            "evidence": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "9",
            "evidence": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application.",
            "section": "Conclusion",
            "page": 13
        }
    ]
}
```


```json
{
    "evidence": [
        {
            "claim_id": "1",
            "evidence": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "section": "Introduction",
            "page": 2
        },
        {
            "claim_id": "2",
            "evidence": "Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer.",
            "section": "Abstract",
            "page": 1
        },
        {
            "claim_id": "3",
            "evidence": "Transformers exhibited greater representational variability and information compression within intermediate layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "4",
            "evidence": "SSMs displayed more stable and consistent representations across layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "5",
            "evidence": "The most substantial changes occur in the intermediate layers.",
            "section": "Section 4.3.2",
            "page": 11
        },
        {
            "claim_id": "6",
            "evidence": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "7",
            "evidence": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "8",
            "evidence": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "9",
            "evidence": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application.",
            "section": "Conclusion",
            "page": 13
        }
    ]
}
```


```json
{
    "evidence": [
        {
            "claim_id": "1",
            "evidence": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "section": "Introduction",
            "page": 2
        },
        {
            "claim_id": "2",
            "evidence": "Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer.",
            "section": "Abstract",
            "page": 1
        },
        {
            "claim_id": "3",
            "evidence": "Transformers exhibited greater representational variability and information compression within intermediate layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "4",
            "evidence": "SSMs displayed more stable and consistent representations across layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "5",
            "evidence": "The most substantial changes occur in the intermediate layers.",
            "section": "Section 4.3.2",
            "page": 11
        },
        {
            "claim_id": "6",
            "evidence": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "7",
            "evidence": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "8",
            "evidence": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "9",
            "evidence": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application.",
            "section": "Conclusion",
            "page": 13
        }
    ]
}
```


```json
{
    "evidence": [
        {
            "claim_id": "1",
            "evidence": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "section": "Introduction",
            "page": 2
        },
        {
            "claim_id": "2",
            "evidence": "Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer.",
            "section": "Abstract",
            "page": 1
        },
        {
            "claim_id": "3",
            "evidence": "Transformers exhibited greater representational variability and information compression within intermediate layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "4",
            "evidence": "SSMs displayed more stable and consistent representations across layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "5",
            "evidence": "The most substantial changes occur in the intermediate layers.",
            "section": "Section 4.3.2",
            "page": 11
        },
        {
            "claim_id": "6",
            "evidence": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "7",
            "evidence": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "8",
            "evidence": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "9",
            "evidence": "These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application.",
            "section": "Conclusion",
            "page": 13
        }
    ]
}
```


```json
{
    "evidence": [
        {
            "claim_id": "1",
            "evidence": "We demonstrate that intermediate layers consistently provide better representations for downstream tasks than the final layers.",
            "section": "Introduction",
            "page": 2
        },
        {
            "claim_id": "2",
            "evidence": "Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer.",
            "section": "Abstract",
            "page": 1
        },
        {
            "claim_id": "3",
            "evidence": "Transformers exhibited greater representational variability and information compression within intermediate layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "4",
            "evidence": "SSMs displayed more stable and consistent representations across layers.",
            "section": "Section 4.3.1",
            "page": 10
        },
        {
            "claim_id": "5",
            "evidence": "The most substantial changes occur in the intermediate layers.",
            "section": "Section 4.3.2",
            "page": 11
        },
        {
            "claim_id": "6",
            "evidence": "Intermediate layers play a pivotal role in adapting to diverse input scenarios.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "7",
            "evidence": "The observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "8",
            "evidence": "By applying a diverse set of evaluation metrics, we highlighted distinct behaviors in Transformer-based architectures and SSMs.",
            "section": "Section 5",
            "page": 12
        },
        {
            "claim_id": "9",
            "evidence": "These findings

Raw Conclusions:


Execution Times:
claims_analysis_time: 707.93 seconds
evidence_analysis_time: 769.78 seconds
conclusions_analysis_time: 833.58 seconds
total_execution_time: 2328.06 seconds
