Claim 1:
Type: performance
Statement: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark.
Location: Section 5.1 Shell Scripting Results
Exact Quote: DocPrompting consistently improves the base models. For example, T5+DocPrompting achieves more than twice higher accuracy in predicting the command name, more than 16 charBLEU points on the entire prediction, and almost 9% of absolute exact match gain, compared to the vanilla T5.

Evidence:
- Evidence Text: T5+DocPrompting achieves more than twice higher accuracy in predicting the command name, more than 16 charBLEU points on the entire prediction, and almost 9% of absolute exact match gain, compared to the vanilla T5.
  Strength: strong
  Location: Section 5.1 Shell Scripting Results
  Limitations: None provided in the section
  Exact Quote: T5+DocPrompting achieves more than twice higher accuracy in predicting the command name, more than 16 charBLEU points on the entire prediction, and almost 9% of absolute exact match gain, compared to the vanilla T5.

- Evidence Text: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation in CoNaLa.
  Strength: strong
  Location: Section 5.1 Shell Scripting Results
  Limitations: None provided in the section
  Exact Quote: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation in CoNaLa.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided shows a significant improvement in the performance metrics of pass@1 and pass@10 when using DocPrompting.
Key Limitations: None provided in the section

--------------------------------------------------

Claim 2:
Type: performance
Statement: On a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo-1.3B by up to absolute 6.9% exact match.
Location: Section 5.1 Shell Scripting Results
Exact Quote: On a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo-1.3B by up to absolute 6.9% exact match.

Evidence:
- Evidence Text: On a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo-1.3B by up to absolute 6.9% exact match.
  Strength: strong
  Location: Section 5.1 Shell Scripting Results
  Limitations: None provided in the section
  Exact Quote: On a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo-1.3B by up to absolute 6.9% exact match.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided shows a significant improvement in the exact match metric when using DocPrompting on the tldr dataset.
Key Limitations: None provided in the section

--------------------------------------------------

Claim 3:
Type: performance
Statement: DocPrompting improves Codex by 6.78 charBLEU score.
Location: Section 5.1 Shell Scripting Results
Exact Quote: DocPrompting improves Codex by 6.78 charBLEU score.

Evidence:
- Evidence Text: DocPrompting improves Codex by 6.78 charBLEU score.
  Strength: strong
  Location: Section 5.1 Shell Scripting Results
  Limitations: None provided in the section
  Exact Quote: DocPrompting improves Codex by 6.78 charBLEU score.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided shows a significant improvement in the charBLEU score when using DocPrompting on Codex.
Key Limitations: None provided in the section

--------------------------------------------------

Claim 4:
Type: performance
Statement: DocPrompting improves CodeT5 by 1.65 BLEU improvement over the state-of-the-art baseline that was initialized with CodeT5.
Location: Section 5.2 Python Programming Results
Exact Quote: CodeT5+DocPrompting yields a 1.65 BLEU improvement over the state-of-the-art baseline that was initialized with CodeT5.

Evidence:
- Evidence Text: CodeT5+DocPrompting yields a 1.65 BLEU improvement over the state-of-the-art baseline that was initialized with CodeT5.
  Strength: strong
  Location: Section 5.2 Python Programming Results
  Limitations: None provided in the section
  Exact Quote: CodeT5+DocPrompting yields a 1.65 BLEU improvement over the state-of-the-art baseline that was initialized with CodeT5.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided shows a significant improvement in the BLEU score when using DocPrompting on CodeT5.
Key Limitations: None provided in the section

--------------------------------------------------

Claim 5:
Type: performance
Statement: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark.
Location: Section 5.1 Shell Scripting Results
Exact Quote: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark.

Evidence:
- Evidence Text: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark.
  Strength: strong
  Location: Section 5.1 Shell Scripting Results
  Limitations: None provided in the section
  Exact Quote: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided shows a significant improvement in the pass@1 metric when using DocPrompting on CodeT5.
Key Limitations: None provided in the section

--------------------------------------------------

Claim 6:
Type: performance
Statement: DocPrompting improves strong base models such as CodeT5 by 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark.
Location: Section 5.1 Shell Scripting Results
Exact Quote: DocPrompting improves strong base models such as CodeT5 by 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark.

Evidence:
- Evidence Text: DocPrompting improves strong base models such as CodeT5 by 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark.
  Strength: strong
  Location: Section 5.1 Shell Scripting Results
  Limitations: None provided in the section
  Exact Quote: DocPrompting improves strong base models such as CodeT5 by 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided shows a significant improvement in the pass@10 metric when using DocPrompting on CodeT5.
Key Limitations: None provided in the section

--------------------------------------------------

Claim 7:
Type: performance
Statement: DocPrompting improves Codex by 6.7 charBLEU score.
Location: Section 5.1 Shell Scripting Results
Exact Quote: DocPrompting improves Codex by 6.7 charBLEU score.

Evidence:
- Evidence Text: DocPrompting improves Codex by 6.7 charBLEU score.
  Strength: strong
  Location: Section 5.1 Shell Scripting Results
  Limitations: None provided in the section
  Exact Quote: DocPrompting improves Codex by 6.7 charBLEU score.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided shows a significant improvement in the charBLEU score when using DocPrompting on Codex.
Key Limitations: None provided in the section

--------------------------------------------------

Claim 8:
Type: performance
Statement: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark.
Location: Section 5.1 Shell Scripting Results
Exact Quote: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark.

Evidence:
- Evidence Text: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark.
  Strength: strong
  Location: Section 5.1 Shell Scripting Results
  Limitations: None provided in the section
  Exact Quote: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided shows a significant improvement in the pass@1 and pass@10 metrics when using DocPrompting on CodeT5.
Key Limitations: None provided in the section

--------------------------------------------------

Claim 9:
Type: performance
Statement: DocPrompting improves strong base models such as CodeT5 by 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark.
Location: Section 5.1 Shell Scripting Results
Exact Quote: DocPrompting improves strong base models such as CodeT5 by 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark.

Evidence:
- Evidence Text: DocPrompting improves strong base models such as CodeT5 by 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark.
  Strength: strong
  Location: Section 5.1 Shell Scripting Results
  Limitations: None provided in the section
  Exact Quote: DocPrompting improves strong base models such as CodeT5 by 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided shows a significant improvement in the pass@10 metric when using DocPrompting on CodeT5.
Key Limitations: None provided in the section

--------------------------------------------------

Claim 10:
Type: performance
Statement: DocPrompting improves Codex by 6.78 charBLEU score.
Location: Section 5.1 Shell Scripting Results
Exact Quote: DocPrompting improves Codex by 6.78 charBLEU score.

Evidence:
- Evidence Text: DocPrompting improves Codex by 6.78 charBLEU score.
  Strength: strong
  Location: Section 5.1 Shell Scripting Results
  Limitations: None provided in the section
  Exact Quote: DocPrompting improves Codex by 6.78 charBLEU score.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided shows a significant improvement in the charBLEU score when using DocPrompting on Codex.
Key Limitations: None provided in the section

--------------------------------------------------

Claim 11:
Type: performance
Statement: DocPrompting improves CodeT5 by 1.65 BLEU improvement over the state-of-the-art baseline that was initialized with CodeT5.
Location: Section 5.2 Python Programming Results
Exact Quote: CodeT5+DocPrompting yields a 1.65 BLEU improvement over the state-of-the-art baseline that was initialized with CodeT5.

Evidence:
- Evidence Text: CodeT5+DocPrompting yields a 1.65 BLEU improvement over the state-of-the-art baseline that was initialized with CodeT5.
  Strength: strong
  Location: Section 5.2 Python Programming Results
  Limitations: None provided in the section
  Exact Quote: CodeT5+DocPrompting yields a 1.65 BLEU improvement over the state-of-the-art baseline that was initialized with CodeT5.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided shows a significant improvement in the BLEU score when using DocPrompting on CodeT5.
Key Limitations: None provided in the section

--------------------------------------------------

