Claim 1:
Type: methodology
Statement: MindMap enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge.
Location: Introduction
Exact Quote: Our method enables LLMs to comprehend KG inputs and infer with a combination of implicit and external knowledge.

Evidence:
- Evidence Text: Experiments on three datasets show MindMap outperforms a series of prompting approaches by a large margin.
  Strength: strong
  Location: Experiments
  Limitations: Experiments are limited to three datasets.
  Exact Quote: We conducted experiments on three datasets to illustrate that MindMap outperforms a series of prompting approaches by a large margin.

- Evidence Text: MindMap achieves significant improvements over baselines on diverse question & answering tasks, especially in medical domains.
  Strength: strong
  Location: Experiments
  Limitations: Performance on unseen or out-of-domain datasets is not evaluated.
  Exact Quote: We evaluate our method on diverse question & answering tasks, especially in medical domains, and show significant improvements over baselines.

- Evidence Text: MindMap is robust to mismatched retrieval knowledge.
  Strength: moderate
  Location: Experiments
  Limitations: Robustness is evaluated only on datasets with mismatched retrieval knowledge.
  Exact Quote: This work underscores how LLM can learn to conduct synergistic inference with KG.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Experimental results demonstrate MindMap's superior performance and robustness.
Key Limitations: Limited to three datasets and mismatched retrieval knowledge.

--------------------------------------------------

Claim 2:
Type: methodology
Statement: MindMap elicits the mind map of LLMs, revealing their reasoning pathways based on the ontology of knowledge.
Location: Introduction
Exact Quote: Our method elicits the mind map of LLMs, which reveals their reasoning pathways based on the ontology of knowledge.

Evidence:
- Evidence Text: MindMap generates a mind map that consolidates retrieved facts from KGs and implicit knowledge from LLMs.
  Strength: strong
  Location: Method
  Limitations: The mind map's interpretability and usefulness are not evaluated.
  Exact Quote: Specifically, MindMap sparks the graph of thoughts of LLMs that consolidates the retrieved facts from KGs and the implicit knowledge from LLMs.

- Evidence Text: MindMap discovers new patterns in input KGs.
  Strength: moderate
  Location: Method
  Limitations: The ability to discover new patterns is not evaluated.
  Exact Quote: Specifically, MindMap sparks the graph of thoughts of LLMs that discovers new patterns in input KGs.

- Evidence Text: MindMap reasons over the mind map to yield final outputs.
  Strength: moderate
  Location: Method
  Limitations: The reasoning process and its impact on final outputs are not evaluated.
  Exact Quote: Specifically, MindMap sparks the graph of thoughts of LLMs that reasons over the mind map to yield final outputs.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The methodology supports the claim that MindMap reasons over the mind map to yield final outputs.
Key Limitations: The reasoning process and its impact on final outputs are not evaluated.

--------------------------------------------------

Claim 3:
Type: performance
Statement: MindMap outperforms retrieval-based baselines in medical question answering tasks.
Location: Experiments
Exact Quote: MindMap outperforms retrieval-based baselines in medical question answering tasks.

Evidence:
- Evidence Text: MindMap achieves higher BERTScore and GPT-4 ranking scores than baselines on GenMedGPT-5k and CMCQA datasets.
  Strength: strong
  Location: Experiments
  Limitations: Performance on other datasets or tasks is not evaluated.
  Exact Quote: In Table 2, various methods are evaluated based on BERTScore, GPT-4 ranking scores, and hallucination quantification. While BERTScore shows similar results among methods, MindMap exhibits a slight improvement, possibly due to the shared tone in medical responses. However, for medical questions, comprehensive domain knowledge is crucial, not well-captured by BERTScore. GPT-4 ranking scores and hallucination quantification reveal that MindMap significantly outperforms others, with an average GPT-4 ranking of 1.8725 and low hallucination scores.

- Evidence Text: MindMap achieves higher accuracy than baselines on ExplainCPE dataset.
  Strength: strong
  Location: Experiments
  Limitations: Performance on other datasets or tasks is not evaluated.
  Exact Quote: Table 6 shows the accuracy scores for ExplainCPE. We calculate the rates of correct, wrong, and failed responses. MindMap outperforms other methods.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Experimental results demonstrate MindMap's superior performance in medical question answering tasks.
Key Limitations: Limited to three datasets and mismatched retrieval knowledge.

--------------------------------------------------

Claim 4:
Type: performance
Statement: MindMap is robust to mismatched retrieval knowledge.
Location: Experiments
Exact Quote: MindMap is robust to mismatched retrieval knowledge.

Evidence:
- Evidence Text: MindMap achieves higher accuracy than baselines on ExplainCPE dataset with mismatched retrieval knowledge.
  Strength: strong
  Location: Experiments
  Limitations: Performance on other datasets or tasks is not evaluated.
  Exact Quote: Table 6 shows the accuracy scores for ExplainCPE. We calculate the rates of correct, wrong, and failed responses. MindMap outperforms other methods.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Experimental results demonstrate MindMap's robustness to mismatched retrieval knowledge.
Key Limitations: Limited to three datasets and mismatched retrieval knowledge.

--------------------------------------------------

