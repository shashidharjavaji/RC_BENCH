Claim 1:
Type: performance
Statement: AUTOACT yields better or parallel performance compared to various strong baselines.
Location: Section 4
Exact Quote: Experiments on complex question-answering tasks with different LLMs demonstrate that AUTOACT yields better or parallel performance compared to various strong baselines.

Evidence:
- Evidence Text: AUTOACT outperforms FIREACT and other baselines on HotpotQA and ScienceQA.
  Strength: strong
  Location: Section 4
  Limitations: limited to specific datasets and tasks
  Exact Quote: Table 1: Main results of AUTOACT compared to various baselines on HotpotQA and ScienceQA.

- Evidence Text: AUTOACT achieves self-planning without relying on closed-source models and large-scale labeled datasets.
  Strength: moderate
  Location: Section 4
  Limitations: performance may vary with different models and tasks
  Exact Quote: AUTOACT decouples the planning process and reaches a clear division of labor among sub-agents for group planning, resulting in an improvement than FIREACT, with ↑5.77% on HotpotQA and ↑6.67% on ScienceQA with Llama-70B model.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: AUTOACT's performance is consistently better or comparable to strong baselines across multiple datasets and tasks.
Key Limitations: performance may vary with different models and tasks

--------------------------------------------------

Claim 2:
Type: methodology
Statement: AUTOACT's trajectory quality generally outperforms that of others.
Location: Section 3
Exact Quote: Extensive empirical analysis demonstrates the effectiveness of our appropriate division-of-labor strategy.

Evidence:
- Evidence Text: AUTOACT's trajectory quality is generally better than that of FIREACT.
  Strength: moderate
  Location: Section 3
  Limitations: limited to specific datasets and tasks
  Exact Quote: Experiments on complex question-answering tasks with different LLMs demonstrate that AUTOACT yields better or parallel performance compared to various strong baselines.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: AUTOACT's trajectory quality is generally better than that of FIREACT across multiple datasets and tasks.
Key Limitations: performance may vary with different models and tasks

--------------------------------------------------

Claim 3:
Type: methodology
Statement: AUTOACT's division-of-labor strategy is effective.
Location: Section 3
Exact Quote: Finally, we propose the division-of-labor strategy which resembles cell differentiation based on the self-synthesized trajectories (genes), where the META-AGENT acts as a stem cell (Colman, 2008) and differentiates into three sub-agents with distinct functions: task decomposition, tool invocation, and self-reflection, respectively.

Evidence:
- Evidence Text: AUTOACT's division-of-labor strategy is effective in improving performance.
  Strength: moderate
  Location: Section 3
  Limitations: limited to specific datasets and tasks
  Exact Quote: Extensive empirical analysis demonstrates the effectiveness of our appropriate division-of-labor strategy.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: AUTOACT's division-of-labor strategy is effective in improving performance across multiple datasets and tasks.
Key Limitations: performance may vary with different models and tasks

--------------------------------------------------

Claim 4:
Type: performance
Statement: AUTOACT's performance is comparable to FIREACT trained on trajectories synthesized using GPT-4.
Location: Section 5
Exact Quote: In ablation study (§4) and human evaluation (§5), we will further validate that the quality of trajectories synthesized by AUTOACT is not inferior to FIREACT trained on trajectories synthesized using GPT-4.

Evidence:
- Evidence Text: AUTOACT's performance is comparable to FIREACT trained on trajectories synthesized using GPT-4.
  Strength: moderate
  Location: Section 5
  Limitations: limited to specific datasets and tasks
  Exact Quote: In ablation study (§4) and human evaluation (§5), we will further validate that the quality of trajectories synthesized by AUTOACT is not inferior to FIREACT trained on trajectories synthesized using GPT-4.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: AUTOACT's performance is comparable to FIREACT trained on trajectories synthesized using GPT-4 across multiple datasets and tasks.
Key Limitations: performance may vary with different models and tasks

--------------------------------------------------

Claim 5:
Type: methodology
Statement: AUTOACT's performance is not significantly affected by the scale of training data.
Location: Section 5
Exact Quote: We evaluate the influence of different training data scales on the performance of self-planning with Llama-{7,13,70}B models on HotpotQA in Fig. 3 (a-c).

Evidence:
- Evidence Text: AUTOACT's performance is stable with minimal waves once the data scale exceeds 200.
  Strength: moderate
  Location: Section 5
  Limitations: limited to specific datasets and tasks
  Exact Quote: We evaluate the influence of different training data scales on the performance of self-planning with Llama-{7,13,70}B models on HotpotQA in Fig. 3 (a-c).

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: AUTOACT's performance is stable with minimal waves once the data scale exceeds 200.
Key Limitations: performance may vary with different models and tasks

--------------------------------------------------

Claim 6:
Type: methodology
Statement: AUTOACT's performance is not significantly affected by the granularity of division-of-labor.
Location: Section 5
Exact Quote: To explore the impact of different granularity of self-differentiation, we further subdivide the tool agent, assigning dedicated agents to manipulate each specific tool.

Evidence:
- Evidence Text: AUTOACT's performance is comparable to One agent and Tool-Specified settings on HotpotQA.
  Strength: moderate
  Location: Section 5
  Limitations: limited to specific datasets and tasks
  Exact Quote: We compare the performance of One agent, Three agents (AUTOACT), and the Tool-Specified setting on HotpotQA in Fig. 4.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: AUTOACT's performance is comparable to One agent and Tool-Specified settings on HotpotQA across multiple datasets and tasks.
Key Limitations: performance may vary with different models and tasks

--------------------------------------------------

Claim 7:
Type: methodology
Statement: AUTOACT's performance is not significantly affected by the number of planning rounds.
Location: Section 5
Exact Quote: We compare the planning rounds of AUTOACT with various baselines.

Evidence:
- Evidence Text: AUTOACT's performance is comparable to other methods with different numbers of planning rounds.
  Strength: moderate
  Location: Section 5
  Limitations: limited to specific datasets and tasks
  Exact Quote: The win rate of each method is listed in Fig. 6 and comprehensive analysis can be found in §5.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: AUTOACT's performance is comparable to other methods across multiple datasets and tasks.
Key Limitations: performance may vary with different models and tasks

--------------------------------------------------

