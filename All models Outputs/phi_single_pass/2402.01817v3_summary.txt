Claim 1:
Type: methodology/result
Statement: LLMs cannot, by themselves, do planning or self-verification
Location: Section 2
Exact Quote: Despite initial claims about the planning capabilities of LLMs (Bairi et al., 2023; Yao et al., 2023b; Shinn et al., 2023; Huang et al., 2023) several recent studies confirm that LLMs are not actually able to generate executable plans when they are used in autonomous modes.

Evidence:
- Evidence Text: On average, only about 12% of the plans that the best LLM (GPT-4) generates are actually executable without errors and goal-reaching.
  Strength: strong
  Location: Section 2.1
  Limitations: performance on specific planning problem instances
  Exact Quote: On average, only about 12% of the plans that the best LLM (GPT-4) generates are actually executable without errors and goal-reaching.

- Evidence Text: Performance deteriorates if the names of the actions and objects in the domain are obfuscated.
  Strength: moderate
  Location: Section 2.1
  Limitations: generalizability to other domains
  Exact Quote: Performance deteriorates if the names of the actions and objects in the domain are obfuscated–a change that doesn’t in any way affect the performance of the standard AI planners.

- Evidence Text: LLMs are no better at verifying solutions.
  Strength: strong
  Location: Section 2.2
  Limitations: dependence on external verifiers
  Exact Quote: More interestingly, they are no better at verifying solutions.

- Evidence Text: LLMs cannot self-improve by generating synthetic data.
  Strength: strong
  Location: Section 2.3
  Limitations: lack of self-critique and self-improvement capabilities
  Exact Quote: Contrary to their claim of “self-improvement”, works like (Wang et al., 2022) actually heavily depend on external knowledge (crafted seed examples) and critics (filtering step).

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided from multiple studies and experiments supports the claim that LLMs lack planning and self-verification capabilities.
Key Limitations: The limitations mentioned are specific to the experimental setup and may not generalize to all LLMs or planning tasks.

--------------------------------------------------

Claim 2:
Type: contribution
Statement: LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end format translators.
Location: Section 1
Exact Quote: We argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end format translators.

Evidence:
- Evidence Text: LLMs can be a rich source of approximate models of world/domain dynamics and user preferences.
  Strength: moderate
  Location: Section 3.4
  Limitations: requires human verification and refinement
  Exact Quote: The fact that LLMs are often good at extracting planning knowledge can indeed be gainfully leveraged.

- Evidence Text: LLM-Modulo framework avoids inheriting the expressiveness and search-complexity limitations of traditional symbolic planners, while retaining their soundness guarantees.
  Strength: moderate
  Location: Section 3.5
  Limitations: requires external verifiers and human interaction
  Exact Quote: The LLM-Modulo architecture puts no such restrictions.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence provided supports the claim that LLMs can play more meaningful roles in planning/reasoning tasks when combined with external verifiers and human interaction.
Key Limitations: The limitations mentioned are specific to the LLM-Modulo framework and may not apply to other approaches.

--------------------------------------------------

Claim 3:
Type: methodology/result
Statement: LLM-Modulo Framework combines the strengths of LLMs with external model-based verifiers in a tighter bi-directional interaction regime.
Location: Section 3
Exact Quote: We present a vision of LLM-Modulo Frameworks that combines the strengths of LLMs with external model-based verifiers in a tighter bi-directional interaction regime.

Evidence:
- Evidence Text: LLM-Modulo framework leverages the idea generation/approximate knowledge provision capabilities of LLMs with external verifiers.
  Strength: moderate
  Location: Section 3.4
  Limitations: requires human interaction for model acquisition and specification refinement
  Exact Quote: The LLM-Modulo architecture is a 'Generate-Test-Critique' loop, with the LLM generating candidate plans and a bank of critics critiquing the candidate.

- Evidence Text: LLM-Modulo framework avoids inheriting the expressiveness and search-complexity limitations of traditional symbolic planners, while retaining their soundness guarantees.
  Strength: moderate
  Location: Section 3.5
  Limitations: requires external verifiers and human interaction
  Exact Quote: The LLM-Modulo architecture puts no such restrictions.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence provided supports the claim that LLM-Modulo framework combines the strengths of LLMs with external verifiers in a tighter bi-directional interaction regime.
Key Limitations: The limitations mentioned are specific to the LLM-Modulo framework and may not apply to other approaches.

--------------------------------------------------

Claim 4:
Type: methodology/result
Statement: The models driving the external verifiers themselves can be acquired with the help of LLMs.
Location: Section 3
Exact Quote: We will show how the models driving the external verifiers themselves can be acquired with the help of LLMs.

Evidence:
- Evidence Text: LLMs can be a rich source of approximate models of world/domain dynamics and user preferences.
  Strength: moderate
  Location: Section 3.4
  Limitations: requires human verification and refinement
  Exact Quote: The fact that LLMs are often good at extracting planning knowledge can indeed be gainfully leveraged.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence provided supports the claim that LLMs can help acquire models for external verifiers.
Key Limitations: The limitations mentioned are specific to the LLM-Modulo framework and may not apply to other approaches.

--------------------------------------------------

Claim 5:
Type: contribution
Statement: LLM-Modulo Framework provides a better neuro-symbolic approach that offers tighter integration between LLMs and symbolic components.
Location: Section 3
Exact Quote: We will also argue that rather than simply pipelining LLMs and symbolic components, this LLM-Modulo Framework provides a better neuro-symbolic approach that offers tighter integration between LLMs and symbolic components.

Evidence:
- Evidence Text: LLM-Modulo framework avoids inheriting the expressiveness and search-complexity limitations of traditional symbolic planners, while retaining their soundness guarantees.
  Strength: moderate
  Location: Section 3.5
  Limitations: requires external verifiers and human interaction
  Exact Quote: The LLM-Modulo architecture puts no such restrictions.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence provided supports the claim that LLM-Modulo framework provides a better neuro-symbolic approach.
Key Limitations: The limitations mentioned are specific to the LLM-Modulo framework and may not apply to other approaches.

--------------------------------------------------

Claim 6:
Type: contribution
Statement: LLM-Modulo Framework extends the scope of model-based planning/reasoning regimes towards more flexible knowledge, problem and preference specifications.
Location: Section 3
Exact Quote: We will also argue that rather than simply pipelining LLMs and symbolic components, this LLM-Modulo Framework provides a better neuro-symbolic approach that offers tighter integration between LLMs and symbolic components, extending the scope of model-based planning/reasoning regimes towards more flexible knowledge, problem and preference specifications.

Evidence:
- Evidence Text: The LLM-Modulo architecture is a 'Generate-Test-Critique' loop, with the LLM generating candidate plans and a bank of critics critiquing the candidate.
  Strength: moderate
  Location: Section 3.4
  Limitations: requires human interaction for model acquisition and specification refinement
  Exact Quote: The LLM-Modulo architecture is a 'Generate-Test-Critique' loop, with the LLM generating candidate plans and a bank of critics critiquing the candidate.

- Evidence Text: The LLM-Modulo framework avoids inheriting the expressiveness and search-complexity limitations of traditional symbolic planners, while retaining their soundness guarantees.
  Strength: moderate
  Location: Section 3.5
  Limitations: requires external verifiers and human interaction
  Exact Quote: The LLM-Modulo architecture puts no such restrictions.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence provided supports the claim that LLM-Modulo framework extends the scope of model-based planning/reasoning regimes.
Key Limitations: The limitations mentioned are specific to the LLM-Modulo framework and may not apply to other approaches.

--------------------------------------------------

Claim 7:
Type: contribution
Statement: LLM-Modulo Framework can be applied to a wide variety of planning or reasoning tasks.
Location: Section 5
Exact Quote: While we focused on PDDL planning tasks for the sake of concreteness, we believe that the essence of LLM-Modulo framework is equally applicable to other scenarios involving planning and reasoning.

Evidence:
- Evidence Text: We have applied the LLM-Modulo framework to classical planning domains and to a recent travel planning benchmark.
  Strength: moderate
  Location: Section 5
  Limitations: preliminary results, limited to specific domains
  Exact Quote: We have applied the LLM-Modulo framework to classical planning domains (as reported in (Valmeekam et al., 2023c)) and to a recent travel planning benchmark (as reported in (Gundawar et al., 2024)).

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence provided supports the claim that LLM-Modulo framework can be applied to a wide variety of planning or reasoning tasks.
Key Limitations: The evidence provided is limited to specific domains and preliminary results.

--------------------------------------------------

