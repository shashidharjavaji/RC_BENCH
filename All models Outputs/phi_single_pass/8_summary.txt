Claim 1:
Type: result
Statement: REPLUG can improve the performance of GPT-3 (175B) on language modeling by 6.3%.
Location: Abstract
Exact Quote: REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%.

Evidence:
- Evidence Text: REPLUG with the tuned retriever improves GPT-3 (175B) language modeling by 6.3%.
  Strength: strong
  Location: Abstract
  Limitations: specific experimental setup and dataset used
  Exact Quote: REPLUG with the tuned retriever significantly improves the performance of GPT-3 (175B) on language modeling by 6.3%.

- Evidence Text: REPLUG LSR improves GPT-3 (175B) language modeling by 7.7%.
  Strength: strong
  Location: Table 1
  Limitations: specific experimental setup and dataset used
  Exact Quote: REPLUG LSR results in 7.7% improvement over baselines compared to 4.7% improvement of REPLUG averaged over the 8 models.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The results are based on experiments conducted on the Pile dataset, which is a diverse language modeling benchmark. The improvement in perplexity is a direct measure of the model's performance.
Key Limitations: The results are specific to the Pile dataset and may not generalize to other datasets or tasks.

--------------------------------------------------

Claim 2:
Type: result
Statement: REPLUG with the tuned retriever improves Codex (175B) on MMLU by 5.1%.
Location: Abstract
Exact Quote: REPLUG with the tuned retriever improves Codex (175B) on five-shot MMLU by 5.1%.

Evidence:
- Evidence Text: REPLUG LSR improves Codex (175B) performance on MMLU by 5.1%.
  Strength: strong
  Location: Table 2
  Limitations: specific experimental setup and dataset used
  Exact Quote: REPLUG LSR improves the original Codex by 5.1%.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The results are based on experiments conducted on the MMLU dataset, which is a multiple choice QA dataset. The improvement in accuracy is a direct measure of the model's performance.
Key Limitations: The results are specific to the MMLU dataset and may not generalize to other datasets or tasks.

--------------------------------------------------

Claim 3:
Type: result
Statement: REPLUG LSR outperforms Atlas, a retrieval-augmented language model, on MMLU.
Location: Abstract
Exact Quote: REPLUG LSR largely outperforms the previous retrieval-augmented language model, Atlas, demonstrating the effectiveness of our black-box retrieval language model setting.

Evidence:
- Evidence Text: REPLUG LSR improves Codex (175B) performance on MMLU by 5.1%, while Atlas improves by 4.6%.
  Strength: strong
  Location: Table 2
  Limitations: specific experimental setup and dataset used
  Exact Quote: REPLUG LSR improves the original Codex by 5.1%.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The results are based on experiments conducted on the MMLU dataset, which is a multiple choice QA dataset. The improvement in accuracy is a direct measure of the model's performance.
Key Limitations: The results are specific to the MMLU dataset and may not generalize to other datasets or tasks.

--------------------------------------------------

Claim 4:
Type: result
Statement: REPLUG LSR improves Codex on NQ and TQA.
Location: Abstract
Exact Quote: REPLUG LSR significantly improves the performance of the original Codex by 12.0% on NQ and 5.0% on TQA.

Evidence:
- Evidence Text: REPLUG LSR improves Codex (175B) performance on NQ by 12.0% and on TQA by 5.0%.
  Strength: strong
  Location: Table 3
  Limitations: specific experimental setup and dataset used
  Exact Quote: REPLUG LSR significantly improves the performance of the original Codex by 12.0% on NQ and 5.0% on TQA.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The results are based on experiments conducted on the NQ and TQA datasets, which are open-domain QA datasets. The improvement in accuracy is a direct measure of the model's performance.
Key Limitations: The results are specific to the NQ and TQA datasets and may not generalize to other datasets or tasks.

--------------------------------------------------

Claim 5:
Type: contribution
Statement: REPLUG is applicable to diverse language models.
Location: Section 7.1
Exact Quote: We further study whether REPLUG could enhance diverse language model families that have been pre-trained using different data and methods.

Evidence:
- Evidence Text: We evaluate each model on Wikitext-103 test data and report its perplexity.
  Strength: moderate
  Location: Section 7.1
  Limitations: specific experimental setup and dataset used
  Exact Quote: We evaluate each model on Wikitext-103 test data and report its perplexity.

- Evidence Text: REPLUG improves the perplexity of all the model families.
  Strength: strong
  Location: Section 7.1
  Limitations: specific experimental setup and dataset used
  Exact Quote: REPLUG improves the perplexity of all the model families, which indicates that REPLUG is applicable to diverse language models.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The results are based on experiments conducted on the Wikitext-103 dataset, which is a diverse language modeling benchmark. The improvement in perplexity is a direct measure of the model's performance.
Key Limitations: The results are specific to the Wikitext-103 dataset and may not generalize to other datasets or tasks.

--------------------------------------------------

Claim 6:
Type: result
Statement: REPLUG performance gain does not simply come from the ensembling effect.
Location: Section 7.2
Exact Quote: We observed that ensembling random documents leads to worse performance, indicating that the performance gains of REPLUG do not come from the ensembling effect.

Evidence:
- Evidence Text: Ensembling random documents leads to worse performance.
  Strength: strong
  Location: Section 7.2
  Limitations: specific experimental setup and dataset used
  Exact Quote: Ensembling random documents leads to worse performance.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The results are based on experiments conducted on the Pile dataset, which is a diverse language modeling benchmark. The comparison between ensembling random documents and ensembling retrieved documents shows that the performance gains of REPLUG are due to the relevance of the retrieved documents.
Key Limitations: The results are specific to the Pile dataset and may not generalize to other datasets or tasks.

--------------------------------------------------

Claim 7:
Type: result
Statement: LSR retriever outperforms other off-the-shelf retrievers.
Location: Section 7.3
Exact Quote: Among all off-the-shelf retrievers, the sparse retriever BM25 performs best. However, it still lags behind our LM-supervised retriever (Contriever LSR), demonstrating the effectiveness of our training scheme that adapts the retriever to LMs.

Evidence:
- Evidence Text: BM25 performs best among off-the-shelf retrievers, but lags behind LM-supervised Contriever LSR.
  Strength: strong
  Location: Section 7.3
  Limitations: specific experimental setup and dataset used
  Exact Quote: Among all off-the-shelf retrievers, the sparse retriever BM25 performs best. However, it still lags behind our LM-supervised retriever (Contriever LSR).

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The results are based on experiments conducted on the Wikitext-103 dataset, which is a diverse language modeling benchmark. The comparison between BM25 and LM-supervised Contriever LSR shows that the performance gains of REPLUG are due to the relevance of the retrieved documents.
Key Limitations: The results are specific to the Wikitext-103 dataset and may not generalize to other datasets or tasks.

--------------------------------------------------

