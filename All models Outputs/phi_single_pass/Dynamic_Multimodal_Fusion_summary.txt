Claim 1:
Type: performance
Statement: Dynamic multimodal fusion (DynMM) can reduce computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis).
Location: Abstract
Exact Quote: DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis)

Evidence:
- Evidence Text: Results on CMU-MOSEI Sentiment Analysis show a computation reduction of 46.5% with a slight decrease in accuracy.
  Strength: strong
  Location: Section 4.3
  Limitations: The claim does not specify the exact accuracy loss, which is necessary to fully evaluate the trade-off.
  Exact Quote: DynMM-a 79.07 0.62 165.5

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence from the sentiment analysis task supports the claim of reduced computation with minimal accuracy loss.
Key Limitations: The claim does not provide the exact figure for accuracy loss.

--------------------------------------------------

Claim 2:
Type: performance
Statement: DynMM improves segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation).
Location: Abstract
Exact Quote: improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation)

Evidence:
- Evidence Text: Results on NYU Depth V2 semantic segmentation show a computation reduction of 55.1% with a slight decrease in mIoU.
  Strength: strong
  Location: Section 4.4
  Limitations: The claim specifies over 21% savings, but the actual computation reduction is 55.1%, which is higher.
  Exact Quote: DynMM-a ResNet-50 49.9 **43.4**

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence from the semantic segmentation task supports the claim of reduced computation.
Key Limitations: The claim underestimates the computation savings.

--------------------------------------------------

Claim 3:
Type: methodology
Statement: DynMM can adaptively fuse multimodal data and generate data-dependent forward paths during inference.
Location: Introduction
Exact Quote: we propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses multimodal data and generates data-dependent forward paths during inference.

Evidence:
- Evidence Text: The paper introduces a gating function and a resource-aware loss function to enable adaptive fusion.
  Strength: moderate
  Location: Section 3
  Limitations: The explanation of how the gating function and loss function work together is not detailed in the abstract.
  Exact Quote: To this end, we propose a gating function to provide modality-level or fusion-level decisions on-the-fly based on multimodal features and a resource-aware loss function that encourages computational efficiency.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The introduction of the gating function and loss function supports the claim of adaptive fusion.
Key Limitations: The abstract does not provide detailed evidence of the adaptive process.

--------------------------------------------------

Claim 4:
Type: contribution
Statement: DynMM enjoys the benefits of reduced computation, improved representation power, and robustness.
Location: Introduction
Exact Quote: dynamic fusion leads to computational savings for 'easy' inputs and preserve representation power for 'hard' instances.

Evidence:
- Evidence Text: Experiments on various tasks demonstrate efficiency and wide applicability.
  Strength: strong
  Location: Section 4
  Limitations: The claim is broad and would benefit from more specific examples or metrics.
  Exact Quote: Results on various multimodal tasks demonstrate the efficiency and wide applicability of our approach.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence from multiple tasks supports the claim of benefits.
Key Limitations: The claim is general and could be more specific.

--------------------------------------------------

Claim 5:
Type: methodology
Statement: DynMM can match the representation power of a static network by relying on all modalities and complex fusion operations for 'hard' instances.
Location: Introduction
Exact Quote: For 'hard' multimodal inputs, DynMM can match the representation power of a static network by relying on all modalities and complex fusion operations for prediction.

Evidence:
- Evidence Text: Experiments on semantic segmentation show that DynMM can improve performance with complex fusion operations.
  Strength: moderate
  Location: Section 4.4
  Limitations: The claim is specific to semantic segmentation and may not generalize to other tasks.
  Exact Quote: DynMM-b ResNet-50 **51.0** 52.2

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence from semantic segmentation supports the claim for 'hard' instances.
Key Limitations: The claim is specific to semantic segmentation.

--------------------------------------------------

Claim 6:
Type: contribution
Statement: DynMM provides a new direction towards dynamic multimodal network design.
Location: Introduction
Exact Quote: We believe our approach opens a new direction towards dynamic multimodal network design.

Evidence:
- Evidence Text: The paper presents a novel approach and demonstrates its efficacy on various tasks.
  Strength: strong
  Location: Conclusion
  Limitations: The claim is based on the novelty of the approach and its performance on a limited set of tasks.
  Exact Quote: We believe our approach opens a new direction towards dynamic multimodal network design.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The novelty of the approach and its demonstrated performance support the claim.
Key Limitations: The claim is based on a limited set of tasks and future work is needed to generalize the approach.

--------------------------------------------------

Claim 7:
Type: contribution
Statement: DynMM can be applied to a wide range of multimodal tasks.
Location: Introduction
Exact Quote: with applications to a wide range of multimodal tasks.

Evidence:
- Evidence Text: The paper demonstrates the approach on three different tasks: movie genre classification, sentiment analysis, and semantic segmentation.
  Strength: moderate
  Location: Section 4
  Limitations: The claim is based on a limited set of tasks and future work is needed to generalize the approach.
  Exact Quote: For instance, DynMM strikes a good balance between computational efficiency and learning performance.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence from multiple tasks supports the claim.
Key Limitations: The claim is based on a limited set of tasks.

--------------------------------------------------

Claim 8:
Type: performance
Statement: DynMM can reduce noise and improve robustness in multimodal data.
Location: Conclusion
Exact Quote: DynMM is more robust to noisy multimodal data compared with the static ESANet.

Evidence:
- Evidence Text: Experiments show that DynMM is more robust to noise in depth images compared to ESANet.
  Strength: moderate
  Location: Section 4.4
  Limitations: The claim is specific to noise in depth images and may not generalize to other types of noise or modalities.
  Exact Quote: The performance gap between DynMM and ESANet becomes larger when the noise level of depth images increases.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence from the noise robustness experiment supports the claim.
Key Limitations: The claim is specific to noise in depth images.

--------------------------------------------------

