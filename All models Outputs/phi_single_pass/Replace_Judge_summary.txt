Claim 1:
Type: result
Statement: Using a Panel of LLm evaluators (PoLL) correlates better with human judgements compared to a single large judge (GPT-4).
Location: Section 4.1
Exact Quote: We find that using an instantiation of PoLL correlates better with human judgements compared to a single large judge (GPT-4), while being over seven times cheaper.

Evidence:
- Evidence Text: Cohen’s κ correlation with human judgements
  Strength: strong
  Location: Section 4.1
  Limitations: correlation does not imply causation
  Exact Quote: PoLL **0.917** **0.778**

- Evidence Text: Pearson and Kendall-Tau correlations with human judgements
  Strength: strong
  Location: Section 4.2
  Limitations: correlation does not imply causation
  Exact Quote: PoLL is best correlated with the gold rankings, particularly at the top of the ranked list.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence shows strong correlation with human judgements, which supports the claim that PoLL is a better evaluator.
Key Limitations: Correlation does not imply causation.

--------------------------------------------------

Claim 2:
Type: result
Statement: Using PoLL reduces intra-model bias.
Location: Section 4.4
Exact Quote: We observe that overall, PoLL has the smallest spread in scores, with a standard deviation of 2.2, compared to EM and individual judges.

Evidence:
- Evidence Text: Standard deviation of scores
  Strength: moderate
  Location: Section 4.4
  Limitations: Standard deviation alone does not fully capture bias.
  Exact Quote: PoLL has the smallest spread in scores, with a standard deviation of 2.2

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The smaller spread in scores suggests reduced bias, but standard deviation alone does not fully capture bias.
Key Limitations: Standard deviation alone does not fully capture bias.

--------------------------------------------------

Claim 3:
Type: result
Statement: Using PoLL is over seven times less expensive than using GPT-4.
Location: Section 4.5
Exact Quote: Depending on the ratio of input-to-output tokens in a given task, running the entire three model PoLL is seven to eight times less expensive than running a single GPT-4 judge.

Evidence:
- Evidence Text: Cost comparison
  Strength: strong
  Location: Section 4.5
  Limitations: Costs may vary based on input-to-output token ratio.
  Exact Quote: Depending on the ratio of input-to-output tokens in a given task, running the entire three model PoLL is seven to eight times less expensive than running a single GPT-4 judge.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The cost comparison is clear and provides strong evidence for the claim.
Key Limitations: Costs may vary based on input-to-output token ratio.

--------------------------------------------------

Claim 4:
Type: result
Statement: PoLL performs well consistently across different settings.
Location: Section 5
Exact Quote: PoLL is not only an effective method for evaluating LLM performance, but also reduces intra-model bias, latency, and cost.

Evidence:
- Evidence Text: Performance across three evaluator settings
  Strength: moderate
  Location: Section 5
  Limitations: Limited number of settings and models evaluated.
  Exact Quote: In this paper, we showed how a Panel of LLM Evaluators composed of smaller models is not only an effective method for evaluating LLM performance, but also reduces intra-model bias, latency, and cost.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the results presented, but the number of settings and models evaluated is limited.
Key Limitations: Limited number of settings and models evaluated.

--------------------------------------------------

