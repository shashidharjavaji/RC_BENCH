Claim 1:
Type: result
Statement: Popular NLI models rely heavily on shallow heuristics rather than true understanding of language structure
Location: Results section (Section 5)
Exact Quote: All models achieved high scores on the MNLI test set... However, they all performed poorly—with accuracies less than 10% in most cases, when chance is 50%—on the cases where the heuristics make incorrect predictions

Evidence:
- Evidence Text: Performance on HANS non-entailment cases <10% for all models
  Strength: strong
  Location: Results section, Figure 1b
  Limitations: Limited to specific heuristics tested
  Exact Quote: they all performed poorly—with accuracies less than 10% in most cases, when chance is 50%—on the cases where the heuristics make incorrect predictions

- Evidence Text: High performance on MNLI test set
  Strength: strong
  Location: Results section, Figure 1a
  Limitations: MNLI may not represent full range of inference types
  Exact Quote: All models achieved high scores on the MNLI test set

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Consistent poor performance across multiple models on targeted test cases, despite high MNLI scores
Key Limitations: Limited to specific heuristic types tested

--------------------------------------------------

Claim 2:
Type: result
Statement: Models' poor performance stems more from insufficient signal in training data than architectural limitations
Location: Discussion section
Exact Quote: Other sources of evidence suggest that the models' failure is due in large part to insufficient signal from the MNLI training set, rather than the models' representational capacities alone.

Evidence:
- Evidence Text: BERT's strong syntactic abilities but poor HANS performance
  Strength: moderate
  Location: Discussion section
  Limitations: Indirect evidence through previous work
  Exact Quote: Despite this evidence that BERT has access to relevant syntactic information, its accuracy was 0% on the subject-object swap cases

- Evidence Text: Improved performance with augmented training data
  Strength: strong
  Location: Section 7
  Limitations: Limited exploration of training conditions
  Exact Quote: In general, the models trained on the augmented MNLI performed very well on HANS

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Multiple pieces of evidence support training data as key factor, but some reliance on indirect evidence
Key Limitations: Limited exploration of different training approaches

--------------------------------------------------

Claim 3:
Type: performance
Statement: BERT performs better than other models on lexical overlap and constituent cases
Location: Results section
Exact Quote: BERT did slightly worse than SPINN on the subsequence cases, but performed noticeably less poorly than all other models at both the constituent and lexical overlap cases

Evidence:
- Evidence Text: Comparative model performance numbers
  Strength: strong
  Location: Results section
  Limitations: Still poor absolute performance
  Exact Quote: Within the lexical overlap cases, BERT achieved 39% accuracy on conjunction...but 0% accuracy on subject/object swap

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Direct comparative performance numbers support claim
Key Limitations: Better relative performance still represents poor absolute performance

--------------------------------------------------

