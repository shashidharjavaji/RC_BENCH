Claim 1:
Type: performance
Statement: Yorùbá language performance in reading comprehension tasks is consistently worse than English across multiple LLM models
Location: Section 3 Experiments
Exact Quote: Table 4 reports the results showing that Yorùbá consistently performs worse than English (e.g., losing 0.4 in Rouge-1)

Evidence:
- Evidence Text: Rouge scores across three models show lower performance for Yorùbá
  Strength: strong
  Location: Table 4
  Limitations: Limited to 3 models and automatic metrics only
  Exact Quote: GPT4O ENG/YOR: 0.39/0.34 (R-1), 0.23/0.19 (R-2), 0.30/0.27 (R-L)

- Evidence Text: Performance gap in comparable length documents
  Strength: moderate
  Location: Table 5
  Limitations: Very small sample size of only 6 comparable documents
  Exact Quote: ENG: 0.45/0.23/0.30 vs YOR: 0.32/0.09/0.19 for R-1/R-2/R-L

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Consistent pattern across multiple models and metrics, though limited by dataset size
Key Limitations: Small comparable document set, reliance on automatic metrics only

--------------------------------------------------

Claim 2:
Type: result
Statement: There are accuracy discrepancies in Wikipedia articles across languages
Location: Section 1 Introduction
Exact Quote: we identify inaccuracies in the English-language version of some Wikipedia articles (26 incorrect answers out of 1,566 humanly analyzed questions in the English-language subset of articles)

Evidence:
- Evidence Text: Manual annotation findings of incorrect English answers
  Strength: moderate
  Location: Section 2.2 Dataset creation
  Limitations: Limited to specific topic areas, may not be representative
  Exact Quote: questions with correct answers in Yorùbá, but incorrect in English, where they annotated the Yorùbá appropriately, but flagged the English portion incorrect (there were 26 questions in the category)

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Direct human annotation provides reliable evidence, though scope is limited
Key Limitations: Sample size relatively small compared to full Wikipedia

--------------------------------------------------

Claim 3:
Type: performance
Statement: Model performance deteriorates with longer document lengths in Yorùbá
Location: Section 3 Experiments
Exact Quote: We can see a drop in performance when the Yorùbá documents reach 1,500 words, which shows the challenges that current models face in long-context understanding of low-resource languages

Evidence:
- Evidence Text: Length analysis showing performance decline
  Strength: moderate
  Location: Section 3/Figure 1
  Limitations: Specific threshold point, may vary by model
  Exact Quote: Model performance changes with the length of the document, as shown in Figure 1

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Clear performance pattern shown in analysis, though specific to tested models
Key Limitations: Limited number of long documents in Yorùbá for comparison

--------------------------------------------------

