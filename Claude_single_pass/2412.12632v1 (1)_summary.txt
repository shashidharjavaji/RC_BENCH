Claim 1:
Type: result
Statement: External knowledge equipped with CoE can more effectively help LLMs generate correct answers compared to Non-CoE when dealing with irrelevant information
Location: Section 5.2 - Finding-1
Exact Quote: External knowledge equipped with CoE can help LLMs generate correct answers more effectively than Non-CoE

Evidence:
- Evidence Text: CoE achieves average accuracy of 92.0% across five LLMs and two datasets, outperforming Non-CoE variants SenP and WordP by 22.5% and 16.3% respectively
  Strength: strong
  Location: Section 5.2
  Limitations: Limited to specific datasets and LLM models tested
  Exact Quote: experimental results show that CoE achieves an average accuracy of 92.0% across five LLMs and two datasets, outperforming Non-CoE variants SenP and WordP by 22.5% and 16.3%, respectively

- Evidence Text: Statistical significance shown through Mann-Whitney tests
  Strength: strong
  Location: Section 5.2
  Limitations: Specific statistical threshold not provided
  Exact Quote: Mann-Whitney tests on all experiment groups of Non-CoE. The results of the hypothesis test show that the improvement in CoE across all types of Non-CoE is statistically significant

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Clear performance improvements shown across multiple models and datasets with statistical significance
Key Limitations: Limited to specific experimental settings and models tested

--------------------------------------------------

Claim 2:
Type: result
Statement: LLMs exhibit higher faithfulness to answers supported by CoE compared to Non-CoE even when CoE contains factual errors
Location: Section 6.2 - Finding-3
Exact Quote: LLMs exhibit significant faithfulness to the answer supported by CoE although it contains factual errors

Evidence:
- Evidence Text: Under CoE, average Following Rate reaches 85.4%, which is 20.6% and 16.2% higher than SenP and WordP types under Non-CoE
  Strength: strong
  Location: Section 6.2
  Limitations: May raise concerns about LLMs being too faithful to incorrect information
  Exact Quote: under CoE, the average FR reaches 85.4%, which is 20.6% and 16.2% higher than the SenP and WordP types under Non-CoE respectively

- Evidence Text: Statistical significance through Mann-Whitney tests
  Strength: strong
  Location: Section 6.2
  Limitations: Specific p-value not provided
  Exact Quote: Mann-Whitney tests confirmed statistically significant improvements of CoE over all Non-CoE groups (p < 0.05)

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Clear evidence of higher faithfulness rates with statistical significance
Key Limitations: High faithfulness to incorrect information could be problematic

--------------------------------------------------

Claim 3:
Type: result
Statement: CoE-guided retrieval strategy improves LLM accuracy in naive RAG framework
Location: Section 8.4 - Finding-7
Exact Quote: For the subject case, CoE-guided retrieval could improve the LLMs' accuracy in the naive framework

Evidence:
- Evidence Text: RAG+ScopeCoE achieves average ACC of 77.8% and 81.6% on HotpotQA and 2WikiMultihopQA, outperforming RAG by 10.4% and 28.7%
  Strength: strong
  Location: Section 8.4
  Limitations: Limited to specific RAG implementation and datasets
  Exact Quote: RAG+ScopeCoE achieves average ACC of 77.8% and 81.6% on HotpotQA and 2WikiMultihopQA respectively, outperforming RAG by 10.4% and 28.7%

- Evidence Text: More efficient knowledge utilization with fewer knowledge pieces (4.6/4.8 vs 5)
  Strength: moderate
  Location: Section 8.4
  Limitations: Marginal difference in number of pieces used
  Exact Quote: ScopeCoE can help LLMs generate more accurate outputs with fewer knowledge pieces (4.6 for HotpotQA and 4.8 for 2WikiMultihopQA) compared to the naive framework (5 pieces)

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Clear performance improvements shown, though limited to specific implementation
Key Limitations: Only tested on one RAG implementation and specific datasets

--------------------------------------------------

