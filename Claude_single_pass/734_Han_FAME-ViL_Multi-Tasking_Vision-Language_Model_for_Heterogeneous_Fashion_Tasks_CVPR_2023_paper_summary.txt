Claim 1:
Type: performance
Statement: FAME-ViL saves 61.5% parameters while outperforming single-task models
Location: Introduction
Exact Quote: Extensive experiments on four fashion tasks show that our method significantly outperforms the previous single-task state-of-the-art with 61.5% parameter saving

Evidence:
- Evidence Text: Parameter savings quantification across tasks
  Strength: strong
  Location: Results Tables 1-3
  Limitations: Exact calculation methodology not detailed
  Exact Quote: FAME-ViL saves 67.33% parameters compared to baseline models while achieving superior performance across all metrics

- Evidence Text: Performance improvements across tasks
  Strength: strong
  Location: Tables 1-3
  Limitations: Some improvements are modest
  Exact Quote: FAME-ViL outperforms FAME-ViL(ST) on XMR (83.14 vs 82.61), TGIR (58.29 vs 55.36), SCR (91.44 vs 90.27), and FIC (65.50 vs 63.67)

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Comprehensive experimental results across multiple tasks demonstrate both parameter savings and performance improvements
Key Limitations: Exact parameter calculation details missing

--------------------------------------------------

Claim 2:
Type: methodology
Statement: Multi-teacher distillation effectively prevents negative transfer in multi-task learning
Location: Section 4.3
Exact Quote: Without MTD, the baseline MTL model is prone to overfit on TGIR after about 20k iterations due to much less training data than other tasks

Evidence:
- Evidence Text: Validation performance curves
  Strength: strong
  Location: Figure 5
  Limitations: Limited to specific tasks and architectures
  Exact Quote: Encouragingly, our MTD yields consistent and significant performance boost per task, rendering it a more stable and effective learning strategy

- Evidence Text: Comparative analysis with other methods
  Strength: moderate
  Location: Section 4.3
  Limitations: Limited comparison methods
  Exact Quote: Neither IAS nor IMTLG can improve over the baseline MTL, regardless of overfitting or not

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Evidence shows clear improvement in preventing overfitting compared to baselines
Key Limitations: Limited comparison to alternative approaches

--------------------------------------------------

Claim 3:
Type: methodology
Statement: Cross-Attention Adapter (XAA) enables effective cross-modality interaction
Location: Section 3.1
Exact Quote: We construct an XAA module by further adding another Multi-Head Cross Attention (MHXA) layer at the bottom of a TSA

Evidence:
- Evidence Text: Ablation study results
  Strength: strong
  Location: Table 4
  Limitations: Limited architectural variations tested
  Exact Quote: XAA can bring in 3%-6% relative improvements for XMR and TGIR

- Evidence Text: Complementary effects with TSA
  Strength: moderate
  Location: Section 4.2
  Limitations: Specific to proposed architecture
  Exact Quote: When TSA and XAA work together, the model can achieve better relative performance than the sum of their gains

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Ablation studies demonstrate clear benefits of XAA, especially in combination with TSA
Key Limitations: Limited exploration of alternative cross-attention mechanisms

--------------------------------------------------

