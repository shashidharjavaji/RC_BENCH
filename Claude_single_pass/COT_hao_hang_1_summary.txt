Claim 1:
Type: result
Statement: Chain-of-thought prompting is an emergent ability that only appears in language models above ~100B parameters
Location: Section 3.2
Exact Quote: chain-of-thought prompting is an emergent ability of model scale. That is, chain-of-thought prompting does not positively impact performance for small models, and only yields performance gains when used with models of ~100B parameters.

Evidence:
- Evidence Text: Performance gains shown across multiple model sizes in Figure 4
  Strength: strong
  Location: Figure 4
  Limitations: Limited to three model families (LaMDA, GPT, PaLM)
  Exact Quote: Figure 4: Chain-of-thought prompting enables large language models to solve challenging math problems. Notably, chain-of-thought reasoning is an emergent ability of increasing model scale.

- Evidence Text: Qualitative analysis of smaller models' outputs
  Strength: moderate
  Location: Section 3.2
  Limitations: Qualitative only, details of analysis not provided
  Exact Quote: We qualitatively found that models of smaller scale produced fluent but illogical chains of thought, leading to lower performance than standard prompting.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Consistent pattern shown across multiple model families and tasks, with both quantitative and qualitative evidence
Key Limitations: Limited number of model families tested, exact parameter threshold not precisely determined

--------------------------------------------------

Claim 2:
Type: performance
Statement: Chain-of-thought prompting achieves state-of-the-art performance on several math reasoning benchmarks
Location: Section 3.2
Exact Quote: Third, chain-of-thought prompting via GPT-3 175B and PaLM 540B compares favorably to prior state of the art, which typically finetunes a task-specific model on a labeled training dataset.

Evidence:
- Evidence Text: PaLM 540B results on GSM8K, SVAMP, and MAWPS benchmarks
  Strength: strong
  Location: Table 1
  Limitations: Results only from largest model variant
  Exact Quote: PaLM with chain-of-thought prompting reaches within 2% of the state of the art (Appendix Table 2).

- Evidence Text: Detailed performance comparison across model sizes and types
  Strength: strong
  Location: Table 1
  Limitations: External calculator needed for best results
  Exact Quote: As shown in Table 1, we see that adding a calculator significantly boosts performance of chain-of-thought prompting on most tasks.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: high
Justification: Clear performance improvements shown, though requires largest models and sometimes external calculator
Key Limitations: State-of-the-art results dependent on largest models and additional components like calculators

--------------------------------------------------

Claim 3:
Type: methodology
Statement: Chain-of-thought prompting is robust across different annotators and prompts
Location: Section 3.4
Exact Quote: Although there is variance among different chain of thought annotations...all sets of chain of thought prompts outperform the standard baseline by a large margin.

Evidence:
- Evidence Text: Results from three different annotators
  Strength: strong
  Location: Figure 6
  Limitations: Limited number of annotators tested
  Exact Quote: all sets of chain of thought prompts outperform the standard baseline by a large margin

- Evidence Text: Results with different exemplar sets from GSM8K
  Strength: strong
  Location: Section 3.4
  Limitations: Only tested on math problems
  Exact Quote: Figure 6 shows that these prompts performed comparably with our manually written exemplars, also substantially outperforming standard prompting

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Consistent improvement over baseline shown across different variations, though with some variance
Key Limitations: Limited number of annotators and domains tested

--------------------------------------------------

