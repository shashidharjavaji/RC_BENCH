{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 2502.12568v2.pdf, results already exist.\n",
      "Extracting claims...\n",
      "Processing shashi_1_papers_trimmed/2409.15915v1.pdf...\n",
      "[                                        ] (0/1=[==                                      ] ( 1/19=[====                                    ] ( 2/19=[======                                  ] ( 3/19=[========                                ] ( 4/19=[==========                              ] ( 5/19=[============                            ] ( 6/19=[==============                          ] ( 7/19=[================                        ] ( 8/19=[==================                      ] ( 9/19==[=====================                   ] (10/1=[=======================                 ] (11/1=[=========================               ] (12/1=[===========================             ] (13/1=[=============================           ] (14/1=[===============================         ] (15/1=[=================================       ] (16/1=[===================================     ] (17/1=[=====================================   ] (18/1=[========================================] (19/19]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 390\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError analyzing paper: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 390\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[46], line 365\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    363\u001b[0m total_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracting claims...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 365\u001b[0m claims \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_claims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzing evidence...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    367\u001b[0m evidence_results \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39manalyze_evidence(filename, claims)\n",
      "Cell \u001b[0;32mIn[46], line 73\u001b[0m, in \u001b[0;36mPaperAnalyzer.get_claims\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to extract text from PDF\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m claims_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124mAnalyze this research paper and extract ALL possible claims made by the authors.\u001b[39m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124mPaper text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 73\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m120\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_times[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_sleep_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m120\u001b[39m\n\u001b[1;32m     75\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate_content(claims_prompt)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import google.generativeai as genai\n",
    "import pdfplumber\n",
    "import pymupdf4llm\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class PaperAnalyzer:\n",
    "    def __init__(self):\n",
    "\n",
    "        api_key = os.getenv(\"GEMINI_AI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API key not found. Please set it in the .env file.\")\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "        self.paper_text = None\n",
    "        self.execution_times = {\n",
    "            \"claims_analysis\": 0,\n",
    "            \"evidence_analysis\": 0,\n",
    "            \"conclusions_analysis\": 0,\n",
    "            \"total_time\": 0,\n",
    "            \"total_sleep_time\": 0,\n",
    "            \"actual_processing_time\": 0\n",
    "        }\n",
    "\n",
    "    def extract_text_from_pdf(self, filename: str) -> str:\n",
    "        \"\"\"Extract text from PDF file using PyMuPDF\"\"\"\n",
    "        try:\n",
    "            self.paper_text = pymupdf4llm.to_markdown(filename)\n",
    "            return self.paper_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from PDF: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def get_claims(self, filename: str) -> dict:\n",
    "        \"\"\"Extract all claims from the paper\"\"\"\n",
    "        if not self.paper_text:\n",
    "            text = self.extract_text_from_pdf(filename)\n",
    "        else:\n",
    "            text = self.paper_text\n",
    "        start_time = time.time()\n",
    "        if not self.paper_text:\n",
    "            raise Exception(\"Failed to extract text from PDF\")\n",
    "        \n",
    "        claims_prompt = f\"\"\"\n",
    "        Analyze this research paper and extract ALL possible claims made by the authors.\n",
    "        Paper text: {text}\n",
    "        \n",
    "        Your task is to identify all statements in the text that meet the following criteria for a claim:\n",
    "        1. Makes a specific, testable assertion about results, methods, or contributions\n",
    "        2. Represents a novel finding, improvement, or advancement\n",
    "        3. Presents a clear position or conclusion\n",
    "        \n",
    "        Make sure to:\n",
    "        1. Include both major and minor claims\n",
    "        2. Don't miss any claims\n",
    "        3. Present each claim as a separate item\n",
    "        4. Don't repear any claim\n",
    "        \n",
    "        Return ONLY the following JSON structure:\n",
    "        {{\n",
    "            \"claims\": [\n",
    "                {{\n",
    "                    \"claim_id\": 1,\n",
    "                    \"claim_text\": \"statement of the claim\",\n",
    "                    \"location\": \"section/paragraph where this claim appears\",\n",
    "                    \"claim_type\": \"Nature of the claim\",\n",
    "                    \"exact_quote\": \"complete verbatim text containing the claim\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        time.sleep(120)\n",
    "        self.execution_times[\"total_sleep_time\"] += 120\n",
    "        response = self.model.generate_content(claims_prompt)\n",
    "        self.execution_times[\"claims_analysis\"] = time.time() - start_time\n",
    "        print(response.text)\n",
    "        return self._parse_json_response(response.text)\n",
    "\n",
    "    def analyze_evidence(self, filename: str, claims: dict) -> list:\n",
    "        \"\"\"Find evidence for each claim\"\"\"\n",
    "        if not self.paper_text:\n",
    "            text = self.extract_text_from_pdf(filename)\n",
    "        else:\n",
    "            text = self.paper_text\n",
    "        start_time = time.time()\n",
    "        evidence_results = []\n",
    "        for claim in claims['claims']:\n",
    "            evidence_prompt = f\"\"\"\n",
    "            Paper text: {text}\n",
    "            \n",
    "            For the following claim from the paper:\n",
    "            \"{claim['claim_text']}\"\n",
    "            \n",
    "            Please identify relevant evidence that:\n",
    "            1. Directly supports or contradicts the claim's specific assertion\n",
    "            2. Is presented with experimental results, data, or concrete examples\n",
    "            3. Can be traced to specific methods, results, or discussion sections\n",
    "            4. Is not from the abstract or introduction\n",
    "            \n",
    "            If NO evidence is found for the given Claim, return:\n",
    "            {{\n",
    "                \"claim_id\": {claim['claim_id']},\n",
    "                \"evidence\": [],\n",
    "                \"no_evidence_reason\": \"Explain why no evidence was found (e.g., 'Claim is unsupported', 'Claim is theoretical without empirical evidence', etc.)\"\n",
    "            }}\n",
    "            ELSE:\n",
    "            Return ONLY the following JSON structure:\n",
    "            {{\n",
    "                \"claim_id\": {claim['claim_id']},\n",
    "                \"evidence\": [\n",
    "                    {{\n",
    "                        \"evidence_id\": 1,\n",
    "                        \"evidence_text\": \"specific experimental result/data point\",\n",
    "                        \"evidence_type\": \"primary/secondary\",\n",
    "                        \"strength\": \"strong/moderate/weak\",\n",
    "                        \"limitations\": \"stated limitations or assumptions\",\n",
    "                        \"location\": \"specific section & paragraph\",\n",
    "                        \"exact_quote\": \"verbatim text from paper\"\n",
    "                    }}\n",
    "                ]\n",
    "            }}\n",
    "            \"\"\"\n",
    "            time.sleep(120)\n",
    "            self.execution_times[\"total_sleep_time\"] += 120\n",
    "            response = self.model.generate_content(evidence_prompt)\n",
    "            result = self._parse_json_response(response.text)\n",
    "            if result:\n",
    "                evidence_results.append(result)\n",
    "        self.execution_times[\"evidence_analysis\"] = time.time() - start_time\n",
    "        print(evidence_results)\n",
    "        return evidence_results\n",
    "\n",
    "    def analyze_conclusions(self, filename: str, claims: dict, evidence_results: list) -> dict:\n",
    "        \"\"\"Analyze conclusions considering claims and evidence one at a time\"\"\"\n",
    "        if not self.paper_text:\n",
    "            text = self.extract_text_from_pdf(filename)\n",
    "        else:\n",
    "            text = self.paper_text\n",
    "        start_time = time.time()\n",
    "        all_conclusions = []\n",
    "        claims_list = claims.get('claims', [])\n",
    "        for claim in claims_list:\n",
    "            claim_id = claim.get('claim_id')\n",
    "            claim_evidence = next((e['evidence'] for e in evidence_results if e.get('claim_id') == claim_id), [])\n",
    "            evidence_text = []\n",
    "            for idx, evidence in enumerate(claim_evidence, 1):\n",
    "                evidence_text.append(\n",
    "                    f\"  Evidence {idx}:\\n\"\n",
    "                    f\"    - Text: {evidence.get('evidence_text', 'No text provided')}\\n\"\n",
    "                    f\"    - Strength: {evidence.get('strength', 'Not specified')}\\n\"\n",
    "                    f\"    - Limitations: {evidence.get('limitations', 'None specified')}\\n\"\n",
    "                    f\"    - Location: {evidence.get('location', 'Location not specified')}\"\n",
    "                )\n",
    "            single_analysis = f\"\"\"\n",
    "            Claim {claim_id}:\n",
    "            Statement: {claim.get('claim_text', 'No text provided')}\n",
    "            Location: {claim.get('location', 'Location not specified')}\n",
    "            \n",
    "            Evidence Summary:\n",
    "            {{'\\n'.join(evidence_text)}}\n",
    "            \"\"\"\n",
    "            single_conclusion_prompt = f\"\"\"\n",
    "            Paper text: {text}\n",
    "            \n",
    "            Analyze the following claim and its supporting evidence:\n",
    "            {single_analysis}\n",
    "            \n",
    "            Provide a comprehensive conclusion analysis following these guidelines:\n",
    "            \n",
    "            1. Evidence Assessment:\n",
    "            - Evaluate the strength and quality of ALL evidence presented\n",
    "            - Consider both supporting and contradicting evidence\n",
    "            - Assess the methodology and reliability of evidence\n",
    "            \n",
    "            2. Conclusion Analysis:\n",
    "            - Determine what the authors concluded about this specific claim\n",
    "            - Evaluate if the conclusion is justified by the evidence\n",
    "            - Consider the relationship between evidence quality and conclusion strength\n",
    "            \n",
    "            3. Robustness Evaluation:\n",
    "            - Assess how well the evidence supports the conclusion\n",
    "            - Consider methodological strengths and weaknesses\n",
    "            - Evaluate the consistency of evidence\n",
    "            \n",
    "            4. Limitations Analysis:\n",
    "            - Identify specific limitations in both evidence and conclusion\n",
    "            - Consider gaps in methodology or data\n",
    "            - Note any potential biases or confounding factors\n",
    "            \n",
    "            Return ONLY the following JSON structure:\n",
    "            {{\n",
    "                \"conclusions\": [\n",
    "                    {{\n",
    "                        \"claim_id\": \"{claim_id}\",\n",
    "                        \"author_conclusion\": \"detailed description of authors' conclusion based on evidence\",\n",
    "                        \"conclusion_justified\": true/false,\n",
    "                        \"justification_explanation\": \"detailed explanation of why conclusion is/isn't justified\",\n",
    "                        \"robustness_analysis\": \"comprehensive analysis of evidence strength and reliability\",\n",
    "                        \"limitations\": \"specific limitations and caveats\",\n",
    "                        \"location\": \"section/paragraph where conclusion appears\",\n",
    "                        \"evidence_alignment\": \"analysis of how well evidence aligns with conclusion\",\n",
    "                        \"confidence_level\": \"high/medium/low based on evidence quality\"\n",
    "                    }}\n",
    "                ]\n",
    "            }}\n",
    "            \"\"\"\n",
    "            try:\n",
    "                time.sleep(120)\n",
    "                self.execution_times[\"total_sleep_time\"] += 120\n",
    "                response = self.model.generate_content(single_conclusion_prompt)\n",
    "                result = self._parse_json_response(response.text)\n",
    "                if result and isinstance(result, dict) and 'conclusions' in result and result['conclusions']:\n",
    "                    conclusion = result['conclusions'][0]\n",
    "                    all_conclusions.append(conclusion)\n",
    "                else:\n",
    "                    all_conclusions.append({\n",
    "                        \"claim_id\": claim_id,\n",
    "                        \"author_conclusion\": \"No conclusion available\",\n",
    "                        \"conclusion_justified\": False,\n",
    "                        \"justification_explanation\": \"Analysis not available\",\n",
    "                        \"robustness_analysis\": \"No robustness analysis available\",\n",
    "                        \"limitations\": \"No limitations analysis available\",\n",
    "                        \"location\": \"Location not specified\",\n",
    "                        \"evidence_alignment\": \"No alignment analysis available\",\n",
    "                        \"confidence_level\": \"low\"\n",
    "                    })\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing conclusion for claim {claim_id}: {str(e)}\")\n",
    "                all_conclusions.append({\n",
    "                    \"claim_id\": claim_id,\n",
    "                    \"author_conclusion\": \"Error in analysis\",\n",
    "                    \"conclusion_justified\": False,\n",
    "                    \"justification_explanation\": \"Analysis failed\",\n",
    "                    \"robustness_analysis\": \"Analysis failed\",\n",
    "                    \"limitations\": \"Analysis failed\",\n",
    "                    \"location\": \"Location not specified\",\n",
    "                    \"evidence_alignment\": \"Analysis failed\",\n",
    "                    \"confidence_level\": \"low\"\n",
    "                })\n",
    "        self.execution_times[\"conclusions_analysis\"] = time.time() - start_time\n",
    "        return {\n",
    "            \"conclusions\": all_conclusions,\n",
    "            \"analysis_metadata\": {\n",
    "                \"total_claims_analyzed\": len(claims_list),\n",
    "                \"claims_with_conclusions\": len(all_conclusions),\n",
    "                \"analysis_timestamp\": str(datetime.datetime.now())\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    def _parse_json_response(self, response: str) -> dict:\n",
    "        try:\n",
    "            response = response.strip()\n",
    "            response = re.sub(r'^`*``json|```*$', '', response).strip()\n",
    "            if not response.startswith('{'):\n",
    "                response = '{' + response\n",
    "            if not response.endswith('}'):\n",
    "                response += '}'\n",
    "            response = re.sub(r\"([{\\[,:\\s])(['])((?:[^'\\\\\\\\]|\\\\\\\\.)*)(\\2)([}\\],\\s])\", r'\\1\"\\3\"\\5', response)\n",
    "            response = re.sub(r'\\\\(?![\"\\\\bfnrt/])', r'\\\\\\\\', response)\n",
    "            response = re.sub(r'([{,]\\s*)(\\w+)(\\s*:)', lambda m: f'{m.group(1)}\"{m.group(2)}\"{m.group(3)}', response)\n",
    "            response = re.sub(r',\\s*([}\\]])', r'\\1', response)\n",
    "            response = re.sub(r'\\bNaN\\b', 'null', response)\n",
    "            response = re.sub(r'\\b(?:Infinity|-Infinity)\\b', 'null', response)\n",
    "            result = json.loads(response)\n",
    "            return result if isinstance(result, dict) else {}\n",
    "        except json.JSONDecodeError as e:\n",
    "            error_position = e.pos\n",
    "            snippet_start = max(error_position - 20, 0)\n",
    "            snippet_end = min(error_position + 20, len(response))\n",
    "            error_snippet = response[snippet_start:snippet_end]\n",
    "            print(f\"JSON parsing failed at position {error_position}: {e.msg}\")\n",
    "            print(f\"Error snippet: ...{error_snippet}...\")\n",
    "            return {'error': 'Failed to parse JSON', 'reason': str(e)}\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error during JSON parsing: {str(e)}\")\n",
    "            return {'error': 'Unexpected parsing error', 'exception': str(e)}\n",
    "            \n",
    "    def combine_results(self, claims: dict, evidence_results: list, conclusions: dict) -> dict:\n",
    "        final_results = {\n",
    "            \"paper_analysis\": []\n",
    "        }\n",
    "        conclusions_dict = { c['claim_id']: c for c in conclusions.get('conclusions', []) } if conclusions else {}\n",
    "        evidence_dict = { e['claim_id']: e.get('evidence', []) for e in evidence_results if isinstance(e, dict) }\n",
    "        for claim in claims.get('claims', []):\n",
    "            claim_id = claim['claim_id']\n",
    "            conclusion = conclusions_dict.get(claim_id, {})\n",
    "            evidence = evidence_dict.get(claim_id, [])\n",
    "            analysis = {\n",
    "                \"claim_id\": claim_id,\n",
    "                \"claim\": claim.get('claim_text', ''),\n",
    "                \"claim_location\": claim.get('location', 'Location not specified'),\n",
    "                \"evidence\": evidence,\n",
    "                \"evidence_locations\": [ev.get('location', 'Location not specified') for ev in evidence],\n",
    "                \"conclusion\": {\n",
    "                    \"author_conclusion\": conclusion.get('author_conclusion', 'No conclusion available'),\n",
    "                    \"conclusion_justified\": conclusion.get('conclusion_justified', False),\n",
    "                    \"robustness_analysis\": conclusion.get('robustness_analysis', 'No robustness analysis available'),\n",
    "                    \"limitations\": conclusion.get('limitations', 'No limitations analysis available'),\n",
    "                    \"conclusion_location\": conclusion.get('location', 'Location not specified')\n",
    "                }\n",
    "            }\n",
    "            final_results['paper_analysis'].append(analysis)\n",
    "        final_results[\"execution_times\"] = {\n",
    "            \"claims_analysis_time\": f\"{self.execution_times['claims_analysis']:.2f} seconds\",\n",
    "            \"evidence_analysis_time\": f\"{self.execution_times['evidence_analysis']:.2f} seconds\",\n",
    "            \"conclusions_analysis_time\": f\"{self.execution_times['conclusions_analysis']:.2f} seconds\",\n",
    "            \"total_sleep_time\": f\"{self.execution_times['total_sleep_time']:.2f} seconds\",\n",
    "            \"actual_processing_time\": f\"{(self.execution_times['total_time'] - self.execution_times['total_sleep_time']):.2f} seconds\",\n",
    "            \"total_execution_time\": f\"{self.execution_times['total_time']:.2f} seconds\"\n",
    "        }\n",
    "        return final_results\n",
    "        \n",
    "    def print_analysis_results(self, final_results: dict):\n",
    "        print(\"\\n=== Complete Paper Analysis ===\\n\")\n",
    "        for analysis in final_results['paper_analysis']:\n",
    "            print(f\"Claim {analysis['claim_id']}:\")\n",
    "            print(f\"Statement: {analysis['claim']}\")\n",
    "            print(\"\\nEvidence:\")\n",
    "            for evidence in analysis['evidence']:\n",
    "                print(f\"- {evidence['evidence_text']}\")\n",
    "                print(f\"  Strength: {evidence['strength']}\")\n",
    "                print(f\"  Limitations: {evidence['limitations']}\")\n",
    "            print(\"\\nConclusion:\")\n",
    "            print(f\"Author's Conclusion: {analysis['conclusion']['author_conclusion']}\")\n",
    "            print(f\"Justified by Evidence: {'Yes' if analysis['conclusion']['conclusion_justified'] else 'No'}\")\n",
    "            print(f\"Robustness: {analysis['conclusion']['robustness_analysis']}\")\n",
    "            print(f\"Limitations: {analysis['conclusion']['limitations']}\")\n",
    "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "\n",
    "def results_exist(base_filename: str, output_folder: str) -> bool:\n",
    "    \"\"\"Check if results already exist for the given file.\"\"\"\n",
    "    output_dir = Path(output_folder)\n",
    "    analysis_path = output_dir / f'{base_filename}_analysis.json'\n",
    "    # Check if all expected output files exist\n",
    "    return any(file.exists() for file in [analysis_path])\n",
    "\n",
    "\n",
    "def main():\n",
    "    import datetime\n",
    "    analyzer = PaperAnalyzer()\n",
    "    input_folder = 'shashi_1_papers_trimmed'\n",
    "    output_folder = 'Gemini_one_by_one_shashi'\n",
    "    pdf_files = [f for f in os.listdir(input_folder) if f.endswith('.pdf')]\n",
    "    \n",
    "    for filename in pdf_files:\n",
    "        basefile_name = Path(filename).stem\n",
    "\n",
    "        \n",
    "        if results_exist(basefile_name, output_folder):\n",
    "            print(f\"Skipping {filename}, results already exist.\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            filename = f\"{input_folder}/{filename}\"\n",
    "            final_output_path = f'Gemini_one_by_one_shashi/{basefile_name}_analysis.json'\n",
    "            intermediate_output_path = f'Gemini_one_by_one_shashi/{basefile_name}_intermediate.json'\n",
    "            if os.path.exists(final_output_path):\n",
    "                print(f\"Skipping {filename}, already processed.\")\n",
    "                continue\n",
    "            total_start_time = time.time()\n",
    "            print(\"Extracting claims...\")\n",
    "            claims = analyzer.get_claims(filename)\n",
    "            print(\"Analyzing evidence...\")\n",
    "            evidence_results = analyzer.analyze_evidence(filename, claims)\n",
    "            print(\"Analyzing conclusions...\")\n",
    "            conclusions = analyzer.analyze_conclusions(filename, claims, evidence_results)\n",
    "            analyzer.execution_times[\"total_time\"] = time.time() - total_start_time\n",
    "            final_results = analyzer.combine_results(claims, evidence_results, conclusions)\n",
    "            analyzer.print_analysis_results(final_results)\n",
    "            os.makedirs('Gemini_one_by_one_shashi', exist_ok=True)\n",
    "            with open(f'Gemini_one_by_one_shashi/{basefile_name}_analysis.json', 'w') as f:\n",
    "                json.dump(final_results, f, indent=4)\n",
    "            os.makedirs('Gemini_one_by_one_shashi', exist_ok=True)\n",
    "            intermediate_results = {\n",
    "                \"claims\": claims,\n",
    "                \"evidence\": evidence_results,\n",
    "                \"conclusions\": conclusions,\n",
    "                \"execution_times\": final_results[\"execution_times\"]\n",
    "            }\n",
    "            with open(f'Gemini_one_by_one_shashi/{basefile_name}_intermediate.json', 'w') as f:\n",
    "                json.dump(intermediate_results, f, indent=4)\n",
    "            print(\"Intermediate results saved to 'Gemini_one_by_one' folder\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing paper: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis of shashi_1_papers/2502.12568v2.pdf\n",
      "Processing shashi_1_papers/2502.12568v2.pdf...\n",
      "[                                        ] (0/1==[===                                     ] ( 1/1==[======                                  ] ( 2/13==[=========                               ] ( 3/1==[============                            ] ( 4/13==[===============                         ] ( 5/1==[==================                      ] ( 6/13==[=====================                   ] ( 7/1==[========================                ] ( 8/13==[===========================             ] ( 9/1==[==============================          ] (10/13==[=================================       ] (11/1==[====================================    ] (12/13===[========================================] (13/13]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing chunk 1: 429 Resource has been exhausted (e.g. check quota).\n",
      "ERROR:root:Error processing chunk 2: 429 Resource has been exhausted (e.g. check quota).\n",
      "ERROR:root:Error processing chunk 3: 429 Resource has been exhausted (e.g. check quota).\n",
      "ERROR:root:Error processing chunk 4: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing response...\n",
      "Error parsing response: Expecting ',' delimiter: line 1 column 822 (char 821)\n",
      "Raw response: \n",
      "{\n",
      "  \"analysis\": [\n",
      "    {\n",
      "      \"claim_id\": 1,\n",
      "      \"claim\": {\n",
      "        \"text\": \"CogWriter demonstrates significantly faster processing compared to the baseline model.\",\n",
      "        \"type\": \"performance\",\n",
      "        \"location\": \"Appendix A.2, Inference Time\",\n",
      "        \"exact_quote\": \"our approach demonstrates a significant reduction in generation time, achieving approximately 50% faster processing compared to the baseline model.\"\n",
      "      },\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"evidence_text\": \"Inference time comparison using LLaMA-3.3-70B on 4 NVIDIA A100 GPUs, with each condition tested three times and only considering outputs achieving 100% completion rate. Figure 6 illustrates the results.\",\n",
      "          \"strength\": \"moderate\",\n",
      "          \"limitations\": \"Limited to LLaMA-3.3-70B due to issues with other models.  Doesn't specify the exact tasks used for comparison. Only outputs with 100% completion rate were considered, which might skew the results.\",\n",
      "          \"location\": \"Appendix A.2, Inference Time\",\n",
      "          \"exact_quote\": \"All experiments were performed on 4 NVIDIA A100 GPUs, with each condition tested three times...We leveraged vLLM for inference acceleration...To ensure a fair comparison, we only considered outputs achieving 100% completion rate. Figure 6 illustrates the inference time comparison\"\n",
      "        }\n",
      "      ],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": true,\n",
      "        \"robustness\": \"medium\",\n",
      "        \"justification\": \"The claim of faster processing is supported by the experiment and Figure 6, though the limitations related to model selection and completion rate filtering reduce the overall robustness.\",\n",
      "        \"key_limitations\": \"Limited model selection, potential bias from 100% completion rate filter, lack of detail on specific tasks used.\",\n",
      "        \"confidence_level\": \"medium\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"claim_id\": 2,\n",
      "      \"claim\": {\n",
      "        \"text\": \"CogWriter consumes significantly more tokens than baseline methods.\",\n",
      "        \"type\": \"performance\",\n",
      "        \"location\": \"Appendix A.2, Token Consumption\",\n",
      "        \"exact_quote\": \"CogWriter consumes approximately 2.8 times more output tokens and 10 times more total tokens compared to baseline methods.\"\n",
      "      },\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"evidence_text\": \"Analysis comparing token consumption of CogWriter with baseline methods.\",\n",
      "          \"strength\": \"strong\",\n",
      "          \"limitations\": \"No specific data or figures are presented to support the 2.8x and 10x numbers.  The explanation for the increased consumption is qualitative rather than quantitative.\",\n",
      "          \"location\": \"Appendix A.2, Token Consumption\",\n",
      "          \"exact_quote\": \"Our analysis reveals that CogWriter consumes approximately 2.8 times more output tokens and 10 times more total tokens compared to baseline methods.\"\n",
      "        }\n",
      "      ],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": true,\n",
      "        \"robustness\": \"medium\",\n",
      "        \"justification\": \"While the claim of higher token consumption is stated clearly, the lack of precise numerical data and reliance on qualitative explanations weakens the evidence. More detailed data would significantly improve the robustness.\",\n",
      "        \"key_limitations\": \"Lack of quantitative data, reliance on qualitative explanations for increased consumption.\",\n",
      "        \"confidence_level\": \"medium\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"claim_id\": 3,\n",
      "      \"claim\": {\n",
      "        \"text\": \"CogWriter achieves a more substantial improvement in Average Accuracy over GPT-4o-mini compared to the improvement GPT-4o achieves over GPT-4o-mini.\",\n",
      "        \"type\": \"performance\",\n",
      "        \"location\": \"Appendix A.2, Token Consumption\",\n",
      "        \"exact_quote\": \"it achieves only a marginal improvement in Average Accuracy (0.08)...In contrast, CogWriter demonstrates a more substantial improvement of 0.16 in Average Accuracy over GPT-4o-mini.\"\n",
      "      },\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"evidence_text\": \"Comparison of Average Accuracy improvement between GPT-4o over GPT-4o-mini (0.08) and CogWriter over GPT-4o-mini (0.16), referencing Table 1.\",\n",
      "          \"strength\": \"moderate\",\n",
      "          \"limitations\": \"The provided text segment does not include Table 1, making it impossible to verify the accuracy figures. Without Table 1, this evidence is weak.\",\n",
      "          \"location\": \"Appendix A.2, Token Consumption\",\n",
      "          \"exact_quote\": \"it achieves only a marginal improvement in Average Accuracy (0.08)...In contrast, CogWriter demonstrates a more substantial improvement of 0.16 in Average Accuracy over GPT-4o-mini\"\n",
      "        }\n",
      "      ],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": false,\n",
      "        \"robustness\": \"low\",\n",
      "        \"justification\": \"The claim of greater accuracy improvement relies entirely on figures presented in the missing Table 1. Without access to that table, the evidence is insufficient to support the claim.\",\n",
      "        \"key_limitations\": \"Missing Table 1 containing the accuracy figures.\",\n",
      "        \"confidence_level\": \"low\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "Error analyzing 2502.12568v2.pdf: No valid analyses generated from any chunk\n",
      "Starting analysis of shashi_1_papers/2409.15915v1.pdf\n",
      "Processing shashi_1_papers/2409.15915v1.pdf...\n",
      "[                                        ] (0/1=[==                                      ] ( 1/19=[====                                    ] ( 2/19=[======                                  ] ( 3/19=[========                                ] ( 4/19=[==========                              ] ( 5/19=[============                            ] ( 6/19=[==============                          ] ( 7/19=[================                        ] ( 8/19=[==================                      ] ( 9/19==[=====================                   ] (10/1=[=======================                 ] (11/1=[=========================               ] (12/1=[===========================             ] (13/1=[=============================           ] (14/1=[===============================         ] (15/1=[=================================       ] (16/1=[===================================     ] (17/1=[=====================================   ] (18/1=[========================================] (19/19]\n",
      "Parsing response...\n",
      "Error parsing response: Expecting ',' delimiter: line 1 column 735 (char 734)\n",
      "Raw response: \n",
      "{\n",
      "  \"analysis\": [\n",
      "    {\n",
      "      \"claim_id\": 1,\n",
      "      \"claim\": {\n",
      "        \"text\": \"Current LLM-symbolic planning pipelines often require multiple iterations of expert intervention to refine and validate the generated action schemas.\",\n",
      "        \"type\": \"methodology\",\n",
      "        \"location\": \"Section 1/Paragraph 3\",\n",
      "        \"exact_quote\": \"Thus, current pipelines often require multiple iterations of expert intervention to refine and validate the generated action schemas.\"\n",
      "      },\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"evidence_text\": \"Guan et al. (2023) reported that the expert took 59 iterations to fix schema errors for a single task domain.\",\n",
      "          \"strength\": \"moderate\",\n",
      "          \"limitations\": \"Only one study is cited.  It's unclear how representative this example is.\",\n",
      "          \"location\": \"Section 1/Paragraph 3\",\n",
      "          \"exact_quote\": \"For instance, Guan et al. (2023) reported that the expert took 59 iterations to fix schema errors for a single task domain.\"\n",
      "        }\n",
      "      ],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": true,\n",
      "        \"robustness\": \"medium\",\n",
      "        \"justification\": \"The provided evidence supports the claim, showing a specific case where extensive expert intervention was required. However, more evidence from diverse studies would strengthen the claim.\",\n",
      "        \"key_limitations\": \"Limited supporting evidence.  Generalizability is unclear.\",\n",
      "        \"confidence_level\": \"medium\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"claim_id\": 2,\n",
      "      \"claim\": {\n",
      "        \"text\": \"A single expert’s interpretation of ambiguous natural language descriptions might not align with the user’s actual intent.\",\n",
      "        \"type\": \"methodology\",\n",
      "        \"location\": \"Section 1/Paragraph 3-4\",\n",
      "        \"exact_quote\": \"This creates a critical vulnerability: the potential for interpretation mismatch between the expert and the user. Experts, while knowledgeable, inevitably bring their own subjective interpretations to the task descriptions, often formalizing them in a single, specific way. This limits the system to a single perspective of the task. However, unlike formal language designed to have an exact, context-independent meaning, natural language inherently contains ambiguities that yield diverse valid interpretations of the same description.\"\n",
      "      },\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"evidence_text\": \"Figure 2 illustrates a scenario where different interpretations of a task are possible.\",\n",
      "          \"strength\": \"weak\",\n",
      "          \"limitations\": \"Figure 2 is illustrative, not empirical evidence. It doesn't demonstrate actual mismatches, only the potential for them.\",\n",
      "          \"location\": \"Section 1/Paragraph 4, Figure 2\",\n",
      "          \"exact_quote\": \"See Figure 2\"\n",
      "        },\n",
      "        {\n",
      "          \"evidence_text\": \"Reference to Moravcsik (1983) regarding the ambiguity of natural language.\",\n",
      "          \"strength\": \"moderate\",\n",
      "          \"limitations\": \"The specific argument from Moravcsik (1983) isn't detailed, making it difficult to assess its relevance directly.\",\n",
      "          \"location\": \"Section 1/Paragraph 4\",\n",
      "          \"exact_quote\": \"(Moravcsik 1983)\"\n",
      "        }\n",
      "      ],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": true,\n",
      "        \"robustness\": \"low\",\n",
      "        \"justification\": \"The claim is conceptually plausible and aligns with the general understanding of language ambiguity.  However,  it lacks strong empirical evidence demonstrating actual mismatches in practice.  The cited figure is illustrative, not data-driven.\",\n",
      "        \"key_limitations\": \"Relies primarily on theoretical arguments and illustrative examples. Lacks empirical data.\",\n",
      "        \"confidence_level\": \"low\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"claim_id\": 3,\n",
      "      \"claim\": {\n",
      "        \"text\": \"The proposed pipeline maintains superiority in planning over the direct LLM planning approach.\",\n",
      "        \"type\": \"performance\",\n",
      "        \"location\": \"Section 1/Paragraph 5\",\n",
      "        \"exact_quote\": \"Our experiments demonstrate that without expert intervention, our pipeline generates sound action plans competitive with direct LLM-based plan generation, even in short-horizon planning tasks.\"\n",
      "      },\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"evidence_text\": \"“Our experiments demonstrate that... our pipeline generates sound action plans competitive with...”\",\n",
      "          \"strength\": \"weak\",\n",
      "          \"limitations\": \"The claim mentions experiments but provides no details about their design, metrics, or results.\",\n",
      "          \"location\": \"Section 1/Paragraph 5\",\n",
      "          \"exact_quote\": \"Our experiments demonstrate that without expert intervention, our pipeline generates sound action plans competitive with direct LLM-based plan generation, even in short-horizon planning tasks.\"\n",
      "        }\n",
      "      ],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": false,\n",
      "        \"robustness\": \"low\",\n",
      "        \"justification\": \"The claim about superiority is not justified as no experimental results are presented. The phrase \\\"competitive with\\\" is vague and does not necessarily imply superiority.\",\n",
      "        \"key_limitations\": \"No data or specific results are provided to support the claim of superiority.\",\n",
      "        \"confidence_level\": \"low\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing chunk 2: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing response...\n",
      "Error parsing response: Expecting ',' delimiter: line 1 column 2011 (char 2010)\n",
      "Raw response: \n",
      "{\n",
      "  \"analysis\": [\n",
      "    {\n",
      "      \"claim_id\": 1,\n",
      "      \"claim\": {\n",
      "        \"text\": \"Semantic equivalence across different representations holds true in our context.\",\n",
      "        \"type\": \"result\",\n",
      "        \"location\": \"Section 5, Hypothesis H1\",\n",
      "        \"exact_quote\": \"Semantic equivalence across different representations, as discussed by Weaver, holds true in our context.\"\n",
      "      },\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"evidence_text\": \"Higher cosine similarity between matched pairs compared to mismatched ones using text-embedding-ada-002 and sentence-t5-xl.\",\n",
      "          \"strength\": \"moderate\",\n",
      "          \"limitations\": \"Relies on cosine similarity which might not fully capture semantic equivalence. Specific values not provided.\",\n",
      "          \"location\": \"Section 5.2\",\n",
      "          \"exact_quote\": \"both models demonstrated higher cosine similarity between matched pairs compared to mismatched ones (see Figure 5).\"\n",
      "        }\n",
      "      ],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": true,\n",
      "        \"robustness\": \"medium\",\n",
      "        \"justification\": \"The evidence of higher cosine similarity for matched pairs suggests semantic equivalence, but more rigorous evaluation methods would strengthen the claim.\",\n",
      "        \"key_limitations\": \"Lack of specific similarity values and reliance solely on cosine similarity.\",\n",
      "        \"confidence_level\": \"medium\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"claim_id\": 2,\n",
      "      \"claim\": {\n",
      "        \"text\": \"Ambiguity in natural language descriptions leads to multiple interpretations.\",\n",
      "        \"type\": \"result\",\n",
      "        \"location\": \"Section 5, Hypothesis H2\",\n",
      "        \"exact_quote\": \"Ambiguity in natural language descriptions leads to multiple interpretations.\"\n",
      "      },\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"evidence_text\": \"Significantly increased number of distinct solvable schema sets generated from layman descriptions (e.g., from 3419 to 8039 when LLM# = 10 w/o CP).\",\n",
      "          \"strength\": \"strong\",\n",
      "          \"limitations\": \"The connection between number of schema sets and \\\"interpretations\\\" is not explicitly established, although implied.\",\n",
      "          \"location\": \"Section 5.3\",\n",
      "          \"exact_quote\": \"when confronted with inherently ambiguous layman descriptions from non-expert participants, our pipeline generates a significantly increased number of distinct solvable schema sets (e.g., from 3419 to 8039 when LLM# = 10 w/o CP)\"\n",
      "        }\n",
      "      ],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": true,\n",
      "        \"robustness\": \"medium\",\n",
      "        \"justification\": \"Increased number of solvable schema sets from ambiguous descriptions suggests multiple valid interpretations.  Further clarification on how schema sets represent different interpretations would be beneficial.\",\n",
      "        \"key_limitations\": \"Indirect link between schema sets and interpretations.\",\n",
      "        \"confidence_level\": \"medium\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"claim_id\": 3,\n",
      "      \"claim\": {\n",
      "        \"text\": \"Our pipeline produces multiple solvable candidate sets of action schemas and plans without expert intervention.\",\n",
      "        \"type\": \"contribution\",\n",
      "        \"location\": \"Section 5, Hypothesis H3\",\n",
      "        \"exact_quote\": \"Our pipeline produces multiple solvable candidate sets of action schemas and plans without expert intervention\"\n",
      "      },\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"evidence_text\": \"Deploying 10 LLM instances is sufficient to generate solvable schema sets for all test domains.\",\n",
      "          \"strength\": \"strong\",\n",
      "          \"limitations\": \"Limited information about the complexity and representativeness of the test domains.\",\n",
      "          \"location\": \"Section 5.3\",\n",
      "          \"exact_quote\": \"deploying 10 LLM instances is sufficient to generate solvable schema sets for all test domains\"\n",
      "        },\n",
      "        {\n",
      "          \"evidence_text\": \"Conformal prediction filtering improves efficiency, increasing the ratio of solvable schemas from 10.9% to 23.0%.\",\n",
      "          \"strength\": \"strong\",\n",
      "          \"limitations\": \"Details about the filtering process and its potential impact on solution diversity not provided.\",\n",
      "          \"location\": \"Section 5.3\",\n",
      "          \"exact_quote\": \"the ratio of solvable schemas (verified by the planner) increased from 10.9% to 23.0%\"\n",
      "        }\n",
      "      ],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": true,\n",
      "        \"robustness\": \"medium\",\n",
      "        \"justification\": \"Evidence shows the pipeline generates solvable schema sets without expert input, strengthened by the improvement achieved through conformal prediction.\",\n",
      "        \"key_limitations\": \"Limited information on test domain complexity and filtering details.\",\n",
      "        \"confidence_level\": \"medium\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"claim_id\": 4,\n",
      "      \"claim\": {\n",
      "        \"text\": \"Our pipeline outperforms direct LLM planning approaches in plan quality.\",\n",
      "        \"type\": \"performance\",\n",
      "        \"location\": \"Section 5, Hypothesis H4\",\n",
      "        \"exact_quote\": \"Our pipeline outperforms direct LLM planning approaches in plan quality\"\n",
      "      },\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"evidence_text\": \"Human evaluation showing our pipeline's plans achieving an average rank of 2.97 compared to 3.58 for ToT and 1.79 for gold standard.\",\n",
      "          \"strength\": \"moderate\",\n",
      "          \"limitations\": \"Small number of assessors (four) and limited details about the evaluation methodology.\",\n",
      "          \"location\": \"Section 5.4, Table 3\",\n",
      "          \"exact_quote\": \"The results, summarized in Table 3, clearly support H4.\"\n",
      "        },\n",
      "        {\n",
      "          \"evidence_text\": \"Success in solving the Sussman Anomaly problem where ToT approaches fail.\",\n",
      "          \"strength\": \"strong\",\n",
      "          \"limitations\": \"Single, specific problem instance.  Generalizability of this superiority not established.\",\n",
      "          \"location\": \"Section 5.4, Table 4\",\n",
      "          \"exact_quote\": \"As shown in Table 4, ToT approaches using various LLMs [...] consistently fail to solve this problem.\"\n",
      "        }\n",
      "      ],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": true,\n",
      "        \"robustness\": \"medium\",\n",
      "        \"justification\": \"Human evaluation and performance on the Sussman Anomaly provide evidence for superior plan quality, but further evaluation on a broader range of problems would enhance robustness.\",\n",
      "        \"key_limitations\": \"Small assessor pool, limited evaluation details, and reliance on a single problem instance for demonstrating specific capability.\",\n",
      "        \"confidence_level\": \"medium\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing chunk 4: 429 Resource has been exhausted (e.g. check quota).\n",
      "ERROR:root:Error processing chunk 5: 429 Resource has been exhausted (e.g. check quota).\n",
      "ERROR:root:Error processing chunk 6: 429 Resource has been exhausted (e.g. check quota).\n",
      "ERROR:root:Error processing chunk 7: 429 Resource has been exhausted (e.g. check quota).\n",
      "ERROR:root:Error processing chunk 8: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing 2409.15915v1.pdf: No valid analyses generated from any chunk\n",
      "Starting analysis of shashi_1_papers/2405.04215v1.pdf\n",
      "Processing shashi_1_papers/2405.04215v1.pdf...\n",
      "[                                        ] (0/2[=                                       ] ( 1/2=[===                                     ] ( 2/2[====                                    ] ( 3/26=[======                                  ] ( 4/26[=======                                 ] ( 5/2=[=========                               ] ( 6/2[==========                              ] ( 7/26=[============                            ] ( 8/26[=============                           ] ( 9/2=[===============                         ] (10/2[================                        ] (11/26=[==================                      ] (12/26=[====================                    ] (13/26[=====================                   ] (14/2=[=======================                 ] (15/2[========================                ] (16/26=[==========================              ] (17/26[===========================             ] (18/2=[=============================           ] (19/2[==============================          ] (20/26=[================================        ] (21/26[=================================       ] (22/2=[===================================     ] (23/2[====================================    ] (24/26=[======================================  ] (25/26=[========================================] (26/26]\n",
      "Parsing response...\n",
      "Error parsing response: Expecting ',' delimiter: line 1 column 1067 (char 1066)\n",
      "Raw response: \n",
      "{\n",
      "  \"analysis\": [\n",
      "    {\n",
      "      \"claim_id\": 1,\n",
      "      \"claim\": {\n",
      "        \"text\": \"NL2Plan is the first domain-agnostic offline natural language planning system and uses an LLM to generate complete PDDL descriptions and corresponding plans based on only a few sentences of natural language, without needing any domain-specific adaptations.\",\n",
      "        \"type\": \"contribution\",\n",
      "        \"location\": \"Introduction/Paragraph 4\",\n",
      "        \"exact_quote\": \"In this paper, we generalize and build upon the existing natural language-to-PDDL systems, adding pre-processing steps and automated common sense feedback to create NL2Plan. NL2Plan is, to our knowledge, the first domain-agnostic offline natural language planning system and uses an LLM to generate complete PDDL descriptions and corresponding plans based on only a few sentences of natural language, without needing any domain-specific adaptations.\"\n",
      "      },\n",
      "      \"evidence\": [],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": false,\n",
      "        \"robustness\": \"low\",\n",
      "        \"justification\": \"The claim of being the \\\"first\\\" is difficult to definitively verify without an exhaustive literature review which is not provided.  While the paper positions NL2Plan as a novel contribution, it primarily builds upon existing techniques. The claim about requiring \\\"only a few sentences\\\" is vague and lacks quantification.\",\n",
      "        \"key_limitations\": \"Lack of concrete evidence and comparative analysis to support the \\\"first\\\" and \\\"domain-agnostic\\\" claims. Vague description of input text length.\",\n",
      "        \"confidence_level\": \"low\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"claim_id\": 2,\n",
      "      \"claim\": {\n",
      "        \"text\": \"NL2Plan correctly solves 10 out of 15 tasks, a clear improvement from directly applying an LLM which solves only 2.\",\n",
      "        \"type\": \"result/performance\",\n",
      "        \"location\": \"Introduction/Paragraph 5\",\n",
      "        \"exact_quote\": \"We find that NL2Plan correctly solves 10 out of 15 tasks, a clear improvement from directly applying an LLM which solves only 2.\"\n",
      "      },\n",
      "      \"evidence\": [],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": false,\n",
      "        \"robustness\": \"low\",\n",
      "        \"justification\": \"The claim of solving 10 out of 15 tasks needs supporting data and details on the experimental setup.  The nature of the tasks, the baseline LLM used, and the metrics for evaluating \\\"correctness\\\" are missing.  Without these details, the significance of the improvement cannot be assessed.\",\n",
      "        \"key_limitations\": \"Missing experimental details, task descriptions, baseline specifications, and success criteria.\",\n",
      "        \"confidence_level\": \"low\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"claim_id\": 3,\n",
      "      \"claim\": {\n",
      "        \"text\": \"NL2Plan’s use of a classical planner allows it to identify 2 out of 5 failure cases and return “No plan found” instead.\",\n",
      "        \"type\": \"methodology/advantage\",\n",
      "        \"location\": \"Introduction/Paragraph 5\",\n",
      "        \"exact_quote\": \"Furthermore, while many LLM-driven methods are unaware of when they fail and simply return invalid solutions, NL2Plan’s use of a classical planner allows it to identify 2 out of 5 failure cases and return “No plan found” instead.\"\n",
      "      },\n",
      "      \"evidence\": [],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": false,\n",
      "        \"robustness\": \"low\",\n",
      "        \"justification\": \"While logically sound that using a classical planner can lead to the detection of unsolvable tasks, this claim requires further details on how these failures are identified and reported.  Without more information about the specific cases, the significance of this feature remains unclear.\",\n",
      "        \"key_limitations\": \"Lacks details on how the failure detection mechanism works and the nature of the failure cases.\",\n",
      "        \"confidence_level\": \"low\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing chunk 2: 429 Resource has been exhausted (e.g. check quota).\n",
      "ERROR:root:Error processing chunk 3: 429 Resource has been exhausted (e.g. check quota).\n",
      "ERROR:root:Error processing chunk 4: 429 Resource has been exhausted (e.g. check quota).\n",
      "ERROR:root:Error processing chunk 5: 429 Resource has been exhausted (e.g. check quota).\n",
      "ERROR:root:Error processing chunk 6: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error analyzing 2405.04215v1.pdf: No valid analyses generated from any chunk\n",
      "Starting analysis of shashi_1_papers/2501.18817v1.pdf\n",
      "Processing shashi_1_papers/2501.18817v1.pdf...\n",
      "[                                        ] (0/2[=                                       ] ( 1/2=[===                                     ] ( 2/2[====                                    ] ( 3/25=[======                                  ] ( 4/25=[========                                ] ( 5/25[=========                               ] ( 6/2=[===========                             ] ( 7/2[============                            ] ( 8/25=[==============                          ] ( 9/25=[================                        ] (10/25[=================                       ] (11/2=[===================                     ] (12/2[====================                    ] (13/25=[======================                  ] (14/25=[========================                ] (15/25[=========================               ] (16/2=[===========================             ] (17/2[============================            ] (18/25=[==============================          ] (19/25=[================================        ] (20/25[=================================       ] (21/2=[===================================     ] (22/2[====================================    ] (23/25=[======================================  ] (24/25=[========================================] (25/25]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error processing chunk 1: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing response...\n",
      "Error parsing response: Expecting ',' delimiter: line 1 column 698 (char 697)\n",
      "Raw response: \n",
      "{\n",
      "  \"analysis\": [\n",
      "    {\n",
      "      \"claim_id\": 1,\n",
      "      \"claim\": {\n",
      "        \"text\": \"The example solution correctly solves a simple BlocksWorld task.\",\n",
      "        \"type\": \"methodology\",\n",
      "        \"location\": \"Section 3, Paragraph 2\",\n",
      "        \"exact_quote\": \"The example solution, provided with the intent of making the solution format clear, correctly solves a simple BlocksWorld task that is not present in the experiment dataset.\"\n",
      "      },\n",
      "      \"evidence\": [],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": false,\n",
      "        \"robustness\": \"low\",\n",
      "        \"justification\": \"The paper mentions the existence of an example solution and its purpose but provides no details about the task itself or how it's solved.  Without this information, the claim of its correctness cannot be verified.\",\n",
      "        \"key_limitations\": \"Lack of specifics about the example solution and task.\",\n",
      "        \"confidence_level\": \"low\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"claim_id\": 2,\n",
      "      \"claim\": {\n",
      "        \"text\": \"The handwritten strategy follows the same structure as the LLM-generated strategies.\",\n",
      "        \"type\": \"methodology\",\n",
      "        \"location\": \"Section 3, Paragraph 6\",\n",
      "        \"exact_quote\": \"Our handwritten strategy follows the same structure as the LLM-generated strategies; that is, a high-level description of BlocksWorld, a list of key constraints, a set of generalised instructions for solving a BlocksWorld task, and some common mistakes that we predicted could arise during the solving process.\"\n",
      "      },\n",
      "      \"evidence\": [],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": true,\n",
      "        \"robustness\": \"medium\",\n",
      "        \"justification\": \"The paper explicitly states the structural components of both the handwritten and LLM-generated strategies. This claim is verifiable by examining the strategies, even though they're not provided within the text itself.\",\n",
      "        \"key_limitations\": \"Strategies themselves aren't shown for direct verification.\",\n",
      "        \"confidence_level\": \"medium\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"claim_id\": 3,\n",
      "      \"claim\": {\n",
      "        \"text\": \"Error correction improves the success rate of all six experiment variations.\",\n",
      "        \"type\": \"result\",\n",
      "        \"location\": \"Section 4.2, Paragraph 1\",\n",
      "        \"exact_quote\": \"Error correction improves the success rate of all six experiment variations.\"\n",
      "      },\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"evidence_text\": \"Table 2 shows increased success rates across all variations after each round of error correction.\",\n",
      "          \"strength\": \"strong\",\n",
      "          \"limitations\": \"Table only shows aggregate success rates, not individual task performance.\",\n",
      "          \"location\": \"Section 4.2, Table 2\",\n",
      "          \"exact_quote\": \"N/A (Data in Table 2)\"\n",
      "        }\n",
      "      ],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": true,\n",
      "        \"robustness\": \"high\",\n",
      "        \"justification\": \"Table 2 clearly supports this claim by showing improved success rates after each correction round across all experimental conditions.\",\n",
      "        \"key_limitations\": \"Lack of detail about the distribution of improvements.  While all variations improved, the magnitude and consistency of the improvements across tasks within a variation are unknown.\",\n",
      "        \"confidence_level\": \"high\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"claim_id\": 4,\n",
      "      \"claim\": {\n",
      "        \"text\": \"Incorporating any of the three generated strategies enhances the ability of o1-mini to solve BlocksWorld tasks.\",\n",
      "        \"type\": \"result\",\n",
      "        \"location\": \"Section 4.2, Paragraph 4\",\n",
      "        \"exact_quote\": \"Incorporating any of the three generated strategies into the task prompt substantially enhances the ability of o1-mini to solve BlocksWorld tasks.\"\n",
      "      },\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"evidence_text\": \"Table 2 shows higher success rates for o1-mini with generated strategies compared to baseline o1-mini.\",\n",
      "          \"strength\": \"strong\",\n",
      "          \"limitations\": \"Only aggregate success rates are presented; the consistency of improvement across individual tasks isn't shown.\",\n",
      "          \"location\": \"Section 4.2, Table 2\",\n",
      "          \"exact_quote\": \"N/A (Data in Table 2)\"\n",
      "        }\n",
      "      ],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": true,\n",
      "        \"robustness\": \"high\",\n",
      "        \"justification\": \"Table 2 directly supports this claim by showing higher success rates for o1-mini when using generated strategies compared to the baseline.\",\n",
      "        \"key_limitations\": \"While the aggregate improvement is clear, the variance in performance across individual tasks is unknown.\",\n",
      "        \"confidence_level\": \"high\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"claim_id\": 5,\n",
      "      \"claim\": {\n",
      "        \"text\": \"Incorporating the handwritten strategy results in near-perfect success from o1-mini.\",\n",
      "        \"type\": \"result\",\n",
      "        \"location\": \"Section 4.2, Paragraph 5\",\n",
      "        \"exact_quote\": \"Incorporating our handwritten strategy into the base prompt results in near-perfect success from o1-mini.\"\n",
      "      },\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"evidence_text\": \"Table 2 shows a 98% success rate in the initial round for o1-mini with the handwritten strategy.\",\n",
      "          \"strength\": \"strong\",\n",
      "          \"limitations\": \"The specific task where it failed isn't detailed.\",\n",
      "          \"location\": \"Section 4.2, Table 2\",\n",
      "          \"exact_quote\": \"N/A (Data in Table 2)\"\n",
      "        }\n",
      "      ],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": true,\n",
      "        \"robustness\": \"high\",\n",
      "        \"justification\": \"The 98% initial success rate presented in Table 2 strongly supports the claim of 'near-perfect' success.\",\n",
      "        \"key_limitations\": \"While near-perfect, understanding the failure case could provide insights.\",\n",
      "        \"confidence_level\": \"high\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 436\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[38;5;28mprint\u001b[39m(paper)\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 436\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[43], line 419\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    416\u001b[0m total_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    418\u001b[0m analyzer\u001b[38;5;241m.\u001b[39mextract_text_from_pdf(filename_with_path)\n\u001b[0;32m--> 419\u001b[0m analysis_results \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_paper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_with_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m analyzer\u001b[38;5;241m.\u001b[39mexecution_times[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m total_start_time\n\u001b[1;32m    422\u001b[0m claims, evidence_results, conclusions, final_results \u001b[38;5;241m=\u001b[39m analyzer\u001b[38;5;241m.\u001b[39mcombine_results(analysis_results)\n",
      "Cell \u001b[0;32mIn[43], line 111\u001b[0m, in \u001b[0;36mSinglePassPaperAnalyzer.analyze_paper\u001b[0;34m(self, filename)\u001b[0m\n\u001b[1;32m     57\u001b[0m comprehensive_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;124mAnalyze this \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mportion_text\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m research paper and provide a comprehensive evaluation.\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_indicator\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124mEnsure the response is a properly formatted JSON object.\u001b[39m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# Add rate limiting\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m90\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecution_times[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_sleep_time\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m90\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m# Get response from Gemini\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pymupdf4llm\n",
    "import time\n",
    "import datetime\n",
    "import logging\n",
    "from typing import Dict, List, Any\n",
    "import re\n",
    "import os, time\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class SinglePassPaperAnalyzer:\n",
    "    def __init__(self):\n",
    "\n",
    "        api_key = os.getenv(\"GEMINI_AI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API key not found. Please set it in the .env file.\")\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "        self.paper_text = None\n",
    "        self.execution_times = {\n",
    "        \"single_pass_analysis\": 0,\n",
    "        \"total_time\": 0,\n",
    "        \"total_sleep_time\": 0,  # Add this\n",
    "        \"actual_processing_time\": 0  # Add this\n",
    "    }\n",
    "        \n",
    "    def extract_text_from_pdf(self, filename: str) -> str:\n",
    "        \"\"\"Extract text from PDF file using PyMuPDF\"\"\"\n",
    "        try:\n",
    "            self.paper_text = pymupdf4llm.to_markdown(filename)\n",
    "            return self.paper_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from PDF: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def analyze_paper(self, filename):\n",
    "        \"\"\"Perform comprehensive single-pass analysis of the paper with improved error handling\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        if not self.paper_text:\n",
    "            text = self.extract_text_from_pdf(filename)\n",
    "        else:\n",
    "            text = self.paper_text\n",
    "                \n",
    "        if not text:\n",
    "            raise Exception(\"Failed to extract text from PDF\")\n",
    "        \n",
    "        # Split text into chunks if it's too long\n",
    "        max_chunk_length = 12000  # Adjust based on model's context window\n",
    "        chunks = [text[i:i + max_chunk_length] for i in range(0, len(text), max_chunk_length)]\n",
    "        \n",
    "        all_analyses = []\n",
    "        claim_id_counter = 1\n",
    "        \n",
    "        for chunk_idx, chunk in enumerate(chunks):\n",
    "            # Create chunk indicator text\n",
    "            chunk_indicator = \"This is part \" + str(chunk_idx + 1) + \" of \" + str(len(chunks)) + \".\" if len(chunks) > 1 else \"\"\n",
    "            portion_text = \"portion of the\" if len(chunks) > 1 else \"\"\n",
    "            \n",
    "            comprehensive_prompt = f\"\"\"\n",
    "            Analyze this {portion_text} research paper and provide a comprehensive evaluation.\n",
    "            {chunk_indicator}\n",
    "            \n",
    "            Paper text: {chunk}\n",
    "\n",
    "            Follow these guidelines:\n",
    "\n",
    "            1. Identify ALL claims in this text where each claim:\n",
    "            - Makes a specific, verifiable assertion\n",
    "            - Is supported by concrete evidence\n",
    "            - Represents findings, contributions, or methodological advantages\n",
    "            - Can be from any section except abstract\n",
    "\n",
    "            2. For each identified claim:\n",
    "            - Extract ALL supporting or contradicting evidence\n",
    "            - Evaluate the evidence strength and limitations\n",
    "            - Assess how well conclusions align with evidence\n",
    "\n",
    "            Return ONLY a valid JSON object with the following structure:\n",
    "            {{\n",
    "                \"analysis\": [\n",
    "                    {{\n",
    "                        \"claim_id\": number,\n",
    "                        \"claim\": {{\n",
    "                            \"text\": \"statement of the claim\",\n",
    "                            \"type\": \"methodology/result/contribution/performance\",\n",
    "                            \"location\": \"section/paragraph\",\n",
    "                            \"exact_quote\": \"verbatim text from paper\"\n",
    "                        }},\n",
    "                        \"evidence\": [\n",
    "                            {{\n",
    "                                \"evidence_text\": \"specific experimental result/data\",\n",
    "                                \"strength\": \"strong/moderate/weak\",\n",
    "                                \"limitations\": \"specific limitations\",\n",
    "                                \"location\": \"section/paragraph\",\n",
    "                                \"exact_quote\": \"verbatim text from paper\"\n",
    "                            }}\n",
    "                        ],\n",
    "                        \"evaluation\": {{\n",
    "                            \"conclusion_justified\": true/false,\n",
    "                            \"robustness\": \"high/medium/low\",\n",
    "                            \"justification\": \"explanation\",\n",
    "                            \"key_limitations\": \"critical limitations\",\n",
    "                            \"confidence_level\": \"high/medium/low\"\n",
    "                        }}\n",
    "                    }}\n",
    "                ]\n",
    "            }}\n",
    "\n",
    "            Ensure the response is a properly formatted JSON object.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Add rate limiting\n",
    "            time.sleep(90)\n",
    "            self.execution_times[\"total_sleep_time\"] += 90\n",
    "            \n",
    "            try:\n",
    "                # Get response from Gemini\n",
    "                response = self.model.generate_content(comprehensive_prompt)\n",
    "                \n",
    "                # Parse response\n",
    "                chunk_results = self._parse_json_response(response.text)\n",
    "                \n",
    "                if chunk_results and 'analysis' in chunk_results:\n",
    "                    # Update claim IDs to ensure uniqueness across chunks\n",
    "                    for analysis in chunk_results['analysis']:\n",
    "                        analysis['claim_id'] = claim_id_counter\n",
    "                        claim_id_counter += 1\n",
    "                    all_analyses.extend(chunk_results['analysis'])\n",
    "            \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error processing chunk {chunk_idx + 1}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if not all_analyses:\n",
    "            raise Exception(\"No valid analyses generated from any chunk\")\n",
    "\n",
    "        result = {\"analysis\": all_analyses}\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        self.execution_times[\"single_pass_analysis\"] = elapsed_time\n",
    "        self.execution_times[\"actual_processing_time\"] = elapsed_time - self.execution_times[\"total_sleep_time\"]\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _parse_json_response(self, response: str) -> Dict:\n",
    "        \"\"\"Parse JSON response with better error handling\"\"\"\n",
    "        try:\n",
    "            print(\"Parsing response...\")\n",
    "            \n",
    "            # Clean up the response\n",
    "            response = response.strip()\n",
    "            if response.startswith('```json'):\n",
    "                response = response[7:]\n",
    "            if response.endswith('```'):\n",
    "                response = response[:-3]\n",
    "            \n",
    "            # Find the JSON content\n",
    "            start_idx = response.find('{')\n",
    "            end_idx = response.rfind('}') + 1\n",
    "            \n",
    "            if start_idx == -1 or end_idx == 0:\n",
    "                raise ValueError(\"No JSON content found in response\")\n",
    "                \n",
    "            json_str = response[start_idx:end_idx]\n",
    "            \n",
    "            # Clean up the JSON string\n",
    "            json_str = (\n",
    "                json_str\n",
    "                .replace('\\n', ' ')\n",
    "                .replace('\\r', ' ')\n",
    "                .replace('\\t', ' ')\n",
    "                .replace('\\\\', '\\\\\\\\')\n",
    "                .replace('\"{', '{')\n",
    "                .replace('}\"', '}')\n",
    "                .replace(\"'\", '\"')\n",
    "            )\n",
    "            \n",
    "            # Remove any invalid control characters\n",
    "            json_str = ''.join(char for char in json_str if ord(char) >= 32 or char in '\\n\\r\\t')\n",
    "            \n",
    "            # Try to parse the JSON\n",
    "            try:\n",
    "                result = json.loads(json_str)\n",
    "            except json.JSONDecodeError:\n",
    "                # If parsing fails, try to fix common issues\n",
    "                json_str = json_str.replace('None', 'null')\n",
    "                json_str = json_str.replace('True', 'true')\n",
    "                json_str = json_str.replace('False', 'false')\n",
    "                # Remove trailing commas\n",
    "                json_str = json_str.replace(',}', '}')\n",
    "                json_str = json_str.replace(',]', ']')\n",
    "                result = json.loads(json_str)\n",
    "            \n",
    "            print(\"Successfully parsed JSON response\")\n",
    "            return result\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing response: {str(e)}\")\n",
    "            print(\"Raw response:\", response)\n",
    "            # Return a default structure instead of raising an error\n",
    "            if 'claims' in response.lower():\n",
    "                return {\"claims\": []}\n",
    "            elif 'evidence' in response.lower():\n",
    "                return {\"evidence_sets\": []}\n",
    "            elif 'conclusion' in response.lower():\n",
    "                return {\"conclusions\": []}\n",
    "            return {}\n",
    "\n",
    "        \n",
    "\n",
    "    def combine_results(self, analysis_results: Dict) -> tuple:\n",
    "        \"\"\"Restructure the single-pass analysis results into the desired format\"\"\"\n",
    "        claims = {\n",
    "            \"claims\": [\n",
    "                {\n",
    "                    \"claim_id\": item[\"claim_id\"],\n",
    "                    \"claim_text\": item[\"claim\"][\"text\"],\n",
    "                    \"location\": item[\"claim\"][\"location\"],\n",
    "                    \"claim_type\": item[\"claim\"][\"type\"],\n",
    "                    \"exact_quote\": item[\"claim\"][\"exact_quote\"]\n",
    "                }\n",
    "                for item in analysis_results[\"analysis\"]\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        evidence_results = [\n",
    "            {\n",
    "                \"claim_id\": item[\"claim_id\"],\n",
    "                \"evidence\": [\n",
    "                    {\n",
    "                        \"evidence_id\": idx + 1,\n",
    "                        \"evidence_text\": ev[\"evidence_text\"],\n",
    "                        \"evidence_type\": \"primary\",\n",
    "                        \"strength\": ev[\"strength\"],\n",
    "                        \"limitations\": ev[\"limitations\"],\n",
    "                        \"location\": ev[\"location\"],\n",
    "                        \"exact_quote\": ev[\"exact_quote\"]\n",
    "                    }\n",
    "                    for idx, ev in enumerate(item[\"evidence\"])\n",
    "                ]\n",
    "            }\n",
    "            for item in analysis_results[\"analysis\"]\n",
    "        ]\n",
    "        \n",
    "        conclusions = {\n",
    "            \"conclusions\": [\n",
    "                {\n",
    "                    \"claim_id\": item[\"claim_id\"],\n",
    "                    \"author_conclusion\": item[\"evaluation\"][\"justification\"],\n",
    "                    \"conclusion_justified\": item[\"evaluation\"][\"conclusion_justified\"],\n",
    "                    \"robustness_analysis\": item[\"evaluation\"][\"robustness\"],\n",
    "                    \"limitations\": item[\"evaluation\"][\"key_limitations\"],\n",
    "                    \"evidence_alignment\": item[\"evaluation\"][\"justification\"],\n",
    "                    \"confidence_level\": item[\"evaluation\"][\"confidence_level\"]\n",
    "                }\n",
    "                for item in analysis_results[\"analysis\"]\n",
    "            ],\n",
    "            \"analysis_metadata\": {\n",
    "                \"total_claims_analyzed\": len(analysis_results[\"analysis\"]),\n",
    "                \"claims_with_conclusions\": len(analysis_results[\"analysis\"]),\n",
    "                \"analysis_timestamp\": str(datetime.datetime.now())\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        final_results = {\n",
    "            \"paper_analysis\": []\n",
    "        }\n",
    "        \n",
    "        for item in analysis_results[\"analysis\"]:\n",
    "            claim_id = item[\"claim_id\"]\n",
    "            analysis = {\n",
    "                \"claim_id\": claim_id,\n",
    "                \"claim\": item[\"claim\"][\"text\"],\n",
    "                \"claim_location\": item[\"claim\"][\"location\"],\n",
    "                \"evidence\": item[\"evidence\"],\n",
    "                \"evidence_locations\": [ev[\"location\"] for ev in item[\"evidence\"]],\n",
    "                \"conclusion\": {\n",
    "                    \"author_conclusion\": item[\"evaluation\"][\"justification\"],\n",
    "                    \"conclusion_justified\": item[\"evaluation\"][\"conclusion_justified\"],\n",
    "                    \"robustness_analysis\": item[\"evaluation\"][\"robustness\"],\n",
    "                    \"limitations\": item[\"evaluation\"][\"key_limitations\"],\n",
    "                    \"conclusion_location\": item[\"claim\"][\"location\"]\n",
    "                }\n",
    "            }\n",
    "            final_results[\"paper_analysis\"].append(analysis)\n",
    "        \n",
    "        return claims, evidence_results, conclusions, final_results\n",
    "\n",
    "    def print_analysis_results(self, final_results: Dict):\n",
    "        \"\"\"Print the analysis results in a readable format\"\"\"\n",
    "        print(\"\\n=== Complete Paper Analysis ===\\n\")\n",
    "        \n",
    "        for analysis in final_results['paper_analysis']:\n",
    "            print(f\"Claim {analysis['claim_id']}:\")\n",
    "            print(f\"Statement: {analysis['claim']}\")\n",
    "            print(\"\\nEvidence:\")\n",
    "            for evidence in analysis['evidence']:\n",
    "                print(f\"- {evidence['evidence_text']}\")\n",
    "                print(f\"  Strength: {evidence['strength']}\")\n",
    "                print(f\"  Limitations: {evidence['limitations']}\")\n",
    "            \n",
    "            print(\"\\nConclusion:\")\n",
    "            print(f\"Author's Conclusion: {analysis['conclusion']['author_conclusion']}\")\n",
    "            print(f\"Justified by Evidence: {'Yes' if analysis['conclusion']['conclusion_justified'] else 'No'}\")\n",
    "            print(f\"Robustness: {analysis['conclusion']['robustness_analysis']}\")\n",
    "            print(f\"Limitations: {analysis['conclusion']['limitations']}\")\n",
    "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "    def save_results(self, results: Dict, base_filename: str):\n",
    "        \"\"\"Save analysis results to files\"\"\"\n",
    "        output_dir = Path('Gemini_all_at_once_shashi')\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "\n",
    "        results[\"execution_times\"] = {\n",
    "        \"single_pass_analysis_time\": f\"{self.execution_times['single_pass_analysis']:.2f} seconds\",\n",
    "        \"total_sleep_time\": f\"{self.execution_times['total_sleep_time']:.2f} seconds\",\n",
    "        \"actual_processing_time\": f\"{self.execution_times['actual_processing_time']:.2f} seconds\",\n",
    "        \"total_execution_time\": f\"{self.execution_times['total_time']:.2f} seconds\"\n",
    "        }\n",
    "        # Save full JSON results\n",
    "        json_path = output_dir / f'{base_filename}_analysis.json'\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "\n",
    "\n",
    "        # results[\"execution_times\"] = {\n",
    "        # \"single_pass_analysis_time\": f\"{self.execution_times['single_pass_analysis']:.2f} seconds\",\n",
    "        # \"total_execution_time\": f\"{self.execution_times['total_time']:.2f} seconds\"\n",
    "        #  }\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        # Save readable text summary\n",
    "        text_path = output_dir / f'{base_filename}_summary.txt'\n",
    "        with open(text_path, 'w', encoding='utf-8') as f:\n",
    "            for analysis in results['analysis']:\n",
    "                f.write(f\"Claim {analysis['claim_id']}:\\n\")\n",
    "                f.write(f\"Type: {analysis['claim']['type']}\\n\")\n",
    "                f.write(f\"Statement: {analysis['claim']['text']}\\n\")\n",
    "                f.write(f\"Location: {analysis['claim']['location']}\\n\")\n",
    "                f.write(f\"Exact Quote: {analysis['claim']['exact_quote']}\\n\\n\")\n",
    "                \n",
    "                f.write(\"Evidence:\\n\")\n",
    "                for evidence in analysis['evidence']:\n",
    "                    f.write(f\"- Evidence Text: {evidence['evidence_text']}\\n\")\n",
    "                    f.write(f\"  Strength: {evidence['strength']}\\n\")\n",
    "                    f.write(f\"  Location: {evidence['location']}\\n\")\n",
    "                    f.write(f\"  Limitations: {evidence['limitations']}\\n\")\n",
    "                    f.write(f\"  Exact Quote: {evidence['exact_quote']}\\n\\n\")\n",
    "                \n",
    "                eval_data = analysis['evaluation']\n",
    "                f.write(\"Evaluation:\\n\")\n",
    "                f.write(f\"Conclusion Justified: {'Yes' if eval_data['conclusion_justified'] else 'No'}\\n\")\n",
    "                f.write(f\"Robustness: {eval_data['robustness']}\\n\")\n",
    "                f.write(f\"Confidence Level: {eval_data['confidence_level']}\\n\")\n",
    "                f.write(f\"Justification: {eval_data['justification']}\\n\")\n",
    "                f.write(f\"Key Limitations: {eval_data['key_limitations']}\\n\")\n",
    "                \n",
    "                f.write(\"\\n\" + \"-\"*50 + \"\\n\\n\")\n",
    "        \n",
    "        # Generate summary statistics\n",
    "        stats_path = output_dir / f'{base_filename}_statistics.txt'\n",
    "        with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "            total_claims = len(results['analysis'])\n",
    "            justified_claims = sum(1 for a in results['analysis'] \n",
    "                                 if a['evaluation']['conclusion_justified'])\n",
    "            \n",
    "            f.write(\"Analysis Statistics:\\n\")\n",
    "            f.write(f\"Total Claims Analyzed: {total_claims}\\n\")\n",
    "            f.write(f\"Justified Claims: {justified_claims}\\n\")\n",
    "            \n",
    "            # Evidence strength distribution\n",
    "            strength_levels = {}\n",
    "            for analysis in results['analysis']:\n",
    "                for evidence in analysis['evidence']:\n",
    "                    strength = evidence['strength']\n",
    "                    strength_levels[strength] = strength_levels.get(strength, 0) + 1\n",
    "            \n",
    "            f.write(\"\\nEvidence Strength Distribution:\\n\")\n",
    "            total_evidence = sum(strength_levels.values())\n",
    "            for strength, count in strength_levels.items():\n",
    "                f.write(f\"{strength}: {count} pieces ({count/total_evidence*100:.1f}%)\\n\")\n",
    "\n",
    "\n",
    "def results_exist(base_filename: str, output_folder: str) -> bool:\n",
    "    \"\"\"Check if results already exist for the given file.\"\"\"\n",
    "    output_dir = Path(output_folder)\n",
    "    analysis_path = output_dir / f'{base_filename}_analysis.json'\n",
    "    summary_path = output_dir / f'{base_filename}_summary.txt'\n",
    "    statistics_path = output_dir / f'{base_filename}_statistics.txt'\n",
    "    \n",
    "    # Check if all expected output files exist\n",
    "    return all(file.exists() for file in [analysis_path, summary_path, statistics_path])\n",
    "\n",
    "def main():\n",
    "    input_folder = 'shashi_1_papers'\n",
    "    output_folder = 'Gemini_all_at_once_shashi'\n",
    "    Path(output_folder).mkdir(exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(input_folder) if f.endswith('.pdf')]\n",
    "    failed_papers = []  # To keep track of any failures\n",
    "\n",
    "    for filename in pdf_files:\n",
    "        base_filename = Path(filename).stem\n",
    "        \n",
    "        if results_exist(base_filename, output_folder):\n",
    "            print(f\"Skipping {filename}, results already exist.\")\n",
    "            continue\n",
    "        \n",
    "        filename_with_path = f\"{input_folder}/{filename}\"\n",
    "        analyzer = SinglePassPaperAnalyzer()\n",
    "        \n",
    "        try:\n",
    "            print(f\"Starting analysis of {filename_with_path}\")\n",
    "            total_start_time = time.time()\n",
    "\n",
    "            analyzer.extract_text_from_pdf(filename_with_path)\n",
    "            analysis_results = analyzer.analyze_paper(filename_with_path)\n",
    "            analyzer.execution_times[\"total_time\"] = time.time() - total_start_time\n",
    "\n",
    "            claims, evidence_results, conclusions, final_results = analyzer.combine_results(analysis_results)\n",
    "            analyzer.print_analysis_results(final_results)\n",
    "            analyzer.save_results(final_results, base_filename)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {filename}: {str(e)}\")\n",
    "            failed_papers.append(filename_with_path)\n",
    "\n",
    "    if failed_papers:\n",
    "        print(\"Failed to process the following papers:\")\n",
    "        for paper in failed_papers:\n",
    "            print(paper)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis of shashi_1_papers/2502.12568v2.pdf\n",
      "Extracting text from PDF...\n",
      "Processing shashi_1_papers/2502.12568v2.pdf...\n",
      "[                                        ] (0/1==[===                                     ] ( 1/1==[======                                  ] ( 2/13==[=========                               ] ( 3/1==[============                            ] ( 4/13==[===============                         ] ( 5/1==[==================                      ] ( 6/13==[=====================                   ] ( 7/1==[========================                ] ( 8/13==[===========================             ] ( 9/1==[==============================          ] (10/13==[=================================       ] (11/1==[====================================    ] (12/13===[========================================] (13/13]\n",
      "Extracting claims...\n",
      "Processing file: shashi_1_papers/2502.12568v2.pdf\n",
      "Error in get_all_claims: 429 Resource has been exhausted (e.g. check quota).\n",
      "Extracting evidence...\n",
      "Error in get_all_evidence: 429 Resource has been exhausted (e.g. check quota).\n",
      "Analyzing conclusions...\n",
      "Conclusions analysis completed\n",
      "Results saved to: Gemini_3_prompts_shashi/2502.12568v2_analysis.txt\n",
      "Analysis completed successfully\n",
      "Starting analysis of shashi_1_papers/2409.15915v1.pdf\n",
      "Extracting text from PDF...\n",
      "Processing shashi_1_papers/2409.15915v1.pdf...\n",
      "[                                        ] (0/1=[==                                      ] ( 1/19=[====                                    ] ( 2/19=[======                                  ] ( 3/19=[========                                ] ( 4/19=[==========                              ] ( 5/19=[============                            ] ( 6/19=[==============                          ] ( 7/19=[================                        ] ( 8/19=[==================                      ] ( 9/19==[=====================                   ] (10/1=[=======================                 ] (11/1=[=========================               ] (12/1=[===========================             ] (13/1=[=============================           ] (14/1=[===============================         ] (15/1=[=================================       ] (16/1=[===================================     ] (17/1=[=====================================   ] (18/1=[========================================] (19/19]\n",
      "Extracting claims...\n",
      "Processing file: shashi_1_papers/2409.15915v1.pdf\n",
      "Error in get_all_claims: 429 Resource has been exhausted (e.g. check quota).\n",
      "Extracting evidence...\n",
      "Error in get_all_evidence: 429 Resource has been exhausted (e.g. check quota).\n",
      "Analyzing conclusions...\n",
      "Error in get_all_conclusions: 429 Resource has been exhausted (e.g. check quota).\n",
      "Results saved to: Gemini_3_prompts_shashi/2409.15915v1_analysis.txt\n",
      "Analysis completed successfully\n",
      "Starting analysis of shashi_1_papers/2405.04215v1.pdf\n",
      "Extracting text from PDF...\n",
      "Processing shashi_1_papers/2405.04215v1.pdf...\n",
      "[                                        ] (0/2[=                                       ] ( 1/2=[===                                     ] ( 2/2[====                                    ] ( 3/26=[======                                  ] ( 4/26[=======                                 ] ( 5/2=[=========                               ] ( 6/2[==========                              ] ( 7/26=[============                            ] ( 8/26[=============                           ] ( 9/2=[===============                         ] (10/2[================                        ] (11/26=[==================                      ] (12/26=[====================                    ] (13/26[=====================                   ] (14/2=[=======================                 ] (15/2[========================                ] (16/26=[==========================              ] (17/26[===========================             ] (18/2=[=============================           ] (19/2[==============================          ] (20/26=[================================        ] (21/26[=================================       ] (22/2=[===================================     ] (23/2[====================================    ] (24/26=[======================================  ] (25/26=[========================================] (26/26]\n",
      "Extracting claims...\n",
      "Processing file: shashi_1_papers/2405.04215v1.pdf\n",
      "Error in get_all_claims: 429 Resource has been exhausted (e.g. check quota).\n",
      "Extracting evidence...\n",
      "Error in get_all_evidence: 429 Resource has been exhausted (e.g. check quota).\n",
      "Analyzing conclusions...\n",
      "Error in get_all_conclusions: 429 Resource has been exhausted (e.g. check quota).\n",
      "Results saved to: Gemini_3_prompts_shashi/2405.04215v1_analysis.txt\n",
      "Analysis completed successfully\n",
      "Starting analysis of shashi_1_papers/2501.18817v1.pdf\n",
      "Extracting text from PDF...\n",
      "Processing shashi_1_papers/2501.18817v1.pdf...\n",
      "[                                        ] (0/2[=                                       ] ( 1/2=[===                                     ] ( 2/2[====                                    ] ( 3/25=[======                                  ] ( 4/25=[========                                ] ( 5/25[=========                               ] ( 6/2=[===========                             ] ( 7/2[============                            ] ( 8/25=[==============                          ] ( 9/25=[================                        ] (10/25[=================                       ] (11/2=[===================                     ] (12/2[====================                    ] (13/25=[======================                  ] (14/25=[========================                ] (15/25[=========================               ] (16/2=[===========================             ] (17/2[============================            ] (18/25=[==============================          ] (19/25=[================================        ] (20/25[=================================       ] (21/2=[===================================     ] (22/2[====================================    ] (23/25=[======================================  ] (24/25=[========================================] (25/25]\n",
      "Extracting claims...\n",
      "Processing file: shashi_1_papers/2501.18817v1.pdf\n",
      "Error in get_all_claims: 429 Resource has been exhausted (e.g. check quota).\n",
      "Extracting evidence...\n",
      "Error in get_all_evidence: 429 Resource has been exhausted (e.g. check quota).\n",
      "Analyzing conclusions...\n",
      "Error in get_all_conclusions: 429 Resource has been exhausted (e.g. check quota).\n",
      "Results saved to: Gemini_3_prompts_shashi/2501.18817v1_analysis.txt\n",
      "Analysis completed successfully\n",
      "Starting analysis of shashi_1_papers/2409.08642v2.pdf\n",
      "Extracting text from PDF...\n",
      "Processing shashi_1_papers/2409.08642v2.pdf...\n",
      "[                                        ] (0/1=[==                                      ] ( 1/17=[====                                    ] ( 2/17==[=======                                 ] ( 3/1=[=========                               ] ( 4/1=[===========                             ] ( 5/1==[==============                          ] ( 6/17=[================                        ] ( 7/17=[==================                      ] ( 8/17==[=====================                   ] ( 9/1=[=======================                 ] (10/1=[=========================               ] (11/1==[============================            ] (12/17=[==============================          ] (13/17=[================================        ] (14/17==[===================================     ] (15/1=[=====================================   ] (16/1==[========================================] (17/17]\n",
      "Extracting claims...\n",
      "Processing file: shashi_1_papers/2409.08642v2.pdf\n",
      "Error in get_all_claims: 429 Resource has been exhausted (e.g. check quota).\n",
      "Extracting evidence...\n",
      "Error in get_all_evidence: 429 Resource has been exhausted (e.g. check quota).\n",
      "Analyzing conclusions...\n",
      "Error in get_all_conclusions: 429 Resource has been exhausted (e.g. check quota).\n",
      "Results saved to: Gemini_3_prompts_shashi/2409.08642v2_analysis.txt\n",
      "Analysis completed successfully\n",
      "Starting analysis of shashi_1_papers/2502.12130v1.pdf\n",
      "Extracting text from PDF...\n",
      "Processing shashi_1_papers/2502.12130v1.pdf...\n",
      "[                                        ] (0/3[=                                       ] ( 1/3[==                                      ] ( 2/32[===                                     ] ( 3/3=[=====                                   ] ( 4/3[======                                  ] ( 5/32[=======                                 ] ( 6/3[========                                ] ( 7/32=[==========                              ] ( 8/32[===========                             ] ( 9/3[============                            ] (10/32[=============                           ] (11/3=[===============                         ] (12/3[================                        ] (13/32[=================                       ] (14/3[==================                      ] (15/32=[====================                    ] (16/32[=====================                   ] (17/3[======================                  ] (18/32[=======================                 ] (19/3=[=========================               ] (20/3[==========================              ] (21/32[===========================             ] (22/3[============================            ] (23/32=[==============================          ] (24/32[===============================         ] (25/3[================================        ] (26/32[=================================       ] (27/3=[===================================     ] (28/3[====================================    ] (29/32[=====================================   ] (30/3[======================================  ] (31/32=[========================================] (32/32]\n",
      "Extracting claims...\n",
      "Processing file: shashi_1_papers/2502.12130v1.pdf\n",
      "Error in get_all_claims: 429 Resource has been exhausted (e.g. check quota).\n",
      "Extracting evidence...\n",
      "Error in get_all_evidence: 429 Resource has been exhausted (e.g. check quota).\n",
      "Analyzing conclusions...\n",
      "Error in get_all_conclusions: 429 Resource has been exhausted (e.g. check quota).\n",
      "Results saved to: Gemini_3_prompts_shashi/2502.12130v1_analysis.txt\n",
      "Analysis completed successfully\n",
      "Starting analysis of shashi_1_papers/2410.14255v2.pdf\n",
      "Extracting text from PDF...\n",
      "Processing shashi_1_papers/2410.14255v2.pdf...\n",
      "[                                        ] (0/44[                                        ] ( 1/44[=                                       ] ( 2/4[==                                      ] ( 3/44[===                                     ] ( 4/4[====                                    ] ( 5/44[=====                                   ] ( 6/4[======                                  ] ( 7/44[=======                                 ] ( 8/4[========                                ] ( 9/44[=========                               ] (10/4[==========                              ] (11/4[==========                              ] (12/44[===========                             ] (13/4[============                            ] (14/44[=============                           ] (15/4[==============                          ] (16/44[===============                         ] (17/4[================                        ] (18/44[=================                       ] (19/4[==================                      ] (20/44[===================                     ] (21/4[====================                    ] (22/4[====================                    ] (23/44[=====================                   ] (24/4[======================                  ] (25/44[=======================                 ] (26/4[========================                ] (27/44[=========================               ] (28/4[==========================              ] (29/44[===========================             ] (30/4[============================            ] (31/44[=============================           ] (32/4[==============================          ] (33/4[==============================          ] (34/44[===============================         ] (35/4[================================        ] (36/44[=================================       ] (37/4[==================================      ] (38/44[===================================     ] (39/4[====================================    ] (40/44[=====================================   ] (41/4[======================================  ] (42/44[======================================= ] (43/4[========================================] (44/44]\n",
      "Extracting claims...\n",
      "Processing file: shashi_1_papers/2410.14255v2.pdf\n",
      "Error in get_all_claims: 429 Resource has been exhausted (e.g. check quota).\n",
      "Extracting evidence...\n",
      "Error in get_all_evidence: 429 Resource has been exhausted (e.g. check quota).\n",
      "Analyzing conclusions...\n",
      "Error in get_all_conclusions: 429 Resource has been exhausted (e.g. check quota).\n",
      "Results saved to: Gemini_3_prompts_shashi/2410.14255v2_analysis.txt\n",
      "Analysis completed successfully\n",
      "Starting analysis of shashi_1_papers/2402.02716v1.pdf\n",
      "Extracting text from PDF...\n",
      "Processing shashi_1_papers/2402.02716v1.pdf...\n",
      "[                                        ] (0/9===[====                                    ] (1/9===[========                                ] (2/9====[=============                           ] (3/===[=================                       ] (4/====[======================                  ] (5/9===[==========================              ] (6/9====[===============================         ] (7/===[===================================     ] (8/====[========================================] (9/9]\n",
      "Extracting claims...\n",
      "Processing file: shashi_1_papers/2402.02716v1.pdf\n",
      "Error in get_all_claims: 429 Resource has been exhausted (e.g. check quota).\n",
      "Extracting evidence...\n",
      "Error in get_all_evidence: 429 Resource has been exhausted (e.g. check quota).\n",
      "Analyzing conclusions...\n",
      "Error in get_all_conclusions: 429 Resource has been exhausted (e.g. check quota).\n",
      "Results saved to: Gemini_3_prompts_shashi/2402.02716v1_analysis.txt\n",
      "Analysis completed successfully\n",
      "Starting analysis of shashi_1_papers/2305.16653v1.pdf\n",
      "Extracting text from PDF...\n",
      "Processing shashi_1_papers/2305.16653v1.pdf...\n",
      "[                                        ] (0/43[                                        ] ( 1/43[=                                       ] ( 2/4[==                                      ] ( 3/43[===                                     ] ( 4/4[====                                    ] ( 5/43[=====                                   ] ( 6/4[======                                  ] ( 7/43[=======                                 ] ( 8/4[========                                ] ( 9/43[=========                               ] (10/4[==========                              ] (11/43[===========                             ] (12/4[============                            ] (13/43[=============                           ] (14/43[=============                           ] (15/4[==============                          ] (16/43[===============                         ] (17/4[================                        ] (18/43[=================                       ] (19/4[==================                      ] (20/43[===================                     ] (21/4[====================                    ] (22/43[=====================                   ] (23/4[======================                  ] (24/43[=======================                 ] (25/4[========================                ] (26/43[=========================               ] (27/4[==========================              ] (28/4[==========================              ] (29/43[===========================             ] (30/4[============================            ] (31/43[=============================           ] (32/4[==============================          ] (33/43[===============================         ] (34/4[================================        ] (35/43[=================================       ] (36/4[==================================      ] (37/43[===================================     ] (38/4[====================================    ] (39/43[=====================================   ] (40/4[======================================  ] (41/43[======================================= ] (42/[========================================] (43/43]\n",
      "Extracting claims...\n",
      "Processing file: shashi_1_papers/2305.16653v1.pdf\n",
      "Error in get_all_claims: 429 Resource has been exhausted (e.g. check quota).\n",
      "Extracting evidence...\n",
      "Error in get_all_evidence: 429 Resource has been exhausted (e.g. check quota).\n",
      "Analyzing conclusions...\n",
      "Error in get_all_conclusions: 429 Resource has been exhausted (e.g. check quota).\n",
      "Results saved to: Gemini_3_prompts_shashi/2305.16653v1_analysis.txt\n",
      "Analysis completed successfully\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "import datetime\n",
    "import pymupdf4llm\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "import traceback\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "class PaperAnalyzer:\n",
    "    def __init__(self):\n",
    "        api_key = os.getenv(\"GEMINI_AI_API_KEY\")\n",
    "        if not api_key:\n",
    "            raise ValueError(\"API key not found. Please set it in the .env file.\")\n",
    "        \n",
    "\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel('gemini-1.5-pro')\n",
    "        self.paper_text = None\n",
    "        self.execution_times = {\n",
    "        \"claims_analysis\": 0,\n",
    "        \"evidence_analysis\": 0,\n",
    "        \"conclusions_analysis\": 0,\n",
    "        \"total_time\": 0,\n",
    "        \"total_sleep_time\": 0,  # Track total sleep time\n",
    "        \"actual_processing_time\": 0  # Time without sleep delays\n",
    "            }\n",
    "        \n",
    "\n",
    "\n",
    "    def extract_text_from_pdf(self, filename: str) -> str:\n",
    "        \"\"\"Extract text from PDF file using PyMuPDF\"\"\"\n",
    "        try:\n",
    "            self.paper_text = pymupdf4llm.to_markdown(filename)\n",
    "            return self.paper_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from PDF: {e}\")\n",
    "            return \"\"\n",
    "    def get_all_claims(self, filename: str) -> str:\n",
    "        \"\"\"Get all claims in text format\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            if not self.paper_text:\n",
    "                text = self.extract_text_from_pdf(filename)\n",
    "            else:\n",
    "                text = self.paper_text\n",
    "\n",
    "            print(f\"Processing file: {filename}\")\n",
    "            \n",
    "            claims_prompt = f\"\"\"\n",
    "            Paper text: {text}\n",
    "            \n",
    "            Please identify all statements that meet these criteria for a claim:\n",
    "            1. Makes a specific, testable assertion about results, methods, or contributions\n",
    "            2. Represents a novel finding, improvement, or advancement\n",
    "            3. Presents a clear position or conclusion\n",
    "\n",
    "            Format each claim as follows:\n",
    "            Claim #: [claim text]\n",
    "            Location: [section/paragraph]\n",
    "            Type: [nature of claim]\n",
    "            Quote: [exact quote from text]\n",
    "\n",
    "            List all claims you can find.\n",
    "            \"\"\"\n",
    "            \n",
    "            time.sleep(90)  # Rate limiting\n",
    "            self.execution_times[\"total_sleep_time\"] += 90\n",
    "            response = self.model.generate_content(claims_prompt)\n",
    "            self.execution_times[\"claims_analysis\"] = time.time() - start_time\n",
    "\n",
    "            print(\"Claims extraction completed\")\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error in get_all_claims: {str(e)}\")\n",
    "            return \"Error extracting claims\"\n",
    "\n",
    "    def get_all_evidence(self, filename: str, claims_text: str) -> str:\n",
    "        \"\"\"Get evidence for claims in text format\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            if not self.paper_text:\n",
    "                text = self.extract_text_from_pdf(filename)\n",
    "            else:\n",
    "                text = self.paper_text\n",
    "            \n",
    "            evidence_prompt = f\"\"\"\n",
    "            Paper text: {text}\n",
    "\n",
    "            For these claims:\n",
    "            {claims_text}\n",
    "\n",
    "            Please identify relevant evidence for each claim that:\n",
    "            1. Directly supports or contradicts the claim\n",
    "            2. Is presented with experimental results or data\n",
    "            3. Can be traced to specific methods or results\n",
    "            4. Is not from abstract or introduction\n",
    "\n",
    "            Format for each claim:\n",
    "            Claim #: [repeat claim]\n",
    "            Evidence:\n",
    "            1. [evidence text]\n",
    "    - Strength: [strong/moderate/weak]\n",
    "    - Limitations: [key limitations]\n",
    "    - Location: [section/paragraph]\n",
    "    - Quote: [exact quote]\n",
    "\n",
    "            List all evidence for each claim.\n",
    "            \"\"\"\n",
    "            \n",
    "            time.sleep(90)\n",
    "            self.execution_times[\"total_sleep_time\"] += 90\n",
    "            response = self.model.generate_content(evidence_prompt)\n",
    "            \n",
    "            self.execution_times[\"evidence_analysis\"] = time.time() - start_time\n",
    "            print(\"Evidence extraction completed\")\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error in get_all_evidence: {str(e)}\")\n",
    "            return \"Error extracting evidence\"\n",
    "\n",
    "    def get_all_conclusions(self, filename: str, claims_text: str, evidence_text: str) -> str:\n",
    "        \"\"\"Analyze conclusions in text format\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            if not self.paper_text:\n",
    "                text = self.extract_text_from_pdf(filename)\n",
    "            else:\n",
    "                text = self.paper_text\n",
    "            \n",
    "            conclusions_prompt = f\"\"\"\n",
    "            Paper text: {text}\n",
    "\n",
    "            Based on these claims:\n",
    "            {claims_text}\n",
    "\n",
    "            And their evidence:\n",
    "            {evidence_text}\n",
    "\n",
    "            Please provide conclusions for each claim:\n",
    "            1. Whether the evidence justifies the claim\n",
    "            2. Overall strength of support\n",
    "            3. Important limitations\n",
    "\n",
    "            Format for each claim:\n",
    "            Claim #: [repeat claim]\n",
    "            Conclusion:\n",
    "            - Justified: [yes/no]\n",
    "            - Robustness: [high/medium/low]\n",
    "            - Limitations: [specific limitations]\n",
    "            - Confidence: [high/medium/low]\n",
    "            \"\"\"\n",
    "            \n",
    "            time.sleep(90)\n",
    "            self.execution_times[\"total_sleep_time\"] += 90\n",
    "            response = self.model.generate_content(conclusions_prompt)\n",
    "            \n",
    "            self.execution_times[\"conclusions_analysis\"] = time.time() - start_time\n",
    "            print(\"Conclusions analysis completed\")\n",
    "            return response.text\n",
    "        except Exception as e:\n",
    "            print(f\"Error in get_all_conclusions: {str(e)}\")\n",
    "            return \"Error generating conclusions\"\n",
    "\n",
    "    def analyze_paper(self, filename: str) -> Dict:\n",
    "        \"\"\"Complete paper analysis using text-based approach\"\"\"\n",
    "        try:\n",
    "            total_start_time = time.time()\n",
    "\n",
    "            print(\"Extracting text from PDF...\")\n",
    "            self.extract_text_from_pdf(filename)\n",
    "\n",
    "            print(\"Extracting claims...\")\n",
    "            claims_text = self.get_all_claims(filename)\n",
    "\n",
    "            print(\"Extracting evidence...\")\n",
    "            evidence_text = self.get_all_evidence(filename, claims_text)\n",
    "\n",
    "            print(\"Analyzing conclusions...\")\n",
    "            conclusions_text = self.get_all_conclusions(filename, claims_text, evidence_text)\n",
    "\n",
    "            # Calculate times\n",
    "            total_elapsed = time.time() - total_start_time\n",
    "            self.execution_times[\"total_time\"] = total_elapsed\n",
    "            self.execution_times[\"actual_processing_time\"] = (\n",
    "                total_elapsed - self.execution_times[\"total_sleep_time\"]\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                \"claims\": claims_text,\n",
    "                \"evidence\": evidence_text,\n",
    "                \"conclusions\": conclusions_text,\n",
    "                \"execution_times\": self.execution_times\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in paper analysis: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def save_results(self, results: Dict, filename: str):\n",
    "        \"\"\"Save analysis results in text format\"\"\"\n",
    "        try:\n",
    "            base_filename = filename\n",
    "            output_dir = \"Gemini_3_prompts_shashi\"\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            # Save all results in a single text file\n",
    "            output_filename = f'{output_dir}/{base_filename}_analysis.txt'\n",
    "            with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"=== PAPER ANALYSIS RESULTS ===\\n\\n\")\n",
    "                \n",
    "                f.write(\"=== CLAIMS ===\\n\")\n",
    "                f.write(results['claims'])\n",
    "                f.write(\"\\n\\n\")\n",
    "                \n",
    "                f.write(\"=== EVIDENCE ===\\n\")\n",
    "                f.write(results['evidence'])\n",
    "                f.write(\"\\n\\n\")\n",
    "                \n",
    "                f.write(\"=== CONCLUSIONS ===\\n\")\n",
    "                f.write(results['conclusions'])\n",
    "                f.write(\"\\n\\n\")\n",
    "                \n",
    "                f.write(\"=== EXECUTION TIMES ===\\n\")\n",
    "                f.write(f\"Claims Analysis Time: {results['execution_times']['claims_analysis']:.2f} seconds\\n\")\n",
    "                f.write(f\"Evidence Analysis Time: {results['execution_times']['evidence_analysis']:.2f} seconds\\n\")\n",
    "                f.write(f\"Conclusions Analysis Time: {results['execution_times']['conclusions_analysis']:.2f} seconds\\n\")\n",
    "                f.write(f\"Total Sleep Time: {results['execution_times']['total_sleep_time']:.2f} seconds\\n\")\n",
    "                f.write(f\"Actual Processing Time: {results['execution_times']['actual_processing_time']:.2f} seconds\\n\")\n",
    "                f.write(f\"Total Time: {results['execution_times']['total_time']:.2f} seconds\\n\")\n",
    "\n",
    "            print(f\"Results saved to: {output_filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {str(e)}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    analyzer = PaperAnalyzer()\n",
    "    \n",
    "    # filename = \"Ax_Hao_Hang_2.pdf\"\n",
    "\n",
    "    input_folder = 'shashi_1_papers'\n",
    "\n",
    "    pdf_files = [f for f in os.listdir(input_folder) if f.endswith('.pdf')]\n",
    "\n",
    "    for filename in pdf_files:\n",
    "        basefile_name = Path(filename).stem\n",
    "        try:\n",
    "            filename = f\"{input_folder}/{filename}\"\n",
    "            print(f\"Starting analysis of {filename}\")\n",
    "                \n",
    "            # Analyze paper\n",
    "            results = analyzer.analyze_paper(filename)\n",
    "            \n",
    "            if results:\n",
    "                # Save results in structured format\n",
    "                analyzer.save_results(results, basefile_name)\n",
    "                print(\"Analysis completed successfully\")\n",
    "            else:\n",
    "                print(\"Analysis failed to produce results\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error in main execution: {str(e)}\")\n",
    "            traceback.print_exc()\n",
    "\n",
    "        \n",
    "    #     print(f\"Starting analysis of {filename}\")\n",
    "        \n",
    "    #     # Analyze paper\n",
    "    #     results = analyzer.analyze_paper(filename)\n",
    "        \n",
    "    #     if results:\n",
    "    #         # Save results in structured format\n",
    "    #         analyzer.save_results(results, filename)\n",
    "    #         print(\"Analysis completed successfully\")\n",
    "    #     else:\n",
    "    #         print(\"Analysis failed to produce results\")\n",
    "            \n",
    "    # except Exception as e:\n",
    "    #     print(f\"Error in main execution: {str(e)}\")\n",
    "    #     traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
