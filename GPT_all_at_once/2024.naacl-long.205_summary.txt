Claim 1:
Type: contribution
Statement: Ada-LEval requires significantly more full-text comprehension for its tasks than traditional QA and summarization tasks.
Location: section 4.5.4
Exact Quote: our benchmarks require much overall text understanding to complete the task than traditional long-context benchmarks.

Evidence:
- Evidence Text: Performance comparison of GPT-4-Turbo-1106 on BestAnswer, NarrativeQA, and GovReport shows a sharper performance decline on BestAnswer when text is truncated.
  Strength: moderate
  Location: section 4.5.4
  Limitations: Limited to one model's performance comparison across three datasets.
  Exact Quote: the performance of GPT-4-Turbo-1106 on BestAnswer decreases more dramatically than NarrativeQA and GovReport when text is truncated.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the evidence of a model's performance decline on the BestAnswer task, indicating higher demand for full-text comprehension.
Key Limitations: Performance assessment is limited to GPT-4-Turbo-1106 and may not generalize across models. The evaluation considers a narrow set of tasks.

--------------------------------------------------

Claim 2:
Type: result
Statement: Proprietary models and open-source models both show performance limitations in ultra-long-context settings.
Location: section 4.4
Exact Quote: For the TSort task, GPT-4-Turbo is able to achieve a random guess level accuracy, while Claude fails to give any correct answers.

Evidence:
- Evidence Text: In ultra-long-context settings, even the best performing models do not surpass random guess accuracy on TSort and see sharp performance declines on BestAnswer.
  Strength: strong
  Location: sections 4.4 and 4.2
  Limitations: Data specifically pertaining to ultra-long-context settings; no comparison with human performance.
  Exact Quote: all three models fall sharply from 16k to 32k text length. Meanwhile, they can not give any correct answer when the text length is greater than 32k.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Evidence convincingly shows a general performance bottleneck for both proprietary and open-source LLMs in ultra-long-context settings, which is crucial for future model development.
Key Limitations: Evaluation focused on a subset of models and tasks, potentially overlooking areas where models may perform well in ultra-long contexts.

--------------------------------------------------

