Claim 1:
Type: result
Statement: Suppressing or amplifying the activation of knowledge neurons can accordingly affect the strength of knowledge expression in pretrained Transformers.
Location: Conclusion and Future Directions
Exact Quote: suppressing or amplifying the activation of knowledge neurons can accordingly affect the strength of knowledge expression

Evidence:
- Evidence Text: Suppressing knowledge neurons decreases the correct probability by 29.03% on average, while amplifying knowledge neurons increases the correct probability by 31.17% on average.
  Strength: strong
  Location: Results of suppressing and amplifying knowledge neurons
  Limitations: Analysis based on specific relational facts without considering more diverse or complex knowledge expressions.
  Exact Quote: Suppressing knowledge neurons decreases the correct probability by 29.03% on average. For the baseline, the decreasing ratio is 1.47% on average. Amplifying knowledge neurons increases the correct probability by 31.17% on average. For the baseline, the correct probability even decreases by 1.27%.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provides clear, quantitative effects of neuron manipulation on knowledge expression with significant difference from baseline, indicating precise identification and effect attribution to knowledge neurons.
Key Limitations: Limited by the relational fact-based evaluation, the broader applicability of identifying and manipulating such neurons in diverse or more complex tasks remains untested.

--------------------------------------------------

Claim 2:
Type: result
Statement: Knowledge neurons in pretrained Transformers are positively correlated to knowledge expression.
Location: Conclusion and Future Directions
Exact Quote: knowledge neurons tend to be activated by the corresponding knowledge-expressing prompts

Evidence:
- Evidence Text: Quantitative and qualitative analysis demonstrate that knowledge neurons are more significantly activated by knowledge-expressing prompts compared to control groups.
  Strength: strong
  Location: Quantitative and qualitative analysis on open-domain texts
  Limitations: Analysis conducted within the scope of available datasets and prompt variations; broader external validity needs further testing.
  Exact Quote: the identified knowledge neurons are more significantly activated by knowledge-expressing prompts (T1 = 0.485), compared with the control groups (T2 = 0.019 and T3 = âˆ’0.018)

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Supportive evidence through activation analysis confirms the correlation, providing a solid foundation for the claim of knowledge neuron correlation with knowledge expression.
Key Limitations: External validity constraint as analysis might not cover all forms of knowledge expressions across diverse domains.

--------------------------------------------------

Claim 3:
Type: contribution
Statement: Knowledge neurons provide a foundational mechanism for potentially updating or erasing specific knowledge in pretrained Transformers without fine-tuning.
Location: Conclusion and Future Directions
Exact Quote: we present two preliminary case studies that attempt to utilize knowledge neurons to update or erase knowledge in pretrained Transformers

Evidence:
- Evidence Text: Case studies on updating and erasing relationships show nontrivial success rates and indicate that manipulation of a few neurons can significantly influence knowledge expression.
  Strength: moderate
  Location: Case Studies of updating facts and erasing relations
  Limitations: Preliminary nature of case studies without extensive validations across more scenarios or models.
  Exact Quote: the surgery of knowledge neurons achieves a nontrivial success rate for updating facts, while random neurons are insufficient. We can further improve the success rate by including more top knowledge neurons in the update process.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence from case studies supports the claim, outlining a path towards knowledge manipulation within neural networks without comprehensive retraining.
Key Limitations: Preliminary results from limited case studies necessitate broader empirical scrutiny for general applicability.

--------------------------------------------------

