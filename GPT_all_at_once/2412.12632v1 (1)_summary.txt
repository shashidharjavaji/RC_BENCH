Claim 1:
Type: result
Statement: External knowledge equipped with CoE can help LLMs generate correct answers more effectively than Non-CoE.
Location: Section 5.2 Results and Findings
Exact Quote: CoE achieves an average accuracy of 92.0% across five LLMs and two datasets, outperforming Non-CoE variants SenP and WordP by 22.5% and 16.3%, respectively.

Evidence:
- Evidence Text: CoE attains an average accuracy of 92.0%, significantly higher than Non-CoE variants.
  Strength: strong
  Location: Section 5.2 Results and Findings/Table 2
  Limitations: Does not detail individual LLM performance variation or the specifics of dataset complexity.
  Exact Quote: CoE achieves an average accuracy of 92.0% across five LLMs and two datasets, outperforming Non-CoE variants SenP and WordP by 22.5% and 16.3%, respectively.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Statistical tests (Mann-Whitney) support the improvement of CoE over Non-CoE.
Key Limitations: The study does not explore the impact of varying complexities or domains within datasets.

--------------------------------------------------

Claim 2:
Type: result
Statement: LLMs exhibit higher faithfulness to the answer implicated in CoE (than Non-CoE), even when CoE contains factual errors.
Location: Section 6.2 Results and Findings
Exact Quote: The results show that under CoE, the average Following Rate (FR) reaches 85.4%, which is significantly higher than under Non-CoE.

Evidence:
- Evidence Text: Under CoE, the average FR reaches 85.4%, which is significantly higher than the SenP and WordP types under Non-CoE.
  Strength: strong
  Location: Section 6.2 Results and Findings
  Limitations: Limited by the scope of the study's design to idealized experimental conditions which may not fully replicate real-world scenarios.
  Exact Quote: Under CoE, the average FR reaches 85.4%, which is 20.6% and 16.2% higher than the SenP and WordP types under Non-CoE, respectively.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Statistically significant differences demonstrated using Mann-Whitney tests.
Key Limitations: Does not account for variations in LLM architectures or differences in CoE construction quality.

--------------------------------------------------

Claim 3:
Type: result
Statement: CoE-enhanced LLMs demonstrate higher robustness against knowledge conflict than Non-CoE, evidenced by smaller reductions in accuracy under misinformation.
Location: Section 7.2 Results and Findings
Exact Quote: LLMs augmented with CoE exhibit higher robustness against knowledge conflict than Non-CoE... with average ACC reaching 84.1%.

Evidence:
- Evidence Text: Average ACC for CoE is 84.1%, significantly higher than SenP and WordP under misinformation conditions.
  Strength: strong
  Location: Section 7.2 Results and Findings/Table 4
  Limitations: Does not discuss how misinformation quality may affect LLMs' performance or the interaction between misinformation and accuracy in depth.
  Exact Quote: The average ACC of LLMs under CoE reaches 84.1%, which is 21.4% and 15.3% higher than the SenP and WordP types under Non-CoE, respectively.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Empirical evidence strongly suggests CoE's effectiveness in maintaining higher ACC under misinformation than Non-CoE.
Key Limitations: Analysis is constrained to experimental setups that may not cover all real-world application scenarios.

--------------------------------------------------

Claim 4:
Type: contribution
Statement: CoE-guided retrieval improves LLM accuracy in a naive RAG framework.
Location: Section 8.4 Results and Findings
Exact Quote: RAG+ScopeCoE achieves average ACC of 77.8% and 81.6% on HotpotQA and 2WikiMultihopQA... outperforming RAG.

Evidence:
- Evidence Text: RAG+ScopeCoE outperforms the standard RAG with higher accuracy across two datasets.
  Strength: strong
  Location: Section 8.4 Results and Findings/Table 5
  Limitations: Evaluation limited to two specific datasets and does not explore the broader impact across different domains or task types.
  Exact Quote: RAG+ScopeCoE achieves average ACC of 77.8% and 81.6% on HotpotQA and 2WikiMultihopQA respectively, outperforming RAG.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Substantial improvement in ACC with the CoE-guided retrieval support RAG+ScopeCoE's effectiveness.
Key Limitations: Performance enhancements may vary depending on the complexity and nature of questions and external knowledge's quality.

--------------------------------------------------

