Claim 1:
Type: result
Statement: LLMs possess intrinsic self-knowledge, capable of recognizing unanswerable or unknowable questions.
Location: Introduction & Conclusion sections
Exact Quote: our extensive analysis, involving 20 LLMs including GPT-3, InstructGPT, and LLaMA, discovering an intrinsic capacity for self-knowledge within these models.

Evidence:
- Evidence Text: GPT-4 demonstrated a self-knowledge level with an F1 score of 75.47%, a significant improvement but still behind human benchmark of 84.93%.
  Strength: strong
  Location: Conclusion section
  Limitations: Comparison to human benchmark suggests room for improvement.
  Exact Quote: GPT-4 currently performs best among the tested models, achieving an impressive F1 score of 75.47%.

- Evidence Text: Instruction tuning and in-context learning significantly enhance model self-knowledge.
  Strength: moderate
  Location: Analysis section
  Limitations: Effectiveness varies significantly across different models and input forms.
  Exact Quote: models from the InstructGPT series exhibit a superior level of self-knowledge compared to their GPT-3 counterparts.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Evidence supports the conclusion that LLMs possess self-knowledge, with a clear correlation between enhanced techniques and higher levels of self-awareness. However, the comparison against human capability indicates limitations.
Key Limitations: Does not account for nuances in model performance across varied data sets and real-world scenarios.

--------------------------------------------------

Claim 2:
Type: contribution
Statement: A new dataset, SelfAware, was developed to better evaluate LLMs' ability to discern unanswerable questions across five distinct categories.
Location: Dataset Construction section
Exact Quote: we created a new dataset called SelfAware. This dataset comprises 1,032 unanswerable questions, which are distributed across five distinct categories.

Evidence:
- Evidence Text: SelfAware dataset includes 1,032 unanswerable questions and 2,337 answerable questions, ensuring a broad evaluation.
  Strength: strong
  Location: Dataset Construction section
  Limitations: Focus on unanswerable questions may not fully represent the diverse challenges present in real-world scenarios.
  Exact Quote: Our dataset incorporates 1,032 unanswerable and 2,337 answerable questions.

- Evidence Text: Questions were carefully selected and evaluated by experienced analysts to ensure quality.
  Strength: moderate
  Location: Dataset Construction section
  Limitations: Subjective bias in question selection and evaluation by analysts could affect dataset objectivity.
  Exact Quote: These questions were meticulously evaluated by three seasoned annotation analysts.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The creation of the SelfAware dataset fills a gap in the assessment of LLMs' ability to handle unanswerable questions, with robust processes in place for question selection and categorization.
Key Limitations: Limited by potential biases in the selection process; further external validation of dataset utility and breadth needed.

--------------------------------------------------

Claim 3:
Type: result
Statement: Experimental results demonstrate that model size correlates positively with self-knowledge, exhibiting improved performance with increased parameters.
Location: Analysis section
Exact Quote: an LLMâ€™s self-knowledge tends to enhance with increasing model size, a trend consistent with the scaling law.

Evidence:
- Evidence Text: Larger models consistently show better F1 scores, indicating higher self-knowledge.
  Strength: strong
  Location: Analysis section
  Limitations: Does not consider computational cost and efficiency of scaling model parameters.
  Exact Quote: an augmentation in model parameter size is associated with an elevation in the F1 Score.

- Evidence Text: Notably, Vicuna-13B outperforms LLaMA-65B after instruction tuning, highlighting the impact beyond mere size.
  Strength: moderate
  Location: Analysis section
  Limitations: Single data point; broader research needed to explore instruction tuning's influence across various models and sizes.
  Exact Quote: Among these, Vicuna-13B outperforms the LLaMA-65B, corroborating the efficacy of instruction tuning for enhancing model self-knowledge.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Evidence strongly suggests model size plays a critical role in self-knowledge. The unexpectedly high performance of Vicuna-13B also points to instruction tuning as an influential factor.
Key Limitations: Conclusions based on dataset-specific performance; real-world applicability and efficiency considerations left unaddressed.

--------------------------------------------------

