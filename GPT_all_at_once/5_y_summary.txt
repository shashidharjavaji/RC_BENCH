Claim 1:
Type: contribution
Statement: The ResNet-like architecture can serve as a strong and effective baseline for tabular DL.
Location: Conclusion section
Exact Quote: a simple ResNet-like architecture can serve as an effective baseline

Evidence:
- Evidence Text: ResNet turns out to be an effective baseline that none of the competitors can consistently outperform.
  Strength: strong
  Location: Comparing DL models section
  Limitations: No direct comparison with all existing DL models for tabular data.
  Exact Quote: ResNet turns out to be an effective baseline that none of the competitors can consistently outperform.

- Evidence Text: In ensemble settings, both ResNet and FT-Transformer performances improve, indicating ResNet's effectiveness.
  Strength: moderate
  Location: Results for ensembles of DL models section
  Limitations: Limited by ensemble comparison and specific dataset performance.
  Exact Quote: FT-Transformer and ResNet benefit more from ensembling; in this regime, FT-Transformer outperforms NODE and the gap between ResNet and NODE is significantly reduced.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Supported by experimental results showing ResNet's competitive performance across multiple datasets.
Key Limitations: Some performance evaluations are specific to datasets and might not represent a universal advantage.

--------------------------------------------------

Claim 2:
Type: contribution
Statement: FT-Transformer is a new powerful solution for tabular DL, outperforming other DL solutions on most tasks.
Location: Conclusion section
Exact Quote: FT-Transformer â€” a simple adaptation of the Transformer architecture that outperforms other DL solutions on most of the tasks

Evidence:
- Evidence Text: FT-Transformer demonstrates the best performance on most tasks and becomes a new powerful solution for the field.
  Strength: strong
  Location: Comparing DL models section
  Limitations: Performance might not generalize to all types of tabular data tasks.
  Exact Quote: FT-Transformer performs best on most tasks and becomes a new powerful solution for the field.

- Evidence Text: FT-Transformer's ensemble outperforms NODE and closely matches or exceeds ResNet in several datasets, confirming its effectiveness.
  Strength: strong
  Location: Results for ensembles of DL models section
  Limitations: Comparison primarily against NODE and ResNet; may exclude other competitive architectures.
  Exact Quote: FT-Transformer outperforms NODE and the gap between ResNet and NODE is significantly reduced.

- Evidence Text: FT-Transformer maintains competitive performance across a wider range of tasks, especially where GBDT outperforms ResNet, showcasing its versatility.
  Strength: moderate
  Location: Analyzing why FT-Transformer is better than ResNet section
  Limitations: The observation is tied to specific tasks where GBDT has an edge, not all possible tabular DL scenarios.
  Exact Quote: FT-Transformer shows the most convincing improvements over ResNet exactly on those datasets where GBDT outperforms ResNet.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Strong experimental evidence across a variety of datasets and comparison settings supports the claim.
Key Limitations: Generalizability across all tabular DL tasks not fully established.

--------------------------------------------------

Claim 3:
Type: result
Statement: No universally superior solution exists among GBDT and the best DL models for tabular data.
Location: Conclusion section
Exact Quote: demonstrated that GBDT still dominates on some tasks

Evidence:
- Evidence Text: Comparison between DL models and GBDT shows that GBDT outperforms DL models on some tasks, indicating no universally superior solution.
  Strength: strong
  Location: Comparing DL models and GBDT section
  Limitations: Lack of a comprehensive comparison across all types of tabular data tasks.
  Exact Quote: GBDT outperforms DL models on some tasks

- Evidence Text: DL models show potential to outperform GBDT in ensemble settings, but no clear superior model emerges.
  Strength: moderate
  Location: Results for ensembles of GBDT and the main DL models section
  Limitations: Comparison focuses on ensemble settings, which might not reflect single model performance directly.
  Exact Quote: deep architectures will benefit more from ensembling

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Supported by data indicating both GBDT and DL models' dominance in different tasks but lacks a conclusively superior model.
Key Limitations: Finding is based on currently considered datasets and models; future advancements in DL for tabular data might change the scenario.

--------------------------------------------------

