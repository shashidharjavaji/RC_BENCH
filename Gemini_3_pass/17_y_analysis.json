{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "Combining tabular data from financial semi-structured documents with text transcripts and audio recordings not only improves stock volatility and price movement prediction by 5-12% but also reduces gender bias caused due to audio-based neural networks by over 30%.",
                "type": "Novel finding",
                "location": "Abstract",
                "exact_quote": "Financial prediction is complex due to the stochastic nature of the stock market. Semistructured financial documents present comprehensive financial data in tabular formats, such as earnings, profit-loss statements, and balance sheets, and can often contain more than 100\u2019s tables worth of technical analysis along with a textual discussion of corporate history, and management analysis, compliance, and risks. Existing research focuses on the textual and audio modalities of financial disclosures from company conference calls to forecast stock volatility and price movement, but ignores the rich tabular data available in financial reports. Moreover, the economic realm is still plagued with a severe under-representation of various communities spanning diverse demographics, gender, and native speakers. In this work, we show that combining tabular data from financial semi-structured documents with text transcripts and audio recordings not only improves stock volatility and price movement prediction by 5-12% but also reduces gender bias caused due to audio-based neural networks by over 30%."
            },
            "evidence": [
                {
                    "evidence_text": "We show significant gains (8-12%) in both tasks across attention based (MDRM, VoLTAGE, MMFTR) and Transformer models (M3A) by combining tabular information extracted from financial semi-structured documents with text-audio time series.",
                    "strength": "Strong",
                    "limitations": "None",
                    "location": "Results",
                    "exact_quote": "We show significant gains (8-12%) in both tasks across attention based (MDRM, VoLTAGE, MMFTR) and Transformer models (M3A) by combining tabular information extracted from financial semi-structured documents with text-audio time series."
                },
                {
                    "evidence_text": "Our method helps the underlying neural architectures utilize contextualize information related to compliance, risks, and future plans from audio-textual utterances with technical indicators presented in financial reports.",
                    "strength": "Strong",
                    "limitations": "None",
                    "location": "Results",
                    "exact_quote": "Our method helps the underlying neural architectures utilize contextualize information related to compliance, risks, and future plans from audio-textual utterances with technical indicators presented in financial reports."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "High",
                "justification": "",
                "key_limitations": "None",
                "confidence_level": "High"
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "Supplementing existing conference calls transcripts with the tabular financial data substantially reduces the unintended gender bias in financial prediction tasks and offers a robust and unbiased alternative to gender-sensitive audio features in cases where under-representation of women speakers (only 7% female speakers in SP 500 Earnings calls dataset (Li et al., 2020) and 12% in Merger&Acquisition calls (Sawhney et al., 2021b) dataset) in executive positions induces unneeded correlations in model predictions.",
                "type": "Novel finding",
                "location": "Introduction",
                "exact_quote": "Recent studies such as Sawhney et al. (2021a) have highlighted the downside of utilizing audiobased multimodal approaches for financial risk prediction due to the inherent gender bias induced in learning models due to the imbalance of speaker demographics in call recordings . Audio features such as speakers\u2019 pitch and intensity can vary greatly across genders. Under-representation of female executives in conference calls is amplified by deep learning models, leading to high error disparity between stock predictions across sensitive attributes. We combine tabular from financial semistructured documents input with existing vocalverbal information from audio call recordings to improve stock price movement and volatility prediction. We demonstrate that supplementing existing conference calls transcripts with the tabular financial data substantially reduces the unintended gender bias in financial prediction tasks and offers a robust and unbiased alternative to gender-sensitive audio features in cases where under-representation of women speakers (only 7% female speakers in SP 500 Earnings calls dataset (Li et al., 2020) and 12% in Merger&Acquisition calls (Sawhney et al., 2021b) dataset) in executive positions induces unneeded correlations in model predictions."
            },
            "evidence": [
                {
                    "evidence_text": "We observe that the table modality has the least error disparity.",
                    "strength": "Strong",
                    "limitations": "None",
                    "location": "Results",
                    "exact_quote": "We observe that the table modality has the least error disparity."
                },
                {
                    "evidence_text": "Audio modality has consistently higher error individually as well as in combination with either of the other modalities, while it significantly drops when considering just text and table data.",
                    "strength": "Moderate",
                    "limitations": "Does not provide specific numbers for the reduction in gender bias",
                    "location": "Results",
                    "exact_quote": "Audio modality has consistently higher error individually as well as in combination with either of the other modalities, while it significantly drops when considering just text and table data."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "Medium",
                "justification": "",
                "key_limitations": "Does not provide specific numbers for the reduction in gender bias",
                "confidence_level": "Medium"
            }
        }
    ],
    "execution_times": {
        "single_pass_analysis_time": "294.77 seconds",
        "total_execution_time": "296.91 seconds"
    }
}