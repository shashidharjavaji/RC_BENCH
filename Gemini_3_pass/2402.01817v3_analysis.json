{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "LLM are not, by themselves, able to do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature.",
                "type": "Position",
                "location": "Introduction",
                "exact_quote": "We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature."
            },
            "evidence": [],
            "evaluation": {
                "conclusion_justified": null,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end format translators.",
                "type": "Position",
                "location": "Introduction",
                "exact_quote": "We also argue that LLMs should be viewed as universal approximate knowledge sources that have much more meaningful roles to play in planning/reasoning tasks beyond simple front-end/back-end format translators."
            },
            "evidence": [],
            "evaluation": {
                "conclusion_justified": null,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "LLMs can be a whole lot more than machine translators, extracting reasoning problems embedded in textual format to symbolic representations. They are a kind of approximate knowledge source (albeit sans guarantees) trained on our collective consciousness.",
                "type": "Position",
                "location": "Introduction",
                "exact_quote": "In truth, LLMs can be a whole lot more than machine translating our reasoning problems embedded in textual format to symbolic representations. They are a kind of approximate knowledge source (albeit sans guarantees) trained on our collective consciousness."
            },
            "evidence": [],
            "evaluation": {
                "conclusion_justified": null,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "There is also unwarranted pessimism about the roles LLMs can play in planning/reasoning tasks.",
                "type": "Position",
                "location": "Introduction",
                "exact_quote": "Several efforts (e.g. (Liu et al., 2023; Pan et al., 2023; Xie et al., 2023)) advocate using LLMs only as glorified translators\u2013converting reasoning problems embedded in textual format to symbolic representations, and pawning them off to external classical symbolic solvers (with all their attendant expressivity and search complexity challenges (Doyle & Patil, 1991)).[2]"
            },
            "evidence": [],
            "evaluation": {
                "conclusion_justified": null,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "LLM-Modulo framework proposed in this position paper tackles this challenge.",
                "type": "Position",
                "location": "Introduction",
                "exact_quote": "The goal of this position paper is to introduce some clarity into this confusing state of affairs oscillating between over-optimism and over-pessimism. Simply put, we take the stance that LLMs are amazing giant external non-veridical memories that can serve as powerful cognitive orthotics for human or machine agents, if rightly used. The underlying ngram nature makes them effortlessly intermix what would be considered disparate fields of study (not surprisingly, LLMs are seen to be very good at making/finding analogies!). The challenge is to leverage them without wrongly ascribing to them capabilities they don\u2019t possess. The LLM-Modulo framework (a name loosely inspired by SAT Modulo Theories (Nieuwenhuis & Oliveras, 2006)); see Figure 3. LLMs play a spectrum of roles in this architecture, from guessing candidate plans, to translating those plans into syntactic forms that are more accessible to external critics, to helping end users flesh out incomplete specifications, to helping expert users acquire domain models (that in turn drive model-based critics). All this leveraging of LLMs is done without ascribing to them any planning or verification abilities. The LLM ideas are vetted by external critics, thus ensuring that the plans generated in this architecture can have formal correctness guarantees where possible."
            },
            "evidence": [],
            "evaluation": {
                "conclusion_justified": null,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "LLM\u2019s cannot generate executable plans in autonomous modes.",
                "type": "Finding",
                "location": "Section 2: Planning-centered Limitations of LLMs",
                "exact_quote": "Despite initial claims about the planning capabilities of LLMs (Bairi et al., 2023; Yao et al., 2023b; Shinn et al., 2023; Huang et al., 2022; Hao et al., 2023) several recent studies confirm that LLMs are not actually able to generate executable plans when they are used in autonomous modes (Valmeekam et al., 2023c; Liu et al., 2023; Silver et al., 2022)."
            },
            "evidence": [
                {
                    "evidence_text": "Experimental results from Valmeekam et al. (2023c), Liu et al. (2023), and Silver et al. (2022) show that LLMs fail to generate executable plans in autonomous modes.",
                    "strength": "Strong",
                    "limitations": "Experiments are limited to specific planning domains and problem instances. May not generalize to all planning domains and problem instances.",
                    "location": "Section 2: Planning-centered Limitations of LLMs",
                    "exact_quote": "Despite initial claims about the planning capabilities of LLMs (Bairi et al., 2023; Yao et al., 2023b; Shinn et al., 2023; Huang et al., 2022; Hao et al., 2023) several recent studies confirm that LLMs are not actually able to generate executable plans when they are used in autonomous modes (Valmeekam et al., 2023c; Liu et al., 2023; Silver et al., 2022)."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "",
                "key_limitations": "Limited to specific planning domains and problem instances.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 7,
            "claim": {
                "text": "LLM\u2019s cannot verify plans and thus cannot improve by self-critiquing.",
                "type": "Finding",
                "location": "Section 2: Planning-centered Limitations of LLMs",
                "exact_quote": "There still exists considerable optimism that even if LLMs can\u2019t generate correct solutions in one go, their accuracy might improve in an iterative prompting regime, where LLMs will be able to \u201cself-critique\u201d their candidate solutions and refine them to the point of correctness (Yao et al., 2023b;a; Shinn et al., 2023; Weng et al., 2023; Huang et al., 2022). This belief seems to rest largely on the assumption that verification of correctness should be easier than generation for many reasoning problems\u2013a rather classical argument from computational complexity. There are grounds to be skeptical of this assumption as the complexity of the reasoning task should be irrelevant to LLM performance if what they are doing is approximate retrieval. In general, unless LLMs are trained not just on \u201ccorrect data,\u201d but also on \u201ccorrections data,\u201d there is no a priori reason to believe that their critiques would even be approximately relevant, let alone actually correct."
            },
            "evidence": [
                {
                    "evidence_text": "Experiments in graph coloring show that LLMs are unable to verify solutions and that self-critiquing does not improve performance.",
                    "strength": "Moderate",
                    "limitations": "Experiments are limited to graph coloring. May not generalize to other planning domains.",
                    "location": "Section 2: Planning-centered Limitations of LLMs",
                    "exact_quote": "Our results indicate that in direct mode, LLMs are, perhaps not surprisingly, pretty bad at solving graph coloring instances. More interestingly, they are no better at verifying solutions. In iterative modes, given the inability of LLMs to verify solutions, it should come as no surprise that our experiments also show that the strategy of LLMs self-critiquing their solutions does not improve over the baseline. We report that the performance is in fact worse because the system can\u2019t recognize a correct coloring and thus merrily passes over fortuitously correct colorings it has generated, ending up with a wrong one!"
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "medium",
                "justification": "",
                "key_limitations": "Limited to graph coloring. May not generalize to other planning domains.",
                "confidence_level": "medium"
            }
        }
    ],
    "execution_times": {
        "single_pass_analysis_time": "120.67 seconds",
        "total_execution_time": "341.84 seconds"
    }
}