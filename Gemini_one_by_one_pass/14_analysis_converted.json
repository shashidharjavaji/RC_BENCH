{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "We propose QRNCA to detect query-relevant neurons in LLMs.",
                "type": "",
                "location": "Introduction",
                "exact_quote": "We propose QRNCA to detect query-relevant neurons in LLMs."
            },
            "evidence": [
                {
                    "evidence_text": "We propose QRNCA to detect query-relevant neurons in LLMs. QRNCA aims to extract Query-Relevant (QR) neurons for each input query. The process begins by transforming an open-ended generation task into a multiple-choice question- answering format. By employing prompt engineering, we constrain LLMs to generate only the option letter rather than the complete answer. This approach allows for the examination of long-form generation beyond single tokens.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 1, Introduction",
                    "exact_quote": "We propose QRNCA to detect query-relevant neurons in LLMs. QRNCA aims to extract Query-Relevant (QR) neurons for each input query. The process begins by transforming an open-ended generation task into a multiple-choice question- answering format. By employing prompt engineering, we constrain LLMs to generate only the option letter rather than the complete answer. This approach allows for the examination of long-form generation beyond single tokens."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "QRNCA is architecture-agnostic and can deal with long-form generations.",
                "type": "",
                "location": "Introduction",
                "exact_quote": "QRNCA is architecture-agnostic and can deal with long-form generations."
            },
            "evidence": [
                {
                    "evidence_text": "QRNCA aims to extract Query-Relevant (QR) neurons for each input query. The process begins by transforming an open-ended generation task into a multiple-choice question-answering format.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Section 1, Introduction",
                    "exact_quote": "QRNCA aims to extract Query-Relevant (QR) neurons for each input query. The process begins by transforming an open-ended generation task into a multiple-choice question-answering format."
                },
                {
                    "evidence_text": "QRNCA employs prompt engineering, we constrain LLMs to generate only the option letter rather than the complete answer.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Section 1, Introduction",
                    "exact_quote": "QRNCA employs prompt engineering, we constrain LLMs to generate only the option letter rather than the complete answer."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "Two new datasets: We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language Knowledge.",
                "type": "",
                "location": "Related Work",
                "exact_quote": "Two new datasets: We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language Knowledge."
            },
            "evidence": [
                {
                    "evidence_text": "Additionally, we observe that some neurons with a relatively high attribution score are still shared across clusters. Through case studies (as shown in Table 4), we demonstrate that they express commonly used concepts such as option letters (\u2018\u2018A\u2019\u2019 and \u2018\u2018B\u2019\u2019) or stop words (\u2018\u2018and\u2019\u2019 and \u2018\u2018the\u2019\u2019). Therefore, we count the frequency of each neuron across clusters. If the frequency is higher than u% of the total clusters, we assign the given neuron to the common neuron set.",
                    "strength": "strong",
                    "limitations": "The case studies are limited to a small number of clusters and neurons.",
                    "location": "Section 4.4: Common Neurons",
                    "exact_quote": "Additionally, we observe that some neurons with a relatively high attribution score are still shared across clusters. Through case studies (as shown in Table 4), we demonstrate that they express commonly used concepts such as option letters (\u2018\u2018A\u2019\u2019 and \u2018\u2018B\u2019\u2019) or stop words (\u2018\u2018and\u2019\u2019 and \u2018\u2018the\u2019\u2019). Therefore, we count the frequency of each neuron across clusters. If the frequency is higher than u% of the total clusters, we assign the given neuron to the common neuron set."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "We visualize distributions of detected neurons and we are the first to show that there are visible localized regions in Llama.",
                "type": "",
                "location": "Analyzing Detected QR Neurons",
                "exact_quote": "We visualize distributions of detected neurons and we are the first to show that there are visible localized regions in Llama."
            },
            "evidence": [
                {
                    "evidence_text": "* Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "5.4 Are There Localized Regions in LLMs?",
                    "exact_quote": "Figure 4 displays the geographical locations of QR neurons in llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "Common neurons are concentrated in the top layer, predominantly expressing frequently used tokens.",
                "type": "",
                "location": "Analyzing Detected QR Neurons",
                "exact_quote": "Common neurons are concentrated in the top layer, predominantly expressing frequently used tokens."
            },
            "evidence": [
                {
                    "evidence_text": "Common neurons are predominantly located in the top layer.",
                    "strength": "strong",
                    "limitations": "none",
                    "location": "Section 5.4.",
                    "exact_quote": "we observe that common neurons tend to appear at the top layer."
                },
                {
                    "evidence_text": "The top layer is mainly responsible for next-token prediction.",
                    "strength": "moderate",
                    "limitations": "assumes that the LLM processes information hierarchically",
                    "location": "Section 5.4.",
                    "exact_quote": "the top layers are then mainly responsible for next-token prediction, which may explain the visible regions for different subject domains."
                },
                {
                    "evidence_text": "Common neurons express frequently used tokens.",
                    "strength": "strong",
                    "limitations": "none",
                    "location": "Section 5.5",
                    "exact_quote": "We project the matrix W[D] in Equation 1 to the vocabulary space and select the top-k tokens with the highest probability. Table 4 lists the predicted tokens, which include common words, punctuation marks, and option letters."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "QRNCA might be useful for knowledge editing and neuron-based prediction.",
                "type": "",
                "location": "Conclusion",
                "exact_quote": "QRNCA might be useful for knowledge editing and neuron-based prediction."
            },
            "evidence": [
                {
                    "evidence_text": "Knowledge editing is possible using QRNCA.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 6.1 Knowledge Editing",
                    "exact_quote": "We adjust the values of QR neurons by either boosting or suppressing them to determine if we can change the prediction of a query from incorrect to correct or vice versa. Table 5 presents the success rates of knowledge editing on our constructed language datasets."
                },
                {
                    "evidence_text": "Neuron-based prediction accuracy is high for QRNCA.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 6.2 Neuron-Based Prediction",
                    "exact_quote": "We experiment on a specifically constructed MMLU (Hendrycks et al. 2020) validation set with a different set of questions than those used to determine the QR neurons (see Section B in the SM for details on our experimental strategy). The results are summarised in Table 6. We observe that the accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM)."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        }
    ],
    "execution_times": {
        "single_pass_analysis_time": "1298.39 seconds",
        "total_execution_time": "1298.39 seconds"
    }
}