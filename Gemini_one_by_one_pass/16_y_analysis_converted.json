{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "We propose QRNCA to detect query-relevant neurons in LLMs.",
                "type": "",
                "location": "Introduction",
                "exact_quote": "\"We propose QRNCA to detect query-relevant neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations.\""
            },
            "evidence": [
                {
                    "evidence_text": "Our framework is architecture-agnostic and capable of handling long-form generation.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Introduction, Paragraph 1",
                    "exact_quote": "\"We propose QRNCA to detect query-relevant neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations.\""
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "QRNCA outperforms baseline methods significantly.",
                "type": "",
                "location": "Introduction",
                "exact_quote": "QRNCA outperforms other baseline methods since it achieves higher PCR, evidenced by its comparatively large probability change ratio scores."
            },
            "evidence": [
                {
                    "evidence_text": "QRNCA (p=0.05) outperforms other baseline methods significantly since it achieves higher PCR, evidenced by its comparatively large probability change ratio scores.",
                    "strength": "strong",
                    "limitations": "none stated",
                    "location": "Results, Section 5.3",
                    "exact_quote": "QRNCA outperforms other baseline methods since it achieves higher PCR, evidenced by its comparatively large probability change ratio scores."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "There are localized knowledge regions in LLMs.",
                "type": "",
                "location": "Introduction",
                "exact_quote": "Distinct localized regions emerge in the middle layers, particularly for domain-specific neurons."
            },
            "evidence": [
                {
                    "evidence_text": "Distinct localized regions emerge in the middle layers, particularly for domain-specific neurons.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Section 5.4 Are There Localized Regions in LLMs?",
                    "exact_quote": "Distinct localized regions emerge in the middle layers, particularly for domain-specific neurons."
                },
                {
                    "evidence_text": "Language-specific neurons are more sparsely distributed, indicating that LLMs likely draw on linguistic knowledge at all processing levels.",
                    "strength": "moderate",
                    "limitations": "None stated.",
                    "location": "Section 5.4 Are There Localized Regions in LLMs?",
                    "exact_quote": "Language-specific neurons are more sparsely distributed, indicating that LLMs likely draw on linguistic knowledge at all processing levels."
                },
                {
                    "evidence_text": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Section 5.4 Are There Localized Regions in LLMs?",
                    "exact_quote": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "QRNCA might be useful for knowledge editing and neuron-based prediction.",
                "type": "",
                "location": "Introduction",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "For this goal, we adjust the values of QR neurons by either boosting or suppressing them to determine if we can change the prediction of a query from incorrect to correct or vice versa. Table 5 presents the success rates of knowledge editing on our constructed language datasets.",
                    "strength": "strong",
                    "limitations": "The results are limited to the constructed language datasets and may not generalize to other domains or languages.",
                    "location": "Section 6.1 Knowledge Editing",
                    "exact_quote": null
                },
                {
                    "evidence_text": "Since we harvest QR neurons for queries in different subject domains, we can group all neurons for a domain to obtain a set of domain-specific neurons.",
                    "strength": "strong",
                    "limitations": "The results are limited to the constructed language datasets and may not generalize to other domains or languages.",
                    "location": "Section 6.2 Neuron-Based Prediction",
                    "exact_quote": null
                },
                {
                    "evidence_text": "We experiment on a specifically constructed MMLU (Hendrycks et al. 2020) validation set with a different set of questions than those used to determine the QR neurons (see Section B in the SM for details on our experimental strategy).",
                    "strength": "strong",
                    "limitations": "The results are limited to the constructed language datasets and may not generalize to other domains or languages.",
                    "location": "Section 6.2 Neuron-Based Prediction",
                    "exact_quote": null
                },
                {
                    "evidence_text": "The results are summarized in Table 6. We observe that the accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM).",
                    "strength": "strong",
                    "limitations": "The results are limited to the constructed language datasets and may not generalize to other domains or languages.",
                    "location": "Section 6.2 Neuron-Based Prediction",
                    "exact_quote": null
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "For a given prompt q (\u2018\u2018Paris is the capital of \u2019\u2019), the probability of the correct answer predicted by an LLM can be formulated as:",
                "type": "",
                "location": "Background",
                "exact_quote": "Given a prompt q (\u2018\u2018Paris is the capital of \u2019\u2019), the probability of the correct answer predicted by an LLM can be formulated as:"
            },
            "evidence": [
                {
                    "evidence_text": "Given a prompt q (\u2018\u2018Paris is the capital of \u2019\u2019), the probability of the correct answer predicted by an LLM can be formulated as:",
                    "strength": "strong",
                    "limitations": null,
                    "location": "Background, Eq 2",
                    "exact_quote": "Given a prompt q (\u2018\u2018Paris is the capital of \u2019\u2019), the probability of the correct answer predicted by an LLM can be formulated as:"
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "We identify a coarse set of knowledge neurons whose attribution scores are greater than a threshold t.",
                "type": "",
                "location": "Background",
                "exact_quote": "We identify a coarse set of knowledge neurons whose attribution scores are greater than a threshold t."
            },
            "evidence": [
                {
                    "evidence_text": "We identify a coarse set of knowledge neurons whose attribution scores are greater than a threshold t.",
                    "strength": "strong",
                    "limitations": "The threshold t is a user-defined parameter.",
                    "location": "Section 4.2, Paragraph 1",
                    "exact_quote": "We identify a coarse set of knowledge neurons whose attribution scores are greater than a threshold t."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 7,
            "claim": {
                "text": "The framework is architecture-agnostic and capable of handling long-form generation.",
                "type": "",
                "location": "Locating Query-Relevant (QR) Neurons in Autoregressive LLMs",
                "exact_quote": "Unlike Knowledge Attribution (Dai et al. 2022) which effectively identifies neurons linked to factual queries, its applicability is limited to encoder-only architectures, and it mandates the output to be a single-token word. To address these constraints, we propose a new framework named Query-Relevant Neuron Cluster Attribution (QRNCA)."
            },
            "evidence": [
                {
                    "evidence_text": "Unlike Knowledge Attribution (Dai et al. 2022) which effectively identifies neurons linked to factual queries, its applicability is limited to encoder-only architectures, and it mandates the output to be a single-token word. To address these constraints, we propose a new framework named Query-Relevant Neuron Cluster Attribution (QRNCA).",
                    "strength": "strong",
                    "limitations": null,
                    "location": "Section 4",
                    "exact_quote": "Unlike Knowledge Attribution (Dai et al. 2022) which effectively identifies neurons linked to factual queries, its applicability is limited to encoder-only architectures, and it mandates the output to be a single-token word. To address these constraints, we propose a new framework named Query-Relevant Neuron Cluster Attribution (QRNCA)."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 8,
            "claim": {
                "text": "Our primary objective is to identify a set of QR neurons for a given input.",
                "type": "",
                "location": "Locating Query-Relevant (QR) Neurons in Autoregressive LLMs",
                "exact_quote": "Our primary objective is to identify a set of QR neurons for a given input query."
            },
            "evidence": [
                {
                    "evidence_text": "Our primary objective is to identify a set of QR neurons for a given input.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Introduction",
                    "exact_quote": "Our primary objective is to identify a set of QR neurons for a given input query."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        }
    ],
    "execution_times": {
        "single_pass_analysis_time": "112.73 seconds",
        "total_execution_time": "2876.61 seconds"
    }
}