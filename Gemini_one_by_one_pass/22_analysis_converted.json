{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "QRNCA outperforms baseline methods significantly.",
                "type": "",
                "location": "Abstract",
                "exact_quote": "Our QRNCA method outperforms other baseline methods significantly, evidenced by its higher PCR."
            },
            "evidence": [
                {
                    "evidence_text": "Our QRNCA method outperforms other baseline methods significantly, evidenced by its higher PCR.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Section 5.3, Results",
                    "exact_quote": "Our QRNCA method outperforms other baseline methods significantly, evidenced by its higher PCR."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "QRNCA can effectively locate query-relevant neurons.",
                "type": "",
                "location": "Abstract",
                "exact_quote": "Our experimental results show that our method outperforms existing baselines in identifying associated neurons."
            },
            "evidence": [
                {
                    "evidence_text": "Our experimental results show that our method outperforms existing baselines in identifying associated neurons.",
                    "strength": "strong",
                    "limitations": "The results are specific to the datasets used in the study.",
                    "location": "Section 5.3",
                    "exact_quote": "Our experimental results show that our method outperforms existing baselines in identifying associated neurons."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "QRNCA can handle long-form text generation.",
                "type": "",
                "location": "Abstract",
                "exact_quote": "To evaluate the effectiveness of our detected neurons, we build two multi-choice QA datasets spanning diverse domains and languages. Empirical evaluations demonstrate that our method outperforms baseline methods significantly."
            },
            "evidence": [
                {
                    "evidence_text": "To evaluate the effectiveness of our detected neurons, we build two multi-choice QA datasets spanning diverse domains and languages. Empirical evaluations demonstrate that our method outperforms baseline methods significantly.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Section 1 Introduction",
                    "exact_quote": "To evaluate the effectiveness of our detected neurons, we build two multi-choice QA datasets spanning diverse domains and languages. Empirical evaluations demonstrate that our method outperforms baseline methods significantly."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "There are visible localized knowledge regions in LLMs.",
                "type": "",
                "location": "Introduction",
                "exact_quote": "Our experimental results show that domain-specific neurons are mainly centralized in the middle layers (10-15), suggesting specific neuron patterns."
            },
            "evidence": [
                {
                    "evidence_text": "Our experimental results show that domain-specific neurons are mainly centralized in the middle layers (10-15), suggesting specific neuron patterns.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Results - Are There Localized Regions in LLMs?",
                    "exact_quote": "Our experimental results show that domain-specific neurons are mainly centralized in the middle layers (10-15), suggesting specific neuron patterns."
                },
                {
                    "evidence_text": "In contrast, language-specific neurons are more sparsely distributed, which indicates that LLMs tend to allow the storage of multiple domain-specific concepts in a single neuron (polysemantic).",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Results - Are There Localized Regions in LLMs?",
                    "exact_quote": "In contrast, language-specific neurons are more sparsely distributed, which indicates that LLMs tend to allow the storage of multiple domain-specific concepts in a single neuron (polysemantic)."
                },
                {
                    "evidence_text": "Based on prior studies, LLMs process and represent information in a hierarchical manner (Geva et al. 2022; Wendler et al. 2024; Tang et al. 2024). The early layers are primarily responsible for extracting low-level features, while the middle layers begin to integrate this information, forming more complex semantic representations. The late layers are typically dedicated to generating the final output. Therefore, we suppose that domain-specific knowledge representation is built in the middle layer and the top layers are then mainly responsible for next-token prediction, which may explain the visible regions for different subject domains.",
                    "strength": "moderate",
                    "limitations": "Relies on prior studies",
                    "location": "Results - Are There Localized Regions in LLMs? - Discussion",
                    "exact_quote": "Based on prior studies, LLMs process and represent information in a hierarchical manner (Geva et al. 2022; Wendler et al. 2024; Tang et al. 2024). The early layers are primarily responsible for extracting low-level features, while the middle layers begin to integrate this information, forming more complex semantic representations. The late layers are typically dedicated to generating the final output. Therefore, we suppose that domain-specific knowledge representation is built in the middle layer and the top layers are then mainly responsible for next-token prediction, which may explain the visible regions for different subject domains."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "QRNCA can be used for knowledge editing and neuron-based prediction.",
                "type": "",
                "location": "Introduction",
                "exact_quote": "We provide two usage examples to showcase the potential applications of our detected QR neurons: Knowledge Editing and Neuron-Based Prediction."
            },
            "evidence": [
                {
                    "evidence_text": "We provide two usage examples to showcase the potential applications of our detected QR neurons: Knowledge Editing and Neuron-Based Prediction.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Section 6",
                    "exact_quote": "We provide two usage examples to showcase the potential applications of our detected QR neurons: Knowledge Editing and Neuron-Based Prediction."
                },
                {
                    "evidence_text": "Table 5 presents the successful rates of knowledge editing on our constructed language datasets. Our observations indicate that QRNCA achieves higher success rates than other baselines.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Section 6.1",
                    "exact_quote": "Table 5 presents the successful rates of knowledge editing on our constructed language datasets. Our observations indicate that QRNCA achieves higher success rates than other baselines."
                },
                {
                    "evidence_text": "The results are summarised in Table 6. We observe that the accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM). This suggests that the activity of identified neurons can reflect the model\u2019s reasoning process to some extent.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Section 6.2",
                    "exact_quote": "The results are summarised in Table 6. We observe that the accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM). This suggests that the activity of identified neurons can reflect the model\u2019s reasoning process to some extent."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "QRNCA is a scalable method.",
                "type": "",
                "location": "Related Work",
                "exact_quote": "\"We propose QRNCA to detect QR neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations.\""
            },
            "evidence": [
                {
                    "evidence_text": "We propose QRNCA to detect QR neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations.",
                    "strength": "strong",
                    "limitations": "n/a",
                    "location": "Related Work > Locating Knowledge in LLMs section, 1st paragraph",
                    "exact_quote": "\"We propose QRNCA to detect QR neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations.\""
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 7,
            "claim": {
                "text": "QRNCA is a novel gradient-based attribution method.",
                "type": "",
                "location": "Related Work",
                "exact_quote": "QRNCA aims to extract Query-Relevant (QR) neurons for each input query. The process begins by transforming an open-ended generation task into a multiple-choice question-answering format. By employing prompt engineering, we constrain LLMs to generate only the option letter rather than the complete answer. This approach allows for the examination of long-form generation beyond single tokens."
            },
            "evidence": [
                {
                    "evidence_text": "QRNCA aims to extract Query-Relevant (QR) neurons for each input query. The process begins by transforming an open-ended generation task into a multiple-choice question-answering format. By employing prompt engineering, we constrain LLMs to generate only the option letter rather than the complete answer. This approach allows for the examination of long-form generation beyond single tokens.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Section 1 Introduction, Paragraph 3",
                    "exact_quote": "QRNCA aims to extract Query-Relevant (QR) neurons for each input query. The process begins by transforming an open-ended generation task into a multiple-choice question-answering format. By employing prompt engineering, we constrain LLMs to generate only the option letter rather than the complete answer. This approach allows for the examination of long-form generation beyond single tokens."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 8,
            "claim": {
                "text": "LLMs tend to complete the formation of domain-specific concepts within middle layers.",
                "type": "",
                "location": "Results",
                "exact_quote": "The QR neurons are predominantly located in the middle layers (15-18) and the top layers (around 30), as depicted in Figure 2b"
            },
            "evidence": [
                {
                    "evidence_text": "The QR neurons are predominantly located in the middle layers (15-18) and the top layers (around 30), as depicted in Figure 2b",
                    "strength": "strong",
                    "limitations": "none",
                    "location": "Section 5.2",
                    "exact_quote": "The QR neurons are predominantly located in the middle layers (15-18) and the top layers (around 30), as depicted in Figure 2b"
                },
                {
                    "evidence_text": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages",
                    "strength": "strong",
                    "limitations": "none",
                    "location": "Section 5.4",
                    "exact_quote": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages"
                },
                {
                    "evidence_text": "The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains",
                    "strength": "moderate",
                    "limitations": "none",
                    "location": "Section 5.4",
                    "exact_quote": "The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains"
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 9,
            "claim": {
                "text": "Common neurons are not critical for specific queries.",
                "type": "",
                "location": "Results",
                "exact_quote": "Table 4 lists the predicted tokens, which include common words, punctuation marks, and option letters. These findings reinforce the notion that common neurons are not critical for specific queries."
            },
            "evidence": [
                {
                    "evidence_text": "Table 4 lists the predicted tokens, which include common words, punctuation marks, and option letters. These findings reinforce the notion that common neurons are not critical for specific queries.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 5.5, The Function of Common Neurons",
                    "exact_quote": "Table 4 lists the predicted tokens, which include common words, punctuation marks, and option letters. These findings reinforce the notion that common neurons are not critical for specific queries."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 10,
            "claim": {
                "text": "Common neurons tend to appear at the top layer.",
                "type": "",
                "location": "Results",
                "exact_quote": "We also analyzed the token predicted by QR neurons, but we found that middle-layer neurons do not have a clear semantic meaning and human-readable concepts mostly appear in the top layer (Wendler et al. 2024)."
            },
            "evidence": [
                {
                    "evidence_text": "We also analyzed the token predicted by QR neurons, but we found that middle-layer neurons do not have a clear semantic meaning and human-readable concepts mostly appear in the top layer (Wendler et al. 2024).",
                    "strength": "strong",
                    "limitations": "This observation is based on the assumption that the predicted tokens by QR neurons can reflect their semantic meaning.",
                    "location": "Section 5.5: The Function of Common Neurons",
                    "exact_quote": "We also analyzed the token predicted by QR neurons, but we found that middle-layer neurons do not have a clear semantic meaning and human-readable concepts mostly appear in the top layer (Wendler et al. 2024)."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        }
    ],
    "execution_times": {
        "single_pass_analysis_time": "2093.84 seconds",
        "total_execution_time": "2093.84 seconds"
    }
}