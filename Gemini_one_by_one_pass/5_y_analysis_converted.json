{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "A scalable method is proposed to detect query-relevant neurons in LLMs.",
                "type": "primary",
                "location": "Abstract",
                "exact_quote": "Our QRNCA method achieves a boosting ratio of 41.2 on the language dataset, the highest among the baselines. Additionally, both our proposed ICA and the removal of common neurons can bring benefits to locating neurons, which is validated by the worse performance of the two QRNCA variants."
            },
            "evidence": [
                {
                    "evidence_text": "Our QRNCA method achieves a boosting ratio of 41.2 on the language dataset, the highest among the baselines. Additionally, both our proposed ICA and the removal of common neurons can bring benefits to locating neurons, which is validated by the worse performance of the two QRNCA variants.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Results > Neuron Attribution",
                    "exact_quote": "Our QRNCA method achieves a boosting ratio of 41.2 on the language dataset, the highest among the baselines. Additionally, both our proposed ICA and the removal of common neurons can bring benefits to locating neurons, which is validated by the worse performance of the two QRNCA variants."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "Two new datasets are curated to evaluate the proposed method.",
                "type": "primary",
                "location": "Abstract",
                "exact_quote": "\"To evaluate the effectiveness of our detected neurons, we build two multi-choice QA datasets spanning diverse domains and languages."
            },
            "evidence": [
                {
                    "evidence_text": "To evaluate the effectiveness of our detected neurons, we build two multi-choice QA datasets spanning diverse domains and languages.",
                    "strength": "strong",
                    "limitations": "The datasets are constructed to evaluate the performance of the proposed method specifically and may not be generalizable to other tasks or models.",
                    "location": "Section 1 Introduction, Paragraph 4",
                    "exact_quote": "\"To evaluate the effectiveness of our detected neurons, we build two multi-choice QA datasets spanning diverse domains and languages."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "The study provides insights into the existence of localized knowledge regions in LLMs.",
                "type": "primary",
                "location": "Abstract",
                "exact_quote": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains."
            },
            "evidence": [
                {
                    "evidence_text": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains.",
                    "strength": "strong",
                    "limitations": "The findings are based on a single LLM (Llama-2-7B)",
                    "location": "Section 5.4 Are There Localized Regions in LLMs?",
                    "exact_quote": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "QRNCA can be utilized for knowledge editing and neuron-based prediction.",
                "type": "secondary",
                "location": "Abstract",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "We provide two usage examples to showcase the potential applications of our detected QR neurons: Knowledge Editing and Neuron-Based Prediction.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Section 6 Potential Applications",
                    "exact_quote": ""
                },
                {
                    "evidence_text": "Table 5 presents the successful rates of knowledge editing on our constructed language datasets. Our observations indicate that QRNCA achieves higher success rates than other baselines.",
                    "strength": "strong",
                    "limitations": "The success rates are measured on a limited dataset.",
                    "location": "Section 6.1 Knowledge Editing",
                    "exact_quote": ""
                },
                {
                    "evidence_text": "The results are summarised in Table 6. We observe that the accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model.",
                    "strength": "strong",
                    "limitations": "The accuracy is measured on a limited dataset.",
                    "location": "Section 6.2 Neuron-Based Prediction",
                    "exact_quote": ""
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "FFNs in Transformers store refined knowledge representations.",
                "type": "primary",
                "location": "Introduction",
                "exact_quote": "We find that FFNs play a key role in refining the knowledge representations learned in Transformers."
            },
            "evidence": [
                {
                    "evidence_text": "We find that FFNs play a key role in refining the knowledge representations learned in Transformers.",
                    "strength": "strong",
                    "limitations": "Based on experimental results on specific datasets",
                    "location": "Section 3, Paragraph 2",
                    "exact_quote": "We find that FFNs play a key role in refining the knowledge representations learned in Transformers."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "Gradient-based methods can quantify the sensitivity of model outputs to internal components.",
                "type": "primary",
                "location": "Introduction",
                "exact_quote": "Gradient-based attribution methods (Dai et al. 2022) measure the sensitivity of model outputs to internal components in response to specific inputs, which enables the effective identification of neurons relevant to particular queries."
            },
            "evidence": [
                {
                    "evidence_text": "Gradient-based attribution methods (Dai et al. 2022) measure the sensitivity of model outputs to internal components in response to specific inputs, which enables the effective identification of neurons relevant to particular queries.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Section 2: Related Work, Paragraph 2",
                    "exact_quote": "Gradient-based attribution methods (Dai et al. 2022) measure the sensitivity of model outputs to internal components in response to specific inputs, which enables the effective identification of neurons relevant to particular queries."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 7,
            "claim": {
                "text": "Causality-based methods employ causal mediation analysis to locate factual associations.",
                "type": "primary",
                "location": "Introduction",
                "exact_quote": "Causality-based methods employ causal mediation analysis to pinpoint layers within LLMs that store factual associations."
            },
            "evidence": [
                {
                    "evidence_text": "Causality-based methods employ causal mediation analysis to pinpoint layers within LLMs that store factual associations.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Introduction",
                    "exact_quote": "Causality-based methods employ causal mediation analysis to pinpoint layers within LLMs that store factual associations."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 8,
            "claim": {
                "text": "LLMs tend to complete the formation of domain-specific concepts within the middle layers.",
                "type": "primary",
                "location": "Results",
                "exact_quote": "notably certain regions are visible in the middle layers (10-15) suggesting specific neuron patterns"
            },
            "evidence": [
                {
                    "evidence_text": "\u201cnotably certain regions are visible in the middle layers (10-15) suggesting specific neuron patterns.\u201d",
                    "strength": "strong",
                    "limitations": "This evidence is specific to the middle layers of LLMs.",
                    "location": "Section 5.4, 4th paragraph",
                    "exact_quote": "notably certain regions are visible in the middle layers (10-15) suggesting specific neuron patterns"
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 9,
            "claim": {
                "text": "Language-specific neurons are more sparsely distributed and exhibit a less localized structure.",
                "type": "primary",
                "location": "Results",
                "exact_quote": "Regarding language-specific neurons, their distribution across different layers appears more sparse and less localized compared to domain-specific neurons. This suggests that the processing of linguistic knowledge requires engagement with language-specific neurons at multiple stages of information processing."
            },
            "evidence": [
                {
                    "evidence_text": "Regarding language-specific neurons, their distribution across different layers appears more sparse and less localized compared to domain-specific neurons. This suggests that the processing of linguistic knowledge requires engagement with language-specific neurons at multiple stages of information processing.",
                    "strength": "strong",
                    "limitations": "This result may vary depending on the specific language and task being considered.",
                    "location": "Section 5.4, Paragraph 1",
                    "exact_quote": "Regarding language-specific neurons, their distribution across different layers appears more sparse and less localized compared to domain-specific neurons. This suggests that the processing of linguistic knowledge requires engagement with language-specific neurons at multiple stages of information processing."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 10,
            "claim": {
                "text": "QR neurons have a higher impact on the prediction of correct answers.",
                "type": "",
                "location": "Results",
                "exact_quote": ""
            },
            "evidence": [],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 11,
            "claim": {
                "text": "The geographical heatmap of detected QR neurons reveals a sparse but distinct distribution, with visible regions for different domains.",
                "type": "primary",
                "location": "Results",
                "exact_quote": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages."
            },
            "evidence": [
                {
                    "evidence_text": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Section 5.4, paragraph 2",
                    "exact_quote": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 12,
            "claim": {
                "text": "Common neurons are associated with frequently used tokens and punctuation marks.",
                "type": "primary",
                "location": "Results",
                "exact_quote": "Table 4 lists the tokens predicted by the common neurons, which include common words, punctuation marks, and option letters."
            },
            "evidence": [
                {
                    "evidence_text": "Table 4 lists the predicted tokens, which include common words, punctuation marks, and option letters.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Section 5.5",
                    "exact_quote": "Table 4 lists the tokens predicted by the common neurons, which include common words, punctuation marks, and option letters."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 13,
            "claim": {
                "text": "The collected neuron-based prediction results are very close to the results of the prompt-based prediction.",
                "type": "primary",
                "location": "Results",
                "exact_quote": "The accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM). This suggests that the activity of identified neurons can reflect the model\u2019s reasoning process to some extent."
            },
            "evidence": [
                {
                    "evidence_text": "The accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM). This suggests that the activity of identified neurons can reflect the model\u2019s reasoning process to some extent.",
                    "strength": "moderate",
                    "limitations": " neuron-based predictions perform marginally worse than prompt-based predictions.",
                    "location": "Section 6.2: Neuron-Based Prediction",
                    "exact_quote": "The accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM). This suggests that the activity of identified neurons can reflect the model\u2019s reasoning process to some extent."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        }
    ],
    "execution_times": {
        "single_pass_analysis_time": "115.56 seconds",
        "total_execution_time": "2678.84 seconds"
    }
}