{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "We propose Query-Relevant Neuron Cluster Attribution (QRNCA), a novel architecture-agnostic framework capable of identifying query-relevant neurons in LLMs.",
                "type": "",
                "location": "Introduction",
                "exact_quote": "\"We propose Query-Relevant Neuron Cluster Attribution (QRNCA), a novel architecture-agnostic framework capable of identifying query-relevant neurons in LLMs.\""
            },
            "evidence": [
                {
                    "evidence_text": "We propose Query-Relevant Neuron Cluster Attribution (QRNCA), a novel architecture-agnostic framework capable of identifying query-relevant neurons in LLMs.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Introduction, Paragraph 1",
                    "exact_quote": "\"We propose Query-Relevant Neuron Cluster Attribution (QRNCA), a novel architecture-agnostic framework capable of identifying query-relevant neurons in LLMs.\""
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "QRNCA allows for the examination of long-form answers beyond triplet facts by employing the proxy task of multi-choice question answering.",
                "type": "",
                "location": "Introduction",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "As previous works have largely focused on locating entity-related (often _single-token) facts in smaller models, several key_questions remain unanswered: (1) How can we effectively lo_cate query-relevant neurons in contemporary autoregressive_ _LLMs, such as Llama and Mistral?",
                    "strength": "strong",
                    "limitations": "The evidence is an indirect statement.",
                    "location": "Introduction",
                    "exact_quote": "As previous works have largely focused on locating entity-related (often _single-token) facts in smaller models, several key_questions remain unanswered: (1) How can we effectively lo_cate query-relevant neurons in contemporary autoregressive_ _LLMs, such as Llama and Mistral?"
                },
                {
                    "evidence_text": "To evaluate the effectiveness of our detected neurons, we conduct two multi-choice QA datasets spanning diverse domains and languages. Empirical evaluations demonstrate that our proposed method outperforms baseline methods significantly.",
                    "strength": "strong",
                    "limitations": "The effectiveness is only demonstrated for multi-choice QA datasets.",
                    "location": "Related Work",
                    "exact_quote": "To evaluate the effectiveness of our detected neurons, we conduct two multi-choice QA datasets spanning diverse domains and languages. Empirical evaluations demonstrate that our proposed method outperforms baseline methods significantly."
                },
                {
                    "evidence_text": "In this study, we introduce a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA), a novel architecture-agnostic framework capable of identifying query-relevant neurons in LLMs.",
                    "strength": "moderate",
                    "limitations": "The evidence is a statement of the framework's capability.",
                    "location": "Introduction",
                    "exact_quote": "In this study, we introduce a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA), a novel architecture-agnostic framework capable of identifying query-relevant neurons in LLMs."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "There are visible localized knowledge regions in LLMs.",
                "type": "",
                "location": "Introduction",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Our findings indicate that distinct localized regions emerge in the middle layers, particularly for domain-specific concept neurons. This suggests that LLMs tend to complete the formation of domain-specific concepts within these middle layers. Conversely, language-specific neurons are more sparsely distributed, indicating that LLMs likely draw on linguistic knowledge at all processing levels.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Section 5.4",
                    "exact_quote": "Our findings indicate that distinct localized regions emerge in the middle layers, particularly for domain-specific concept neurons. This suggests that LLMs tend to complete the formation of domain-specific concepts within these middle layers. Conversely, language-specific neurons are more sparsely distributed, indicating that LLMs likely draw on linguistic knowledge at all processing levels."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "QRNCA might be useful for knowledge editing and neuron-based prediction.",
                "type": "",
                "location": "Conclusion",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "For this goal, we adjust the values of QR neurons by either boosting or suppressing them to determine if we can change the prediction of a query from incorrect to correct or vice versa. Table 5 presents the success rates of knowledge editing on our constructed language datasets. Our observations indicate that QRNCA achieves higher success rates than other baselines.",
                    "strength": "strong",
                    "limitations": "Only tested on language datasets",
                    "location": "Section 6.1 Knowledge Editing",
                    "exact_quote": null
                },
                {
                    "evidence_text": "The results are summarised in Table 6. We observe that the accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM). This suggests that the activity of identified neurons can reflect the model\u2019s reasoning process to some extent.",
                    "strength": "strong",
                    "limitations": "Only tested on selected domains",
                    "location": "Section 6.2 Neuron-Based Prediction",
                    "exact_quote": null
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "Two new datasets are created: Domain Knowledge and Language Knowledge.",
                "type": "",
                "location": "Related Work",
                "exact_quote": "We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge."
            },
            "evidence": [
                {
                    "evidence_text": "We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Section 2.3 Background",
                    "exact_quote": "We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "The geographical locations of the detected neurons in Llama are visualized in 2D heatmaps.",
                "type": "",
                "location": "Analyzing Detected QR Neurons",
                "exact_quote": "The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns. In contrast, language neurons are more sparsely distributed with smaller regions, and languages like Arabic and Russian exhibit less localized properties."
            },
            "evidence": [
                {
                    "evidence_text": "Given our ability to identify QR neurons for each query, it is intriguing to explore whether LLMs exhibit localized regions for each domain or language, analogous to the functional localizations in the human brain (Brett, Johnsrude, and Owen 2002). To investigate this, we visualize domainor language-specific neurons on a 2D geographical heatmap. The width of the heatmap corresponds to the dimension of FFNs in Llama-2-7B (11008), and the length represents the layer depth (32). We accumulate the value of naica(n[l]i[)][ to populate the heatmap. Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns. In contrast, language neurons are more sparsely distributed with smaller regions, and languages like Arabic and Russian exhibit less localized properties.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Results: Are there localized regions in LLMs",
                    "exact_quote": "The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns. In contrast, language neurons are more sparsely distributed with smaller regions, and languages like Arabic and Russian exhibit less localized properties."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 7,
            "claim": {
                "text": "Common neurons are not critical for specific queries.",
                "type": "",
                "location": "Analyzing Detected QR Neurons",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "QR neurons have more influence on the correct answer while exerting a relatively low impact on unrelated queries.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Section 5.3, Paragraph 2",
                    "exact_quote": "Our QRNCA method consistently outperforms other baselines, evidenced by its higher PCR. This indicates that our identified QR neurons significantly affect the probability of correct answers while exerting a relatively low impact on unrelated queries."
                },
                {
                    "evidence_text": "The removal of common neurons enhances the precision in identifying critical neural correlates.",
                    "strength": "moderate",
                    "limitations": "N/A",
                    "location": "Section 4.4, Paragraph 1",
                    "exact_quote": "Refining the extraction of QR neurons by excluding these common neurons enhances the precision in identifying critical neural correlates."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 8,
            "claim": {
                "text": "The activity of identified neurons can reflect the model\u2019s reasoning process to some extent.",
                "type": "",
                "location": "Potential Applications",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Table 6 presents the accuracy of neuron-based prediction on selected domains in comparison with the standard prompt-based model prediction.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Section 6.2 Neuron-Based Prediction",
                    "exact_quote": "Table 6 presents the accuracy of neuron-based prediction on selected domains in comparison with the standard prompt-based model prediction."
                },
                {
                    "evidence_text": "We observe that the accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM).",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Section 6.2 Neuron-Based Prediction",
                    "exact_quote": "We observe that the accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM)."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        }
    ],
    "execution_times": {
        "single_pass_analysis_time": "1692.86 seconds",
        "total_execution_time": "1692.86 seconds"
    }
}