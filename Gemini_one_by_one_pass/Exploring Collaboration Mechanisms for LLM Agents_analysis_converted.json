{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "We propose QRNCA to detect query-relevant neurons in LLMs.",
                "type": "secondary",
                "location": "Introduction",
                "exact_quote": "We propose QRNCA to detect query-relevant neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations."
            },
            "evidence": [
                {
                    "evidence_text": "No direct evidence that demonstrates the evidence is from the experiment.",
                    "strength": "weak",
                    "limitations": "The evidence only shows the method and its potential use cases.",
                    "location": "Section 4: Locating Query-Relevant (QR) Neurons in Autoregressive LLMs.",
                    "exact_quote": "We propose QRNCA to detect query-relevant neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "No conclusion available",
                "key_limitations": "No limitations analysis available",
                "confidence_level": "null"
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "Two new datasets: We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge.",
                "type": "primary",
                "location": "Related Work",
                "exact_quote": "We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge."
            },
            "evidence": [
                {
                    "evidence_text": "We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Section 2: Related Work, Paragraph 2",
                    "exact_quote": "We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "No conclusion available",
                "key_limitations": "No limitations analysis available",
                "confidence_level": "null"
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "We visualize distributions of detected neurons and we are the first to show that there are visible localized regions in Llama.",
                "type": "primary",
                "location": "Related Work",
                "exact_quote": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages."
            },
            "evidence": [
                {
                    "evidence_text": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages.",
                    "strength": "strong",
                    "limitations": "We observe that domain-specific neurons are built in the middle layer and the top layers are then mainly responsible for next-token prediction, which may explain the visible regions for different subject domains",
                    "location": "Section 5.4",
                    "exact_quote": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "No conclusion available",
                "key_limitations": "No limitations analysis available",
                "confidence_level": "null"
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "QRNCA might be useful for knowledge editing and neuron-based prediction.",
                "type": "primary",
                "location": "Related Work",
                "exact_quote": "Apart from using the metric of PCR in Section 5.3, we are also interested in whether the detected QR neurons can be used for knowledge editing. For this goal, we adjust the values of QR neurons by either boosting or suppressing them to determine if we can change the prediction of a query from incorrect to correct or vice versa."
            },
            "evidence": [
                {
                    "evidence_text": "Apart from using the metric of PCR in Section 5.3, we are also interested in whether the detected QR neurons can be used for knowledge editing. For this goal, we adjust the values of QR neurons by either boosting or suppressing them to determine if we can change the prediction of a query from incorrect to correct or vice versa.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Section 6.1, Paragraph 1",
                    "exact_quote": "Apart from using the metric of PCR in Section 5.3, we are also interested in whether the detected QR neurons can be used for knowledge editing. For this goal, we adjust the values of QR neurons by either boosting or suppressing them to determine if we can change the prediction of a query from incorrect to correct or vice versa."
                },
                {
                    "evidence_text": "Table 5 presents the successful rates of knowledge editing on our constructed language datasets. Our observations indicate that QRNCA achieves higher success rates than other baselines.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Section 6.1, Paragraph 2",
                    "exact_quote": "Table 5 presents the successful rates of knowledge editing on our constructed language datasets. Our observations indicate that QRNCA achieves higher success rates than other baselines."
                },
                {
                    "evidence_text": "The intuition behind neuron-based prediction is that for a domain-specific question, if the corresponding localized regions are properly activated, the LLM is more likely to generate truthful answers. Otherwise, the LLM may produce hallucinated answers. To this end, we test whether the correct answers to domain-specific questions can be predicted solely based on the activity of the associated neurons.",
                    "strength": "moderate",
                    "limitations": "None stated",
                    "location": "Section 6.2, Paragraph 1",
                    "exact_quote": "The intuition behind neuron-based prediction is that for a domain-specific question, if the corresponding localized regions are properly activated, the LLM is more likely to generate truthful answers. Otherwise, the LLM may produce hallucinated answers. To this end, we test whether the correct answers to domain-specific questions can be predicted solely based on the activity of the associated neurons."
                },
                {
                    "evidence_text": "The results are summarised in Table 6. We observe that the accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM).",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Section 6.2, Paragraph 2",
                    "exact_quote": "The results are summarised in Table 6. We observe that the accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM)."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "No conclusion available",
                "key_limitations": "No limitations analysis available",
                "confidence_level": "null"
            }
        }
    ],
    "execution_times": {
        "single_pass_analysis_time": "897.38 seconds",
        "total_execution_time": "897.38 seconds"
    }
}