Claim 1:
Type: performance
Statement: DocPrompting outperforms existing code generation models that cannot generalize to unseen functions and libraries.
Location: introduction
Exact Quote: In contrast, when human programmers use functions and libraries for the first time, they frequently refer to textual resources such as code manuals and documentation, to explore and understand the available functionality. Inspired by this observation, we introduce DocPrompting: a natural-language-to-code generation approach that explicitly leverages code documentation by (1) retrieving the relevant documentation pieces given a natural language (NL) intent, and (2) generating code based on the NL intent and the retrieved documentation.

Evidence:
- Evidence Text: Existing models inherently cannot generalize to generate such unseen usages.
  Strength: strong
  Location: introduction
  Limitations: None
  Exact Quote: In contrast, when human programmers use functions and libraries for the first time, they frequently refer to textual resources such as code manuals and documentation, to explore and understand the available functionality. Inspired by this observation, we introduce DocPrompting: a natural-language-to-code generation approach that explicitly leverages code documentation by (1) retrieving the relevant documentation pieces given a natural language (NL) intent, and (2) generating code based on the NL intent and the retrieved documentation. DocPrompting is general: it can be applied to any programming language, and is agnostic to the underlying neural model. We demonstrate that DocPrompting consistently improves NL-to-code models: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo-1.3B by up to absolute 6.9% exact match.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by strong evidence that demonstrates the limitations of existing code generation models and the improvements achieved by DocPrompting.
Key Limitations: The evidence is limited to the specific datasets and models used in the study.

--------------------------------------------------

Claim 2:
Type: methodology
Statement: DocPrompting leverages code documentation to generalize to unseen libraries and functions.
Location: introduction
Exact Quote: In contrast to these existing models, human programmers frequently refer to manuals and documentation when writing code (Nykaza et al., 2002; Lethbridge et al., 2003). This allows humans to easily use functions and libraries they have never seen nor used before. Inspired by this ability, we propose DocPrompting: a code generation approach that learns to retrieve code documentation before generating the code.

Evidence:
- Evidence Text: This way, DocPrompting can leverage newly added documentation, and it can generate code containing unseen and unused functions and libraries.
  Strength: strong
  Location: introduction
  Limitations: None
  Exact Quote: A documentation pool serves as an external data store that can be updated frequently with new contents (e.g., documentation of newly released libraries), without re-training any model component. This way, DocPrompting can leverage newly added documentation, and it can generate code containing unseen and unused functions and libraries.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by strong evidence that explains how DocPrompting utilizes code documentation to handle unseen libraries and functions.
Key Limitations: The effectiveness of DocPrompting may depend on the quality and comprehensiveness of the available documentation.

--------------------------------------------------

Claim 3:
Type: methodology
Statement: DocPrompting can be applied to any programming language and underlying base architecture.
Location: introduction
Exact Quote: DocPrompting is general and applicable to any programming language and underlying base architecture.

Evidence:
- Evidence Text: We demonstrate the effectiveness of DocPrompting on two NL code benchmarks and tasks, across two programming languages, and using several base models: GPT-Neo (Black et al., 2021), T5 (Raffel et al., 2020), CodeT5 (Wang et al., 2021), Fusion-in-Decoder (Izacard and Grave, 2021)), and Codex (Chen et al., 2021).
  Strength: strong
  Location: introduction
  Limitations: None
  Exact Quote: We demonstrate the effectiveness of DocPrompting on two NL code benchmarks and tasks, across two programming languages, and using several base models: GPT-Neo (Black et al., 2021), T5 (Raffel et al., 2020), CodeT5 (Wang et al., 2021), Fusion-in-Decoder (Izacard and Grave, 2021)), and Codex (Chen et al., 2021).

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by strong evidence that demonstrates the general applicability of DocPrompting across different programming languages and base models.
Key Limitations: The claim is limited to the specific benchmarks and models used in the study.

--------------------------------------------------

Claim 4:
Type: performance
Statement: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation in CoNaLa.
Location: introduction
Exact Quote: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo-1.3B by up to absolute 6.9% exact match.

Evidence:
- Evidence Text: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo-1.3B by up to absolute 6.9% exact match.
  Strength: strong
  Location: introduction
  Limitations: None
  Exact Quote: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo-1.3B by up to absolute 6.9% exact match.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by strong evidence that demonstrates the performance improvements achieved by DocPrompting on different benchmarks and models.
Key Limitations: The claim is limited to the specific benchmarks and models used in the study.

--------------------------------------------------

Claim 5:
Type: result
Statement: aLa, this baseline achieved a BLEU score of 39.12, which outperforms the previous state-of-the-art (Beau and Crabb´e, 2022) by 4.92 BLEU points.
Location: part 3
Exact Quote: aLa, this baseline achieved a BLEU score of 39.12, which outperforms the previous state-of-the-art (Beau and Crabb´e, 2022) by 4.92 BLEU points.

Evidence:
- Evidence Text: 39.12
  Strength: strong
  Location: part 3
  Limitations: none
  Exact Quote: aLa, this baseline achieved a BLEU score of 39.12, 

- Evidence Text: 4.92
  Strength: strong
  Location: part 3
  Limitations: none
  Exact Quote: ...outperforms the previous state-of-the-art (Beau and Crabb´e, 2022) by 4.92 BLEU points.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by concrete evidence from the paper.
Key Limitations: none

--------------------------------------------------

Claim 6:
Type: result
Statement: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass @ 1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark.
Location: part 3
Exact Quote: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass @ 1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; 

Evidence:
- Evidence Text: 2.85%
  Strength: strong
  Location: part 3
  Limitations: limited to a specific dataset
  Exact Quote: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass @ 1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; 

- Evidence Text: 52%
  Strength: strong
  Location: part 3
  Limitations: relative gain
  Exact Quote: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass @ 1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; 

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by concrete evidence from the paper, but the results may not generalize to other datasets.
Key Limitations: limited to Python CoNaLa benchmark

--------------------------------------------------

Claim 7:
Type: result
Statement: DocPrompting improves CodeT5 and GPT-Neo-1.3B by up to 6.9% exact match, and Codex by 6.78 charBLEU score on a new Bash dataset tldr.
Location: part 3
Exact Quote: on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo-1.3B by up to 6.9% exact match, and Codex by 6.78 charBLEU score.

Evidence:
- Evidence Text: 6.9%
  Strength: strong
  Location: part 3
  Limitations: limited to a specific dataset
  Exact Quote: on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo-1.3B by up to 6.9% exact match, 

- Evidence Text: 6.78
  Strength: strong
  Location: part 3
  Limitations: different metric
  Exact Quote: and Codex by 6.78 charBLEU score.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by concrete evidence from the paper, but the results may not generalize to other datasets.
Key Limitations: limited to tldr dataset

--------------------------------------------------

Claim 8:
Type: methodology
Statement: Documentation easing the mapping between NL intents and code due to containing both NL descriptions and function signatures.
Location: part 3
Exact Quote: We believe that one of the major reasons is that documentation eases the mapping between NL intents and code, since the documentation contains both NL descriptions and function signatures.

Evidence:
- Evidence Text: Documentation contains both NL descriptions and function signatures.
  Strength: strong
  Location: part 3
  Limitations: assumption
  Exact Quote: We believe that one of the major reasons is that documentation eases the mapping between NL intents and code, since the documentation contains both NL descriptions and function signatures.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by a reasonable assumption.
Key Limitations: relies on documentation quality

--------------------------------------------------

Claim 9:
Type: result
Statement: DocPrompting increases n-gram overlap between NL intents and code snippets.
Location: part 3
Exact Quote: As shown in Figure 4, adding documentation significantly increases the overlap across n-grams, and increase, for example, the unigram overlap from 12% to 24% in tldr.

Evidence:
- Evidence Text: Figure 4
  Strength: strong
  Location: part 3
  Limitations: graphical data
  Exact Quote: As shown in Figure 4, adding documentation significantly increases the overlap across n-grams, and increase, for example, the unigram overlap from 12% to 24% in tldr.

- Evidence Text: 12% to 24%
  Strength: strong
  Location: part 3
  Limitations: specific example
  Exact Quote: As shown in Figure 4, adding documentation significantly increases the overlap across n-grams, and increase, for example, the unigram overlap from *12%* to *24%* in tldr.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by concrete evidence from the paper, but the results may vary depending on the dataset and evaluation metric.
Key Limitations: specific to n-gram overlap

--------------------------------------------------

