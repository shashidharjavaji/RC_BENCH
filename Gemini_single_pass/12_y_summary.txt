Claim 1:
Type: methodology
Statement: There is a significant gap between textual and visual representations in MLLMs, indicating unsatisfactory cross-modal representation alignment.
Location: Introduction
Exact Quote: We first analyzed the representation distribution of textual and visual tokens in MLLM, revealing two important findings: 1) there is a significant gap between textual and visual representations, indicating unsatisfactory cross-modal representation alignment;

Evidence:
- Evidence Text: The distribution of the last token’s representations yielded by LLM for visual or textual token sequences.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: As shown in Figure 1, we have two primary findings: - A significant modality gap remains between the textual and visual tokens despite visual projection;

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence shows that the distribution of textual and visual representations are significantly different, indicating a gap in cross-modal representation alignment.
Key Limitations: None

--------------------------------------------------

Claim 2:
Type: methodology
Statement: Representations of texts that contain and do not contain hallucinations are entangled, making it challenging to distinguish them.
Location: Introduction
Exact Quote: We first analyzed the representation distribution of textual and visual tokens in MLLM, revealing two important findings: ... 2) representations of texts that contain and do not contain hallucinations are entangled, making it challenging to distinguish them.

Evidence:
- Evidence Text: The distribution of the last token’s representations yielded by LLM for visual or textual token sequences.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: As shown in Figure 1, we have two primary findings: ... - Representations of texts that contain and do not contain hallucinations are entangled, making it challenging to differentiate them.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence shows that the representations of texts with and without hallucinations are not well separated, making it difficult to distinguish between them.
Key Limitations: None

--------------------------------------------------

Claim 3:
Type: methodology
Statement: The proposed method, Hallucination Augmented Contrastive Learning (HACL), enhances the alignment between visual and textual representations to alleviate hallucinations.
Location: Introduction
Exact Quote: Inspired by these insights, we propose a simple yet effective method named Hallucination Augmented Cross-Modal Contrastive Learning (HACL). Introducing contrastive learning into MLLMs and using hallucinative text as hard negative samples yields a better cross-modal and more hallucinations-distinguishable representation space.

Evidence:
- Evidence Text: The results of the MMhal-Bench benchmark, which show a 29% increase in overall score when using HACL.
  Strength: strong
  Location: Introduction
  Limitations: The results are specific to the MMhal-Bench benchmark and may not generalize to other tasks.
  Exact Quote: As shown in Subfigure 1 (c), when equipped with HACL, LLaVA achieves a 29% increase in overall score on the MMhal-Bench benchmark [44], as well as an 11% improvement on the MME [12] benchmark.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: medium
Justification: The evidence shows that HACL can improve the alignment between visual and textual representations and reduce hallucinations.
Key Limitations: The results may not generalize to other tasks.

--------------------------------------------------

Claim 4:
Type: methodology
Statement: HACL improves the effectiveness of contrastive learning by introducing hard negative samples that mimic the hallucinative text generated by MLLMs.
Location: Section 3.2
Exact Quote: We propose to improve the effectiveness of contrastive learning by introducing hard negative samples which mimic the hallucinative text generated by MLLMs.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the fact that HACL leverages hallucinative captions, which are similar to the hallucinations generated by MLLMs, to improve contrastive learning.
Key Limitations: The effectiveness of HACL may vary depending on the quality of the hallucinative captions generated.

--------------------------------------------------

Claim 5:
Type: performance
Statement: HACL significantly improves the overall performance of MMHal-Bench and POPE benchmarks, demonstrating its effectiveness in mitigating hallucination.
Location: Section 4.2
Exact Quote: We verify the efficacy of our proposed method in addressing hallucination issues, we leveraged two widely used benchmark evaluation datasets that evaluate the presence of hallucinations in models. These datasets included MMHal-Bench\n[44] and POPE [27]. MMHal-Bench offers a comprehensive evaluation of models that encompasses multiple perspectives, such as attributes, relations, and counting. On the other hand, POPE particularly focuses on hallucinations related to objects. We employed both datasets to measure the effectiveness of our method in addressing hallucination across various scenarios.

Evidence:
- Evidence Text: On MMHal-Bench, MiniGPT-4-HACL exhibited considerable performance gain over MiniGPT-4 [55]. Moreover, compared with LLaVA-RLHF[44], a recently proposed method that uses human feedback and reinforcement learning to address hallucinations, LLaVA-HACL showed an even more significant improvement.
  Strength: strong
  Location: Section 4.2
  Limitations: The results may vary depending on the specific models and datasets used.
  Exact Quote: On MMHal-Bench\n[44], MiniGPT-4-HACL exhibited considerable performance gain over MiniGPT-4 [55]. Moreover, compared with LLaVA-RLHF[44], a recently proposed method that uses human feedback and reinforcement learning to address hallucinations, LLaVA-HACL showed an even more significant improvement.

- Evidence Text: In addition, we obtained consistent results using MMHal-Bench [44] in the POPE evaluation benchmark [27]. Table 2 shows that miniGPT-4-HACL and LLaVA-HACL both demonstrated significant improvements compared to the original model. Of particular note, the average F1 score of LLaVA-HACL increased by 17.8% compared to LLaVA [32], while the Yes ratio decreased from99.55 to 48.25.
  Strength: strong
  Location: Section 4.2
  Limitations: The results may vary depending on the specific models and datasets used.
  Exact Quote: In addition, we obtained consistent results using MMHal-Bench [44] in the POPE evaluation benchmark [27]. Table 2 shows that miniGPT-4-HACL and LLaVA-HACL both demonstrated significant improvements compared to the original model. Of particular note, the average F1 score of LLaVA-HACL increased by 17.8% compared to LLaVA [32], while the Yes ratio decreased from\n99.55 to 48.25.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by strong evidence from two widely used benchmark datasets, demonstrating the effectiveness of HACL in mitigating hallucination.
Key Limitations: The results may vary depending on the specific models and datasets used.

--------------------------------------------------

Claim 6:
Type: result
Statement: HACL has shown effectiveness in solving the issue of hallucination.
Location: Results on Benchmark Tasks
Exact Quote: HACL has shown effectiveness in solving the issue of hallucination.

Evidence:
- Evidence Text: After applying HACL to MiniGPT-4, LLaVA, and LLaVA1.5, all three models exhibited improvements across multiple benchmarks.
  Strength: strong
  Location: MLLM-oriented Multi-modal Benchmarks
  Limitations: None mentioned
  Exact Quote: After implementing HACL, all three models exhibited improvements across multiple benchmarks.

- Evidence Text: For instance, after implementing HACL, LLaVA’s MME score improved from 581.67 to 653.94.
  Strength: strong
  Location: MLLM-oriented Multi-modal Benchmarks
  Limitations: None mentioned
  Exact Quote: For instance, after implementing HACL, LLaVA’s MME score improved from 581.67 to 653.94.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by multiple strong evidence that shows improvements in hallucination detection and overall performance on multiple benchmarks after applying HACL to different MLLMs.
Key Limitations: None mentioned

--------------------------------------------------

Claim 7:
Type: result
Statement: Our approach successfully enhances the performance of original models across a range of VQA datasets.
Location: Results on Benchmark Tasks
Exact Quote: Our experimental results show that our approach successfully enhances the performance of original models across a range of VQA datasets.

Evidence:
- Evidence Text: LLaVA-HACL outperforms LLaVA [32] in terms of consistency and accuracy across all VQA datasets.
  Strength: strong
  Location: Results on Benchmark Tasks
  Limitations: None mentioned
  Exact Quote: LLaVA-HACL outperforms LLaVA [32] in terms of consistency and accuracy across all VQA datasets.

- Evidence Text: Additionally, when compared to LLaVA1.5 [31], LLaVA1.5-HACL achieves better results in General VQA benchmarks and zero-shot VQA tasks.
  Strength: strong
  Location: Results on Benchmark Tasks
  Limitations: None mentioned
  Exact Quote: Additionally, when compared to LLaVA1.5 [31], LLaVA1.5-HACL achieves better results in General VQA benchmarks abd zero-shot VQA tasks

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by multiple strong evidence that shows improvements in VQA performance on different datasets and metrics after applying HACL to LLaVA and LLaVA1.5.
Key Limitations: None mentioned

--------------------------------------------------

Claim 8:
Type: result
Statement: MLLMs may not only mitigate hallucinations but also improve correlations between visual and textual information, which further refines the generalization ability of models.
Location: Results on Benchmark Tasks
Exact Quote: MLLMs may not only mitigate hallucinations but also improve correlations between visual and textual information, which further refines the generalization ability of models.

Evidence:
- Evidence Text: LLaVA-HACL outperforms LLaVA [32] in terms of consistency and accuracy across all VQA datasets.
  Strength: strong
  Location: Results on Benchmark Tasks
  Limitations: None mentioned
  Exact Quote: LLaVA-HACL outperforms LLaVA [32] in terms of consistency and accuracy across all VQA datasets.

- Evidence Text: Additionally, when compared to LLaVA1.5 [31], LLaVA1.5-HACL achieves better results in General VQA benchmarks and zero-shot VQA tasks.
  Strength: strong
  Location: Results on Benchmark Tasks
  Limitations: None mentioned
  Exact Quote: Additionally, when compared to LLaVA1.5 [31], LLaVA1.5-HACL achieves better results in General VQA benchmarks abd zero-shot VQA tasks

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by multiple strong evidence that shows improvements in VQA performance on different datasets and metrics after applying HACL to LLaVA and LLaVA1.5.
Key Limitations: None mentioned

--------------------------------------------------

Claim 9:
Type: contribution
Statement: HACL effectively reduces the occurrence of hallucinations.
Location: Conclusion
Exact Quote: Experimental results demonstrate that incorporating HACL enhances the performance of MLLMs and significantly reduces the occurrence of hallucinations in benchmark evaluations.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by experimental results, but the specific reduction in hallucination occurrence is not quantified.
Key Limitations: The evaluation does not specify which benchmark evaluations were used or how the reduction in hallucination occurrence was measured.

--------------------------------------------------

Claim 10:
Type: contribution
Statement: The proposed method improves the alignment between visual and textual representations.
Location: Conclusion
Exact Quote: This paper addresses the issue of hallucinations in Multi-modal Large Language Models (MLLMs) and proposes a method called Hallucination Augmented Contrastive Learning (HACL) to improve the alignment between visual and textual representations.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the introduction and methodology sections, which describe the HACL method and its goal of improving alignment between visual and textual representations.
Key Limitations: The evaluation does not include specific metrics or results to quantify the improvement in alignment.

--------------------------------------------------

Claim 11:
Type: result
Statement: Contrastive learning and hallucination augmentation reduce the modality gap in data distribution.
Location: 4.5 Visualization
Exact Quote: As illustrated in Figure 4 (a), a substantial modality gap is observable in the data distribution without contrast learning. In Figure 4 (b), after applying contrast learning, although the modal gap decreased, a differentiation in the distribution of hallucination samples and ground truth samples was unattainable. In Figure 4 (c), with the application of hallucination augmentation in contrast learning, not only did the modal gap decrease, but the hallucination sample distribution was also significantly distanced.

Evidence:
- Evidence Text: Figure 4 shows that the modality gap is reduced with contrastive learning and hallucination augmentation.
  Strength: strong
  Location: 4.5 Visualization
  Limitations: The figure does not include quantitative metrics to measure the reduction in modality gap.
  Exact Quote: As illustrated in Figure 4 (a), a substantial modality gap is observable in the data distribution without contrast learning. In Figure 4 (b), after applying contrast learning, although the modal gap decreased, a differentiation in the distribution of hallucination samples and ground truth samples was unattainable. In Figure 4 (c), with the application of hallucination augmentation in contrast learning, not only did the modal gap decrease, but the hallucination sample distribution was also significantly distanced.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is strongly supported by visual evidence in Figure 4, which shows a clear reduction in modality gap with the application of contrastive learning and hallucination augmentation.
Key Limitations: The evaluation does not include specific metrics to quantify the reduction in modality gap.

--------------------------------------------------

Claim 12:
Type: result
Statement: HACL enhances the performance of MLLMs on benchmark evaluations.
Location: Conclusion
Exact Quote: Experimental results demonstrate that incorporating HACL enhances the performance of MLLMs and significantly reduces the occurrence of hallucinations in benchmark evaluations.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the statement that experimental results demonstrate enhanced performance of MLLMs with HACL.
Key Limitations: The evaluation does not specify which benchmark evaluations were used or provide specific performance metrics.

--------------------------------------------------

Claim 13:
Type: result
Statement: Initiating the Visual Encoder during pretraining leads to a modest performance boost.
Location: 4.4 Discussion on Training Paradigm
Exact Quote: Conversely, initiating the Visual Encoder led to a modest performance boost. This might be attributed to the fact that the target parameters our model can optimize extend beyond the learnable interface and incorporate the visual encoder as well.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the statement that initiating the Visual Encoder led to a modest performance boost.
Key Limitations: The evaluation does not provide specific performance metrics or experimental results to support the claim.

--------------------------------------------------

Claim 14:
Type: result
Statement: Activating LLMs during pretraining leads to a significant performance decline.
Location: 4.4 Discussion on Training Paradigm
Exact Quote: We hypothesize that this downturn could be linked to low-quality data in the first pretraining stage and the introduction of additional contrast learning tasks, both of which affect the LLMs’ representation distribution. This culminates in the catastrophic forgetting of the LLMs.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the statement that activating LLMs during pretraining leads to a significant performance decline.
Key Limitations: The evaluation does not provide specific performance metrics or experimental results to support the claim.

--------------------------------------------------

