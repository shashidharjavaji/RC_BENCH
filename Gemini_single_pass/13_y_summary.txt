Claim 1:
Type: performance
Statement: Using templates can substantially improve the results of original BERT on all datasets.
Location: 5.4 Non Fine-Tuned BERT Results
Exact Quote: Using templates can substantially improve the results of original BERT on all datasets.

Evidence:
- Evidence Text: Compared to pooling methods like averaging of last layer or averaging of first and last layers, our methods can improve spearman correlation by more than 10%.
  Strength: strong
  Location: 5.4 Non Fine-Tuned BERT Results
  Limitations: None
  Exact Quote: Compared to pooling methods like averaging of last layer or averaging of first and last layers, our methods can improve spearman correlation by more than 10%.

- Evidence Text: Compared to the postprocess methods: BERT-flow and BERT-whitening, only using the manual template surpasses can these methods.
  Strength: strong
  Location: 5.4 Non Fine-Tuned BERT Results
  Limitations: None
  Exact Quote: Compared to the postprocess methods: BERT-flow and BERT-whitening, only using the manual template surpasses can these methods.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by strong evidence, and the study was conducted using a variety of datasets.
Key Limitations: None

--------------------------------------------------

Claim 2:
Type: performance
Statement: Prompt based contrastive learning objective significantly shortens the gap between the unsupervised and supervised methods.
Location: 5.5 Fine-Tuned BERT Results
Exact Quote: Prompt based contrastive learning objective significantly shortens the gap between the unsupervised and supervised methods.

Evidence:
- Evidence Text: PromptBERTbase achieved a STS12 score of 71.56±0.18 and a STS16 score of 80.60±0.21, which are higher than the results of unsupervised SimCSE-BERTbase (STS12: 68.40, STS16: 78.56) and supervised IS-BERTbase (STS12: 56.77, STS16: 70.16).
  Strength: strong
  Location: 5.5 Fine-Tuned BERT Results and Table 5
  Limitations: None
  Exact Quote: PromptBERTbase achieved a STS12 score of 71.56±0.18 and a STS16 score of 80.60±0.21, which are higher than the results of unsupervised SimCSE-BERTbase (STS12: 68.40, STS16: 78.56) and supervised IS-BERTbase (STS12: 56.77, STS16: 70.16).

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by strong evidence.
Key Limitations: None

--------------------------------------------------

Claim 3:
Type: methodology
Statement: Template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.
Location: 6.1 Template Denoising
Exact Quote: We find the template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.

Evidence:
- Evidence Text: As Table 7 shows, we predict some sentences’ top-5 tokens in the [MASK] tokens. We find the template denoising removes the unrelated tokens like “nothing,no,yes” and helps the model predict more related tokens.
  Strength: strong
  Location: 6.1 Template Denoising
  Limitations: The results are based on a small number of examples.
  Exact Quote: As Table 7 shows, we predict some sentences’ top-5 tokens in the [MASK] tokens. We find the template denoising removes the unrelated tokens like “nothing,no,yes” and helps the model predict more related tokens.

- Evidence Text: The template denoising significantly improves the quality of tokens predicted by MLM head.
  Strength: strong
  Location: 6.1 Template Denoising
  Limitations: None
  Exact Quote: The template denoising significantly improves the quality of tokens predicted by MLM head.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by strong evidence from a limited number of examples.
Key Limitations: The results may not generalize to other datasets or models.

--------------------------------------------------

