Claim 1:
Type: performance
Statement: Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25 fewer parameters.
Location: Abstract
Exact Quote: With a 2 trillion token database,\nour Retrieval-Enhanced Transformer (RETRO)\nobtains comparable performance to GPT-3 and\nJurassic-1 on the Pile, despite using 25 fewer pa\nrameters.

Evidence:
- Evidence Text: RETRO performs comparably to GPT-3 and Jurassic-1 on the Pile, despite using 25 fewer parameters.
  Strength: strong
  Location: Abstract
  Limitations: None
  Exact Quote: With a 2 trillion token database,\nour Retrieval-Enhanced Transformer (RETRO)\nobtains comparable performance to GPT-3 and\nJurassic-1 on the Pile, despite using 25 fewer pa\nrameters.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim that RETRO performs comparably to GPT-3 and Jurassic-1 on the Pile, despite using 25 fewer parameters.
Key Limitations: None

--------------------------------------------------

Claim 2:
Type: methodology
Statement: RETRO combines a frozen BERT retriever, a differentiable encoder and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training.
Location: Abstract
Exact Quote: RETRO combines a frozen BERT retriever, a differentiable encoder and a chunked cross-attention mechanism\nto predict tokens based on an order of magnitude more data than what is typically consumed\nduring training.

Evidence:
- Evidence Text: RETRO combines a frozen BERT retriever, a differentiable encoder, and a chunked cross-attention mechanism.
  Strength: strong
  Location: Abstract
  Limitations: None
  Exact Quote: RETRO combines a frozen BERT retriever, a differentiable encoder and a chunked cross-attention mechanism

- Evidence Text: RETRO can predict tokens based on an order of magnitude more data than what is typically consumed during training.
  Strength: strong
  Location: Abstract
  Limitations: None
  Exact Quote: to predict tokens based on an order of magnitude more data than what is typically consumed\nduring training.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim that RETRO combines a frozen BERT retriever, a differentiable encoder, and a chunked cross-attention mechanism to predict tokens based on an order of magnitude more data than what is typically consumed during training.
Key Limitations: None

--------------------------------------------------

Claim 3:
Type: methodology
Statement: Retrieval models contain one RETRO-block every 3 blocks, starting from layer 6.
Location: Training details
Exact Quote: The retrieval models contain one RETRO-block every 3 blocks, starting from layer 6.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is directly stated in the paper and no contradictory evidence is found.
Key Limitations: None

--------------------------------------------------

Claim 4:
Type: methodology
Statement: Additional CCA layers (3 in the decoder, 1 in the encoder) hold 12M parameters.
Location: Training details
Exact Quote: Additional CCA layers (3 in the decoder, 1 in the encoder) hold 12M parameters.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is directly stated in the paper and no contradictory evidence is found.
Key Limitations: None

--------------------------------------------------

Claim 5:
Type: methodology
Statement: The relative number of extra parameters reduces as we increase the baseline model size.
Location: Training details
Exact Quote: The relative number of extra parameters reduces as we increase the baseline model size.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is directly stated in the paper and no contradictory evidence is found.
Key Limitations: None

--------------------------------------------------

Claim 6:
Type: methodology
Statement: BERT model is trained for 500,000 steps with a batch size of 2,048.
Location: Training details
Exact Quote: We train the BERT model for 500,000 steps with a batch size of 2,048.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is directly stated in the paper and no contradictory evidence is found.
Key Limitations: None

--------------------------------------------------

Claim 7:
Type: performance
Statement: RETRO models may arguably benefit more from evaluation dataset leakage.
Location: Quantifying dataset leakage exploitation
Exact Quote: RETRO models may arguably benefit more from evaluation dataset leakage (i.e. some evaluation data may also be present in the training set).

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is directly stated in the paper and no contradictory evidence is found.
Key Limitations: None

--------------------------------------------------

Claim 8:
Type: performance
Statement: retrieval improves language modelling performance
Location: Quantifying dataset leakage exploitation
Exact Quote: To better understand how retrieval improves language modelling performance, we quantify evaluation likelihood as a function of the overlap between the evaluation and training datasets.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is directly stated in the paper and no contradictory evidence is found.
Key Limitations: None

--------------------------------------------------

Claim 9:
Type: performance
Statement: RETROfitting models quickly surpasses the performance of baseline models and even achieves performance close to that of RETRO models trained from scratch.
Location: 4.3
Exact Quote: RETROfitting models quickly surpasses the performance of baseline models and even achieves performance close to that of RETRO models trained from scratch, as displayed in Fig. 3.

Evidence:
- Evidence Text: RETROfitting resulted in models with performance almost as good as models trained from scratch.
  Strength: strong
  Location: 4.3
  Limitations: None
  Exact Quote: RETROfitting models quickly surpasses the performance of baseline models and even achieves performance close to that of RETRO models trained from scratch, as displayed in Fig. 3.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is directly supported by the experimental results presented in Figure 3.
Key Limitations: None

--------------------------------------------------

Claim 10:
Type: methodology
Statement: RETROfitting only changes a small fraction of the model weights (less than 10% for the 7B model).
Location: 4.3
Exact Quote: This offers an efficient alternative path to enhance transformers with retrieval, requiring only 6 million sequences (3% of the pre-training sequences that we used). Additionally, training exclusively the new weights ensures that the original model performance is exactly maintained when evaluated without retrieval.

Evidence:
- Evidence Text: Only 6 million sequences were added to the pre-trained model during RETROfitting, which constitutes 3% of sequences used in the original pre-training process.
  Strength: strong
  Location: 4.3
  Limitations: None
  Exact Quote: This offers an efficient alternative path to enhance transformers with retrieval, requiring only 6 million sequences (3% of the pre-training sequences that we used).

- Evidence Text: Training RETRO only changed less than 10% of the weights in the 7B model.
  Strength: moderate
  Location: 4.3
  Limitations: It is not clear how many weights were changed in models of other sizes.
  Exact Quote: RETROfitting models quickly surpasses the performance of baseline models and even achieves performance close to that of RETRO models trained from scratch, as displayed in Fig. 3. This offers an efficient alternative path to enhance transformers with retrieval, requiring only 6 million sequences (3% of the pre-training sequences that we used). Additionally, training exclusively the new weights ensures that the original model performance is exactly maintained when evaluated without retrieval.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the experimental details provided in the paper, but it is not clear how generalizable these results are to models of different sizes.
Key Limitations: The claim is specific to the 7B model.

--------------------------------------------------

