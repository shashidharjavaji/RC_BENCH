Claim 1:
Type: result
Statement: Feed-forward layers in transformer-based language models operate as key-value memories.
Location: Abstract
Exact Quote: Transformer layersFeed-forward layers constitute two-thirds of a transformer model’s parameters, yet their role in the network remains under-explored. We show that feed-forward layers in transformerbased language models operate as key-value memories, where each key correlates with textual patterns in the training examples, and each value induces a distribution over the output vocabulary.

Evidence:
- Evidence Text: Feed-forward layers emulate neural memories, where the first parameter matrix in the layer corresponds to keys, and the second parameter matrix to values.
  Strength: strong
  Location: Section 2
  Limitations: None mentioned
  Exact Quote: We show that feed-forward layers emulate neural memories (Sukhbaatar et al., 2015), where the first parameter matrix in the layer corresponds to keys, and the second parameter matrix to values.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim that feed-forward layers in transformer-based language models operate as key-value memories.
Key Limitations: The study was conducted on a limited number of transformer models.

--------------------------------------------------

Claim 2:
Type: result
Statement: Each key correlates with a specific set of human-interpretable input patterns.
Location: Introduction
Exact Quote: We find that each key correlates with a specific set of human-interpretable input patterns, such as n-grams or semantic topics.

Evidence:
- Evidence Text: For example, k2 in Figure 1 is triggered by inputs that describe a period of time and end with “a”.
  Strength: moderate
  Location: Introduction
  Limitations: The example is specific to k2 in Figure 1.
  Exact Quote: For example, k2 in Figure 1 is triggered by inputs that describe a period of time and end with “a”.

- Evidence Text: We manually inspected the keys of a trained language model’s feed-forward layers and found that each key vector ki corresponds to a specific pattern over the input prefix x1, . . ., xj.
  Strength: strong
  Location: Section 3
  Limitations: The inspection was conducted on a limited number of keys.
  Exact Quote: We posit that the key vectors K in feed-forward layers act as pattern detectors over the input sequence, where each individual key vector ki corresponds to a specific pattern over the input prefix x1, . . ., xj.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence provides some support for the claim that each key correlates with a specific set of human-interpretable input patterns, but the evidence is limited.
Key Limitations: The study was conducted on a limited number of transformer models and keys.

--------------------------------------------------

Claim 3:
Type: result
Statement: Each value induces a distribution over the output vocabulary.
Location: Abstract
Exact Quote: ... and each value induces a distribution over the output vocabulary.

Evidence:
- Evidence Text: Figure 1 illustrates how the keys (first parameter matrix) interact with the input to produce coefficients, which are then used to compute a weighted sum of the values (second parameter matrix) as the output.
  Strength: strong
  Location: Introduction
  Limitations: None mentioned
  Exact Quote: Figure 1 illustrates how the keys (first parameter matrix) interact with the input to produce coefficients, which are then used to compute a weighted sum of the values (second parameter matrix) as the output.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim that each value induces a distribution over the output vocabulary.
Key Limitations: The study was conducted on a limited number of transformer models.

--------------------------------------------------

