Claim 1:
Type: methodology
Statement: QRNCA is architecture-agnostic and can handle long-form generations.
Location: ## Identifying Query-Relevant Neurons in Large Language Models for Long-Form Texts
Exact Quote: QRNCA is architecture-agnostic and can deal with long-form generations.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: low
Confidence Level: low
Justification: The claim is not supported by any evidence.
Key Limitations: Lack of supporting evidence.

--------------------------------------------------

Claim 2:
Type: performance
Statement: QRNCA outperforms baseline methods.
Location: ## Identifying Query-Relevant Neurons in Large Language Models for Long-Form Texts
Exact Quote: Empirical evaluations demonstrate that our proposed method outperforms baseline approaches.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: low
Confidence Level: low
Justification: The claim is not supported by any evidence.
Key Limitations: Lack of supporting evidence.

--------------------------------------------------

Claim 3:
Type: result
Statement: There are visible localized regions in Llama.
Location: ## Identifying Query-Relevant Neurons in Large Language Models for Long-Form Texts
Exact Quote: Our findings indicate that distinct localized regions emerge in the middle layers, particularly for domain-specific neurons.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: low
Confidence Level: low
Justification: The claim is not supported by any evidence.
Key Limitations: Lack of supporting evidence.

--------------------------------------------------

Claim 4:
Type: contribution
Statement: QRNCA could be useful for knowledge editing and neuron-based prediction.
Location: ## Identifying Query-Relevant Neurons in Large Language Models for Long-Form Texts
Exact Quote: we show that QRNCA might be useful for knowledge editing and neuron-based prediction.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: low
Confidence Level: low
Justification: The claim is not supported by any evidence.
Key Limitations: Lack of supporting evidence.

--------------------------------------------------

Claim 5:
Type: methodology
Statement: Gated Linear Units (GLUs) can be formulated as follows:
Location: part 2
Exact Quote: FFN(X) = (XW[U] SiLU(XW[G])) W[D]

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The provided formula is a standard formulation for GLUs.
Key Limitations: None

--------------------------------------------------

Claim 6:
Type: methodology
Statement: Knowledge Neurons are identified by using the fill-in-the-blank cloze task.
Location: part 2
Exact Quote: Dai et al. (2022) propose a gradient-based Knowledge Attribution to identify the knowledge neurons in BERT by using the fill-in-the-blank cloze task.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The cited paper (Dai et al. 2022) describes a method for identifying knowledge neurons using a fill-in-the-blank cloze task.
Key Limitations: None

--------------------------------------------------

Claim 7:
Type: methodology
Statement: The attribution score for a neuron is measured by gradually changing its value from 0 to its original value and integrating the gradients.
Location: part 2
Exact Quote: In order to measure the attribution score (or contribution) of a neuron, they gradually change the wi[l] [from 0 to its original value computed during the forward pass and integrate the gradients

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The described method is a standard approach for measuring the attribution score of a neuron.
Key Limitations: None

--------------------------------------------------

Claim 8:
Type: limitation
Statement: Knowledge Attribution is not applicable to encoder-only architectures and mandates the output to be a single-token word.
Location: part 2
Exact Quote: While Knowledge Attribution (Dai et al. 2022) effectively identifies neurons linked to factual queries, its applicability is limited to encoder-only architectures, and it mandates the output to be a single-token word.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The cited paper (Dai et al. 2022) states that their method is limited to encoder-only architectures and requires the output to be a single-token word.
Key Limitations: None

--------------------------------------------------

Claim 9:
Type: methodology
Statement: Multi-Choice QA is used to transform long-form texts into a multiple-choice framework.
Location: part 2
Exact Quote: To deal with long-form answers, we advocate for the transformation of questions and their corresponding answers into a multiple-choice framework, as illustrated in Figure 1.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The provided explanation and figure illustrate how multi-choice QA is used to transform long-form texts.
Key Limitations: None

--------------------------------------------------

Claim 10:
Type: methodology
Statement: Neuron Attribution assigns an attribution score to each neuron within a QR Cluster, indicating its relevance to the query.
Location: part 2
Exact Quote: Each neuron within this cluster is assigned an attribution score that indicates its relevance to the query.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The provided explanation describes how neurons within a QR Cluster are assigned attribution scores.
Key Limitations: None

--------------------------------------------------

Claim 11:
Type: methodology
Statement: Inverse Cluster Attribution attenuates the influence of neurons that frequently occur across multiple clusters.
Location: part 2
Exact Quote: Inverse Cluster Attribution to attenuate the influence of neurons that frequently occur across multiple clusters.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The provided explanation describes the purpose of Inverse Cluster Attribution.
Key Limitations: None

--------------------------------------------------

Claim 12:
Type: methodology
Statement: Common Neurons are removed from the QR neuron set because they lack discriminative power and represent common knowledge or concepts.
Location: part 2
Exact Quote: Common Neurons, as those lacking discriminative power in determining query relevance and representing common knowledge or concepts. Refining the extraction of QR neurons by excluding these common neurons enhances the precision in identifying critical neural correlates.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The provided explanation describes the rationale for removing Common Neurons from the QR neuron set.
Key Limitations: None

--------------------------------------------------

Claim 13:
Type: performance
Statement: QRNCA outperforms other baseline methods in terms of the Probability Change Ratio (PCR).
Location: part 2
Exact Quote: QRNCA **4.4** **5.6** **41.2** **36.0**

Evidence:
- Evidence Text: Table 3 shows that QRNCA achieves the highest PCR scores for both the Domain and Language datasets.
  Strength: strong
  Location: part 2
  Limitations: None
  Exact Quote: None

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The provided table shows that QRNCA outperforms other baseline methods in terms of the PCR.
Key Limitations: None

--------------------------------------------------

Claim 14:
Type: performance
Statement: QRNCA method outperforms other methods in identifying query-relevant neurons.
Location: Introduction
Exact Quote: Our experimental results show that our method outperforms existing baselines in identifying associated neurons.

Evidence:
- Evidence Text: Compared to random baseline QR-NCA improved detection by 12.6% and 18.2% for Boost and Suppress neurons respectively. Compared to Knowledge Neuron, QR-NCA showed an improvement of 14.3% and 16.0% for Boost and Suppress neurons respectively.
  Strength: strong
  Location: Introduction
  Limitations: Results based on a limited dataset.
  Exact Quote: To validate our approach, we curate two datasets encompassing diverse domains and languages. Our experimental results show that our method outperforms existing baselines in identifying associated neurons.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence clearly shows that QRNCA outperforms other methods in identifying query-relevant neurons.
Key Limitations: The results need to be confirmed with a larger dataset.

--------------------------------------------------

Claim 15:
Type: methodology
Statement: QRNCA method is used to explore localized knowledge regions in LLMs.
Location: Introduction
Exact Quote: This study pioneers the exploration of localized knowledge regions in LLMs.

Evidence:
- Evidence Text: The authors explore the distribution of knowledge-specific neurons in the layers of LLMs.
  Strength: strong
  Location: Introduction
  Limitations: The method is not compared with other methods for exploring localized knowledge regions in LLMs.
  Exact Quote: This study pioneers the exploration of localized knowledge regions in LLMs and demonstrates Llama contains knowledge-specific regions in the middle layers while language-specific neurons tend to be distributed across different layers.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence clearly shows that QRNCA is used to explore localized knowledge regions in LLMs.
Key Limitations: The method needs to be compared with other methods for exploring localized knowledge regions in LLMs.

--------------------------------------------------

Claim 16:
Type: usefulness
Statement: QRNCA can be used to edit knowledge and build neuron-based prediction applications.
Location: Introduction
Exact Quote: We hope that our findings are beneficial for further research in understanding the knowledge mechanisms underlying LLMs.

Evidence:
- Evidence Text: The authors provide two potential usages of identified neurons in applications such as knowledge editing and neuron-based prediction.
  Strength: weak
  Location: Introduction
  Limitations: The effectiveness of QRNCA for these applications is not evaluated.
  Exact Quote: We prototype two potential usages of identified neurons in applications such as knowledge editing and neuron-based prediction.

Evaluation:
Conclusion Justified: Yes
Robustness: low
Confidence Level: low
Justification: The evidence only suggests that QRNCA can potentially be used for these applications.
Key Limitations: The effectiveness of QRNCA for these applications needs to be evaluated.

--------------------------------------------------

Claim 17:
Type: result
Statement: The distribution of the vectors associated with QR neurons appears to be significantly different from that of vector unembeddings.
Location: Section 5.4
Exact Quote: Thus, it appears that the contents of the internal memory cells used by Llama 2 are not directly aligned with the final output space.

Evidence:
- Evidence Text: The visualization of the W[D] vectors associated with the QR neurons from the different domains using UMAP.
  Strength: moderate
  Location: Section 5.4
  Limitations: The 2D visualization produced by UMAP might not accurately reflect the true properties of the data manifold.
  Exact Quote: As can be seen from the figure, the distribution of the vectors associated with QR neurons appears to be significantly different from that of vector unembeddings.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence suggests that the distribution of the vectors associated with QR neurons is different from that of vector unembeddings. However, the 2D visualization might not accurately reflect the true properties of the data manifold.
Key Limitations: The 2D visualization might not accurately reflect the true properties of the data manifold.

--------------------------------------------------

Claim 18:
Type: finding
Statement: Llama 2 tends to form an abstract representation, usually human_unreadable, in intermediate layers.
Location: Section 5.4
Exact Quote: This indicates that Llama 2 tends to form an abstract representation, usually human_unreadable, in intermediate layers (Wendler et al. 2024).

Evidence:
- Evidence Text: The predicted tokens associated with the QR neurons are less human-interpretable, including tokens like textt, archivi, Kontrola, totalit´e or Einzeln.
  Strength: moderate
  Location: Section 5.4
  Limitations: The predicted tokens are only a small sample of the possible tokens that could be generated.
  Exact Quote: Apart from the above tokens, there are certain neurons scattered in top layers still representing option letters, which need further refinement. In summary, since the detected neurons centralize in middle layers, it is hard to interpret their predicted tokens. We may need to explore a better semantic space to study their localized regions.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence suggests that the predicted tokens associated with the QR neurons are less human-interpretable. However, the predicted tokens are only a small sample of the possible tokens that could be generated.
Key Limitations: The predicted tokens are only a small sample of the possible tokens that could be generated.

--------------------------------------------------

Claim 19:
Type: finding
Statement: Domain-specific neurons are mainly centralized in middle layers.
Location: Section 5.4
Exact Quote: Domain-specific neurons are mainly centralized in middle layers, and we found the predicted tokens less human-interpretable, including tokens like textt, archivi, Kontrola, totalit´e or Einzeln.

Evidence:
- Evidence Text: The visualization of the W[D] vectors associated with the QR neurons from the different domains using UMAP.
  Strength: moderate
  Location: Section 5.4
  Limitations: The 2D visualization produced by UMAP might not accurately reflect the true properties of the data manifold.
  Exact Quote: As can be seen from the figure, the distribution of the vectors associated with QR neurons appears to be significantly different from that of vector unembeddings.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence suggests that the domain-specific neurons are mainly centralized in middle layers. However, the 2D visualization might not accurately reflect the true properties of the data manifold.
Key Limitations: The 2D visualization might not accurately reflect the true properties of the data manifold.

--------------------------------------------------

Claim 20:
Type: finding
Statement: QR neurons are capable of representing specific semantic meanings.
Location: Section 5.4
Exact Quote: This finding suggests that intermediate states are capable of representing specific semantic meanings.

Evidence:
- Evidence Text: The vocabulary probabilistic predictions are a linear function of the activations in Transformer’s final layer.
  Strength: weak
  Location: Section 5.4
  Limitations: This is a previous study and does not directly support the claim.
  Exact Quote: According to the previous study, Logit Lens[2], the vocabulary probabilistic predictions are a linear function of the activations in Transformer’s final layer but we can obtain reasonable distributions if we apply the same function to the activations of intermediate layers, i.e., an interpretable nexttoken distribution can be obtained by intermediate states.

Evaluation:
Conclusion Justified: Yes
Robustness: low
Confidence Level: low
Justification: The evidence is weak and does not directly support the claim.
Key Limitations: This is a previous study and does not directly support the claim.

--------------------------------------------------

