Claim 1:
Type: contribution
Statement: The Multimodal Large Language Model (MLLM) benchmark, MME, is the first comprehensive evaluation benchmark for MLLMs.
Location: Abstract
Exact Quote: It measures both perception and cognition abilities on a total of 14 subtasks.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The paper provides a detailed description of the MME benchmark, including its subtasks, evaluation metrics, and data collection process.
Key Limitations: None identified

--------------------------------------------------

Claim 2:
Type: methodology
Statement: MME avoids data leakage by using manually designed instruction-answer pairs and collecting data through real photographs and image generation.
Location: Introduction
Exact Quote: The annotations of instruction-answer pairs are all manually designed. For the few public datasets involved in our study, we only use images without directly relying on their original annotations.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The paper provides a clear explanation of the data collection and annotation process, which helps to ensure the quality and reliability of the data.
Key Limitations: None identified

--------------------------------------------------

Claim 3:
Type: methodology
Statement: MME uses concise instructions to avoid the impact of prompt engineering on model output.
Location: Introduction
Exact Quote: Its instructions are designed concisely to avoid the impact of prompt engineering on the model output.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The paper provides a rationale for using concise instructions, but it does not provide empirical evidence to support this claim.
Key Limitations: Lack of empirical evidence

--------------------------------------------------

Claim 4:
Type: contribution
Statement: MME covers both perception and cognition abilities, including 14 subtasks.
Location: Introduction
Exact Quote: It measures both perception and cognition abilities on a total of 14 subtasks.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The paper provides a detailed description of the 14 subtasks included in MME, which cover a wide range of perception and cognition abilities.
Key Limitations: None identified

--------------------------------------------------

Claim 5:
Type: performance
Statement: MME is used to evaluate 30 advanced MLLMs, including GPT-4V, WeMM, and Lion.
Location: Introduction
Exact Quote: A total of 30 advanced MLLMs are comprehensively evaluated on our MME.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The paper provides a table showing the performance of 30 MLLMs on the MME benchmark.
Key Limitations: None identified

--------------------------------------------------

Claim 6:
Type: methodology
Statement: This research paper analyzes the performance of 30 MLLMs on a set of perception and cognition tasks.
Location: Introduction
Exact Quote: We conduct massive experiments to evaluate the zero-shot performance of 30 advanced MLLMs on the 14 subtasks.

Evidence:
- Evidence Text: The authors evaluated 30 MLLMs on 14 subtasks, including existence, count, position, color, OCR, poster, celebrity, commonsense reasoning, logical reasoning, arithmetic reasoning, relational reasoning, and hypothetical reasoning.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: The evaluated MLLMs include BLIP-2 [23], InstructBLIP [12], MiniGPT-4 [59], PandaGPT [39], Multimodal-GPT [15], VisualGLM-6B [5], ImageBindLLM [17], VPGTrans [53], LaVIN [33], mPLUGOwl [48], Octopus [3], Muffin [51], Otter [22], LRVInstruction [28], Cheetor [24], LLaMAAdapter-v2 [14], GIT2 [41], BLIVA [18], Lynx [52], MMICL [54], GPT4V [37], Skywork-MM [4], mPLUG-Owl2 [48], QwenVL-Chat [9], XComposer-VL [7], LLaVA [29], Lion [2], SPHINX [27], InfMLLM [1], and WeMM [6].

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The authors provide a detailed description of the evaluation methodology and results.
Key Limitations: None

--------------------------------------------------

Claim 7:
Type: contribution
Statement: We propose a new benchmark MME to meet the urgent need of MLLM evaluation.
Location: Introduction
Exact Quote: We propose a new benchmark MME to meet the urgent need of MLLM evaluation.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The authors provide a rationale for the need for a new MME benchmark, but they do not provide a detailed description of the benchmark.
Key Limitations: The authors do not provide a detailed description of the MME benchmark.

--------------------------------------------------

Claim 8:
Type: result
Statement: A total of 30 up-to-date MLLMs are evaluated on our MME.
Location: Introduction
Exact Quote: A total of 30 up-to-date MLLMs are evaluated on our MME.

Evidence:
- Evidence Text: The authors evaluated 30 MLLMs on 14 subtasks, including existence, count, position, color, OCR, poster, celebrity, commonsense reasoning, logical reasoning, arithmetic reasoning, relational reasoning, and hypothetical reasoning.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: The evaluated MLLMs include BLIP-2 [23], InstructBLIP [12], MiniGPT-4 [59], PandaGPT [39], Multimodal-GPT [15], VisualGLM-6B [5], ImageBindLLM [17], VPGTrans [53], LaVIN [33], mPLUGOwl [48], Octopus [3], Muffin [51], Otter [22], LRVInstruction [28], Cheetor [24], LLaMAAdapter-v2 [14], GIT2 [41], BLIVA [18], Lynx [52], MMICL [54], GPT4V [37], Skywork-MM [4], mPLUG-Owl2 [48], QwenVL-Chat [9], XComposer-VL [7], LLaVA [29], Lion [2], SPHINX [27], InfMLLM [1], and WeMM [6].

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The authors provide a detailed description of the evaluation methodology and results.
Key Limitations: None

--------------------------------------------------

Claim 9:
Type: contribution
Statement: We summarize the exposed problems in experiments, proving guidance for the evolution of MLLMs.
Location: Introduction
Exact Quote: We summarize the exposed problems in experiments, proving guidance for the evolution of MLLMs.

Evidence:
- Evidence Text: The authors identified four prominent problems exposed in experiments, including inability to follow basic instructions, a lack of basic perception and reasoning, as well as object hallucination.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: It is expected that these findings are instructive for the subsequent model optimization.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The authors provide a summary of the problems exposed in experiments, but they do not provide a detailed analysis of the problems or their implications for the evolution of MLLMs.
Key Limitations: The authors do not provide a detailed analysis of the problems exposed in experiments or their implications for the evolution of MLLMs.

--------------------------------------------------

Claim 10:
Type: result
Statement: MLLMs perform well in a zero-short setting without complex step-by-step reasoning.
Location: Part 3, paragraph 1
Exact Quote: Therefore, we expect MLLMs to perform well in a zero-short setting.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the fact that MLLMs are able to perform numerical calculations end-to-end, which requires them to be able to read the arithmetic problem in the image and output the answer without using complex step-by-step reasoning.
Key Limitations: The claim is only supported by a single experiment, and it is not clear how well MLLMs would perform in other zero-short settings.

--------------------------------------------------

