Claim 1:
Type: methodology
Statement: We aim to evaluate Large Language Models (LLMs) for embodied decision making.
Location: Abstract
Exact Quote: We aim to evaluate Large Language Models (LLMs) for embodied decision making.

Evidence:
- Evidence Text: We propose a generalized interface (EMBODIED AGENT INTERFACE) that supports the formalization of various types of tasks and input-output specifications of LLM-based modules.
  Strength: strong
  Location: Abstract
  Limitations: None
  Exact Quote: We propose a generalized interface (EMBODIED AGENT INTERFACE) that supports the formalization of various types of tasks and input-output specifications of LLM-based modules.

- Evidence Text: Specifically, it allows us to unify 1) a broad set of embodied decision-making tasks involving both state and temporally extended goals, 2) four commonly-used LLM-based modules for decision making: goal interpretation, subgoal decomposition, action sequencing, and transition modeling, and 3) a collection of fine-grained metrics which break down evaluation into various types of errors, such as hallucination errors, affordance errors, various types of planning errors, etc.
  Strength: strong
  Location: Abstract
  Limitations: None
  Exact Quote: Specifically, it allows us to unify 1) a broad set of embodied decision-making tasks involving both state and temporally extended goals, 2) four commonly-used LLM-based modules for decision making: goal interpretation, subgoal decomposition, action sequencing, and transition modeling, and 3) a collection of fine-grained metrics which break down evaluation into various types of errors, such as hallucination errors, affordance errors, various types of planning errors, etc.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is well-supported by the evidence provided in the paper.
Key Limitations: None

--------------------------------------------------

Claim 2:
Type: methodology
Statement: Existing evaluation methods fall short of providing a comprehensive insight due to three key limitations: the lack of standardization of 1) embodied decision-making tasks, 2) modules that an LLM can interface with or be implemented for, and 3) fine-grained evaluation metrics beyond a single success rate.
Location: Introduction
Exact Quote: Existing evaluation methods fall short of providing a comprehensive insight due to three key limitations: the lack of standardization of 1) embodied decision-making tasks, 2) modules that an LLM can interface with or be implemented for, and 3) fine-grained evaluation metrics beyond a single success rate.

Evidence:
- Evidence Text: In this paper, we propose EMBODIED AGENT INTERFACE, to address these challenges.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: In this paper, we propose EMBODIED AGENT INTERFACE, to address these challenges.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is well-supported by the evidence provided in the paper.
Key Limitations: None

--------------------------------------------------

Claim 3:
Type: contribution
Statement: Our benchmark offers a comprehensive assessment of LLMs’ performance for different subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI systems and providing insights for effective and selective use of LLMs in embodied decision making.
Location: Abstract
Exact Quote: Overall, our benchmark offers a comprehensive assessment of LLMs’ performance for different subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI systems and providing insights for effective and selective use of LLMs in embodied decision making.

Evidence:
- Evidence Text: We propose a generalized interface (EMBODIED AGENT INTERFACE) that supports the formalization of various types of tasks and input-output specifications of LLM-based modules.
  Strength: strong
  Location: Abstract
  Limitations: None
  Exact Quote: We propose a generalized interface (EMBODIED AGENT INTERFACE) that supports the formalization of various types of tasks and input-output specifications of LLM-based modules.

- Evidence Text: Specifically, it allows us to unify 1) a broad set of embodied decision-making tasks involving both state and temporally extended goals, 2) four commonly-used LLM-based modules for decision making: goal interpretation, subgoal decomposition, action sequencing, and transition modeling, and 3) a collection of fine-grained metrics which break down evaluation into various types of errors, such as hallucination errors, affordance errors, various types of planning errors, etc.
  Strength: strong
  Location: Abstract
  Limitations: None
  Exact Quote: Specifically, it allows us to unify 1) a broad set of embodied decision-making tasks involving both state and temporally extended goals, 2) four commonly-used LLM-based modules for decision making: goal interpretation, subgoal decomposition, action sequencing, and transition modeling, and 3) a collection of fine-grained metrics which break down evaluation into various types of errors, such as hallucination errors, affordance errors, various types of planning errors, etc.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is well-supported by the evidence provided in the paper.
Key Limitations: None

--------------------------------------------------

Claim 4:
Type: methodology
Statement: In EMBODIED AGENT INTERFACE, a state is represented as a tuple s = _,_, where is the universe of objects, assumed to be a fixed finite set. is a set of relational Boolean features. Each\nf ∈F is a table where each entry is associated with a tuple of objects (o1, · · ·, ok). Each entry has the value of the feature in the state, and k is the arity of the feature.
Location: 2.1 Representation for Objects, States and Actions
Exact Quote: In EMBODIED AGENT INTERFACE, a state is represented as a tuple s = ⟨U, F⟩, where U is the universe of objects, assumed to be a fixed finite set. F is a set of relational Boolean features. Each f ∈ F is a table where each entry is associated with a tuple of objects (o1, · · ·, ok). Each entry has the value of the feature in the state, and k is the arity of the feature.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the following exact quote from the paper.
Key Limitations: None

--------------------------------------------------

Claim 5:
Type: methodology
Statement: In EMBODIED AGENT INTERFACE, goals g, subgoals ϕ, and action sequences ¯a are modeled as linear temporal logic (LTL) formulas.
Location: 2.2 Representation for Goals, Subgoals, Action Sequences, and State-Action Trajectories
Exact Quote: In EMBODIED AGENT INTERFACE, goals g, subgoals ϕ, and action sequences ¯a are modeled as linear temporal logic (LTL) formulas.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the following exact quote from the paper.
Key Limitations: None

--------------------------------------------------

Claim 6:
Type: methodology
Statement: The goal interpretation module takes the state s0 and a natural language instruction lg as input, and generates an LTL goal ˆg, as a formal goal specification which a symbolic planner can conceivably take as input.
Location: 2.3 Ability Module 1: Goal Interpretation G : ⟨s0, lg⟩→ g
Exact Quote: The goal interpretation module takes the state s0 and a natural language instruction lg as input, and generates an LTL goal ˆg, as a formal goal specification which a symbolic planner can conceivably take as input.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the following exact quote from the paper.
Key Limitations: None

--------------------------------------------------

Claim 7:
Type: methodology
Statement: The subgoal decomposition module takes the task ⟨s0, g⟩ as input and generates a sequence of subgoals _ϕ[¯] = {ϕi}i[k]=1[, where each][ ϕ][i][ is an LTL formula.
Location: 2.4 Ability Module 2: Subgoal Decomposition Φ : ⟨s0, g⟩→ ϕ[¯]
Exact Quote: The subgoal decomposition module takes the task ⟨s0, g⟩ as input and generates a sequence of subgoals _ϕ[¯] = {ϕi}i[k]=1[, where each][ ϕ][i][ is an LTL formula.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the following exact quote from the paper.
Key Limitations: None

--------------------------------------------------

Claim 8:
Type: methodology
Statement: The action sequencing module takes the task ⟨s0, g⟩ as input, and the transition model M, and generates an action sequence ¯a = {ai}i[n]=1[.]
Location: 2.5 Ability Module 3: Action Sequencing Q : ⟨s0, g⟩, M → a¯
Exact Quote: The action sequencing module takes the task ⟨s0, g⟩ as input, and the transition model M, and generates an action sequence ¯a = {ai}i[n]=1[.]

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the following exact quote from the paper.
Key Limitations: None

--------------------------------------------------

Claim 9:
Type: methodology
Statement: The transition modeling module takes the task ⟨s0, g⟩ and a set of operator definitions {oi} as input, and generates a PDDL operator definition [11] for each oi.
Location: 2.6 Ability Module 4: Transition Modeling T : ⟨s0, g⟩, o →⟨pre, eff⟩
Exact Quote: The transition modeling module takes the task ⟨s0, g⟩ and a set of operator definitions {oi} as input, and generates a PDDL operator definition [11] for each oi.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the following exact quote from the paper.
Key Limitations: None

--------------------------------------------------

Claim 10:
Type: methodology
Statement: The transition modeling module can be evaluated in two ways. First, the logic matching score for an operator oi compares the generated preconditions and effects against the ground truth operator definition annotated by human experts.
Location: Evaluation Metric
Exact Quote: The transition modeling module can be evaluated in two ways. First, the logic matching score for an operator oi compares the generated prei and effi against the ground truth operator definition annotated by human experts.

Evidence:
- Evidence Text: This comparison uses a surface form matching score to produce an F1-based score between two logic formulas.
  Strength: strong
  Location: Evaluation Metric
  Limitations: None
  Exact Quote: This comparison uses a surface form matching score to produce an F-1 based score between two logic formulas.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim that the transition modeling module can be evaluated by comparing the generated preconditions and effects against the ground truth operator definition annotated by human experts.
Key Limitations: None

--------------------------------------------------

Claim 11:
Type: methodology
Statement: Furthermore, the planning success rate assesses whether the preconditions and effects of different operators enable a viable plan. This is computed by running an external PDDL planner [12] based on generated operator definitions to achieve g from the initial state s0.
Location: Evaluation Metric
Exact Quote: Furthermore, the planning success rate assesses whether the preconditions and effects of different operators enable a viable plan. This is computed by running an external PDDL planner [12] based on generated operator definitions to achieve g from the initial state s0.

Evidence:
- Evidence Text: For simplicity, we only state goals in g (and ignore action subgoals).
  Strength: strong
  Location: Evaluation Metric
  Limitations: None
  Exact Quote: For simplicity, we only state goals in g (and ignore action subgoals).

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim that the planning success rate is assessed by running an external PDDL planner based on generated operator definitions to achieve g from the initial state s0.
Key Limitations: None

--------------------------------------------------

Claim 12:
Type: result
Statement: LLMs generally have difficulties distinguishing intermediate subgoals and final goals.
Location: 4.1 Ability Module Analysis
Exact Quote: LLMs generally have difficulties distinguishing intermediate subgoals and final goals.

Evidence:
- Evidence Text: For example, in the VirtualHome task Drink, GPT-4o predicts some intermediate states as part of the final goal (e.g., open(freezer) and inside(water, glass)).
  Strength: moderate
  Location: 4.1 Ability Module Analysis
  Limitations: The example given is only for GPT-4o and may not represent the behavior of other LLMs.
  Exact Quote: For example, in the VirtualHome task Drink, GPT-4o predicts some intermediate states as part of the final goal (e.g., open(freezer) and inside(water, glass)).

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by a specific example, but the evidence is limited to one LLM and one task.
Key Limitations: The claim may not generalize to all LLMs or all tasks.

--------------------------------------------------

Claim 13:
Type: result
Statement: LLMs tend to translate NL goals word-by-word into their symbolic correspondence, rather than grounding them in the environment state.
Location: 4.1 Ability Module Analysis
Exact Quote: Overall, we observe that LLMs tend to translate NL goals word-by-word into their symbolic correspondence, rather than grounding them in the environment state.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: low
Confidence Level: low
Justification: The claim is based on a general observation of the behavior of LLMs, but no specific examples or data are provided to support it.
Key Limitations: The claim is not supported by concrete evidence.

--------------------------------------------------

Claim 14:
Type: result
Statement: Most errors in subgoal decomposition and action sequencing are runtime errors (rather than syntax errors).
Location: 4.1 Ability Module Analysis
Exact Quote: Most errors are runtime errors (rather than syntax errors).

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: low
Confidence Level: low
Justification: The claim is based on a general observation of the behavior of LLMs, but no specific examples or data are provided to support it.
Key Limitations: The claim is not supported by concrete evidence.

--------------------------------------------------

Claim 15:
Type: result
Statement: LLMs are more likely to make missing-step and additional-step errors than wrong-order or affordance errors.
Location: 4.1 Ability Module Analysis
Exact Quote: Overall, LLMs are more likely to make missing-step and additional-step errors than wrong-order or affordance errors.

Evidence:
- Evidence Text: Missing-step errors occur when a precondition is not satisfied before the execution of an action (e.g., fetching an object without opening the box containing it).
  Strength: moderate
  Location: 4.1 Ability Module Analysis
  Limitations: The example given is only for missing-step errors.
  Exact Quote: Missing-step errors occur when a precondition is not satisfied before the execution of an action (e.g., fetching an object without opening the box containing it).

- Evidence Text: Additional steps form the most frequent errors, even for the most powerful models—it occurs when a goal has already been achieved but the model still predicts to execute an additional action to achieve it (e.g., opening a box twice).
  Strength: moderate
  Location: 4.1 Ability Module Analysis
  Limitations: The example given is only for additional-step errors.
  Exact Quote: Additional steps form the most frequent errors, even for the most powerful models—it occurs when a goal has already been achieved but the model still predicts to execute an additional action to achieve it (e.g., opening a box twice).

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by specific examples and a general observation of the behavior of LLMs.
Key Limitations: The claim may not generalize to all tasks or all LLMs.

--------------------------------------------------

Claim 16:
Type: result
Statement: Object goals are generally easier to achieve than relational goals.
Location: 4.1 Ability Module Analysis
Exact Quote: Shown in Table 5, object goals (such as toggled_on) are generally easier to achieve than relational goals (such as _ontop(agent, chair)).

Evidence:
- Evidence Text: More analysis is provided in Appendix E.2.
  Strength: weak
  Location: 4.1 Ability Module Analysis
  Limitations: The evidence is only a reference to another section of the paper, which was not provided.
  Exact Quote: More analysis is provided in Appendix E.2.

Evaluation:
Conclusion Justified: Yes
Robustness: low
Confidence Level: low
Justification: The claim is supported by a general observation of the behavior of LLMs, but no specific examples or data are provided to support it.
Key Limitations: The claim is not supported by concrete evidence.

--------------------------------------------------

