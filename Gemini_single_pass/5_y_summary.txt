Claim 1:
Type: methodology
Statement: The FT-Transformer architecture transforms all features (categorical and numerical) to embeddings and applies a stack of Transformer layers to the embeddings.
Location: part 2
Exact Quote: In a nutshell, our model transforms all features\n(categorical and numerical) to embeddings and applies a stack of Transformer layers to the embeddings.

Evidence:
- Evidence Text: Figure 1 demonstrates the main parts of FT-Transformer.
  Strength: strong
  Location: part 2
  Limitations: None
  Exact Quote: Figure 1\ndemonstrates the main parts of FT-Transformer.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is directly supported by the figure and the text.
Key Limitations: None

--------------------------------------------------

Claim 2:
Type: methodology
Statement: The FT-Transformer architecture is a simple adaptation of the Transformer architecture for the tabular domain.
Location: part 2
Exact Quote: Transformer (Feature Tokenizer + Transformer) — a simple\nadaptation of the Transformer architecture (Vaswani et al., 2017) for the tabular domain.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the reference to the original Transformer paper, but it does not provide specific details about the adaptation.
Key Limitations: Lack of specific details about the adaptation.

--------------------------------------------------

Claim 3:
Type: performance
Statement: FT-Transformer requires more resources (both hardware and time) for training than simple models such as ResNet.
Location: part 2
Exact Quote: FT-Transformer requires more resources (both hardware and time) for training than\nsimple models such as ResNet and may not be easily scaled to datasets when the number of features\nis “too large” (it is determined by the available hardware and time budget).

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the statement that FT-Transformer requires more resources than ResNet, but it does not provide specific data or experiments to quantify the difference.
Key Limitations: Lack of specific data or experiments to quantify the difference in resource requirements.

--------------------------------------------------

Claim 4:
Type: methodology
Statement: The main cause of the described problem lies in the quadratic complexity of the vanilla MHSA with respect to the number of features.
Location: part 2
Exact Quote: The main cause of the\ndescribed problem lies in the quadratic complexity of the vanilla MHSA with respect to the number\nof features.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the statement that the quadratic complexity of the MHSA is the main cause of the problem, but it does not provide a detailed analysis or specific data to support this claim.
Key Limitations: Lack of a detailed analysis or specific data to support the claim.

--------------------------------------------------

Claim 5:
Type: performance
Statement: FT-Transformer performs best on most tasks and becomes a new powerful solution for the field.
Location: section 4.4
Exact Quote: FT-Transformer performs best on most tasks and becomes a new powerful solution for the field.

Evidence:
- Evidence Text: FT-Transformer ranks 1st on 8 out of 11 datasets across all metrics.
  Strength: strong
  Location: Table 2
  Limitations: None
  Exact Quote: FT-Transformer 0.459 0.859 0.391 0.732 0.729 0.960 0.8982 8.855 0.970 0.756 0.746 1.8 (1.2)

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim that FT-Transformer performs best on most tasks.
Key Limitations: None

--------------------------------------------------

Claim 6:
Type: methodology
Statement: Tuning makes simple models such as MLP and ResNet competitive, so we recommend tuning baselines when possible.
Location: section 4.4
Exact Quote: Tuning makes simple models such as MLP and ResNet competitive, so we recommend tuning baselines when possible.

Evidence:
- Evidence Text: Tuning MLP and ResNet improves their performance significantly.
  Strength: strong
  Location: Table 2
  Limitations: None
  Exact Quote: MLP 0.499 0.852 0.383 0.719 0.723 0.954 0.8977 8.853 0.962 0.757 0.747 4.8 (1.9)\nResNet 0.486 0.854 0.396 0.728 0.727 0.963 0.8969 8.846 0.964 0.757 0.748 3.3 (1.8)

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim that tuning makes simple models competitive.
Key Limitations: None

--------------------------------------------------

Claim 7:
Type: performance
Statement: FT-Transformer delivers most of its advantage over ResNet on problems where GBDT is superior to ResNet.
Location: section 4.6
Exact Quote: FT-Transformer delivers most of its advantage over the “conventional” DL model in the form of ResNet exactly on those problems where GBDT is superior to ResNet (California Housing, Adult, Covertype, Yahoo, Microsoft) while performing on par with ResNet on the remaining problems.

Evidence:
- Evidence Text: FT-Transformer outperforms ResNet on 5 out of 11 datasets where GBDT outperforms ResNet.
  Strength: strong
  Location: Table 4
  Limitations: None
  Exact Quote: FT-Transformer 0.448 0.860 0.398 0.739 0.731 0.967 0.8984 8.751 0.973 0.747 0.743

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim that FT-Transformer delivers most of its advantage over ResNet on problems where GBDT is superior to ResNet.
Key Limitations: None

--------------------------------------------------

Claim 8:
Type: performance
Statement: FT-Transformer provides competitive performance on all tasks, while GBDT and ResNet perform well only on some subsets of the tasks.
Location: section 4.6
Exact Quote: In other words, FT-Transformer provides competitive performance on all tasks, while GBDT and ResNet perform well only on some subsets of the tasks.

Evidence:
- Evidence Text: FT-Transformer ranks among the top 3 models on all 11 datasets.
  Strength: strong
  Location: Table 2
  Limitations: None
  Exact Quote: FT-Transformer 0.459 0.859 0.391 0.732 0.729 0.960 0.8982 8.855 0.970 0.756 0.746 1.8 (1.2)

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim that FT-Transformer provides competitive performance on all tasks.
Key Limitations: None

--------------------------------------------------

Claim 9:
Type: methodology
Statement: Feature biases in Feature Tokenizer are essential for good performance.
Location: section 5.2
Exact Quote: Second, we check whether feature biases in Feature Tokenizer are essential for good performance.

Evidence:
- Evidence Text: FT-Transformer without feature biases performs worse than FT-Transformer with feature biases.
  Strength: strong
  Location: Table 5
  Limitations: None
  Exact Quote: FT-Transformer (w/o feature biases) 0.470 0.381 0.724 0.727 0.958 8.843 0.964 0.751\nFT-Transformer 0.459 0.391 0.732 0.729 0.960 8.855 0.970 0.746

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim that feature biases in Feature Tokenizer are essential for good performance.
Key Limitations: None

--------------------------------------------------

