Claim 1:
Type: contribution
Statement: PROMETHEUS induces fine-grained evaluation capability in language models.
Location: abstract
Exact Quote: PROMETHEUS: INDUCING FINE-GRAINED EVALUATION CAPABILITY IN LANGUAGE MODELS

Evidence:
- Evidence Text: PROMETHEUS obtained a Pearson correlation of 0.897 with human evaluators on 45 customized score rubrics sampled across three test sets.
  Strength: strong
  Location: paragraph 2
  Limitations: might not generalize to other types of tasks or domains
  Exact Quote: On 45 customized score rubrics sampled across three test sets (MT Bench, Vicuna Bench, Feedback Bench), PROMETHEUS obtains a Pearson correlation of 0.897 with human evaluators, which is similar with GPT-4 (0.882), and has a significant gap with GPT-3.5-Turbo (0.392).

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence demonstrates that PROMETHEUS can achieve human-level performance on fine-grained evaluation tasks.
Key Limitations: none identified in the provided text

--------------------------------------------------

Claim 2:
Type: methodology
Statement: PROMETHEUS is open-source, reproducible, and inexpensive.
Location: abstract
Exact Quote: PROMETHEUS is open-source, reproducible, and inexpensive.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: low
Confidence Level: low
Justification: The provided text does not provide any evidence to support this claim.
Key Limitations: no evidence provided

--------------------------------------------------

Claim 3:
Type: methodology
Statement: PROMETHEUS was trained on the FEEDBACK COLLECTION dataset.
Location: paragraph 2
Exact Quote: We use the FEEDBACK COLLECTION to fine-tune Llama-2-Chat-13B in creating PROMETHEUS.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The provided text explicitly states that PROMETHEUS was trained on the FEEDBACK COLLECTION dataset.
Key Limitations: none identified in the provided text

--------------------------------------------------

Claim 4:
Type: performance
Statement: PROMETHEUS outperforms GPT-3.5-Turbo on customized score rubrics.
Location: paragraph 2
Exact Quote: PROMETHEUS showed higher correlation compared to GPT-3.5-Turbo and Llama-2-Chat 70B.

Evidence:
- Evidence Text: PROMETHEUS obtained a Pearson correlation of 0.897 with human evaluators, while GPT-3.5-Turbo obtained a Pearson correlation of 0.392.
  Strength: strong
  Location: paragraph 2
  Limitations: might not generalize to other types of tasks or domains
  Exact Quote: On 45 customized score rubrics sampled across three test sets (MT Bench, Vicuna Bench, Feedback Bench), PROMETHEUS obtains a Pearson correlation of 0.897 with human evaluators, which is similar with GPT-4 (0.882), and has a significant gap with GPT-3.5-Turbo (0.392).

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence demonstrates that PROMETHEUS significantly outperforms GPT-3.5-Turbo on customized score rubrics.
Key Limitations: none identified in the provided text

--------------------------------------------------

Claim 5:
Type: performance
Statement: PROMETHEUS is suitable for evaluating human preference.
Location: paragraph 2
Exact Quote: PROMETHEUS outperforms two state-of-the-art reward models and GPT-3.5-Turbo on human preference datasets.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The provided text states that PROMETHEUS outperforms state-of-the-art reward models on human preference datasets, suggesting that it is suitable for evaluating human preference.
Key Limitations: none identified in the provided text

--------------------------------------------------

Claim 6:
Type: methodology
Statement: The FEEDBACK COLLECTION dataset is constructed with the following considerations: (1) including as many reference materials as possible, (2) maintaining a uniform length among the reference answers for each score (1 to 5) to prevent undesired length bias, (3) maintaining a uniform score distribution to prevent undesired decision bias, and (4) limiting the scope of our instructions and responses to realistic situations where a user is interacting with a LLM.
Location: Paper text
Exact Quote: This is part 2 of 6.\n\nPaper text: n as a fine-grained evaluator. We thus\nintroduce the FEEDBACK COLLECTION, a new dataset for the sole purpose of fine-tuning an opensourced evaluator LLM. Our 4 main considerations during dataset construction are: (1) including as\nmany reference materials (i.e. reference answer, and scoring rubric) as possible, (2) maintaining a\nuniform length among the reference answers for each score (1 to 5) to prevent undesired length bias,\n(3) maintaining a uniform score distribution to prevent undesired decision bias, and (4) limiting the\nscope of our instructions and responses to realistic situations where a user is interacting with a LLM.

Evidence:
- Evidence Text: We thus introduce the FEEDBACK COLLECTION, a new dataset for the sole purpose of fine-tuning an opensourced evaluator LLM.
  Strength: strong
  Location: Paper text
  Limitations: None
  Exact Quote: This is part 2 of 6.\n\nPaper text: n as a fine-grained evaluator. We thus\nintroduce the FEEDBACK COLLECTION, a new dataset for the sole purpose of fine-tuning an opensourced evaluator LLM.

- Evidence Text: Our 4 main considerations during dataset construction are: (1) including as\nmany reference materials (i.e. reference answer, and scoring rubric) as possible, (2) maintaining a\nuniform length among the reference answers for each score (1 to 5) to prevent undesired length bias,\n(3) maintaining a uniform score distribution to prevent undesired decision bias, and (4) limiting the\nscope of our instructions and responses to realistic situations where a user is interacting with a LLM.
  Strength: strong
  Location: Paper text
  Limitations: None
  Exact Quote: Our 4 main considerations during dataset construction are: (1) including as\nmany reference materials (i.e. reference answer, and scoring rubric) as possible, (2) maintaining a\nuniform length among the reference answers for each score (1 to 5) to prevent undesired length bias,\n(3) maintaining a uniform score distribution to prevent undesired decision bias, and (4) limiting the\nscope of our instructions and responses to realistic situations where a user is interacting with a LLM.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by strong evidence from the paper text, which explicitly states the four considerations made during the construction of the FEEDBACK COLLECTION dataset.
Key Limitations: None

--------------------------------------------------

Claim 7:
Type: performance
Statement: PROMETHEUS shows a high correlation with human evaluators.
Location: Figure 3
Exact Quote: Figure 3: The Pearson correlation between scores from human annotators and the score from GPT3.5-Turbo, Prometheus, and GPT-4 on 45 customized score rubrics from the Feedback Bench, Vicuna Bench, and MT Bench. PROMETHEUS shows a high correlation with human evaluators.

Evidence:
- Evidence Text: PROMETHEUS shows a high correlation with human evaluators.
  Strength: strong
  Location: Figure 3
  Limitations: None
  Exact Quote: Figure 3: The Pearson correlation between scores from human annotators and the score from GPT3.5-Turbo, Prometheus, and GPT-4 on 45 customized score rubrics from the Feedback Bench, Vicuna Bench, and MT Bench. PROMETHEUS shows a high correlation with human evaluators.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by strong evidence from Figure 3, which shows that PROMETHEUS obtains a 0.897 Pearson correlation with human evaluators, which is higher than GPT-4 (0.882) and GPT-3.5-Turbo (0.392).
Key Limitations: None

--------------------------------------------------

Claim 8:
Type: performance
Statement: PROMETHEUS shows a win-rate of 58.62% over GPT-4 and 79.57% over GPT-3.5-Turbo in a pairwise comparison of feedback quality.
Location: Figure 4
Exact Quote: Figure 4: Pairwise comparison of the quality of the feedback generated by GPT-4, PROMETHEUS\nand GPT-3.5-Turbo. Annotators are asked to choose which feedback is better at assessing the given\nresponse. PROMETHEUS shows a win-rate of 58.62% over GPT-4 and 79.57% over GPT-3.5-Turbo.

Evidence:
- Evidence Text: PROMETHEUS shows a win-rate of 58.62% over GPT-4 and 79.57% over GPT-3.5-Turbo in a pairwise comparison of feedback quality.
  Strength: strong
  Location: Figure 4
  Limitations: None
  Exact Quote: Figure 4: Pairwise comparison of the quality of the feedback generated by GPT-4, PROMETHEUS\nand GPT-3.5-Turbo. Annotators are asked to choose which feedback is better at assessing the given\nresponse. PROMETHEUS shows a win-rate of 58.62% over GPT-4 and 79.57% over GPT-3.5-Turbo.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by strong evidence from Figure 4, which shows that PROMETHEUS is preferred over GPT-4 58.62% of the times, and over GPT-3.5-Turbo 79.57% of the times.
Key Limitations: None

--------------------------------------------------

Claim 9:
Type: performance
Statement: PROMETHEUS-13B outperforms LLAMA2-CHAT 13B, GPT-3.5-TURBO-0613, and different versions of GPT-4 on Pearson correlation on seen and unseen rubric sets.
Location: Paper text
Exact Quote: In Table 2, the performance of LLAMA-2-CHAT 13B degrades over the 7B model and slightly\nimproves when scaled up to 70B size, indicating that naively increasing the size of a model does\nnot necessarily improve an LLM’s evaluation capabilities. On the other hand, PROMETHEUS 13B\nshows a +0.420 and +0.397 improvement over its base model LLAMA2-CHAT 13B in terms of\nPearson correlation on the seen and unseen rubric set, respectively. Moreover, it even outperforms\nLLAMA2-CHAT 70B, GPT-3.5-TURBO-0613, and different versions of GPT-4.

Evidence:
- Evidence Text: PROMETHEUS-13B shows a +0.420 and +0.397 improvement over its base model LLAMA2-CHAT 13B in terms of\nPearson correlation on the seen and unseen rubric set, respectively.
  Strength: strong
  Location: Paper text
  Limitations: The results are based on a single dataset and may not generalize to other datasets or tasks.
  Exact Quote: In Table 2, the performance of LLAMA-2-CHAT 13B degrades over the 7B model and slightly\nimproves when scaled up to 70B size, indicating that naively increasing the size of a model does\nnot necessarily improve an LLM’s evaluation capabilities. On the other hand, PROMETHEUS 13B\nshows a +0.420 and +0.397 improvement over its base model LLAMA2-CHAT 13B in terms of\nPearson correlation on the seen and unseen rubric set, respectively. Moreover, it even outperforms\nLLAMA2-CHAT 70B, GPT-3.5-TURBO-0613, and different versions of GPT-4.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim that PROMETHEUS-13B outperforms LLAMA2-CHAT 13B, GPT-3.5-TURBO-0613, and different versions of GPT-4 on Pearson correlation on seen and unseen rubric sets.
Key Limitations: The results may not generalize to other datasets or tasks.

--------------------------------------------------

Claim 10:
Type: performance
Statement: When comparing Pearson correlation with GPT-4, PROMETHEUS shows the highest correlation even outperforming GPT-3.5-Turbo.
Location: Paper text
Exact Quote: When comparing Pearson correlation with GPT-4, PROMETHEUS shows the highest correlation even outperforming GPT-3.5-Turbo.

Evidence:
- Evidence Text: When comparing Pearson correlation with GPT-4, PROMETHEUS shows the highest correlation even outperforming GPT-3.5-Turbo.
  Strength: strong
  Location: Paper text
  Limitations: The results are based on a single dataset and may not generalize to other datasets or tasks.
  Exact Quote: When comparing Pearson correlation with GPT-4, PROMETHEUS shows the highest correlation even outperforming GPT-3.5-Turbo.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence strongly supports the claim that PROMETHEUS shows the highest Pearson correlation with GPT-4, even outperforming GPT-3.5-Turbo.
Key Limitations: The results may not generalize to other datasets or tasks.

--------------------------------------------------

Claim 11:
Type: performance
Statement: PROMETHEUS shows superior performance on human preference datasets, indicating its possibility as an universal reward model.
Location: Paper text
Exact Quote: Lastly, we show that PROMETHEUS shows superior performance on human preference datasets, indicating its possibility as an universal reward model.

Evidence:
- Evidence Text: PROMETHEUS shows superior performance on human preference datasets, indicating its possibility as an universal reward model.
  Strength: strong
  Location: Paper text
  Limitations: The results are based on a single dataset and may not generalize to other datasets or tasks.
  Exact Quote: Lastly, we show that PROMETHEUS shows superior performance on human preference datasets, indicating its possibility as an universal reward model.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence strongly supports the claim that PROMETHEUS shows superior performance on human preference datasets.
Key Limitations: The results may not generalize to other datasets or tasks.

--------------------------------------------------

Claim 12:
Type: methodology
Statement: PROMETHEUS was fine-tuned to minimize length bias during evaluation.
Location: Part 6/Paragraph 3
Exact Quote: In order to minimize this effect during fine-tuning PROMETHEUS, one of our main consideration was to maintain a length distribution equal among the score range of 1 to 5.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the statement that length distribution was considered during fine-tuning, but no specific data or metrics are provided to demonstrate the effectiveness of this approach.
Key Limitations: Lack of specific evidence or metrics to support the claim.

--------------------------------------------------

Claim 13:
Type: result
Statement: The FEEDBACK COLLECTION consists of unseen score rubrics against the FEEDBACK BENCH.
Location: Part 6/Paragraph 5
Exact Quote: One of the main considerations of our experiments in Section 5.2 using the FEEDBACK BENCH was testing whether PROMETHEUS could generalize to unseen customized score rubrics.

Evidence:
- Evidence Text: Plot of rouge-L distribution between a random score rubric within the FEEDBACK COLLECTION and a random score rubric within the FEEDBACK BENCH shows low overlap.
  Strength: strong
  Location: Part 6/Paragraph 5
  Limitations: The method used to construct the unseen score rubric subset is not described.
  Exact Quote: As shown in Figure 10, there is a low overlap among the train and test sets, confirming that the FEEDBACK BENCH is valid to be claimed as an unseen test set to measure the evaluation capability of evaluator LMs.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is strongly supported by the provided experimental results and analysis, indicating that the FEEDBACK BENCH indeed contains unseen score rubrics.
Key Limitations: Lack of information on the method used to select the unseen score rubrics.

--------------------------------------------------

Claim 14:
Type: result
Statement: The FEEDBACK COLLECTION includes a variety of expressions, indicating moderate diversity.
Location: Part 6/Paragraph 4
Exact Quote: Our findings suggest a moderate level of diversity. While there is some term repetition, the dataset also showcases a notable range of expressions.

Evidence:
- Evidence Text: Bigram and trigram ratios indicate variety in term usage.
  Strength: moderate
  Location: Part 6/Paragraph 4
  Limitations: The specific metrics or methods used to calculate diversity are not mentioned.
  Exact Quote: The results are shown in Table 7, indicating a variety in how terms are expressed, and our findings suggest a moderate level of diversity.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the analysis of bigram and trigram ratios, although the specific metrics and methods used are not provided.
Key Limitations: Lack of detailed information on the diversity analysis methods.

--------------------------------------------------

