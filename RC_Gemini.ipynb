{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/student-lab-1/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting claims...\n",
      "Processing ICLR_1.pdf...\n",
      "[                                        ] (0/1===[====                                    ] ( 1/10)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/student-lab-1/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\b===[========                                ] ( 2/10===[============                            ] ( 3/10===[================                        ] ( 4/10===[====================                    ] ( 5/10===[========================                ] ( 6/10===[============================            ] ( 7/10===[================================        ] ( 8/10===[====================================    ] ( 9/10===[========================================] (10/10]\n",
      "Analyzing evidence...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 368\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError analyzing paper: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 368\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 337\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;66;03m# Step 2: Analyze evidence\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzing evidence...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 337\u001b[0m evidence_results \u001b[38;5;241m=\u001b[39m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze_evidence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclaims\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m# Step 3: Analyze conclusions\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnalyzing conclusions...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 130\u001b[0m, in \u001b[0;36mPaperAnalyzer.analyze_evidence\u001b[0;34m(self, filename, claims)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m claim \u001b[38;5;129;01min\u001b[39;00m claims[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclaims\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     95\u001b[0m     evidence_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;124m    Paper text: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;124m    \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 130\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m45\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate_content(evidence_prompt)\n\u001b[1;32m    133\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_json_response(response\u001b[38;5;241m.\u001b[39mtext)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "import datetime\n",
    "import pdfplumber\n",
    "from typing import Dict, List, Any\n",
    "import pymupdf4llm\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "class PaperAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel('gemini-pro')\n",
    "        self.paper_text = None\n",
    "        self.execution_times = {\n",
    "        \"claims_analysis\": 0,\n",
    "        \"evidence_analysis\": 0,\n",
    "        \"conclusions_analysis\": 0,\n",
    "        \"total_time\": 0,\n",
    "        \"total_sleep_time\": 0,  # Add this\n",
    "        \"actual_processing_time\": 0  # Add this\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    def extract_text_from_pdf(self, filename: str) -> str:\n",
    "        \"\"\"Extract text from PDF file using PyMuPDF\"\"\"\n",
    "        try:\n",
    "            # Convert PDF to markdown format\n",
    "            self.paper_text = pymupdf4llm.to_markdown(filename)\n",
    "            return self.paper_text\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from PDF: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "\n",
    "        \n",
    "    def get_claims(self, filename: str) -> Dict:\n",
    "        \"\"\"Extract all claims from the paper\"\"\"\n",
    "        # text = self.extract_text_from_pdf(filename)\n",
    "\n",
    "\n",
    "        if not self.paper_text:\n",
    "            text = self.extract_text_from_pdf(filename)\n",
    "        else:\n",
    "            text = self.paper_text\n",
    "    \n",
    "        start_time = time.time()\n",
    "\n",
    "        if not self.paper_text:\n",
    "            raise Exception(\"Failed to extract text from PDF\")\n",
    "\n",
    "        \n",
    "        claims_prompt = f\"\"\"\n",
    "        Analyze this research paper and extract ALL possible claims made by the authors.\n",
    "        Paper text: {text}\n",
    "        \n",
    "        Your task is to identify all statements in the text that meet the following criteria for a claim:\n",
    "        1. Makes a specific, testable assertion about results, methods, or contributions\n",
    "        2. Represents a novel finding, improvement, or advancement\n",
    "        3. Presents a clear position or conclusion\n",
    "\n",
    "        Make sure to:\n",
    "        1. Include both major and minor claims\n",
    "        2. Don't miss any claims\n",
    "        3. Present each claim as a separate item\n",
    "        \n",
    "        Return ONLY the following JSON structure:\n",
    "        {{\n",
    "            \"claims\": [\n",
    "                {{\n",
    "                    \"claim_id\": 1,\n",
    "                    \"claim_text\": \"statement of the claim\",\n",
    "                    \"location\": \"section/paragraph where this claim appears\",\n",
    "                    \"claim_type\": \"Nature of the claim\",\n",
    "                    \"exact_quote\": \"complete verbatim text containing the claim\"\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        time.sleep(45)\n",
    "        self.execution_times[\"total_sleep_time\"] += 45\n",
    "        response = self.model.generate_content(claims_prompt)\n",
    "        # self.execution_times[\"claims_analysis\"] = time.time() - start_time\n",
    "\n",
    "        self.execution_times[\"claims_analysis\"] = time.time() - start_time\n",
    "\n",
    "        return self._parse_json_response(response.text)\n",
    "\n",
    "    def analyze_evidence(self, filename: str, claims: Dict) -> List[Dict]:\n",
    "        \"\"\"Find evidence for each claim\"\"\"\n",
    "        # text = extract_text_from_pdf(filename)\n",
    "\n",
    "        \n",
    "\n",
    "        if not self.paper_text:\n",
    "            text = self.extract_text_from_pdf(filename)\n",
    "        else:\n",
    "            text = self.paper_text\n",
    "\n",
    "        start_time = time.time()\n",
    "        evidence_results = []\n",
    "        \n",
    "        for claim in claims['claims']:\n",
    "            evidence_prompt = f\"\"\"\n",
    "            Paper text: {text}\n",
    "            \n",
    "            For the following claim from the paper:\n",
    "            \"{claim['claim_text']}\"\n",
    "            \n",
    "            Please identify relevant evidence that:\n",
    "            1. Directly supports or contradicts the claim's specific assertion\n",
    "            2. Is presented with experimental results, data, or concrete examples\n",
    "            3. Can be traced to specific methods, results, or discussion sections\n",
    "            4. Is not from the abstract or introduction\n",
    "\n",
    "            If NO evidence is found for the given Claim, return:\n",
    "            {{\n",
    "                \"claim_id\": {claim['claim_id']},\n",
    "                \"evidence\": [],\n",
    "                \"no_evidence_reason\": \"Explain why no evidence was found (e.g., 'Claim is unsupported', 'Claim is theoretical without empirical evidence', etc.)\"\n",
    "            }}\n",
    "            ELSE:\n",
    "            Return ONLY the following JSON structure:\n",
    "            {{\n",
    "                \"claim_id\": {claim['claim_id']},\n",
    "                \"evidence\": [\n",
    "                    {{\n",
    "                        \"evidence_id\": 1,\n",
    "                        \"evidence_text\": \"specific experimental result/data point\",\n",
    "                        \"evidence_type\": \"primary/secondary\",\n",
    "                        \"strength\": \"strong/moderate/weak\",\n",
    "                        \"limitations\": \"stated limitations or assumptions\",\n",
    "                        \"location\": \"specific section & paragraph\",\n",
    "                        \"exact_quote\": \"verbatim text from paper\"\n",
    "                    }}\n",
    "                ]\n",
    "            }}\n",
    "            \"\"\"\n",
    "            time.sleep(45)\n",
    "            self.execution_times[\"total_sleep_time\"] += 45\n",
    "            response = self.model.generate_content(evidence_prompt)\n",
    "            result = self._parse_json_response(response.text)\n",
    "            if result:\n",
    "                evidence_results.append(result)\n",
    "        \n",
    "        self.execution_times[\"evidence_analysis\"] = time.time() - start_time\n",
    "        return evidence_results\n",
    "\n",
    "    def analyze_conclusions(self, filename: str, claims: Dict, evidence_results: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze conclusions considering claims and evidence\"\"\"\n",
    "        # text = extract_text_from_pdf(filename)\n",
    "\n",
    "        if not self.paper_text:\n",
    "            text = self.extract_text_from_pdf(filename)\n",
    "        else:\n",
    "            text = self.paper_text\n",
    "        \n",
    "        # Build evidence summary\n",
    "        def build_evidence_summary(claim_id):\n",
    "            claim_evidence = next((e['evidence'] for e in evidence_results if e.get('claim_id') == claim_id), [])\n",
    "            evidence_text = []\n",
    "            for idx, evidence in enumerate(claim_evidence, 1):\n",
    "                evidence_text.append(\n",
    "                    f\"  Evidence {idx}:\\n\"\n",
    "                    f\"    - Text: {evidence.get('evidence_text', 'No text provided')}\\n\"\n",
    "                    f\"    - Strength: {evidence.get('strength', 'Not specified')}\\n\"\n",
    "                    f\"    - Limitations: {evidence.get('limitations', 'None specified')}\\n\"\n",
    "                    f\"    - Location: {evidence.get('location', 'Location not specified')}\"\n",
    "                )\n",
    "            return \"\\n\".join(evidence_text)\n",
    "\n",
    "        # Create analysis summary\n",
    "        analysis_sections = []\n",
    "        for claim in claims.get('claims', []):\n",
    "            claim_id = claim.get('claim_id')\n",
    "            claim_section = (\n",
    "                f\"\\nClaim {claim_id}:\\n\"\n",
    "                f\"Statement: {claim.get('claim_text', 'No text provided')}\\n\"\n",
    "                f\"Location: {claim.get('location', 'Location not specified')}\\n\"\n",
    "                f\"\\nEvidence Summary:\\n{build_evidence_summary(claim_id)}\"\n",
    "            )\n",
    "            analysis_sections.append(claim_section)\n",
    "\n",
    "        full_analysis = \"\\n\".join(analysis_sections)\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "\n",
    "        conclusions_prompt = f\"\"\"\n",
    "        Paper text: {text}\n",
    "        \n",
    "        Analyze the following claims and their supporting evidence:\n",
    "        {full_analysis}\n",
    "\n",
    "        For each claim, provide a comprehensive conclusion analysis following these guidelines:\n",
    "\n",
    "        1. Evidence Assessment:\n",
    "        - Evaluate the strength and quality of ALL evidence presented\n",
    "        - Consider both supporting and contradicting evidence\n",
    "        - Assess the methodology and reliability of evidence\n",
    "\n",
    "        2. Conclusion Analysis:\n",
    "        - Determine what the authors concluded about each claim\n",
    "        - Evaluate if conclusions are justified by the evidence\n",
    "        - Consider the relationship between evidence quality and conclusion strength\n",
    "\n",
    "        3. Robustness Evaluation:\n",
    "        - Assess how well the evidence supports the conclusions\n",
    "        - Consider methodological strengths and weaknesses\n",
    "        - Evaluate the consistency of evidence across different sources\n",
    "\n",
    "        4. Limitations Analysis:\n",
    "        - Identify specific limitations in both evidence and conclusions\n",
    "        - Consider gaps in methodology or data\n",
    "        - Note any potential biases or confounding factors\n",
    "\n",
    "        Return ONLY the following JSON structure:\n",
    "        {{\n",
    "            \"conclusions\": [\n",
    "                {{\n",
    "                    \"claim_id\": number,\n",
    "                    \"author_conclusion\": \"detailed description of authors' conclusion based on evidence\",\n",
    "                    \"conclusion_justified\": true/false,\n",
    "                    \"justification_explanation\": \"detailed explanation of why conclusion is/isn't justified\",\n",
    "                    \"robustness_analysis\": \"comprehensive analysis of evidence strength and reliability\",\n",
    "                    \"limitations\": \"specific limitations and caveats\",\n",
    "                    \"location\": \"section/paragraph where conclusion appears\",\n",
    "                    \"evidence_alignment\": \"analysis of how well evidence aligns with conclusion\",\n",
    "                    \"confidence_level\": \"high/medium/low based on evidence quality\",\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "        \"\"\"\n",
    "        time.sleep(45)\n",
    "        self.execution_times[\"total_sleep_time\"] += 45\n",
    "        response = self.model.generate_content(conclusions_prompt)\n",
    "        result = self._parse_json_response(response.text)\n",
    "\n",
    "        if not result or not isinstance(result, dict) or 'conclusions' not in result:\n",
    "            return {\"conclusions\": []}\n",
    "\n",
    "        # Ensure complete coverage\n",
    "        claims_ids = set(claim['claim_id'] for claim in claims.get('claims', []))\n",
    "        all_conclusions = result.get('conclusions', [])\n",
    "        \n",
    "        complete_conclusions = []\n",
    "        for claim_id in claims_ids:\n",
    "            existing_conclusion = next(\n",
    "                (c for c in all_conclusions if c.get('claim_id') == claim_id),\n",
    "                None\n",
    "            )\n",
    "            \n",
    "            if existing_conclusion:\n",
    "                complete_conclusions.append(existing_conclusion)\n",
    "            else:\n",
    "                complete_conclusions.append({\n",
    "                    \"claim_id\": claim_id,\n",
    "                    \"author_conclusion\": \"No conclusion available\",\n",
    "                    \"conclusion_justified\": False,\n",
    "                    \"justification_explanation\": \"Analysis not available\",\n",
    "                    \"robustness_analysis\": \"No robustness analysis available\",\n",
    "                    \"limitations\": \"No limitations analysis available\",\n",
    "                    \"location\": \"Location not specified\",\n",
    "                    \"evidence_alignment\": \"No alignment analysis available\",\n",
    "                    \"confidence_level\": \"low\"\n",
    "                })\n",
    "        self.execution_times[\"conclusions_analysis\"] = time.time() - start_time\n",
    "\n",
    "        return {\n",
    "            \"conclusions\": complete_conclusions,\n",
    "            \"analysis_metadata\": {\n",
    "                \"total_claims_analyzed\": len(claims_ids),\n",
    "                \"claims_with_conclusions\": len(all_conclusions),\n",
    "                \"analysis_timestamp\": str(datetime.datetime.now())\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _parse_json_response(self, response: str) -> Dict:\n",
    "        \"\"\"Parse JSON response and handle errors\"\"\"\n",
    "        try:\n",
    "            start_idx = response.find('{')\n",
    "            end_idx = response.rfind('}') + 1\n",
    "            if start_idx == -1 or end_idx == 0:\n",
    "                raise ValueError(\"No JSON content found in response\")\n",
    "                \n",
    "            json_str = response[start_idx:end_idx]\n",
    "            return json.loads(json_str)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing response: {e}\")\n",
    "            print(\"Raw response:\", response)\n",
    "            return None\n",
    "\n",
    "    def combine_results(self, claims: Dict, evidence_results: List[Dict], conclusions: Dict) -> Dict:\n",
    "        \"\"\"Combine all analysis results into a final structured format\"\"\"\n",
    "        final_results = {\n",
    "            \"paper_analysis\": []\n",
    "        }\n",
    "        \n",
    "        conclusions_dict = {\n",
    "            c['claim_id']: c \n",
    "            for c in conclusions.get('conclusions', [])\n",
    "        } if conclusions else {}\n",
    "        \n",
    "        evidence_dict = {\n",
    "            e['claim_id']: e.get('evidence', [])\n",
    "            for e in evidence_results if isinstance(e, dict)\n",
    "        }\n",
    "        \n",
    "        for claim in claims.get('claims', []):\n",
    "            claim_id = claim['claim_id']\n",
    "            conclusion = conclusions_dict.get(claim_id, {})\n",
    "            evidence = evidence_dict.get(claim_id, [])\n",
    "            \n",
    "            analysis = {\n",
    "                \"claim_id\": claim_id,\n",
    "                \"claim\": claim.get('claim_text', ''),\n",
    "                \"claim_location\": claim.get('location', 'Location not specified'),\n",
    "                \"evidence\": evidence,\n",
    "                \"evidence_locations\": [ev.get('location', 'Location not specified') for ev in evidence],\n",
    "                \"conclusion\": {\n",
    "                    \"author_conclusion\": conclusion.get('author_conclusion', 'No conclusion available'),\n",
    "                    \"conclusion_justified\": conclusion.get('conclusion_justified', False),\n",
    "                    \"robustness_analysis\": conclusion.get('robustness_analysis', 'No robustness analysis available'),\n",
    "                    \"limitations\": conclusion.get('limitations', 'No limitations analysis available'),\n",
    "                    \"conclusion_location\": conclusion.get('location', 'Location not specified')\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            final_results['paper_analysis'].append(analysis)\n",
    "\n",
    "        final_results[\"execution_times\"] = {\n",
    "            \"claims_analysis_time\": f\"{self.execution_times['claims_analysis']:.2f} seconds\",\n",
    "            \"evidence_analysis_time\": f\"{self.execution_times['evidence_analysis']:.2f} seconds\",\n",
    "            \"conclusions_analysis_time\": f\"{self.execution_times['conclusions_analysis']:.2f} seconds\",\n",
    "            \"total_sleep_time\": f\"{self.execution_times['total_sleep_time']:.2f} seconds\",\n",
    "            \"actual_processing_time\": f\"{(self.execution_times['total_time'] - self.execution_times['total_sleep_time']):.2f} seconds\",\n",
    "            \"total_execution_time\": f\"{self.execution_times['total_time']:.2f} seconds\"\n",
    "        }\n",
    "\n",
    "        \n",
    "        return final_results\n",
    "\n",
    "    def print_analysis_results(self, final_results: Dict):\n",
    "        \"\"\"Print the analysis results in a readable format\"\"\"\n",
    "        print(\"\\n=== Complete Paper Analysis ===\\n\")\n",
    "        \n",
    "        for analysis in final_results['paper_analysis']:\n",
    "            print(f\"Claim {analysis['claim_id']}:\")\n",
    "            print(f\"Statement: {analysis['claim']}\")\n",
    "            print(\"\\nEvidence:\")\n",
    "            for evidence in analysis['evidence']:\n",
    "                print(f\"- {evidence['evidence_text']}\")\n",
    "                print(f\"  Strength: {evidence['strength']}\")\n",
    "                print(f\"  Limitations: {evidence['limitations']}\")\n",
    "            \n",
    "            print(\"\\nConclusion:\")\n",
    "            print(f\"Author's Conclusion: {analysis['conclusion']['author_conclusion']}\")\n",
    "            print(f\"Justified by Evidence: {'Yes' if analysis['conclusion']['conclusion_justified'] else 'No'}\")\n",
    "            print(f\"Robustness: {analysis['conclusion']['robustness_analysis']}\")\n",
    "            print(f\"Limitations: {analysis['conclusion']['limitations']}\")\n",
    "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "def main():\n",
    "    # Initialize analyzer\n",
    "    api_key = \"AIzaSyAxRZoijGYCA0EisBJTxm1KGs7KBD0Nppo\"\n",
    "    analyzer = PaperAnalyzer(api_key)\n",
    "    \n",
    "    # Analyze paper\n",
    "    filename = \"ICLR_1.pdf\"\n",
    "    try:\n",
    "\n",
    "        total_start_time = time.time()\n",
    "\n",
    "        # Step 1: Extract claims\n",
    "        print(\"Extracting claims...\")\n",
    "        claims = analyzer.get_claims(filename)\n",
    "\n",
    "        # Step 2: Analyze evidence\n",
    "        print(\"Analyzing evidence...\")\n",
    "        evidence_results = analyzer.analyze_evidence(filename, claims)\n",
    "        \n",
    "        # Step 3: Analyze conclusions\n",
    "        print(\"Analyzing conclusions...\")\n",
    "        conclusions = analyzer.analyze_conclusions(filename, claims, evidence_results)\n",
    "        \n",
    "\n",
    "        analyzer.execution_times[\"total_time\"] = time.time() - total_start_time\n",
    "\n",
    "        # Combine results\n",
    "        final_results = analyzer.combine_results(claims, evidence_results, conclusions)\n",
    "        \n",
    "        # Print results\n",
    "        analyzer.print_analysis_results(final_results)\n",
    "        \n",
    "        # Save results\n",
    "        with open('detailed_analysis_results.json', 'w') as f:\n",
    "            json.dump(final_results, f, indent=4)\n",
    "        print(\"Results saved to 'detailed_analysis_results.json'\")\n",
    "        \n",
    "        # Save intermediate results\n",
    "        intermediate_results = {\n",
    "            \"claims\": claims,\n",
    "            \"evidence\": evidence_results,\n",
    "            \"conclusions\": conclusions,\n",
    "            \"execution_times\": final_results[\"execution_times\"]\n",
    "\n",
    "\n",
    "        }\n",
    "        with open('intermediate_results.json', 'w') as f:\n",
    "            json.dump(intermediate_results, f, indent=4)\n",
    "        print(\"Intermediate results saved to 'intermediate_results.json'\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing paper: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis of ICLR_1.pdf\n",
      "Extracting text from PDF...\n",
      "Processing ICLR_1.pdf...\n",
      "[                                        ] (0/1===[====                                    ] ( 1/10===[========                                ] ( 2/10===[============                            ] ( 3/10===[================                        ] ( 4/10===[====================                    ] ( 5/10===[========================                ] ( 6/10===[============================            ] ( 7/10===[================================        ] ( 8/10===[====================================    ] ( 9/10===[========================================] (10/10]\n",
      "Analyzing paper...\n",
      "Parsing response...\n",
      "Raw response: ```json\n",
      "{\n",
      "  \"analysis\": [\n",
      "    {\n",
      "      \"claim_id\": 1,\n",
      "      \"claim\": {\n",
      "        \"text\": \"CQCC outperforms MFCC for acoustic analysis of neurodegenerative disorders.\",\n",
      "        \"type\": \"performance\",\n",
      "        \"location\": \"Section 5.2.1\",\n",
      "        \"exact_quote\": \"As observed from Table 4, it can be observed that, Among the features analyzed, CQCC achieved the highest classification accuracy, with the Random Forest classifier attaining an exceptional 99%, in contrast to the 63.4% accuracy achieved by the Support Vector Machine classifier. CQCC excels due to its sophisticated time-frequency representation, which captures subtle and intricate spectral variations essential for distinguishing pathological speech from healthy speech.\"\n",
      "      },\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"evidence_text\": \"CQCC achieved 99% accuracy with Random Forest classifier and 63.4% accuracy with SVM classifier.\",\n",
      "          \"strength\": \"strong\",\n",
      "          \"limitations\": \"None mentioned in the paper.\",\n",
      "          \"location\": \"Section 5.2.1\",\n",
      "          \"exact_quote\": \"As observed from Table 4, it can be observed that, Among the features analyzed, CQCC achieved the highest classification accuracy, with the Random Forest classifier attaining an exceptional 99%, in contrast to the 63.4% accuracy achieved by the Support Vector Machine classifier.\"\n",
      "        }\n",
      "      ],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": true,\n",
      "        \"robustness\": \"high\",\n",
      "        \"justification\": \"The evidence directly supports the claim of CQCC's superior performance, with high accuracy in both Random Forest and SVM classifiers.\",\n",
      "        \"key_limitations\": \"None identified.\",\n",
      "        \"confidence_level\": \"high\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"claim_id\": 2,\n",
      "      \"claim\": {\n",
      "        \"text\": \"CQCC outperforms MFCC for classification of different neurodegenerative pathologies.\",\n",
      "        \"type\": \"performance\",\n",
      "        \"location\": \"Section 5.2.2\",\n",
      "        \"exact_quote\": \"As observed from the tables that the proposed CQCC features outperform the baseline MFCC features with an absolute increment of 5.6% and 7.7% on RF and SVM classifiers, respectively.\"\n",
      "      },\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"evidence_text\": \"CQCC achieves higher accuracy than MFCC on both RF and SVM classifiers for the classification of different neurodegenerative pathologies.\",\n",
      "          \"strength\": \"strong\",\n",
      "          \"limitations\": \"None mentioned in the paper.\",\n",
      "          \"location\": \"Section 5.2.2\",\n",
      "          \"exact_quote\": \"Table 5 reports the results on baseline as well as proposed feature sets using RF and SVM as classifiers. It can be observed from the tables that the proposed CQCC features outperform the baseline MFCC features with an absolute increment of 5.6% and 7.7% on RF and SVM classifiers, respectively.\"\n",
      "        }\n",
      "      ],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": true,\n",
      "        \"robustness\": \"high\",\n",
      "        \"justification\": \"The evidence directly supports the claim of CQCC's outperformance with higher accuracy for different neurodegenerative pathologies.\",\n",
      "        \"key_limitations\": \"None identified.\",\n",
      "        \"confidence_level\": \"high\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"claim_id\": 3,\n",
      "      \"claim\": {\n",
      "        \"text\": \"CQCC features enhance class separation in LDA plots.\",\n",
      "        \"type\": \"performance\",\n",
      "        \"location\": \"Section 5.2.3\",\n",
      "        \"exact_quote\": \"The LDA plot of CQCC features exhibits a clearer separation, especially between the ALS and Parkinson’s disease classes. ALS samples are tightly clustered on the far left, showing a distinct separation from the Parkinson and Healthy Control classes. Healthy Control samples are spread across a different region, especially in the positive range of the first LDA component, indicating less overlap with Parkinson’s disease samples. This stronger discriminative power indicates that CQCC features are more effective at distinguishing between neurodegenerative disorders and healthy individuals, making them a more robust feature set for classification tasks.\"\n",
      "      },\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"evidence_text\": \"LDA plots show better class separation with CQCC features, especially between ALS and Parkinson's disease classes.\",\n",
      "          \"strength\": \"strong\",\n",
      "          \"limitations\": \"None mentioned in the paper.\",\n",
      "          \"location\": \"Section 5.2.3\",\n",
      "          \"exact_quote\": \"The LDA plot as shown in Figure 3, MFCC features reveals a moderate overlap between the three classes. ALS and Parkinson’s disease samples display a slight separation along the first LDA component, with Parkinson’s samples tending to cluster more closely in a specific region, while ALS shows broader dispersion. Healthy Control samples, although overlapping with ALS and Parkinson, are more distinguishable, particularly in the negative region of the first component. This moderate separability suggests that while MFCC captures useful information related to voice characteristics, it may not be fully sufficient for high-accuracy classification of the three groups. However, the LDA plot of CQCC features exhibits a clearer separation, especially between the ALS and Parkinson’s disease classes. ALS samples are tightly clustered on the far left, showing a distinct separation from the Parkinson and Healthy Control classes. Healthy Control samples are spread across a different region, especially in the positive range of the first LDA component, indicating less overlap with Parkinson’s disease samples. This stronger discriminative power indicates that CQCC features are more effective at distinguishing between neurodegenerative disorders and healthy individuals, making them a more robust feature set for classification tasks.\"\n",
      "        }\n",
      "      ],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": true,\n",
      "        \"robustness\": \"high\",\n",
      "        \"justification\": \"The evidence directly supports the claim of enhanced class separation using CQCC features in LDA plots.\",\n",
      "        \"key_limitations\": \"None identified.\",\n",
      "        \"confidence_level\": \"high\"\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"claim_id\": 4,\n",
      "      \"claim\": {\n",
      "        \"text\": \"CQCC uses a form-invariant property of the Constant-Q Transform.\",\n",
      "        \"type\": \"methodology\",\n",
      "        \"location\": \"Section 3.1.1\",\n",
      "        \"exact_quote\": \"Moreover, this form of the window function applies to practical models involving short-time analysis, such as those that mimic the auditory system’s peripheral processing. For example, Flanagan’s original model (Flanagan, 2013) describes the window function used in mechanical spectral analysis due to the movements of the basilar membrane in the cochlea of the human ear (Gambardella, 1968).\"\n",
      "      },\n",
      "      \"evidence\": [\n",
      "        {\n",
      "          \"evidence_text\": \"CQCC uses a window function that depends on both time and frequency.\",\n",
      "          \"strength\": \"strong\",\n",
      "          \"limitations\": \"None mentioned in the paper.\",\n",
      "          \"location\": \"Section 3.1.1\",\n",
      "          \"exact_quote\": \"However, it is interesting to note that the situation changes if the window function depends on both time and frequency, i.e., ψ(t) _ψ(t, ξ), as in the case of CQT.\"\n",
      "        },\n",
      "        {\n",
      "          \"evidence_text\": \"This window function satisfies the bound input and bound output (BIBO) stability conditions for linear time-invariant filters.\",\n",
      "          \"strength\": \"strong\",\n",
      "          \"limitations\": \"None mentioned in the paper.\",\n",
      "          \"location\": \"Section 3.1.1\",\n",
      "          \"exact_quote\": \"Furthermore, ψ(t, ξ) also adheres to the Bounded Input and Bounded Output (BIBO) stability conditions for an LTI filter, meaning that its impulse response is absolutely integrable (Oppenheim et al., 2001), as expressed in the following condition:\"\n",
      "        }\n",
      "      ],\n",
      "      \"evaluation\": {\n",
      "        \"conclusion_justified\": true,\n",
      "        \"robustness\": \"high\",\n",
      "        \"justification\": \"The evidence supports the claim that CQCC uses a form-invariant property, as it relies on a window function that depends on both time and frequency, and satisfies BIBO stability conditions.\",\n",
      "        \"key_limitations\": \"None identified.\",\n",
      "        \"confidence_level\": \"high\"\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "Successfully parsed JSON response\n",
      "\n",
      "=== Complete Paper Analysis ===\n",
      "\n",
      "Claim 1:\n",
      "Statement: CQCC outperforms MFCC for acoustic analysis of neurodegenerative disorders.\n",
      "\n",
      "Evidence:\n",
      "- CQCC achieved 99% accuracy with Random Forest classifier and 63.4% accuracy with SVM classifier.\n",
      "  Strength: strong\n",
      "  Limitations: None mentioned in the paper.\n",
      "\n",
      "Conclusion:\n",
      "Author's Conclusion: The evidence directly supports the claim of CQCC's superior performance, with high accuracy in both Random Forest and SVM classifiers.\n",
      "Justified by Evidence: Yes\n",
      "Robustness: high\n",
      "Limitations: None identified.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Claim 2:\n",
      "Statement: CQCC outperforms MFCC for classification of different neurodegenerative pathologies.\n",
      "\n",
      "Evidence:\n",
      "- CQCC achieves higher accuracy than MFCC on both RF and SVM classifiers for the classification of different neurodegenerative pathologies.\n",
      "  Strength: strong\n",
      "  Limitations: None mentioned in the paper.\n",
      "\n",
      "Conclusion:\n",
      "Author's Conclusion: The evidence directly supports the claim of CQCC's outperformance with higher accuracy for different neurodegenerative pathologies.\n",
      "Justified by Evidence: Yes\n",
      "Robustness: high\n",
      "Limitations: None identified.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Claim 3:\n",
      "Statement: CQCC features enhance class separation in LDA plots.\n",
      "\n",
      "Evidence:\n",
      "- LDA plots show better class separation with CQCC features, especially between ALS and Parkinson's disease classes.\n",
      "  Strength: strong\n",
      "  Limitations: None mentioned in the paper.\n",
      "\n",
      "Conclusion:\n",
      "Author's Conclusion: The evidence directly supports the claim of enhanced class separation using CQCC features in LDA plots.\n",
      "Justified by Evidence: Yes\n",
      "Robustness: high\n",
      "Limitations: None identified.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Claim 4:\n",
      "Statement: CQCC uses a form-invariant property of the Constant-Q Transform.\n",
      "\n",
      "Evidence:\n",
      "- CQCC uses a window function that depends on both time and frequency.\n",
      "  Strength: strong\n",
      "  Limitations: None mentioned in the paper.\n",
      "- This window function satisfies the bound input and bound output (BIBO) stability conditions for linear time-invariant filters.\n",
      "  Strength: strong\n",
      "  Limitations: None mentioned in the paper.\n",
      "\n",
      "Conclusion:\n",
      "Author's Conclusion: The evidence supports the claim that CQCC uses a form-invariant property, as it relies on a window function that depends on both time and frequency, and satisfies BIBO stability conditions.\n",
      "Justified by Evidence: Yes\n",
      "Robustness: high\n",
      "Limitations: None identified.\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Results saved to 'detailed_analysis_results.json'\n",
      "Intermediate results saved to 'intermediate_results.json'\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pymupdf4llm\n",
    "import time\n",
    "import datetime\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "class SinglePassPaperAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel('gemini-pro')\n",
    "        self.paper_text = None\n",
    "        self.execution_times = {\n",
    "        \"single_pass_analysis\": 0,\n",
    "        \"total_time\": 0,\n",
    "        \"total_sleep_time\": 0,  # Add this\n",
    "        \"actual_processing_time\": 0  # Add this\n",
    "    }\n",
    "        \n",
    "    def extract_text_from_pdf(self, filename: str) -> str:\n",
    "        \"\"\"Extract text from PDF file using PyMuPDF\"\"\"\n",
    "        try:\n",
    "            self.paper_text = pymupdf4llm.to_markdown(filename)\n",
    "            return self.paper_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from PDF: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def analyze_paper(self, filename):\n",
    "        \"\"\"Perform comprehensive single-pass analysis of the paper\"\"\"\n",
    "        start_time = time.time()\n",
    "\n",
    "        if not self.paper_text:\n",
    "            text = self.extract_text_from_pdf(filename)\n",
    "        else:\n",
    "            text = self.paper_text\n",
    "            \n",
    "        if not text:\n",
    "            raise Exception(\"Failed to extract text from PDF\")\n",
    "        \n",
    "        comprehensive_prompt = f\"\"\"\n",
    "        Analyze this research paper and provide a comprehensive evaluation.\n",
    "        Paper text: {text}\n",
    "\n",
    "        Follow these guidelines:\n",
    "\n",
    "        1. Identify ALL claims in the paper where each claim:\n",
    "           - Makes a specific, verifiable assertion\n",
    "           - Is supported by concrete evidence\n",
    "           - Represents findings, contributions, or methodological advantages\n",
    "           - Can be from any section except abstract\n",
    "\n",
    "        2. For each identified claim:\n",
    "           - Extract ALL supporting or contradicting evidence (experimental results, data, or methodology)\n",
    "           - Evaluate the evidence strength and limitations\n",
    "           - Assess how well conclusions align with evidence\n",
    "\n",
    "        Return ONLY the following JSON structure:\n",
    "        {{\n",
    "            \"analysis\": [\n",
    "                {{\n",
    "                    \"claim_id\": number,\n",
    "                    \"claim\": {{\n",
    "                        \"text\": \"statement of the claim\",\n",
    "                        \"type\": \"methodology/result/contribution/performance\",\n",
    "                        \"location\": \"section/paragraph\",\n",
    "                        \"exact_quote\": \"verbatim text from paper\"\n",
    "                    }},\n",
    "                    \"evidence\": [\n",
    "                        {{\n",
    "                            \"evidence_text\": \"specific experimental result/data\",\n",
    "                            \"strength\": \"strong/moderate/weak\",\n",
    "                            \"limitations\": \"specific limitations\",\n",
    "                            \"location\": \"section/paragraph\",\n",
    "                            \"exact_quote\": \"verbatim text from paper\"\n",
    "                        }}\n",
    "                    ],\n",
    "                    \"evaluation\": {{\n",
    "                        \"conclusion_justified\": true/false,\n",
    "                        \"robustness\": \"high/medium/low\",\n",
    "                        \"justification\": \"explanation of evidence-conclusion alignment\",\n",
    "                        \"key_limitations\": \"critical limitations affecting validity\",\n",
    "                        \"confidence_level\": \"high/medium/low\"\n",
    "                    }}\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "\n",
    "        Ensure:\n",
    "        - ALL substantive claims are captured\n",
    "        - Evaluations are objective and well-reasoned\n",
    "        - All locations and quotes are precise\n",
    "        - Multiple pieces of evidence per claim are included when present\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add rate limiting\n",
    "        time.sleep(45)\n",
    "        self.execution_times[\"total_sleep_time\"] += 45\n",
    "        # Get response from Gemini\n",
    "        response = self.model.generate_content(comprehensive_prompt)\n",
    "        # self.execution_times[\"single_pass_analysis\"] = time.time() - start_time\n",
    "\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        self.execution_times[\"single_pass_analysis\"] = elapsed_time\n",
    "        self.execution_times[\"actual_processing_time\"] = elapsed_time - self.execution_times[\"total_sleep_time\"]\n",
    "\n",
    "\n",
    "\n",
    "        return self._parse_json_response(response.text)\n",
    "\n",
    "    def _parse_json_response(self, response: str) -> Dict:\n",
    "        \"\"\"Parse JSON response with better error handling\"\"\"\n",
    "        try:\n",
    "            print(\"Parsing response...\")\n",
    "            print(\"Raw response:\", response)\n",
    "            \n",
    "            start_idx = response.find('{')\n",
    "            end_idx = response.rfind('}') + 1\n",
    "            \n",
    "            if start_idx == -1 or end_idx == 0:\n",
    "                raise ValueError(\"No JSON content found in response\")\n",
    "                \n",
    "            json_str = response[start_idx:end_idx]\n",
    "            result = json.loads(json_str)\n",
    "            \n",
    "            print(\"Successfully parsed JSON response\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing response: {str(e)}\")\n",
    "            print(\"Raw response:\", response)\n",
    "            return None\n",
    "        \n",
    "\n",
    "    def combine_results(self, analysis_results: Dict) -> tuple:\n",
    "        \"\"\"Restructure the single-pass analysis results into the desired format\"\"\"\n",
    "        claims = {\n",
    "            \"claims\": [\n",
    "                {\n",
    "                    \"claim_id\": item[\"claim_id\"],\n",
    "                    \"claim_text\": item[\"claim\"][\"text\"],\n",
    "                    \"location\": item[\"claim\"][\"location\"],\n",
    "                    \"claim_type\": item[\"claim\"][\"type\"],\n",
    "                    \"exact_quote\": item[\"claim\"][\"exact_quote\"]\n",
    "                }\n",
    "                for item in analysis_results[\"analysis\"]\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        evidence_results = [\n",
    "            {\n",
    "                \"claim_id\": item[\"claim_id\"],\n",
    "                \"evidence\": [\n",
    "                    {\n",
    "                        \"evidence_id\": idx + 1,\n",
    "                        \"evidence_text\": ev[\"evidence_text\"],\n",
    "                        \"evidence_type\": \"primary\",\n",
    "                        \"strength\": ev[\"strength\"],\n",
    "                        \"limitations\": ev[\"limitations\"],\n",
    "                        \"location\": ev[\"location\"],\n",
    "                        \"exact_quote\": ev[\"exact_quote\"]\n",
    "                    }\n",
    "                    for idx, ev in enumerate(item[\"evidence\"])\n",
    "                ]\n",
    "            }\n",
    "            for item in analysis_results[\"analysis\"]\n",
    "        ]\n",
    "        \n",
    "        conclusions = {\n",
    "            \"conclusions\": [\n",
    "                {\n",
    "                    \"claim_id\": item[\"claim_id\"],\n",
    "                    \"author_conclusion\": item[\"evaluation\"][\"justification\"],\n",
    "                    \"conclusion_justified\": item[\"evaluation\"][\"conclusion_justified\"],\n",
    "                    \"robustness_analysis\": item[\"evaluation\"][\"robustness\"],\n",
    "                    \"limitations\": item[\"evaluation\"][\"key_limitations\"],\n",
    "                    \"evidence_alignment\": item[\"evaluation\"][\"justification\"],\n",
    "                    \"confidence_level\": item[\"evaluation\"][\"confidence_level\"]\n",
    "                }\n",
    "                for item in analysis_results[\"analysis\"]\n",
    "            ],\n",
    "            \"analysis_metadata\": {\n",
    "                \"total_claims_analyzed\": len(analysis_results[\"analysis\"]),\n",
    "                \"claims_with_conclusions\": len(analysis_results[\"analysis\"]),\n",
    "                \"analysis_timestamp\": str(datetime.datetime.now())\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        final_results = {\n",
    "            \"paper_analysis\": []\n",
    "        }\n",
    "        \n",
    "        for item in analysis_results[\"analysis\"]:\n",
    "            claim_id = item[\"claim_id\"]\n",
    "            analysis = {\n",
    "                \"claim_id\": claim_id,\n",
    "                \"claim\": item[\"claim\"][\"text\"],\n",
    "                \"claim_location\": item[\"claim\"][\"location\"],\n",
    "                \"evidence\": item[\"evidence\"],\n",
    "                \"evidence_locations\": [ev[\"location\"] for ev in item[\"evidence\"]],\n",
    "                \"conclusion\": {\n",
    "                    \"author_conclusion\": item[\"evaluation\"][\"justification\"],\n",
    "                    \"conclusion_justified\": item[\"evaluation\"][\"conclusion_justified\"],\n",
    "                    \"robustness_analysis\": item[\"evaluation\"][\"robustness\"],\n",
    "                    \"limitations\": item[\"evaluation\"][\"key_limitations\"],\n",
    "                    \"conclusion_location\": item[\"claim\"][\"location\"]\n",
    "                }\n",
    "            }\n",
    "            final_results[\"paper_analysis\"].append(analysis)\n",
    "        \n",
    "        return claims, evidence_results, conclusions, final_results\n",
    "\n",
    "    def print_analysis_results(self, final_results: Dict):\n",
    "        \"\"\"Print the analysis results in a readable format\"\"\"\n",
    "        print(\"\\n=== Complete Paper Analysis ===\\n\")\n",
    "        \n",
    "        for analysis in final_results['paper_analysis']:\n",
    "            print(f\"Claim {analysis['claim_id']}:\")\n",
    "            print(f\"Statement: {analysis['claim']}\")\n",
    "            print(\"\\nEvidence:\")\n",
    "            for evidence in analysis['evidence']:\n",
    "                print(f\"- {evidence['evidence_text']}\")\n",
    "                print(f\"  Strength: {evidence['strength']}\")\n",
    "                print(f\"  Limitations: {evidence['limitations']}\")\n",
    "            \n",
    "            print(\"\\nConclusion:\")\n",
    "            print(f\"Author's Conclusion: {analysis['conclusion']['author_conclusion']}\")\n",
    "            print(f\"Justified by Evidence: {'Yes' if analysis['conclusion']['conclusion_justified'] else 'No'}\")\n",
    "            print(f\"Robustness: {analysis['conclusion']['robustness_analysis']}\")\n",
    "            print(f\"Limitations: {analysis['conclusion']['limitations']}\")\n",
    "            print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "    def save_results(self, results: Dict, base_filename: str):\n",
    "        \"\"\"Save analysis results to files\"\"\"\n",
    "        output_dir = Path('analysis_outputs')\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Save full JSON results\n",
    "        json_path = output_dir / f'{base_filename}_analysis.json'\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "\n",
    "\n",
    "        # results[\"execution_times\"] = {\n",
    "        # \"single_pass_analysis_time\": f\"{self.execution_times['single_pass_analysis']:.2f} seconds\",\n",
    "        # \"total_execution_time\": f\"{self.execution_times['total_time']:.2f} seconds\"\n",
    "        #  }\n",
    "\n",
    "\n",
    "        results[\"execution_times\"] = {\n",
    "        \"single_pass_analysis_time\": f\"{self.execution_times['single_pass_analysis']:.2f} seconds\",\n",
    "        \"total_sleep_time\": f\"{self.execution_times['total_sleep_time']:.2f} seconds\",\n",
    "        \"actual_processing_time\": f\"{self.execution_times['actual_processing_time']:.2f} seconds\",\n",
    "        \"total_execution_time\": f\"{self.execution_times['total_time']:.2f} seconds\"\n",
    "        }\n",
    "        \n",
    "        # Save readable text summary\n",
    "        text_path = output_dir / f'{base_filename}_summary.txt'\n",
    "        with open(text_path, 'w', encoding='utf-8') as f:\n",
    "            for analysis in results['analysis']:\n",
    "                f.write(f\"Claim {analysis['claim_id']}:\\n\")\n",
    "                f.write(f\"Type: {analysis['claim']['type']}\\n\")\n",
    "                f.write(f\"Statement: {analysis['claim']['text']}\\n\")\n",
    "                f.write(f\"Location: {analysis['claim']['location']}\\n\")\n",
    "                f.write(f\"Exact Quote: {analysis['claim']['exact_quote']}\\n\\n\")\n",
    "                \n",
    "                f.write(\"Evidence:\\n\")\n",
    "                for evidence in analysis['evidence']:\n",
    "                    f.write(f\"- Evidence Text: {evidence['evidence_text']}\\n\")\n",
    "                    f.write(f\"  Strength: {evidence['strength']}\\n\")\n",
    "                    f.write(f\"  Location: {evidence['location']}\\n\")\n",
    "                    f.write(f\"  Limitations: {evidence['limitations']}\\n\")\n",
    "                    f.write(f\"  Exact Quote: {evidence['exact_quote']}\\n\\n\")\n",
    "                \n",
    "                eval_data = analysis['evaluation']\n",
    "                f.write(\"Evaluation:\\n\")\n",
    "                f.write(f\"Conclusion Justified: {'Yes' if eval_data['conclusion_justified'] else 'No'}\\n\")\n",
    "                f.write(f\"Robustness: {eval_data['robustness']}\\n\")\n",
    "                f.write(f\"Confidence Level: {eval_data['confidence_level']}\\n\")\n",
    "                f.write(f\"Justification: {eval_data['justification']}\\n\")\n",
    "                f.write(f\"Key Limitations: {eval_data['key_limitations']}\\n\")\n",
    "                \n",
    "                f.write(\"\\n\" + \"-\"*50 + \"\\n\\n\")\n",
    "        \n",
    "        # Generate summary statistics\n",
    "        stats_path = output_dir / f'{base_filename}_statistics.txt'\n",
    "        with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "            total_claims = len(results['analysis'])\n",
    "            justified_claims = sum(1 for a in results['analysis'] \n",
    "                                 if a['evaluation']['conclusion_justified'])\n",
    "            \n",
    "            f.write(\"Analysis Statistics:\\n\")\n",
    "            f.write(f\"Total Claims Analyzed: {total_claims}\\n\")\n",
    "            f.write(f\"Justified Claims: {justified_claims}\\n\")\n",
    "            \n",
    "            # Evidence strength distribution\n",
    "            strength_levels = {}\n",
    "            for analysis in results['analysis']:\n",
    "                for evidence in analysis['evidence']:\n",
    "                    strength = evidence['strength']\n",
    "                    strength_levels[strength] = strength_levels.get(strength, 0) + 1\n",
    "            \n",
    "            f.write(\"\\nEvidence Strength Distribution:\\n\")\n",
    "            total_evidence = sum(strength_levels.values())\n",
    "            for strength, count in strength_levels.items():\n",
    "                f.write(f\"{strength}: {count} pieces ({count/total_evidence*100:.1f}%)\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        api_key = \"AIzaSyAxRZoijGYCA0EisBJTxm1KGs7KBD0Nppo\"\n",
    "        analyzer = SinglePassPaperAnalyzer(api_key)\n",
    "        \n",
    "        filename = \"ICLR_1.pdf\"\n",
    "        print(f\"Starting analysis of {filename}\")\n",
    "        \n",
    "\n",
    "        total_start_time = time.time()\n",
    "\n",
    "        # Extract text once at the beginning\n",
    "        print(\"Extracting text from PDF...\")\n",
    "        analyzer.extract_text_from_pdf(filename)\n",
    "        \n",
    "        # Perform single-pass analysis\n",
    "        print(\"Analyzing paper...\")\n",
    "        analysis_results = analyzer.analyze_paper(filename)\n",
    "\n",
    "        analyzer.execution_times[\"total_time\"] = time.time() - total_start_time\n",
    "\n",
    "        \n",
    "        # Restructure results into desired format\n",
    "        claims, evidence_results, conclusions, final_results = analyzer.combine_results(analysis_results)\n",
    "        \n",
    "        # Print results\n",
    "        analyzer.print_analysis_results(final_results)\n",
    "        \n",
    "        # Save detailed results\n",
    "        with open('detailed_analysis_results.json', 'w') as f:\n",
    "            json.dump(final_results, f, indent=4)\n",
    "        print(\"Results saved to 'detailed_analysis_results.json'\")\n",
    "        \n",
    "        # Save intermediate results\n",
    "        intermediate_results = {\n",
    "            \"claims\": claims,\n",
    "            \"evidence\": evidence_results,\n",
    "            \"conclusions\": conclusions,\n",
    "            \"execution_times\": final_results[\"execution_times\"]\n",
    "\n",
    "        }\n",
    "        with open('intermediate_results.json', 'w') as f:\n",
    "            json.dump(intermediate_results, f, indent=4)\n",
    "        print(\"Intermediate results saved to 'intermediate_results.json'\")\n",
    "        \n",
    "        # Save additional analysis outputs\n",
    "        base_filename = Path(filename).stem\n",
    "        analyzer.save_results(analysis_results, base_filename)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing paper: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "import datetime\n",
    "import pymupdf4llm\n",
    "import time\n",
    "from pathlib import Path\n",
    "import os\n",
    "import traceback\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "class PaperAnalyzer:\n",
    "    def __init__(self, api_key: str):\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.model = genai.GenerativeModel('gemini-pro')\n",
    "        self.paper_text = None\n",
    "        self.execution_times = {\n",
    "        \"claims_analysis\": 0,\n",
    "        \"evidence_analysis\": 0,\n",
    "        \"conclusions_analysis\": 0,\n",
    "        \"total_time\": 0,\n",
    "        \"total_sleep_time\": 0,  # Track total sleep time\n",
    "        \"actual_processing_time\": 0  # Time without sleep delays\n",
    "            }\n",
    "        \n",
    "\n",
    "\n",
    "    def extract_text_from_pdf(self, filename: str) -> str:\n",
    "        \"\"\"Extract text from PDF file using PyMuPDF\"\"\"\n",
    "        try:\n",
    "            self.paper_text = pymupdf4llm.to_markdown(filename)\n",
    "            return self.paper_text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting text from PDF: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def get_all_claims(self, filename: str) -> Dict:\n",
    "        \"\"\"Get all claims in one pass\"\"\"\n",
    "        try:\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            if not self.paper_text:\n",
    "                text = self.extract_text_from_pdf(filename)\n",
    "            else:\n",
    "                text = self.paper_text\n",
    "\n",
    "            print(f\"Processing file: {filename}\")\n",
    "            \n",
    "            claims_prompt = f\"\"\"\n",
    "            task is to identify all statements in the text that meet the following criteria for a claim:\n",
    "            1. Makes a specific, testable assertion about results, methods, or contributions\n",
    "            2. Represents a novel finding, improvement, or advancement\n",
    "            3. Presents a clear position or conclusion\n",
    "\n",
    "            Make sure to:\n",
    "            1. Include both major and minor claims\n",
    "            2. Don't miss any claims\n",
    "            3. Present each claim as a separate item\n",
    "            \n",
    "            Return ONLY the following JSON structure:\n",
    "            {{\n",
    "                \"claims\": [\n",
    "                    {{\n",
    "                        \"claim_id\": 1,\n",
    "                        \"claim_text\": \"statement of the claim\",\n",
    "                        \"location\": \"section/paragraph where this claim appears\",\n",
    "                        \"claim_type\": \"Nature of the claim\",\n",
    "                        \"exact_quote\": \"complete verbatim text containing the claim\"\n",
    "                    }}\n",
    "                ]\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            time.sleep(45)  # Rate limiting\n",
    "            self.execution_times[\"total_sleep_time\"] += 45\n",
    "            response = self.model.generate_content(claims_prompt)\n",
    "            self.execution_times[\"claims_analysis\"] = time.time() - start_time\n",
    "\n",
    "            result = self._parse_json_response(response.text)\n",
    "            print(\"Claims extraction completed\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Error in get_all_claims: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_all_evidence(self, filename: str, claims: Dict) -> Dict:\n",
    "        \"\"\"Get evidence for all claims in one pass\"\"\"\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            if not self.paper_text:\n",
    "                text = self.extract_text_from_pdf(filename)\n",
    "            else:\n",
    "                text = self.paper_text\n",
    "            \n",
    "            claims_text = \"\\n\".join([f\"Claim {c['claim_id']}: {c['claim_text']}\" \n",
    "                                   for c in claims['claims']])\n",
    "            print(\"Processing evidence for claims:\", claims_text)\n",
    "            \n",
    "            evidence_prompt = f\"\"\"\n",
    "            Paper text: {text}\n",
    "\n",
    "            For these claims:\n",
    "            {claims_text}\n",
    "\n",
    "             Please identify relevant evidence that:\n",
    "            1. Directly supports or contradicts the claim's specific assertion\n",
    "            2. Is presented with experimental results, data, or concrete examples\n",
    "            3. Can be traced to specific methods, results, or discussion sections\n",
    "            4. Is not from the abstract or introduction\n",
    "\n",
    "            Return ONLY the following JSON:\n",
    "            {{\n",
    "                \"evidence_sets\": [\n",
    "                    {{\n",
    "                        \"claim_id\": number,\n",
    "                        \"evidence\": [\n",
    "                            {{\n",
    "                                \"evidence_id\": number,\n",
    "                                \"evidence_text\": \"specific evidence\",\n",
    "                                \"strength\": \"strong/moderate/weak\",\n",
    "                                \"limitations\": \"key limitations\",\n",
    "                                \"location\": \"section/paragraph\",\n",
    "                                \"exact_quote\": \"verbatim text\"\n",
    "                            }}\n",
    "                        ]\n",
    "                    }}\n",
    "                ]\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            time.sleep(45)  # Rate limiting\n",
    "            self.execution_times[\"total_sleep_time\"] += 45\n",
    "            response = self.model.generate_content(evidence_prompt)\n",
    "            \n",
    "            result = self._parse_json_response(response.text)\n",
    "            self.execution_times[\"evidence_analysis\"] = time.time() - start_time\n",
    "            print(\"Evidence extraction completed\")\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            print(f\"Error in get_all_evidence: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_all_conclusions(self, filename: str, claims: Dict, evidence_sets: Dict) -> Dict:\n",
    "        \"\"\"Analyze conclusions for all claims and evidence in one pass\"\"\"\n",
    "        try:\n",
    "\n",
    "            start_time = time.time()\n",
    "            if not self.paper_text:\n",
    "                text = self.extract_text_from_pdf(filename)\n",
    "            else:\n",
    "                text = self.paper_text\n",
    "            \n",
    "            # Create summary of claims and evidence for the prompt\n",
    "            analysis_summary = []\n",
    "            for claim in claims['claims']:\n",
    "                claim_id = claim['claim_id']\n",
    "                claim_evidence = next((e['evidence'] for e in evidence_sets['evidence_sets'] \n",
    "                                    if e['claim_id'] == claim_id), [])\n",
    "                \n",
    "                summary = f\"\\nClaim {claim_id}: {claim['claim_text']}\\n\"\n",
    "                summary += \"Evidence:\\n\"\n",
    "                for evidence in claim_evidence:\n",
    "                    summary += f\"- {evidence['evidence_text']}\\n\"\n",
    "                analysis_summary.append(summary)\n",
    "            \n",
    "            analysis_text = \"\\n\".join(analysis_summary)\n",
    "            \n",
    "            conclusions_prompt = f\"\"\"\n",
    "            Paper text: {text}\n",
    "\n",
    "            Analyze these claims and their evidence:\n",
    "            {analysis_text}\n",
    "\n",
    "            For each claim-evidence pair, evaluate:\n",
    "            1. Whether the evidence justifies the claim\n",
    "            2. The overall strength of support\n",
    "            3. Any important limitations\n",
    "\n",
    "            Return ONLY the following JSON:\n",
    "            {{\n",
    "                \"conclusions\": [\n",
    "                    {{\n",
    "                        \"claim_id\": number,\n",
    "                        \"conclusion_justified\": true/false,\n",
    "                        \"robustness\": \"high/medium/low\",\n",
    "                        \"key_limitations\": \"specific limitations\",\n",
    "                        \"confidence_level\": \"high/medium/low\"\n",
    "                    }}\n",
    "                ]\n",
    "            }}\n",
    "            \"\"\"\n",
    "            \n",
    "            time.sleep(45)  # Rate limiting\n",
    "            self.execution_times[\"total_sleep_time\"] += 45\n",
    "            response = self.model.generate_content(conclusions_prompt)\n",
    "            \n",
    "            result = self._parse_json_response(response.text)\n",
    "\n",
    "            self.execution_times[\"conclusions_analysis\"] = time.time() - start_time\n",
    "\n",
    "            print(\"Conclusions analysis completed\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in get_all_conclusions: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _parse_json_response(self, response: str) -> Dict:\n",
    "        \"\"\"Parse JSON response with better error handling\"\"\"\n",
    "        try:\n",
    "            print(\"Parsing response...\")\n",
    "            print(\"Raw response:\", response)\n",
    "            \n",
    "            start_idx = response.find('{')\n",
    "            end_idx = response.rfind('}') + 1\n",
    "            \n",
    "            if start_idx == -1 or end_idx == 0:\n",
    "                raise ValueError(\"No JSON content found in response\")\n",
    "                \n",
    "            json_str = response[start_idx:end_idx]\n",
    "            result = json.loads(json_str)\n",
    "            \n",
    "            print(\"Successfully parsed JSON response\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing response: {str(e)}\")\n",
    "            print(\"Raw response:\", response)\n",
    "            raise\n",
    "\n",
    "    def analyze_paper(self, filename: str) -> Dict:\n",
    "        \"\"\"Complete paper analysis using three-prompt approach\"\"\"\n",
    "        try:\n",
    "\n",
    "            total_start_time = time.time()\n",
    "\n",
    "            # Extract text once at the beginning\n",
    "            print(\"Extracting text from PDF...\")\n",
    "            self.extract_text_from_pdf(filename)\n",
    "\n",
    "            # Get all claims\n",
    "            print(\"Extracting claims...\")\n",
    "            claims = self.get_all_claims(filename)\n",
    "            if not claims:\n",
    "                raise Exception(\"Failed to extract claims\")\n",
    "\n",
    "            # Get evidence for all claims\n",
    "            print(\"Extracting evidence...\")\n",
    "            evidence_sets = self.get_all_evidence(filename, claims)\n",
    "            if not evidence_sets:\n",
    "                raise Exception(\"Failed to extract evidence\")\n",
    "\n",
    "            # Get conclusions for all claim-evidence pairs\n",
    "            print(\"Analyzing conclusions...\")\n",
    "            conclusions = self.get_all_conclusions(filename, claims, evidence_sets)\n",
    "            if not conclusions:\n",
    "                raise Exception(\"Failed to generate conclusions\")\n",
    "\n",
    "\n",
    "\n",
    "# Calculate times\n",
    "            total_elapsed = time.time() - total_start_time\n",
    "            self.execution_times[\"total_time\"] = total_elapsed\n",
    "            self.execution_times[\"actual_processing_time\"] = (\n",
    "                total_elapsed - self.execution_times[\"total_sleep_time\"]\n",
    "            )\n",
    "\n",
    "            # Structure final results\n",
    "            final_results = {\n",
    "                \"paper_analysis\": []\n",
    "            }\n",
    "\n",
    "            for claim in claims['claims']:\n",
    "                claim_id = claim['claim_id']\n",
    "                \n",
    "                # Get evidence for this claim\n",
    "                evidence = next((e['evidence'] for e in evidence_sets['evidence_sets'] \n",
    "                            if e['claim_id'] == claim_id), [])\n",
    "                \n",
    "                # Get conclusion for this claim\n",
    "                conclusion = next((c for c in conclusions['conclusions'] \n",
    "                                if c['claim_id'] == claim_id), {})\n",
    "\n",
    "                analysis_item = {\n",
    "                    \"claim_id\": claim_id,\n",
    "                    \"claim\": {\n",
    "                        \"text\": claim['claim_text'],\n",
    "                        \"location\": claim['location'],\n",
    "                        \"type\": claim['claim_type'],\n",
    "                        \"exact_quote\": claim['exact_quote']\n",
    "                    },\n",
    "                    \"evidence\": evidence,\n",
    "                    \"conclusion\": {\n",
    "                        \"conclusion_justified\": conclusion.get('conclusion_justified', False),\n",
    "                        \"robustness\": conclusion.get('robustness', 'Not evaluated'),\n",
    "                        \"limitations\": conclusion.get('key_limitations', 'Not specified'),\n",
    "                        \"confidence_level\": conclusion.get('confidence_level', 'low')\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                final_results['paper_analysis'].append(analysis_item)\n",
    "\n",
    "            \n",
    "            # Add timing information\n",
    "            final_results[\"execution_times\"] = {\n",
    "                \"claims_analysis_time\": f\"{self.execution_times['claims_analysis']:.2f} seconds\",\n",
    "                \"evidence_analysis_time\": f\"{self.execution_times['evidence_analysis']:.2f} seconds\",\n",
    "                \"conclusions_analysis_time\": f\"{self.execution_times['conclusions_analysis']:.2f} seconds\",\n",
    "                \"total_time\": f\"{self.execution_times['total_time']:.2f} seconds\",\n",
    "                \"rate_limiting_sleep_time\": f\"{self.execution_times['total_sleep_time']:.2f} seconds\",\n",
    "                \"actual_processing_time\": f\"{self.execution_times['actual_processing_time']:.2f} seconds\"\n",
    "            }\n",
    "            \n",
    "            \n",
    "            \n",
    "            return final_results\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in paper analysis: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def save_results(self, results: Dict, filename: str):\n",
    "        \"\"\"Save analysis results in multiple formats\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            base_filename = Path(filename).stem\n",
    "\n",
    "            # Create output directory\n",
    "            os.makedirs('analysis_outputs', exist_ok=True)\n",
    "\n",
    "            # Save detailed JSON results\n",
    "            json_filename = f'analysis_outputs/{base_filename}_analysis_{timestamp}.json'\n",
    "            with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, indent=4)\n",
    "\n",
    "            # Save human-readable summary\n",
    "            summary_filename = f'analysis_outputs/{base_filename}_summary_{timestamp}.txt'\n",
    "            with open(summary_filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"=== Paper Analysis Summary ===\\n\\n\")\n",
    "                \n",
    "                for analysis in results['paper_analysis']:\n",
    "                    f.write(f\"Claim {analysis['claim_id']}:\\n\")\n",
    "                    f.write(f\"Statement: {analysis['claim']['text']}\\n\")\n",
    "                    f.write(f\"Location: {analysis['claim']['location']}\\n\")\n",
    "                    f.write(f\"Type: {analysis['claim']['type']}\\n\")\n",
    "                    f.write(f\"Quote: {analysis['claim']['exact_quote']}\\n\\n\")\n",
    "                    \n",
    "                    f.write(\"Evidence:\\n\")\n",
    "                    for evidence in analysis['evidence']:\n",
    "                        f.write(f\"- {evidence['evidence_text']}\\n\")\n",
    "                        f.write(f\"  Strength: {evidence['strength']}\\n\")\n",
    "                        f.write(f\"  Location: {evidence['location']}\\n\")\n",
    "                        f.write(f\"  Limitations: {evidence['limitations']}\\n\")\n",
    "                        f.write(f\"  Quote: {evidence['exact_quote']}\\n\\n\")\n",
    "                    \n",
    "                    f.write(\"Conclusion:\\n\")\n",
    "                    f.write(f\"Justified: {analysis['conclusion']['conclusion_justified']}\\n\")\n",
    "                    f.write(f\"Robustness: {analysis['conclusion']['robustness']}\\n\")\n",
    "                    f.write(f\"Limitations: {analysis['conclusion']['limitations']}\\n\")\n",
    "                    f.write(f\"Confidence: {analysis['conclusion']['confidence_level']}\\n\")\n",
    "                    f.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "\n",
    "\n",
    "                    f.write(\"\\nExecution Times:\\n\")\n",
    "                    f.write(f\"Claims Analysis (with delays): {self.execution_times['claims_analysis']:.2f} seconds\\n\")\n",
    "                    f.write(f\"Evidence Analysis (with delays): {self.execution_times['evidence_analysis']:.2f} seconds\\n\")\n",
    "                    f.write(f\"Conclusions Analysis (with delays): {self.execution_times['conclusions_analysis']:.2f} seconds\\n\")\n",
    "                    f.write(f\"Total Rate Limiting Sleep Time: {self.execution_times['total_sleep_time']:.2f} seconds\\n\")\n",
    "                    f.write(f\"Actual Processing Time: {self.execution_times['actual_processing_time']:.2f} seconds\\n\")\n",
    "                    f.write(f\"Total Execution Time: {self.execution_times['total_time']:.2f} seconds\\n\")\n",
    "\n",
    "            # Save statistics\n",
    "            stats_filename = f'analysis_outputs/{base_filename}_stats_{timestamp}.txt'\n",
    "            with open(stats_filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"Analysis Statistics:\\n\\n\")\n",
    "                f.write(f\"Total Claims Analyzed: {len(results['paper_analysis'])}\\n\")\n",
    "                \n",
    "                # Evidence statistics\n",
    "                total_evidence = sum(len(analysis['evidence']) for analysis in results['paper_analysis'])\n",
    "                f.write(f\"Total Evidence Pieces: {total_evidence}\\n\")\n",
    "                \n",
    "                # Confidence distribution\n",
    "                confidence_levels = {}\n",
    "                for analysis in results['paper_analysis']:\n",
    "                    level = analysis['conclusion']['confidence_level']\n",
    "                    confidence_levels[level] = confidence_levels.get(level, 0) + 1\n",
    "                \n",
    "                f.write(\"\\nConfidence Level Distribution:\\n\")\n",
    "                for level, count in confidence_levels.items():\n",
    "                    f.write(f\"{level}: {count} claims\\n\")\n",
    "\n",
    "            print(f\"Results saved to analysis_outputs/:\")\n",
    "            print(f\"- Detailed analysis: {json_filename}\")\n",
    "            print(f\"- Summary: {summary_filename}\")\n",
    "            print(f\"- Statistics: {stats_filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving results: {str(e)}\")\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        api_key = \"AIzaSyAxRZoijGYCA0EisBJTxm1KGs7KBD0Nppo\"\n",
    "        analyzer = PaperAnalyzer(api_key)\n",
    "        \n",
    "        filename = \"ICLR_1.pdf\"\n",
    "        print(f\"Starting analysis of {filename}\")\n",
    "        \n",
    "        # Analyze paper\n",
    "        results = analyzer.analyze_paper(filename)\n",
    "        \n",
    "        if results:\n",
    "            # Save results in structured format\n",
    "            analyzer.save_results(results, filename)\n",
    "            print(\"Analysis completed successfully\")\n",
    "        else:\n",
    "            print(\"Analysis failed to produce results\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {str(e)}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
