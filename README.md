
# ğŸ“Š RC_BENCH
Repository for RC_BENCH project

**Paper Title:**  
#### â€œCanâ€¯AIâ€¯Validateâ€¯Science? Benchmarkingâ€¯LLMs for Accurate Scientific Claimâ€¯â†’â€¯Evidenceâ€¯Reasoningâ€ 

---

## ğŸ¯ Mission

Provide a transparent benchmark (**CLAIMâ€‘BENCH**) and reproducible code for evaluating longâ€‘context LLMs on sentenceâ€‘level claim evidence identification for scientific reasoning.

---

![](Results/figures/architecture.png)

## ğŸ“ Repo Structure

```text
RC_BENCH/
â”‚
â”œâ”€â”€ Code/                                           # All scripts and Jupyter notebooks
â”‚   â”œâ”€â”€ .DS_Store                                  # System file (can be ignored)
â”‚   â”œâ”€â”€ 3_open_models_combined_3_prompts.py        # Script: combines 3 open models using 3 prompts
â”‚   â”œâ”€â”€ 3_open_models_combined_all_at_once.py      # Script: combines 3 open models, runs all at once
â”‚   â”œâ”€â”€ 3_open_models_combined_one_by_one.py       # Script: combines 3 open models, runs one by one
â”‚   â”œâ”€â”€ Annotation_tool_v1.py                      # Main annotation tool script (version 1)
â”‚   â”œâ”€â”€ RC_Claude.ipynb                            # Jupyter notebook for RC_Claude model analysis
â”‚   â”œâ”€â”€ RC_GPT.ipynb                               # Jupyter notebook for RC_GPT model analysis
â”‚   â””â”€â”€ RC_Gemini.ipynb                            # Jupyter notebook for RC_Gemini model analysis
â”‚
â”œâ”€â”€ Data/                                           # All input data files
â”‚   â”œâ”€â”€ .DS_Store                                  # System file (can be ignored)
â”‚   â””â”€â”€ all_papers/                                # Directory: collection of PDF papers for annotation
â”‚
â”œâ”€â”€ Results/                                        # All outputs, annotations, and figures
â”‚   â”œâ”€â”€ .DS_Store                                  # System file (can be ignored)
â”‚   â”œâ”€â”€ All models Outputs/                        # Model output files (claims, evidence, conclusions, etc.)
â”‚   â”œâ”€â”€ Statistics/                                # Aggregated results/statistics (merged or processed)
â”‚   â”œâ”€â”€ all_annotations/                           # Annotated results from the tool
â”‚   â”œâ”€â”€ all_inter_annotations/                     # Intermediate or cross-annotator results
â”‚   â””â”€â”€ figures/                                   # All figures/plots for results and tool screenshots
â”‚
â”œâ”€â”€ .env                                           # API keys and secrets (not tracked, see .gitignore)
â”œâ”€â”€ .gitignore                                     # Should include .env, .DS_Store, and other files to ignore
â”œâ”€â”€ README.md                                      # General project documentation
â””â”€â”€ ANNOTATION_TOOL.md                             # Documentation for the annotation tool
```
## ğŸ“ Annotator Guidelines

### Task Description

Your task is to identify all statements in the text that qualify as claims under the following criteria:

1. **ğŸ¯ Specificity**: The statement makes a specific, testable assertion about results, methods, or contributions.
2. **âœ¨ Novelty**: The statement represents a novel finding, improvement, or advancement.
3. **ğŸ’¡ Clarity**: The statement presents a clear position or conclusion.

### Requirements

- âœ… Include both major and minor claims.
- âœ… Ensure no claim is overlooked.
- âœ… Present each claim as a separate item.

## ğŸ” Evidence Identification

For each identified claim, find and document relevant evidence that:

1. **ğŸ”— Relevance**: Directly supports or contradicts the claim's specific assertion.
2. **ğŸ“Š Concrete Support**: Is presented with experimental results, data, or concrete examples.
3. **ğŸ“ Traceability**: Can be traced to specific methods, results, or discussion sections in the text.
4. **âŒ Exclusions**: Evidence must not be derived from the abstract or introduction sections of the text.

## âœ… Conclusion Analysis

- **âš–ï¸ Justification**: Evaluate whether the conclusions drawn in the text are justified by the evidence provided.

## ğŸ“‹ Annotation Format

Each annotation should be formatted as follows:

```json
{
    "Claim_id": "<unique_identifier>",
    "Claim_text": "<text_of_the_claim>",
    "Evidence_text": "<text_supporting_or_contradicting_the_claim>",
    "Justification_Conclusion": "<evaluator's_comment_on_evidence_justification>"
}
```


## ğŸ“ˆ Results
![](Results/figures/precision_recall_scatter_improved.png)

![](Results/figures/Sentence_dist_analysis.png)


## ğŸ’» Installation

### Prerequisites
- Python 3.7 or higher
- PyQt5
- PyMuPDF (fitz)

## ğŸ“š Citations

If you use RC_BENCH in your research, please cite:

```bibtex
@article{rcbench2024,
  title={Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim -> Evidence Reasoning},
  author={SR Javaji et al.},
  journal={arxiv},
  year={2025}
}
```

