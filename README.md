
# ğŸ“Š RC_BENCH
Repository for RC_BENCH project

![](architecture.png)

## ğŸ“ Annotator Guidelines

### Task Description

Your task is to identify all statements in the text that qualify as claims under the following criteria:

1. **ğŸ¯ Specificity**: The statement makes a specific, testable assertion about results, methods, or contributions.
2. **âœ¨ Novelty**: The statement represents a novel finding, improvement, or advancement.
3. **ğŸ’¡ Clarity**: The statement presents a clear position or conclusion.

### Requirements

- âœ… Include both major and minor claims.
- âœ… Ensure no claim is overlooked.
- âœ… Present each claim as a separate item.

## ğŸ” Evidence Identification

For each identified claim, find and document relevant evidence that:

1. **ğŸ”— Relevance**: Directly supports or contradicts the claim's specific assertion.
2. **ğŸ“Š Concrete Support**: Is presented with experimental results, data, or concrete examples.
3. **ğŸ“ Traceability**: Can be traced to specific methods, results, or discussion sections in the text.
4. **âŒ Exclusions**: Evidence must not be derived from the abstract or introduction sections of the text.

## âœ… Conclusion Analysis

- **âš–ï¸ Justification**: Evaluate whether the conclusions drawn in the text are justified by the evidence provided.

## ğŸ“‹ Annotation Format

Each annotation should be formatted as follows:

```json
{
    "Claim_id": "<unique_identifier>",
    "Claim_text": "<text_of_the_claim>",
    "Evidence_text": "<text_supporting_or_contradicting_the_claim>",
    "Justification_Conclusion": "<evaluator's_comment_on_evidence_justification>"
}
```

## ğŸ“ Folders and Files Description 

For each model in `{Gemini, Claude, GPT}`, the following folder structure is maintained:

### ğŸ“‚ `{model}_3_prompts/`
- Contains outputs from the 3-prompts code execution
- Each prompt runs independently for claims, evidence, and conclusions

### ğŸ“‚ `{model}_one_by_one/`
- Houses outputs from iterative building process:
  1. Get all claims first
  2. For each claim, fetch corresponding evidence
  3. For each claim-evidence pair, generate conclusion

### ğŸ“‚ `{model}_all_at_one/`
- Stores outputs from single-prompt execution
- All tasks (claims, evidence, conclusions) processed in one go

```
project/
â”œâ”€â”€ {model}_3_prompts/
â”œâ”€â”€ {model}_one_by_one/
â””â”€â”€ {model}_all_at_one/
```

*Note: Replace `{model}` with Gemini, Claude, or GPT accordingly.*


# ğŸ–Šï¸ Annotation tool README

# PDF Annotation Tool

A desktop application for annotating PDF documents with claims and corresponding evidence. This tool allows users to select text from PDF documents, organize them into claims and evidence pairs, and save the annotations in a structured JSON format.


![](annotation_tool_screenshot.png)
## âœ¨ Features

- **ğŸ“„ PDF Viewing**
  - Load and view PDF documents
  - Smooth scrolling and navigation
  - Zoom controls (buttons and keyboard shortcuts)
  - Automatic fit-to-width functionality

- **âœï¸ Text Selection**
  - Click and drag to select text
  - Multiple selections can be combined
  - Preview and edit selected text
  - Clear selection option

- **ğŸ“ Annotation Management**
  - Create claims from selected text
  - Add multiple pieces of evidence for each claim
  - Clear organization of claims and their corresponding evidence
  - Visual hierarchy for better understanding

- **ğŸ¨ User-Friendly Interface**
  - Intuitive controls
  - Visual feedback for actions
  - Status bar messages
  - Keyboard shortcuts


## ğŸ“ˆ Results
![](precision_recall_scatter_improved.png)

![](Sentence_dist_analysis.png)


## ğŸ’» Installation

### Prerequisites
- Python 3.7 or higher
- PyQt5
- PyMuPDF (fitz)

## ğŸ“š Citations

If you use RC_BENCH in your research, please cite:

```bibtex
@article{rcbench2024,
  title={Can AI Validate Science? Benchmarking LLMs for Accurate Scientific Claim -> Evidence Reasoning},
  author={SR Javaji et al.},
  journal={arxiv},
  year={2025}
}
```

