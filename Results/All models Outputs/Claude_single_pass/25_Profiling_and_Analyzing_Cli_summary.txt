Claim 1:
Type: result
Statement: The authors successfully extracted 10,393 scientific statements from IPCC AR6 reports with associated uncertainty levels and glossary terms
Location: Results section
Exact Quote: We obtained 10,393 statements, which is in excess of the 8,094 statements extracted by Lacombe, Wu, and Dilworth (2023)

Evidence:
- Evidence Text: Detailed breakdown of extracted statements showing 9,252 statements with confidence levels and 1,488 with likelihood levels
  Strength: strong
  Location: Overall statement profile section
  Limitations: Some statements may have been missed or incorrectly extracted
  Exact Quote: We denote the 10,393 statements as set S; the subset of 9,252 statements with confidence levels as set C = {s ∈ S, where sc ̸= ϕ}; the subset of 1,488 statements with likelihood levels as set L = {s ∈ S, where sl ̸= ϕ}

- Evidence Text: Comparison to previous work showing more comprehensive extraction
  Strength: moderate
  Location: Overall statement profile section
  Limitations: Does not fully validate accuracy of extraction
  Exact Quote: in excess of the 8,094 statements extracted by Lacombe, Wu, and Dilworth (2023)

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: high
Justification: The numerical evidence clearly shows successful extraction of a large number of statements, exceeding previous work
Key Limitations: Accuracy and completeness of extraction not fully validated

--------------------------------------------------

Claim 2:
Type: result
Statement: Over 90% of the statements in IPCC reports have confidence levels above medium, indicating they are confident scientific findings rather than hypotheses
Location: Overall statement profile section
Exact Quote: Over 90% of the overall statements have confidence levels above medium (i.e., medium, high, or very high)

Evidence:
- Evidence Text: Distribution analysis of confidence levels across reports
  Strength: strong
  Location: Overall statement profile section
  Limitations: Relies on correct extraction and classification of confidence levels
  Exact Quote: high confidence is the most common confidence level for statements in most chapters, except for those in the chapter bodies of the WGI and WGIII reports

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Clear quantitative evidence from comprehensive analysis of confidence levels
Key Limitations: Assumes accurate extraction and classification of confidence levels

--------------------------------------------------

Claim 3:
Type: result
Statement: The semantic similarity method for linking related statements achieves high precision but potentially low recall
Location: Case Study 1 section
Exact Quote: This outcome demonstrates the high precision of this semantic similarity-based method. However, the method may miss valid links as the recall is undetermined

Evidence:
- Evidence Text: Only one pair of statements found above similarity threshold
  Strength: moderate
  Location: Case Study 1 section
  Limitations: Limited test case focusing only on wetland-related statements
  Exact Quote: Table 2 shows the result that only one pair of statements from set D (of 12 wetland-related statements) and N are above the threshold

- Evidence Text: Manual verification of found pair confirms validity
  Strength: moderate
  Location: Case Study 1 section
  Limitations: Small sample size
  Exact Quote: Upon reading these statements, we confirm that they are essentially the same statement pitched at different levels of detail

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Evidence shows method works accurately but may be too restrictive
Key Limitations: Limited testing scope, recall not quantified

--------------------------------------------------

Claim 4:
Type: result
Statement: GPT-generated summaries of wetland restoration statements contain inaccuracies and hallucinations compared to extracted statements
Location: Case Study 3 section
Exact Quote: Unlike our method, the zero-shot GPT model often produces statements that cite inaccurate IPCC sections

Evidence:
- Evidence Text: Example of incorrect section citations
  Strength: strong
  Location: Case Study 3 section
  Limitations: Limited to specific examples
  Exact Quote: all three generated statements that cite 'WGII Section 6.5' are incorrect – The term 'wetland' does not appear in that section

- Evidence Text: RAG-based GPT still shows issues
  Strength: moderate
  Location: Case Study 3 section
  Limitations: Qualitative assessment
  Exact Quote: However, it still tends to excessively condense content and generate hallucinations, similar to the zero-shot GPT model

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Clear evidence of specific inaccuracies in GPT outputs compared to extracted statements
Key Limitations: Limited number of test cases examined

--------------------------------------------------

