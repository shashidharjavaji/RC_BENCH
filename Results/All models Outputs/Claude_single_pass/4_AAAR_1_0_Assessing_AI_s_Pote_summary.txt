Claim 1:
Type: result
Statement: Closed-source LLMs generally outperform open-source LLMs on the AAAR-1.0 benchmark tasks
Location: Abstract & Results sections
Exact Quote: Closed-source LLMs generally outperform open-source LLMs on AAAR-1.0

Evidence:
- Evidence Text: On EQINFER task, Claude 3.5 (61.10%), GPT-4 (49.85%), and o1 (59.49%) significantly outperformed open source models like Llama 3.1 (38.13%)
  Strength: strong
  Location: EQUATIONINFERENCE Results
  Limitations: Limited to one specific task type
  Exact Quote: ...closed-source LLMs generally achieve superior accuracy, probably owing to the richer scientific knowledge from the larger model parameters

- Evidence Text: On EXPDESIGN task, closed-source models showed better S-F1 scores (47-53%) compared to open source models (34-43%)
  Strength: strong
  Location: EXPERIMENTDESIGN Results
  Limitations: Specific to experimental design capabilities
  Exact Quote: For the experiment design, the closed-source LLMs generally outperform open-source LLMs

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Consistent performance advantage shown across multiple tasks with quantitative metrics
Key Limitations: Limited to research-oriented tasks, may not generalize to other domains

--------------------------------------------------

Claim 2:
Type: finding
Statement: Extending input modality or enlarging input context does not guarantee enhanced performance for LLMs
Location: Abstract & Results sections
Exact Quote: Contrary to human behaviour, neither extending the input modality (i.e., leveraging text and figures) nor enlarging the input context guarantees enhanced performance

Evidence:
- Evidence Text: Adding figure inputs decreased performance for GPT-4o from 53.00 to 50.11 S-F1 score on EXPDESIGN
  Strength: moderate
  Location: Multi-Modal Input Ablation results
  Limitations: Tested on limited number of models
  Exact Quote: w/ figures 50.11 48.94 51.59 58.53 27.87 34.30

- Evidence Text: Increasing context length beyond 1000 words showed no improvement for GPT-4-Turbo and GPT-4o
  Strength: moderate
  Location: Context Scaling Investigation
  Limitations: Specific to equation inference task
  Exact Quote: scaling up input length gradually boosts the performances at the first 1,000 words, but stabilizes afterwards

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Multiple experiments show lack of improvement with additional modalities/context
Key Limitations: Limited exploration of different modality types and context structures

--------------------------------------------------

Claim 3:
Type: finding
Statement: LLM-generated weaknesses often lack domain knowledge and tend to be vague
Location: Abstract & Results
Exact Quote: LLM-generated weaknesses often lack ample domain knowledge, especially on cutting-edge research topics, leading to the vague weaknesses applicable to various papers

Evidence:
- Evidence Text: Lower ITF-IDF scores for LLM-generated weaknesses compared to human reviewers (5.95 vs 7.69)
  Strength: strong
  Location: PAPERWEAKNESS Results
  Limitations: Metric may not capture all aspects of review quality
  Exact Quote: there is still a considerable gap in the weakness diversity between the LLMs and human experts

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Quantitative metrics support claim but could benefit from more detailed analysis
Key Limitations: Limited qualitative analysis of weakness content

--------------------------------------------------

