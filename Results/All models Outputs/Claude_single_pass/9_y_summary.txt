Claim 1:
Type: result
Statement: Image prompts cast into transformer embedding space do not encode interpretable semantics
Location: Section 3.1
Exact Quote: We decode image prompts aligned to the GPT-J embedding space into language, and measure their agreement with the input image and its human annotations

Evidence:
- Evidence Text: No significant difference between CLIPScores of real decoded prompts vs random embeddings
  Strength: strong
  Location: Section 3.1
  Limitations: Limited to CLIP-based evaluation metric
  Exact Quote: A Kolmogorov-Smirnov test shows no significant difference (D = .037, p > .5) between CLIPScore distributions comparing real decoded prompts and random embeddings to images

- Evidence Text: BERTScores show negligible difference between real and random prompts
  Strength: moderate
  Location: Table 2
  Limitations: BERTScore may not capture all semantic aspects
  Exact Quote: Real and random prompts are negligibly different, confirming that inputs to GPT-J do not readily encode interpretable semantics

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Multiple evaluation metrics (CLIP and BERT scores) consistently show no meaningful difference between real and random embeddings
Key Limitations: Relies on automated metrics rather than human evaluation

--------------------------------------------------

Claim 2:
Type: result
Statement: Multimodal neurons in transformer MLPs consistently translate specific visual concepts into related text
Location: Section 2.2
Exact Quote: To evaluate whether uk translates an image representation into semantically related text, we compare decoder(Wout[k]) to image content

Evidence:
- Evidence Text: CLIPScores of decoded neuron tokens competitive with GPT captions
  Strength: strong
  Location: Table 1
  Limitations: Limited to CLIP-based evaluation
  Exact Quote: tokens decoded from multimodal neurons perform competitively with GPT image captions on CLIPScore

- Evidence Text: IoU comparisons show better object segmentation than random neurons
  Strength: strong
  Location: Section 3.2
  Limitations: Limited to 12 COCO categories
  Exact Quote: across all categories, the receptive fields of multimodal neurons better segment the object in each image than randomly sampled neurons from the same layers

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Multiple quantitative measures (CLIPScore, IoU) demonstrate consistent concept-specific activation and translation
Key Limitations: Analysis limited to specific object categories and automated metrics

--------------------------------------------------

Claim 3:
Type: result
Statement: Multimodal neurons causally affect model output in image captioning
Location: Section 3.3
Exact Quote: To investigate how strongly multimodal neurons causally affect model output, we successively ablate units sorted by gk,c

Evidence:
- Evidence Text: Ablating top-scoring units decreases token probability by 80%
  Strength: strong
  Location: Section 3.3
  Limitations: Only measures effect on single token probability
  Exact Quote: ablating the same number of top-scoring units decreases token probability by 80% on average

- Evidence Text: Random unit ablation has minimal effect on token probability
  Strength: strong
  Location: Section 3.3
  Limitations: Control comparison only
  Exact Quote: When up to 6400 random units are ablated, we find that the probability of token c is largely unaffected

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: high
Justification: Clear causal effect demonstrated through ablation studies with appropriate controls
Key Limitations: Analysis focused primarily on token probabilities rather than overall caption quality

--------------------------------------------------

