Claim 1:
Type: performance
Statement: Using a Panel of LLm evaluators (PoLL) composed of smaller models correlates better with human judgements compared to a single large judge while being over 7x cheaper
Location: Section 1 Introduction
Exact Quote: We show that using an instantiation of PoLL correlates better with human judgements compared to a single large judge (GPT-4), while being over seven times cheaper

Evidence:
- Evidence Text: Cohen's kappa correlation results across QA datasets
  Strength: strong
  Location: Section 4.1, Table 1
  Limitations: Limited to specific QA tasks
  Exact Quote: PoLL has the strongest correlation across various tasks, while GPT-4 is one of the weaker evaluators on this particular task setup

- Evidence Text: Cost comparison between PoLL and GPT-4
  Strength: strong
  Location: Section 4.5
  Limitations: Costs may vary over time
  Exact Quote: running the entire three model PoLL is seven to eight times less expensive than running a single GPT-4 judge

- Evidence Text: Chatbot Arena correlation results
  Strength: strong
  Location: Table 2
  Limitations: Limited to one specific evaluation setting
  Exact Quote: PoLL is best correlated with the gold rankings, particularly at the top of the ranked list

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Multiple evaluation settings consistently show PoLL outperforming GPT-4 while having clear cost advantages
Key Limitations: Performance advantages vary by task type

--------------------------------------------------

Claim 2:
Type: result
Statement: PoLL reduces intra-model bias compared to single model judges
Location: Section 4.4
Exact Quote: We observe that overall, PoLL has the smallest spread in scores, with a standard deviation of 2.2, compared to EM and individual judges

Evidence:
- Evidence Text: Spread analysis on multi-hop datasets
  Strength: strong
  Location: Section 4.4
  Limitations: Limited to specific datasets
  Exact Quote: PoLL has the smallest spread in scores, with a standard deviation of 2.2, compared to EM and individual judges. GPT-3.5 has the highest spread, with a standard deviation of 6.1

- Evidence Text: Chatbot Arena ranking analysis
  Strength: moderate
  Location: Section 4.4
  Limitations: Single evaluation scenario
  Exact Quote: GPT-4 judge ranks another GPT-4 variant in position 2, higher than its actual position 4

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Quantitative evidence shows reduced variance and bias, but limited to specific evaluation settings
Key Limitations: Could benefit from more diverse evaluation scenarios

--------------------------------------------------

Claim 3:
Type: result
Statement: GPT-4 is a relatively weak judge in some scenarios and exhibits high variance with minor prompt changes
Location: Section 1 Introduction
Exact Quote: In some scenarios, GPT-4 is a relatively weak judge, exhibiting high variance with minor changes to the prompt

Evidence:
- Evidence Text: Prompt variation experiments
  Strength: strong
  Location: Section 4.3, Table 3
  Limitations: Limited to QA tasks
  Exact Quote: we can see how the correlation between GPT-4 and human annotators varies as the prompt changes

- Evidence Text: Performance on KILT evaluations
  Strength: moderate
  Location: Section 4.1
  Limitations: Specific to one type of task
  Exact Quote: GPT-4 is one of the weaker evaluators on this particular task setup

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Clear evidence of prompt sensitivity and weak performance in specific scenarios, though limited to certain tasks
Key Limitations: Findings may not generalize to all evaluation scenarios

--------------------------------------------------

