Claim 1:
Type: methodology
Statement: Original BERT layers fail to improve sentence embedding performance due to static token embedding bias and ineffective utilization of BERT layers.
Location: Observations & Introduction
Exact Quote: original BERT layers actually damage the quality of sentence embeddings. However, if we treat static token embeddings as word embedding, it still yields unsatisfactory results compared to GloVe

Evidence:
- Evidence Text: Static token embeddings averaging outperforms GloVe when biases are removed, showing the negative impact of embedding biases.
  Strength: strong
  Location: Observation 2 & Discussion
  Limitations: Does not account for the effectiveness of BERT's dynamic representations.
  Exact Quote: simply removing these biased tokens and using the average of the remaining token embeddings as sentence representation. It can outperform the Glove and even achieve results comparable to post-processing methods BERT-flow and BERT-whitening

- Evidence Text: Prompt-based sentence embeddings, particularly with denoising, effectively utilize BERT layers, avoiding embedding bias.
  Strength: strong
  Location: Template Denoising & Prompt Based Contrastive Learning
  Limitations: Manual generation of templates may not be optimal.
  Exact Quote: Prompt-based method can avoid embedding bias and utilize the original BERT layers. We find original BERT can achieve reasonable performance with the help of the template in sentence embeddings

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by both experimental results and logical analysis regarding the impact of embedding biases and the effectiveness of the proposed method.
Key Limitations: Reliance on manual templates and lack of comprehensive evaluation across diverse datasets.

--------------------------------------------------

Claim 2:
Type: performance
Statement: Prompt-based sentence embedding methods achieve significant improvements over traditional methods, including GloVe and BERT-flow.
Location: Non Fine-Tuned & Fine-Tuned BERT Results
Exact Quote: Using templates can substantially improve the results of original BERT on all datasets. Compared to pooling methods like averaging of last layer or averaging of first and last layers, our methods can improve spearman correlation by more than 10%

Evidence:
- Evidence Text: Comparative results on STS tasks show prompt-based methods outperforming GloVe, BERT-flow, and BERT-whitening.
  Strength: strong
  Location: Non Fine-Tuned BERT Results
  Limitations: May not extend to all NLP tasks or datasets beyond STS.
  Exact Quote: Compared to pooling methods like averaging of last layer or averaging of first and last layers, our methods can improve spearman correlation by more than 10%

- Evidence Text: PromptBERT achieves state-of-the-art results in unsupervised and supervised settings on STS tasks.
  Strength: strong
  Location: Fine-Tuned BERT Results & Stability in Unsupervised Contrastive Learning
  Limitations: Result stability and generalizability to other tasks not fully addressed.
  Exact Quote: Our method achieves state-of-the-art results in both unsupervised and supervised settings.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence from direct experimental comparisons and relative performance metrics strongly supports the claim of methodological advancements and performance improvements.
Key Limitations: Limited analysis on the impact of different types of prompts or evaluation on the broader range of NLP tasks.

--------------------------------------------------

