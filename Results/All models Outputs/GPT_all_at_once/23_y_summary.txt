Claim 1:
Type: contribution
Statement: MME is the first comprehensive MLLM Evaluation benchmark addressing both perception and cognition across 14 subtasks.
Location: Introduction section
Exact Quote: presenting the first comprehensive MLLM Evaluation benchmark MME [...] It measures both perception and cognition abilities on a total of 14 subtasks.

Evidence:
- Evidence Text: MME includes perception tasks such as object existence, count, position, and color recognition, as well as cognition tasks like commonsense reasoning, numerical calculation, text translation, and code reasoning.
  Strength: strong
  Location: Data Collection section
  Limitations: Only covers 14 subtasks, may not capture all aspects of MLLM capabilities.
  Exact Quote: Apart from OCR, the perception includes the recognition of coarse-grained and fine-grained objects. The cognition includes commonsense reasoning, numerical calculation, text translation, and code reasoning.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: Well-defined subtasks and manual design of instruction-answer pairs align well with claim, but broader aspects of MLLM capabilities may be underexplored.
Key Limitations: Limited to 14 specific subtasks; may not represent all MLLM capabilities comprehensively.

--------------------------------------------------

Claim 2:
Type: methodology
Statement: The design of MME aims to avoid prompt engineering and allows fair comparison among MLLMs.
Location: Instruction Design section
Exact Quote: The instructions of MME are designed concisely to avoid the impact of prompt engineering on the model output.

Evidence:
- Evidence Text: Instructions consist of a concise question followed by 'Please answer yes or no,' providing a uniform standard for evaluating models.
  Strength: strong
  Location: Instruction Design section
  Limitations: Simplicity may not capture nuances in more complex interactions or model abilities.
  Exact Quote: the instruction consists of two parts, including a concise question and a description 'Please answer yes or no.'

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The methodological precautions against prompt engineering support the claim, yet its simplicity might not fully test MLLMs' adaptability in varied contexts.
Key Limitations: Focused primarily on 'yes or no' responses; complexity of AI interactions might be understated.

--------------------------------------------------

Claim 3:
Type: result
Statement: Existing MLLMs exhibit significant gaps in following instructions, perception accuracy, reasoning capabilities, and avoiding object hallucination.
Location: Analysis section
Exact Quote: We conclude four common problems that largely affect the performance of MLLMs.

Evidence:
- Evidence Text: Issues include failure to adhere to concise instruction design, misidentifying objects or counting, incorrect reasoning despite having correct information, and answering based on non-existent objects.
  Strength: strong
  Location: Analysis section
  Limitations: Analysis based on specific tasks within MME; may not generalize across all tasks MLLMs could encounter.
  Exact Quote: inability to follow basic instructions, a lack of basic perception and reasoning, as well as object hallucination

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: Direct observation of MLLM performance on standardized tasks underlines significant deficiencies, thus strongly justifying the claim.
Key Limitations: Insights are confined to the scope of MME configuration and may not encompass the full range of multimodal interactions.

--------------------------------------------------

