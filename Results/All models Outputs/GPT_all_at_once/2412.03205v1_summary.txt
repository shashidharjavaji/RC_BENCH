Claim 1:
Type: contribution
Statement: The U-MATH benchmark fills a crucial gap in evaluating LLMs’ university-level mathematical capabilities, offering a diverse set of problems.
Location: Introduction/Section 1
Exact Quote: To address these gaps, we introduce U-MATH, a novel benchmark of 1,100 unpublished open-ended university-level problems sourced from teaching materials.

Evidence:
- Evidence Text: U-MATH consists of 1,100 university-level problems including a 20% focus on multimodal (image-based) problems.
  Strength: strong
  Location: Conclusion/Section 5
  Limitations: Does not cover full range of advanced topics and may introduce biases towards certain problem types and difficulty levels.
  Exact Quote: U-MATH includes 1,100 unpublished free-form problems from real teaching materials, covering 6 core mathematical subjects, with 20% involving image-based reasoning.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The provision of a wide array of problems across multiple mathematical disciplines and inclusion of multimodal problems justifies the utility and novelty of U-MATH in evaluating LLMs for university-level math.
Key Limitations: Limited scope in advanced topics beyond the provided disciplines could miss out on evaluating complex mathematical reasoning in underrepresented areas.

--------------------------------------------------

Claim 2:
Type: contribution
Statement: U-MATH leads to a more challenging and comprehensive LLM evaluation by including a significant portion of problems requiring image understanding.
Location: Section 3/Paragraph 2
Exact Quote: About 20% of problems require image understanding to be solved.

Evidence:
- Evidence Text: 225 of the U-MATH problems require visual elements for solving, aiming to mirror real-world scenarios and challenge models to handle both traditional and visual problem-solving.
  Strength: strong
  Location: Background/Section 2
  Limitations: Exclusion of non-textual problems from the µ-MATH meta-evaluation set could limit comprehension of LLM capabilities in visual reasoning.
  Exact Quote: Our U-MATH dataset improves on existing benchmarks with 225 of 1,100 university-level problems that require visual elements (graph, table, diagram) to be solved.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The inclusion of problems requiring visual elements addresses a critical gap in evaluating LLMs’ abilities to process and solve multimodal mathematical problems.
Key Limitations: The assessment of visual reasoning capabilities may not be fully captured due to the methodology of excluding image-based problems from the meta-evaluation set.

--------------------------------------------------

Claim 3:
Type: contribution
Statement: The µ-MATH meta-evaluation benchmark is designed to rigorously assess the quality of LLM judges on a subset of U-MATH problems, enhancing understanding of LLM evaluation capabilities.
Location: Section 3.3
Exact Quote: Additionally, we introduce a set of 1084 meta-evaluation tasks sourced from U-MATH problems and designed to rigorously assess the quality of LLM judges.

Evidence:
- Evidence Text: µ-MATH uses a binary classification task based on problem statement, reference answer, and a solution to evaluate, providing a focused approach to assess LLM judges with macro-averaged F1-score, PPV, TPR, NPV, and TNR.
  Strength: strong
  Location: Experiments and Results/Section 4.1
  Limitations: Focus on text-only problems for meta-evaluation may not fully ascertain LLMs' judgment accuracy across all types of mathematical problems, especially visual ones.
  Exact Quote: A tested model is provided with a problem statement, a reference answer, and a solution to evaluate.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: medium
Justification: The structured approach to create the µ-MATH benchmark with a clear evaluation metric effectively enables a deeper insight into the evaluative capabilities of LLM judges, highlighting a critical aspect of LLM performance.
Key Limitations: The exclusion of visual problems from the meta-evaluation might underrepresent the challenges in LLMs' judgment capabilities concerning multimodal mathematical reasoning.

--------------------------------------------------

