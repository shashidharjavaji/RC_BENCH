{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "In-Context RALM, a simple document reading mechanism can have a large impact, and that substantial gains can also be made by adapting the document ranking to the LM task.",
                "type": "Advancements",
                "location": "Introduction",
                "exact_quote": "Thus, we show that many of the benefits of RALM can be achieved while working with off-the-shelf LMs, even via API access."
            },
            "evidence": [
                {
                    "evidence_text": "In Section 4 we evaluate the application of off-the-shelf retrievers to our framework. In this minimal-effort setting, we found that In-Context RALM led to LM performance gains equivalent to increasing the LM\u2019s number of parameters by 2\u20133 across all of the text corpora we examined.",
                    "strength": "Strong",
                    "limitations": "None",
                    "location": "Introduction",
                    "exact_quote": "In Section 4 we evaluate the application of off-the-shelf retrievers to our framework. In this minimal-effort setting, we found that In-Context RALM led to LM performance gains equivalent to increasing the LM\u2019s number of parameters by 2\u20133 across all of the text corpora we examined."
                },
                {
                    "evidence_text": "In Section 6 we investigate methods for adapting document ranking to the LM task, a relatively under-explored RALM degree of freedom. Our adaptation methods range from using a small LM to perform zero-shot ranking of the retrieved documents, up to training a dedicated bidirectional reranker by employing self-supervision via the LM signal. These methods lead to further gains in the LM task corresponding to an additional size increase of 2 in the LM architecture.",
                    "strength": "Strong",
                    "limitations": "None",
                    "location": "Introduction",
                    "exact_quote": "In Section 6 we investigate methods for adapting document ranking to the LM task, a relatively under-explored RALM degree of freedom. Our adaptation methods range from using a small LM to perform zero-shot ranking of the retrieved documents, up to training a dedicated bidirectional reranker by employing self-supervision via the LM signal. These methods lead to further gains in the LM task corresponding to an additional size increase of 2 in the LM architecture."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "Using a BM25 retriever (Robertson and Zaragoza, 2009) to find the grounding documents yields significant gains",
                "type": "Advancements",
                "location": "Introduction",
                "exact_quote": "Specifically, we consider a simple but powerful RALM framework, dubbed In-Context RALM: leaving the LM architecture unchanged and prepending the selected documents to the input, without modifying the LM. Section 4 describes our experimental setup. To show the wide applicability of our framework, we performed LM experiments on a suite of five diverse corpora: WikiText-103 (Merity et al., 2016), RealNews (Zellers et al., 2019), and three datasets from The Pile (Gao et al., 2021): ArXiv, Stack Exchange, and FreeLaw. We use open-source LMs ranging from 110M to 66B parameters (from the GPT-2, GPT-Neo, OPT, and LLaMA model families). In Section 5 we evaluate the application of off-the-shelf retrievers to our framework. In this minimal-effort setting, we found that In-Context RALM led to LM performance gains equivalent to increasing the LM\u2019s number of parameters by 2\u20133 across all of the text corpora we examined. In Section 6 we investigate methods for adapting document ranking to the LM task, a relatively under-explored RALM degree of freedom. Our adaptation methods range from using a small LM to perform zero-shot ranking of the retrieved documents, up to training a dedicated bidirectional reranker by employing self-supervision via the LM signal. These methods lead to further gains in the LM task corresponding to an additional size increase of 2 in the LM architecture. As a concrete example of the gains, a 345M parameter GPT-2 enhanced by In-Context RALM outperforms a 762M parameter GPT-2 when employing an off-the-shelf BM25 retriever (Robertson and Zaragoza, 2009), and outperforms a 1.5B parameter GPT-2 when employing our trained LM-oriented reranker (see Figure 2)."
            },
            "evidence": [
                {
                    "evidence_text": "In Section 5 we evaluate the application of off-the-shelf retrievers to our framework. In this minimal-effort setting, we found that In-Context RALM led to LM performance gains equivalent to increasing the LM\u2019s number of parameters by 2\u20133 across all of the text corpora we examined.",
                    "strength": "Strong",
                    "limitations": "None",
                    "location": "Introduction",
                    "exact_quote": "In Section 5 we evaluate the application of off-the-shelf retrievers to our framework. In this minimal-effort setting, we found that In-Context RALM led to LM performance gains equivalent to increasing the LM\u2019s number of parameters by 2\u20133 across all of the text corpora we examined."
                },
                {
                    "evidence_text": "Table 2: Results of models from the LLaMA family, measured by word-level perplexity on the test set of WikiText-103.",
                    "strength": "Strong",
                    "limitations": "None",
                    "location": "Section 5",
                    "exact_quote": "Table 2: Results of models from the LLaMA family, measured by word-level perplexity on the test set of WikiText-103."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "Retrieval augmented language models (RALMs) add an operation that retrieves one or more documents from an external corpus, and condition the above LM predictions on these documents.",
                "type": "Advancements",
                "location": "Related Work",
                "exact_quote": "Retrieval augmented language models (RALMs) add an operation that retrieves one or more documents from an external corpus, and condition the above LM predictions on these documents."
            },
            "evidence": [
                {
                    "evidence_text": "Retrieval augmented language models (RALMs) add an operation that retrieves one or more documents from an external corpus, and condition the above LM predictions on these documents.",
                    "strength": "Strong",
                    "limitations": "None",
                    "location": "Related Work",
                    "exact_quote": "Retrieval augmented language models (RALMs) add an operation that retrieves one or more documents from an external corpus, and condition the above LM predictions on these documents."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "Prior work couples the retrieval stride s and the retrieval query length \u2113 (Borgeaud et al., 2022). In 5, we show that enforcing s = \u2113 degrades LM performance.",
                "type": "Advancements",
                "location": "3.1 In-Context RALM",
                "exact_quote": "Our experiments in this section provided us with a recommended configuration for applying In-Context RALM: applying a sparse BM25 retriever that receives \u2113 = 32 query tokens and is applied as frequently as possible. Practically, we retrieve every s = 4 tokens (\u2113 and s are defined in \u00a73). Table 1 shows for the GPT-2 models that across all the examined corpora, employing In-Context RALM with an off-the-shelf retriever improved LM perplexity to a sufficient extent that it matched that of a 2\u20133 larger model. Figure 4 and Tables 2 and 5 show that this trend holds across model sizes up to 66B parameters, for both WikiText-103 and RealNews. Prior work couples the retrieval stride s and the retrieval query length \u2113 (Borgeaud et al., 2022). In 5, we show that enforcing s = \u2113 degrades LM performance."
            },
            "evidence": [
                {
                    "evidence_text": "Prior work couples the retrieval stride s and the retrieval query length \u2113 (Borgeaud et al., 2022). In 5, we show that enforcing s = \u2113 degrades LM performance.",
                    "strength": "Strong",
                    "limitations": "None",
                    "location": "3.1 In-Context RALM",
                    "exact_quote": "Prior work couples the retrieval stride s and the retrieval query length \u2113 (Borgeaud et al., 2022). In 5, we show that enforcing s = \u2113 degrades LM performance."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "The fact that Wikipedia was part of their training data allowed us to investigate the usefulness of In-Context RALM for corpora seen during training.",
                "type": "Advancements",
                "location": "Experimental Details",
                "exact_quote": "The rest of the models brought two further benefits. First, they allowed us to investigate how our methods scale to models larger than GPT-2. Second, the fact that Wikipedia was part of their training data allowed us to investigate the usefulness of In-Context RALM for corpora seen during training. The helpfulness of such retrieval has been demonstrated for previous RALM methods (Khandelwal et al., 2020) and has also been justified theoretically by Levine et al. (2022c)."
            },
            "evidence": [
                {
                    "evidence_text": "The rest of the models brought two further benefits. First, they allowed us to investigate how our methods scale to models larger than GPT-2. Second, the fact that Wikipedia was part of their training data allowed us to investigate the usefulness of In-Context RALM for corpora seen during training. The helpfulness of such retrieval has been demonstrated for previous RALM methods (Khandelwal et al., 2020) and has also been justified theoretically by Levine et al. (2022c).",
                    "strength": "Strong",
                    "limitations": "None",
                    "location": "Experimental Details",
                    "exact_quote": "The rest of the models brought two further benefits. First, they allowed us to investigate how our methods scale to models larger than GPT-2. Second, the fact that Wikipedia was part of their training data allowed us to investigate the usefulness of In-Context RALM for corpora seen during training. The helpfulness of such retrieval has been demonstrated for previous RALM methods (Khandelwal et al., 2020) and has also been justified theoretically by Levine et al. (2022c)."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        }
    ],
    "execution_times": {
        "single_pass_analysis_time": "108.24 seconds",
        "total_execution_time": "326.85 seconds"
    }
}