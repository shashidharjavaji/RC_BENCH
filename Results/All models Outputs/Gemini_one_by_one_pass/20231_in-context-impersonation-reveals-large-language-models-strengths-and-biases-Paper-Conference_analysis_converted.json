{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "We propose a novel architecture-agnostic framework named Query-Relevant Neuron Cluster Attribution (QRNCA) designed to identify query-relevant neurons in LLMs.",
                "type": "",
                "location": "Introduction / Paragraph 2",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "We propose a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA), a novel architecture-agnostic framework capable of identifying query-relevant neurons in LLMs.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Section 3: Background",
                    "exact_quote": "We propose a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA), a novel architecture-agnostic framework capable of identifying query-relevant neurons in LLMs."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "QRNCA aims to extract Query-Relevant (QR) neurons for each input query.",
                "type": "",
                "location": "Introduction / Paragraph 2",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Our framework first resorts to the proxy task of Multi-Choice QA to deal with long-form texts. Starting with a given query, the framework employs Neuron Attribution to derive a QR Cluster. Each neuron within this cluster is assigned an attribution score that indicates its relevance to the query.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Methods, Section 4.1",
                    "exact_quote": "Our framework first resorts to the proxy task of Multi-Choice QA to deal with long-form texts. Starting with a given query, the framework employs Neuron Attribution to derive a QR Cluster. Each neuron within this cluster is assigned an attribution score that indicates its relevance to the query."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "The principal advantages of our framework are its architecture-agnostic nature and its capability of handling long-form text generation effectively.",
                "type": "",
                "location": "Introduction / Paragraph 2",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Our proposed method outperforms baseline approaches.",
                    "strength": "strong",
                    "limitations": "No limitations are mentioned.",
                    "location": "Section 5.1",
                    "exact_quote": "Table 3 presents the comparisons of different knowledge locating methods for Llama-2-7B. The metric here is the Probability Change Ratio (PCR) described in Section 5.1. Details are shown in Table A2 in the appendix. Our QRNCA method consistently outperforms other baselines, evidenced by its higher PCR. This indicates that our identified QR neurons significantly affect the probability of correct answers while exerting a relatively low impact on unrelated queries."
                },
                {
                    "evidence_text": "QRNCA achieves higher success rates than other baselines on knowledge editing.",
                    "strength": "strong",
                    "limitations": "No limitations are mentioned.",
                    "location": "Section 6.1",
                    "exact_quote": "Table 5 presents the successful rates of knowledge editing on our constructed language datasets. Our observations indicate that QRNCA achieves higher success rates than other baselines."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "Empirical evaluations demonstrate that our proposed method outperforms baseline approaches.",
                "type": "",
                "location": "Introduction / Paragraph 2",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "We evaluate the effectiveness of our detected neurons by building two multi-choice QA datasets spanning diverse domains and languages. Empirical evaluations demonstrate that our method outperforms baseline methods significantly. Further, analysis of neuron distributions reveals the presence of visible localized regions, particularly within different domains. Finally, we show potential applications of our detected neurons in knowledge editing and neuron-based prediction.",
                    "strength": "strong",
                    "limitations": "The evaluation is conducted on two self-constructed datasets and may not generalize to other datasets.",
                    "location": "Section 1, Introduction",
                    "exact_quote": "Empirical evaluations demonstrate that our method outperforms baseline methods significantly."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "Distinct localized regions emerge in the middle layers, particularly for domain-specific neurons.",
                "type": "",
                "location": "Results / Paragraph 4",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns in these layers.",
                    "strength": "strong",
                    "limitations": "The experimental setup may affect the specific neuron patterns observed.",
                    "location": "Section 5.4, paragraph 3",
                    "exact_quote": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns in these layers."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "Language-specific neurons are more sparsely distributed, indicating that LLMs likely draw on linguistic knowledge at all processing levels.",
                "type": "",
                "location": "Results / Paragraph 4",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Regarding language-specific neurons, their role in accessing linguistic knowledge across different layers likely accounts for their more sparse and distributed locations.",
                    "strength": "strong",
                    "limitations": "Linguistic knowledge may not be the only factor contributing to the sparse distribution.",
                    "location": "Section 5.4, Paragraph 2",
                    "exact_quote": ""
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 7,
            "claim": {
                "text": "Our main contribution is four-fold:",
                "type": "",
                "location": "Results / Paragraph 4",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "- A scalable method: we propose QRNCA to detect\nquery-relevant neurons in LLMs; the QRNCA method is\narchitecture-agnostic and can deal with long-form generations.",
                    "strength": "strong",
                    "limitations": "none stated",
                    "location": "Section 3 Background",
                    "exact_quote": "- A scalable method: we propose QRNCA to detect\nquery-relevant neurons in LLMs; the QRNCA method is\narchitecture-agnostic and can deal with long-form generations."
                },
                {
                    "evidence_text": "- Two new datasets: we curate two multi-choice\nQA datasets that contain different types of knowledge,\nnamely Domain Knowledge and Language knowledge.",
                    "strength": "strong",
                    "limitations": "none stated",
                    "location": "Section 3 Background",
                    "exact_quote": "- Two new datasets: we curate two multi-choice\nQA datasets that contain different types of knowledge,\nnamely Domain Knowledge and Language knowledge."
                },
                {
                    "evidence_text": "- In-depth studies: we visualize distributions of detected\nneurons and we are the first to show that there are visible\nlocalized regions in Llama.",
                    "strength": "strong",
                    "limitations": "none stated",
                    "location": "Section 3 Background",
                    "exact_quote": "- In-depth studies: we visualize distributions of detected\nneurons and we are the first to show that there are visible\nlocalized regions in Llama."
                },
                {
                    "evidence_text": "- Potential applications: we show that QRNCA might be useful for knowledge editing\nand neuron-based prediction.",
                    "strength": "strong",
                    "limitations": "none stated",
                    "location": "Section 3 Background",
                    "exact_quote": "- Potential applications: we show that QRNCA might be useful for knowledge editing\nand neuron-based prediction."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 8,
            "claim": {
                "text": "We propose QRNCA to detect query-relevant neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations.",
                "type": "",
                "location": "Results / Paragraph 4",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "We propose QRNCA to detect query-relevant neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Section 1 Introduction",
                    "exact_quote": "We propose QRNCA to detect query-relevant neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 9,
            "claim": {
                "text": "We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge.",
                "type": "",
                "location": "Results / Paragraph 4",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Section 2: Related Work",
                    "exact_quote": "We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 10,
            "claim": {
                "text": "We visualize distributions of detected neurons and we are the first to show that there are visible localized regions in Llama.",
                "type": "",
                "location": "Results / Paragraph 4",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Our findings indicate that distinct localized regions emerge in the middle layers, particularly for domain-specific neurons.",
                    "strength": "strong",
                    "limitations": "The study is limited to Llama-2-7B and Mistral-7B, and the results may not generalize to other LLMs.",
                    "location": "Section 5.4, Paragraph 2",
                    "exact_quote": "Our findings indicate that distinct localized regions emerge in the middle layers, particularly for domain-specific neurons. Conversely, language-specific neurons are more sparsely distributed, indicating that LLMs tend to complete the formation of domain-specific concepts within these middle layers. Conversely, language-specific neurons are more sparsely distributed, indicating that LLMs likely draw on linguistic knowledge at all processing levels."
                },
                {
                    "evidence_text": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains.",
                    "strength": "strong",
                    "limitations": "The study is limited to Llama-2-7B and Mistral-7B, and the results may not generalize to other LLMs.",
                    "location": "Section 5.4, Paragraph 2",
                    "exact_quote": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns. In contrast, language neurons are more sparsely distributed with smaller regions, and languages like Arabic and Russian exhibit less localized properties."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 11,
            "claim": {
                "text": "QRNCA might be useful for knowledge editing and neuron-based prediction.",
                "type": "",
                "location": "Results / Paragraph 4",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "We provide two usage examples to showcase the potential applications of our detected QR neurons: Knowledge Editing and Neuron-Based Prediction.",
                    "strength": "strong",
                    "limitations": "The examples are illustrative and may not generalize to all cases.",
                    "location": "Section 6 Potential Applications, Paragraph 1",
                    "exact_quote": "We provide two usage examples to showcase the potential applications of our detected QR neurons: Knowledge Editing and Neuron-Based Prediction."
                },
                {
                    "evidence_text": "For this goal, we adjust the values of QR neurons by either boosting or suppressing them to determine if we can change the prediction of a query from incorrect to correct or vice versa.",
                    "strength": "moderate",
                    "limitations": "The results may be sensitive to the choice of QR neurons and the amount of adjustment.",
                    "location": "Section 6.1 Knowledge Editing, Paragraph 2",
                    "exact_quote": "For this goal, we adjust the values of QR neurons by either boosting or suppressing them to determine if we can change the prediction of a query from incorrect to correct or vice versa."
                },
                {
                    "evidence_text": "Table 5 presents the success rates of knowledge editing on our constructed language datasets.",
                    "strength": "strong",
                    "limitations": "The results are based on a limited number of datasets and may not generalize to other languages or domains.",
                    "location": "Section 6.1 Knowledge Editing, Paragraph 3",
                    "exact_quote": "Table 5 presents the success rates of knowledge editing on our constructed language datasets."
                },
                {
                    "evidence_text": "The intuition behind neuron-based prediction is that for a domain-specific question, if the corresponding localized regions are properly activated, the LLM is more likely to generate truthful answers.",
                    "strength": "moderate",
                    "limitations": "The accuracy of neuron-based prediction may depend on the quality of the identified QR neurons and the complexity of the question.",
                    "location": "Section 6.2 Neuron-Based Prediction, Paragraph 1",
                    "exact_quote": "The intuition behind neuron-based prediction is that for a domain-specific question, if the corresponding localized regions are properly activated, the LLM is more likely to generate truthful answers."
                },
                {
                    "evidence_text": "The results are summarised in Table 6.",
                    "strength": "strong",
                    "limitations": "The results are based on a single LLM and may not generalize to other models.",
                    "location": "Section 6.2 Neuron-Based Prediction, Paragraph 3",
                    "exact_quote": "The results are summarised in Table 6."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "No robustness analysis available",
                "justification": "",
                "key_limitations": "No limitations analysis available",
                "confidence_level": ""
            }
        }
    ],
    "execution_times": {
        "single_pass_analysis_time": "1093.94 seconds",
        "total_execution_time": "2293.28 seconds"
    }
}