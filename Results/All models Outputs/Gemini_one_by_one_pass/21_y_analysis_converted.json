{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "We propose QRNCA to detect query-relevant neurons in LLMs.",
                "type": "",
                "location": "Introduction",
                "exact_quote": "We propose QRNCA to detect query-relevant neurons in LLMs."
            },
            "evidence": [
                {
                    "evidence_text": "We introduce a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA)",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Introduction, Paragraph 2",
                    "exact_quote": "We introduce a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA)"
                },
                {
                    "evidence_text": "QRNCA aims to extract Query-Relevant neurons for each input query.",
                    "strength": "moderate",
                    "limitations": "None stated",
                    "location": "Introduction, Paragraph 2",
                    "exact_quote": "QRNCA aims to extract Query-Relevant (QR) neurons for each input query."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "QRNCA can deal with long-form text generation.",
                "type": "",
                "location": "Introduction",
                "exact_quote": "QRNCA can deal with long-form text generation."
            },
            "evidence": [
                {
                    "evidence_text": "To address the first two questions, we introduce a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA) designed to identify query-relevant neurons in LLMs. The principal advantages of our framework are its architecture-agnostic nature and its capability of handling long-form text generation effectively, as shown in Table 1.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Introduction, third paragraph",
                    "exact_quote": "To address the first two questions, we introduce a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA) designed to identify query-relevant neurons in LLMs. The principal advantages of our framework are its architecture-agnostic nature and its capability of handling long-form text generation effectively, as shown in Table 1."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "There are visible localized regions of knowledge in LLMs.",
                "type": "",
                "location": "Introduction",
                "exact_quote": "There are visible localized regions of knowledge in LLMs."
            },
            "evidence": [
                {
                    "evidence_text": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains.",
                    "strength": "strong",
                    "limitations": "Note that our framework can be easily extended to larger-size LLMs.",
                    "location": "Section 5.4, Paragraph 2",
                    "exact_quote": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "QRNCA outperforms baseline approaches.",
                "type": "",
                "location": "Related Work",
                "exact_quote": "QRNCA outperforms baseline approaches."
            },
            "evidence": [
                {
                    "evidence_text": "Our QRNCA method consistently outperforms other baseline approaches, evidenced by its higher Probability Change Ratio (PCR). This indicates that our identified QR neurons significantly affect the probability of correct answers while exerting a relatively low impact on unrelated queries.",
                    "strength": "strong",
                    "limitations": "This method's performance is dependent on the quality and diversity of the constructed dataset.",
                    "location": "Section 5.3, Paragraph 2",
                    "exact_quote": "Our QRNCA method consistently outperforms other baseline approaches, evidenced by its higher Probability Change Ratio (PCR). This indicates that our identified QR neurons significantly affect the probability of correct answers while exerting a relatively low impact on unrelated queries."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "Common neurons are concentrated in the top layer.",
                "type": "",
                "location": "Locating Query-Relevant (QR) Neurons in Autoregressive LLMs",
                "exact_quote": "Common neurons are concentrated in the top layer."
            },
            "evidence": [
                {
                    "evidence_text": "Figure A2 shows that common neurons tend to appear at the top layer.",
                    "strength": "strong",
                    "limitations": "This finding is based on analysis of the Llama-2-7B model.",
                    "location": "Section A, Supplementary Material",
                    "exact_quote": "We also analyzed the token predicted by QR neurons, but we found that middle-layer neurons do not have a clear semantic meaning and human-readable concepts mostly appear in the top layer."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "QRNCA might be useful for knowledge editing and neuron-based prediction.",
                "type": "",
                "location": "Potential Applications",
                "exact_quote": "QRNCA might be useful for knowledge editing and neuron-based prediction."
            },
            "evidence": [
                {
                    "evidence_text": "Table 5 presents the success rates of knowledge editing on our constructed language datasets. Our observations indicate that QRNCA achieves higher success rates than other baselines.",
                    "strength": "strong",
                    "limitations": "the experiments were conducted on a constructed dataset, and the results may not generalize to real-world scenarios",
                    "location": "Section 6.1",
                    "exact_quote": "Table 5 presents the success rates of knowledge editing on our constructed language datasets. \u2206 measures how well we can flip the predictions (correct _incorrect or vice versa)."
                },
                {
                    "evidence_text": "The results are summarised in Table 6. We observe that the accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM).",
                    "strength": "moderate",
                    "limitations": "the experiments were conducted on a small dataset, and the results may not generalize to larger datasets",
                    "location": "Section 6.2",
                    "exact_quote": "Table 6: Accuracy of neuron-based prediction on selected domains in comparison with the standard prompt-based model prediction."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        }
    ],
    "execution_times": {
        "single_pass_analysis_time": "135.59 seconds",
        "total_execution_time": "1305.59 seconds"
    }
}