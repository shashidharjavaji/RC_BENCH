{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "We propose QRNCA, a novel architecture-agnostic framework capable of identifying query-relevant neurons in LLMs",
                "type": "",
                "location": "Abstract",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "We introduce a novel framework, QRNCA, for identifying query-relevant neurons in contemporary autoregressive language models.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 6, Conclusion",
                    "exact_quote": "\"We introduce a novel framework, QRNCA, for identifying query-relevant neurons in contemporary autoregressive language models."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "QRNCA allows for the examination of long-form answers beyond triplet facts by employing the proxy task of multi-choice question answering",
                "type": "",
                "location": "Abstract",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "To address the first two questions, we introduce a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA) designed to identify query-relevant neurons in LLMs. The principal advantages of our framework are its architecture-agnostic nature and its capability of handling long-form text generation effectively, as shown in Table 1.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Introduction section",
                    "exact_quote": "To address the first two questions, we introduce a novel framework named Query-Relevant Neuron Cluster Attribution (QRNCA) designed to identify query-relevant neurons in LLMs. The principal advantages of our framework are its architecture-agnostic nature and its capability of handling long-form text generation effectively, as shown in Table 1."
                },
                {
                    "evidence_text": "QRNCA aims to extract Query-Relevant (QR) neurons for each input query. The process begins by transforming an open-ended generation task into a multiple-choice question-answering format. By employing prompt engineering, we constrain LLMs to generate only the option letter rather than the complete answer. This approach allows for the examination of long-form generation beyond single tokens.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Methods section",
                    "exact_quote": "QRNCA aims to extract Query-Relevant (QR) neurons for each input query. The process begins by transforming an open-ended generation task into a multiple-choice question-answering format. By employing prompt engineering, we constrain LLMs to generate only the option letter rather than the complete answer. This approach allows for the examination of long-form generation beyond single tokens."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "There are visible localized regions in LLMs, particularly within different domains",
                "type": "",
                "location": "Abstract",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns.",
                    "strength": "strong",
                    "limitations": "This finding is based on the constructed dataset and the used LLM (Llama-2-7B). The results may vary with different datasets and models.",
                    "location": "Section 5.4, paragraph 3",
                    "exact_quote": "The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns."
                },
                {
                    "evidence_text": "In contrast, language-specific neurons are more sparsely distributed with smaller regions, and languages like Arabic and Russian exhibit less localized properties.",
                    "strength": "strong",
                    "limitations": "This finding is based on the constructed dataset and the used LLM (Llama-2-7B). The results may vary with different datasets and models.",
                    "location": "Section 5.4, paragraph 3",
                    "exact_quote": "In contrast, language-specific neurons are more sparsely distributed with smaller regions, and languages like Arabic and Russian exhibit less localized properties."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "Our method outperforms baseline methods significantly",
                "type": "",
                "location": "Introduction",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Our QRNCA method consistently outperforms other baseline methods, evidenced by its higher PCR.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Section 5.3, Paragraph 3",
                    "exact_quote": "Our QRNCA method consistently outperforms other baseline methods, evidenced by its higher PCR."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "Two new datasets are curated: Domain Knowledge and Language knowledge",
                "type": "",
                "location": "Introduction",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 2. Related Work, Paragraph 2.1. Locating Knowledge in LLMs",
                    "exact_quote": "We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "Visible localized regions emerge in the middle layers, particularly for domain-specific neurons",
                "type": "",
                "location": "Introduction",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns.",
                    "strength": "strong",
                    "limitations": "The study is limited to two LLM models and two datasets, so the findings may not generalize to other models or datasets.",
                    "location": "Section 5.4, paragraph 2",
                    "exact_quote": "The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 7,
            "claim": {
                "text": "Language-specific neurons are more sparsely distributed, indicating that LLMs likely draw on linguistic knowledge at all processing levels",
                "type": "",
                "location": "Introduction",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "We observed that common neurons are concentrated in the top layer, predominantly expressing frequently used tokens.",
                    "strength": "strong",
                    "limitations": "The analysis was performed on a specific LLM (Llama-2-7B) and might not generalize to other models.",
                    "location": "Section 5.5 The Function of Common Neurons",
                    "exact_quote": "We observed that common neurons are concentrated in the top layer, predominantly expressing frequently used tokens."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 8,
            "claim": {
                "text": "Common neurons are concentrated in the top layer, predominantly expressing frequently used tokens",
                "type": "",
                "location": "Introduction",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Furthermore, Figure A2 in the Supplementary Material (SM) illustrates the geographical locations of the common neurons. Common neurons tend to appear at the top layer (as shown in Figure A2 in the SM).",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Section 5.3: QR Neurons Can Impact the Knowledge Expression, Paragraph 2",
                    "exact_quote": "Furthermore, Figure A2 in the Supplementary Material (SM) illustrates the geographical locations of the common neurons. Common neurons tend to appear at the top layer (as shown in Figure A2 in the SM)."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 9,
            "claim": {
                "text": "QRNCA might be useful for knowledge editing and neuron-based prediction",
                "type": "",
                "location": "Introduction",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "The success rate of knowledge editing on our constructed language datasets.",
                    "strength": "strong",
                    "limitations": "The study is limited to a specific set of language datasets and may not generalize to other types of knowledge or different languages.",
                    "location": "Section 6.1 Knowledge Editing",
                    "exact_quote": null
                },
                {
                    "evidence_text": "The accuracy of neuron-based prediction on selected domains in comparison with the standard prompt-based model prediction.",
                    "strength": "moderate",
                    "limitations": "The study is limited to a specific set of domains and may not generalize to other types of knowledge or different domains.",
                    "location": "Section 6.2 Neuron-Based Prediction",
                    "exact_quote": null
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        }
    ],
    "execution_times": {
        "single_pass_analysis_time": "1033.22 seconds",
        "total_execution_time": "15296.64 seconds"
    }
}