{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "We introduce Query-Relevant Neuron Cluster Attribution (QRNCA), a novel architecture-agnostic framework capable of identifying query-relevant neurons in LLMs.",
                "type": "",
                "location": "Introduction",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "We propose QRNCA to detect QR neurons for each input query. The process begins by transforming an open-ended generation task into a multiple-choice question-answering format.",
                    "strength": "moderate",
                    "limitations": "The effectiveness of QRNCA depends on the quality of the multiple-choice question-answering format.",
                    "location": "Section 4.1 Multi-Choice QA Transformation",
                    "exact_quote": "We propose QRNCA to detect QR neurons for each input query. The process begins by transforming an open-ended generation task into a multiple-choice-question-answering format. By employing prompt engineering, we constrain LLMs to generate only the option letter rather than the complete answer. This approach allows for the examination of long-form generation beyond single tokens."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "QRNCA allows for the examination of long-form answers beyond triplet facts by employing the proxy task of multi-choice question answering.",
                "type": "",
                "location": "Introduction",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "To evaluate the effectiveness of our detected neurons, we build two multi-choice QA datasets spanning diverse domains and languages. Empirical evaluations demonstrate that our method outperforms baseline methods significantly.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 1 Introduction",
                    "exact_quote": "To evaluate the effectiveness of our detected neurons, we build two multi-choice QA datasets spanning diverse domains and languages. Empirical evaluations demonstrate that our method outperforms baseline methods significantly."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "There are visible localized regions in LLMs analogous to the localized functional regions observed in human brains.",
                "type": "",
                "location": "Introduction",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Section 5.4",
                    "exact_quote": "The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains."
                },
                {
                    "evidence_text": "Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Section 5.4",
                    "exact_quote": "Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns."
                },
                {
                    "evidence_text": "In contrast, language neurons are more sparsely distributed and languages like Arabic and Russian exhibit less localized properties.",
                    "strength": "strong",
                    "limitations": "None stated",
                    "location": "Section 5.4",
                    "exact_quote": "In contrast, language neurons are more sparsely distributed and languages like Arabic and Russian exhibit less localized properties."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "Our main contribution is four-fold:",
                "type": "",
                "location": "Related Work",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "We propose a novel gradient-based attribution method aimed at locating input-output knowledge within LLMs.",
                    "strength": "strong",
                    "limitations": "no limitations stated",
                    "location": "Section 3",
                    "exact_quote": "We propose a novel gradient-based attribution method aimed at locating input-output knowledge within LLMs."
                },
                {
                    "evidence_text": "Unlike previous methodologies, our approach mainly focuses on long-form texts beyond entity facts.",
                    "strength": "strong",
                    "limitations": "no limitations stated",
                    "location": "Section 3",
                    "exact_quote": "Unlike previous methodologies, our approach mainly focuses on long-form texts beyond entity facts."
                },
                {
                    "evidence_text": "We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge.",
                    "strength": "strong",
                    "limitations": "no limitations stated",
                    "location": "Section 3",
                    "exact_quote": "We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge."
                },
                {
                    "evidence_text": "We visualize distributions of detected neurons and we are the first to show that there are visible localized regions in Llama.",
                    "strength": "strong",
                    "limitations": "no limitations stated",
                    "location": "Section 3",
                    "exact_quote": "We visualize distributions of detected neurons and we are the first to show that there are visible localized regions in Llama."
                },
                {
                    "evidence_text": "In summary, our main contribution is four-fold: (1) A scalable method: we propose QRNCA to detect knowledge-relevant neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations. (2) Two new datasets: we curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge. (3) In-depth studies: we visualize distributions of detected neurons and we are the first to show that there are visible localized regions in Llama. (4) Potential applications: we show that QRNCA might be useful for knowledge editing and neuron-based prediction.",
                    "strength": "strong",
                    "limitations": "no limitations stated",
                    "location": "Section 3",
                    "exact_quote": "In summary, our main contribution is four-fold: (1) A scalable method: we propose QRNCA to detect knowledge-relevant neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations. (2) Two new datasets: we curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge. (3) In-depth studies: we visualize distributions of detected neurons and we are the first to show that there are visible localized regions in Llama. (4) Potential applications: we show that QRNCA might be useful for knowledge editing and neuron-based prediction."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "We propose QRNCA to detect query-relevant neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations.",
                "type": "",
                "location": "Related Work",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "We propose QRNCA to detect query-relevant neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations.",
                    "strength": "weak",
                    "limitations": "No empirical evidence is provided",
                    "location": "Introduction\\n### 1 Introduction",
                    "exact_quote": "We propose QRNCA to detect query-relevant neurons in LLMs; the QRNCA method is architecture-agnostic and can deal with long-form generations."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "We curate two multi-choice QA datasets that contain different types of knowledge, namely Domain Knowledge and Language knowledge.",
                "type": "",
                "location": "Related Work",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "We construct two datasets to locate knowledge neurons that cover two different categories: sub_ject domains and languages._",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Section 5.1 Experimental Settings",
                    "exact_quote": ""
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 7,
            "claim": {
                "text": "We visualize distributions of detected neurons and we are the first to show that there are visible localized regions in Llama.",
                "type": "",
                "location": "Related Work",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "The geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns.",
                    "strength": "strong",
                    "limitations": "None stated.",
                    "location": "Results, Section 5.4",
                    "exact_quote": "Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        },
        {
            "claim_id": 8,
            "claim": {
                "text": "QRNCA might be useful for knowledge editing and neuron-based prediction.",
                "type": "",
                "location": "Related Work",
                "exact_quote": ""
            },
            "evidence": [
                {
                    "evidence_text": "Apart from using the metric of PCR in Section 5.3, we are also interested in whether the detected QR neurons can be used for knowledge editing. For this goal, we adjust the values of QR neurons by either boosting or suppressing them to determine if we can change the prediction of a query from incorrect to correct or vice versa. Table 5 presents the success rates of knowledge editing on our constructed language datasets. Our observations indicate that QRNCA achieves higher success rates than other baselines.",
                    "strength": "strong",
                    "limitations": "The experiments are conducted on a constructed language dataset, which may not generalize to real-world scenarios.",
                    "location": "Section 6.1 Knowledge Editing",
                    "exact_quote": "Apart from using the metric of PCR in Section 5.3, we are also interested in whether the detected QR neurons can be used for knowledge editing. For this goal, we adjust the values of QR neurons by either boosting or suppressing them to determine if we can change the prediction of a query from incorrect to correct or vice versa. Table 5 presents the success rates of knowledge editing on our constructed language datasets. Our observations indicate that QRNCA achieves higher success rates than other baselines."
                },
                {
                    "evidence_text": "The intuition behind neuron-based prediction is that for a domain-specific question, if the corresponding localized regions are properly activated, the LLM is more likely to generate truthful answers. Otherwise, the LLM may produce hallucinated answers. To this end, we test whether the correct answers to domain-specific questions can be predicted solely based on the activity of the associated neurons.",
                    "strength": "strong",
                    "limitations": "The experiments are conducted on a specific constructed MMLU validation set, which may not generalize to other datasets.",
                    "location": "Section 6.2 Neuron-Based Prediction",
                    "exact_quote": "The intuition behind neuron-based prediction is that for a domain-specific question, if the corresponding localized regions are properly activated, the LLM is more likely to generate truthful answers. Otherwise, the LLM may produce hallucinated answers. To this end, we test whether the correct answers to domain-specific questions can be predicted solely based on the activity of the associated neurons."
                },
                {
                    "evidence_text": "Since we harvest QR neurons for queries in different subject domains, we can group all neurons for a domain to obtain a set of domain-specific neurons. We experiment on a specifically constructed MMLU (Hendrycks et al., 2020) validation set with a different set of questions than those used to determine the QR neurons (see Section B in the SM for details on our experimental strategy). The results are summarised in Table 6. We observe that the accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM). This suggests that the activity of identified neurons can reflect the model\u2019s reasoning process to some extent. Investigating how this finding could be leveraged in applications like fact-checking and hallucination detection presents a promising line of future work.",
                    "strength": "strong",
                    "limitations": "The experiments are conducted on a specific constructed MMLU validation set, which may not generalize to other datasets.",
                    "location": "Section 6.2 Neuron-Based Prediction",
                    "exact_quote": "Since we harvest QR neurons for queries in different subject domains, we can group all neurons for a domain to obtain a set of domain-specific neurons. We experiment on a specifically constructed MMLU (Hendrycks et al., 2020) validation set with a different set of questions than those used to determine the QR neurons (see Section B in the SM for details on our experimental strategy). The results are summarised in Table 6. We observe that the accuracy of the neuron-based predictions is very close to the accuracy of the prompt-based method of using the entire model (the used templates are shown in Table A3 in the SM). This suggests that the activity of identified neurons can reflect the model\u2019s reasoning process to some extent. Investigating how this finding could be leveraged in applications like fact-checking and hallucination detection presents a promising line of future work."
                }
            ],
            "evaluation": {
                "conclusion_justified": false,
                "robustness": "",
                "justification": "",
                "key_limitations": "",
                "confidence_level": ""
            }
        }
    ],
    "execution_times": {
        "single_pass_analysis_time": "1707.78 seconds",
        "total_execution_time": "1707.78 seconds"
    }
}