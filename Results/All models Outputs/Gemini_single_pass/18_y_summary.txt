Claim 1:
Type: result
Statement: In-Context RALM leads to substantial LM gains across our diverse evaluation suite.
Location: ## 5 The Effectiveness of In-Context RALM with Off-the-Shelf Retrievers
Exact Quote: We now empirically show that despite its simple document reading mechanism, In-Context RALM leads to substantial LM gains across our diverse evaluation suite.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: This claim is supported by the experimental results presented in the paper, which show that In-Context RALM consistently improves LM performance across a variety of datasets and model sizes.
Key Limitations: The evaluation is based on a limited number of datasets and models, and it is possible that the results may not generalize to other settings.

--------------------------------------------------

Claim 2:
Type: result
Statement: Employing In-Context RALM with an off-the-shelf retriever improved LM perplexity to a sufficient extent that it matched that of a 2-3 larger model.
Location: ## 5 The Effectiveness of In-Context RALM with Off-the-Shelf Retrievers
Exact Quote: We begin in this section by investigating the effectiveness of off-the-shelf retrievers for In-Context RALM; we go on in 6 to show that further LM gains can be made by tailoring document rankingfunctionstotheLMtask. The experiments in this section provided us with a recommended configuration for applying In-Context RALM: applying a sparse BM25 retriever that receives ℓ = 32 query tokens and is applied as frequently as possible. Practically, we retrieve every s = 4 tokens (ℓ and s are defined in 3). Table 1 shows for the GPT-2 models that across all the examined corpora, employing In-Context RALM with an off-the-shelf retriever improved LM perplexity to a sufficient extent that it matched that of a 2–3 larger model.

Evidence:
- Evidence Text: Table 1 shows for the GPT-2 models that across all the examined corpora, employing In-Context RALM with an off-the-shelf retriever improved LM perplexity to a sufficient extent that it matched that of a 2–3 larger model.
  Strength: strong
  Location: ## 5 The Effectiveness of In-Context RALM with Off-the-Shelf Retrievers
  Limitations: The results are specific to GPT-2 models and may not generalize to other model architectures.
  Exact Quote: Table 1 shows for the GPT-2 models that across all the examined corpora, employing In-Context RALM with an off-the-shelf retriever improved LM perplexity to a sufficient extent that it matched that of a 2–3 larger model.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: This claim is supported by the results presented in Table 1, which show that In-Context RALM with an off-the-shelf retriever consistently reduces LM perplexity across a variety of datasets and model sizes.
Key Limitations: The evaluation is based on a limited number of datasets and models, and it is possible that the results may not generalize to other settings.

--------------------------------------------------

Claim 3:
Type: result
Statement: BM25 Outperforms Off-the-Shelf Neural Retrievers in Language Modeling
Location: ## 5 The Effectiveness of In-Context RALM with Off-the-Shelf Retrievers
Exact Quote: We experimented with different off-the-shelf general purpose retrievers, and found that the sparse (lexical) BM25 retriever (Robertson and Zaragoza, 2009) outperformed three popular dense (neural) retrievers: the self-supervised retrievers Contriever (Izacard et al., 2022a) and Spider (Ram et al., 2022), as well as a retriever based on the average pooling of BERT embeddings that was used in the RETRO system (Borgeaud et al., 2022).

Evidence:
- Evidence Text: We experimented with different off-the-shelf general purpose retrievers, and found that the sparse (lexical) BM25 retriever (Robertson and Zaragoza, 2009) outperformed three popular dense (neural) retrievers: the self-supervised retrievers Contriever (Izacard et al., 2022a) and Spider (Ram et al., 2022), as well as a retriever based on the average pooling of BERT embeddings that was used in the RETRO system (Borgeaud et al., 2022).
  Strength: strong
  Location: ## 5 The Effectiveness of In-Context RALM with Off-the-Shelf Retrievers
  Limitations: The evaluation is based on a limited number of datasets and models, and it is possible that the results may not generalize to other settings.
  Exact Quote: We experimented with different off-the-shelf general purpose retrievers, and found that the sparse (lexical) BM25 retriever (Robertson and Zaragoza, 2009) outperformed three popular dense (neural) retrievers: the self-supervised retrievers Contriever (Izacard et al., 2022a) and Spider (Ram et al., 2022), as well as a retriever based on the average pooling of BERT embeddings that was used in the RETRO system (Borgeaud et al., 2022).

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: This claim is supported by the experimental results presented in the paper, which show that BM25 consistently outperforms neural retrievers in terms of LM perplexity.
Key Limitations: The evaluation is based on a limited number of datasets and models, and it is possible that the results may not generalize to other settings.

--------------------------------------------------

Claim 4:
Type: performance
Statement: In-Context RALM outperforms the neural alternatives.
Location: Introduction
Exact Quote: This result renders In-Context RALM even more appealing since applying a BM25 retriever is significantly cheaper than the neural alternatives.

Evidence:
- Evidence Text: In-Context RALM achieves better results than neural retrieval alternatives.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: This result renders In-Context RALM even more appealing since applying a BM25 retriever is significantly cheaper than the neural alternatives.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by strong evidence.
Key Limitations: None

--------------------------------------------------

Claim 5:
Type: result
Statement: Frequent retrieval improves language modeling.
Location: 5.2 Frequent Retrieval Improves Language Modeling
Exact Quote: We investigated the effect of varying the retrieval stride s (i.e., the number of tokens between consecutive retrieval operations).

Evidence:
- Evidence Text: LM performance improved as the retrieval operation became more frequent.
  Strength: strong
  Location: 5.2 Frequent Retrieval Improves Language Modeling
  Limitations: None
  Exact Quote: LM performance improved as the retrieval operation became more frequent.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by strong evidence.
Key Limitations: None

--------------------------------------------------

Claim 6:
Type: methodology
Statement: There is an optimal query length for BM25 retrieval.
Location: 5.3 A Contextualization vs. Recency Tradeoff in Query Length
Exact Quote: We also investigated the effect of varying ℓ, the length of the retrieval query for BM25.

Evidence:
- Evidence Text: Figure 6 reveals an interesting tradeoff and a sweet spot around a query length of 32 tokens.
  Strength: strong
  Location: 5.3 A Contextualization vs. Recency Tradeoff in Query Length
  Limitations: None
  Exact Quote: Figure 6 reveals an interesting tradeoff and a sweet spot around a query length of 32 tokens.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by strong evidence.
Key Limitations: None

--------------------------------------------------

Claim 7:
Type: result
Statement: LM-oriented reranking can further improve In-Context RALM.
Location: 6 Improving In-Context RALM with LM-Oriented Reranking
Exact Quote: Since In-Context RALM uses a fixed document reading component by definition, it is natural to ask whether performance can be improved by specializing its document retrieval mechanism to the LM task.

Evidence:
- Evidence Text: Both zero-shot reranking and predictive reranking yield significant gains over the baseline In-Context RALM.
  Strength: strong
  Location: 6 Improving In-Context RALM with LM-Oriented Reranking
  Limitations: None
  Exact Quote: Table 1 shows the results of letting the LM perform zero-shot reranking on the top-16 documents retrieved by BM25 (third row for each of the models). It is evident that reranking yielded consistently better results than simply taking the first result returned by the retriever.\n\nTable 3 shows that a small LM (GPT-2 117M) can be used to rerank the documents for all larger GPT-2 models, with roughly the same performance as having each LM perform reranking for itself, supporting the applicability of this method for LMs that are only accessible via an API.\n\nNext, we trained a reranker to choose one of the top-k documents retrieved by BM25. We refer to this approach as Predictive Reranking, since the reranker learns to choose which document will help in ‘‘predicting’’ the upcoming text. For this process, we assume availability of training data from the target corpus. Our reranker is a classifier that gets a prefix x≤s·j and a document di (for i ∈ [k]), and produces a scalar f (x≤s·j, di) that should resemble the relevance of di for the continuation of x≤s·j. We then normalize these relevance scores:\n\nprank(di|x≤s·j) = ke−exp(f (x≤s·j, di))i′=1 [exp(f [(x≤s·j, di′)])]\n\nand choose the document dˆi such that\nˆi = arg maxprank(di|x≤s·j).\ni∈[k]\nResults Table 1 shows the result of our predictive reranker, trained on WikiText-103. Specifically, we trained it with data produced by GPT-2 110M (S), and tested its effectiveness for all GPT-2 models. We observed significant gains obtained from Predictive Reranking. For example, the perplexity of GPT-2 110M (S) improved from 29.6 to 26.8, and that of GPT-2 1.5B (XL) improved from 16.6 to 15.4. This trend held for the other two models as well. Overall, these results demonstrate that training a reranker with domain-specific data was more effective than zero-shot reranking (Section 6.1). Note that these results—while impressive—still leave room for further improvements, compared to the top-16 BM25 oracle results (see Figure 7). Moreover, the oracle results themselves can be improved by retrieving k > 16 documents via a BM25 retriever, or by training stronger retrievers dedicated to the RALM task. We leave this direction for future work.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by strong evidence.
Key Limitations: None

--------------------------------------------------

Claim 8:
Type: result
Statement: In-Context RALM can be applied to open-domain question answering without additional training.
Location: 7 In-Context RALM for Open-Domain Question Answering
Exact Quote: To test its efficacy in additional scenarios, and specifically downstream tasks, we now turn to evaluate In-Context RALM on open-domain question answering (ODQA; Chen et al., 2017).

Evidence:
- Evidence Text: In-Context RALM shows significant improvements in exact match accuracy on Natural Questions and TriviaQA datasets, without any additional training.
  Strength: strong
  Location: 7 In-Context RALM for Open-Domain Question Answering
  Limitations: None
  Exact Quote: Table 4: Zero-shot results of In-Context RALM on the test set of Natural Questions and TriviaQA measured by exact match. In the open-book setting, we include the top two documents returned by DPR.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by strong evidence.
Key Limitations: None

--------------------------------------------------

