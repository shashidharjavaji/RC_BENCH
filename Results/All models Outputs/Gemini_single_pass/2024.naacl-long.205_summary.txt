Claim 1:
Type: performance
Statement: When the input length scales to 4,000 tokens, most open-source models rapidly deteriorates to random guess level.
Location: Section 5
Exact Quote: When the input length scales to 4,000 tokens, most open-source models rapidly deteriorates to random guess level.

Evidence:
- Evidence Text: Table 10 shows that the performance of GPT-4-Turbo on BestAnswer decreases more dramatically than NarrativeQA and GovReport when text is truncated.
  Strength: strong
  Location: Section 4
  Limitations: The results are based on a single benchmark and may not generalize to other tasks or datasets.
  Exact Quote: From the table 10, the performance of GPT-4Turbo on BestAnswer decreases more dramatically than NarrativeQA and GovReport when text is truncated.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence from Table 10 supports the claim that the performance of open-source models deteriorates rapidly when the input length scales to 4,000 tokens.
Key Limitations: The results are based on a single benchmark and may not generalize to other tasks or datasets.

--------------------------------------------------

Claim 2:
Type: performance
Statement: Proprietary models are also severely limited when it comes to ultra-long settings (32,000+ tokens), and none notably outperforms the random baseline.
Location: Section 5
Exact Quote: In the meanwhile, the capability of proprietary models is also severely limited, \nWhen it comes to the ultra-long setting (32,000+ tokens), no proprietary model notably outperforms the random baseline.

Evidence:
- Evidence Text: Table 10 shows that the performance of all models on the ultra-long setting (32,000+ tokens) is close to the random baseline.
  Strength: strong
  Location: Section 4
  Limitations: The results are based on a single benchmark and may not generalize to other tasks or datasets.
  Exact Quote: When it comes to the ultra-long setting (32,000+ tokens), no proprietary model notably outperforms the random baseline.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence from Table 10 supports the claim that proprietary models are also severely limited when it comes to ultra-long settings (32,000+ tokens).
Key Limitations: The results are based on a single benchmark and may not generalize to other tasks or datasets.

--------------------------------------------------

Claim 3:
Type: methodology
Statement: We present instructions of both tasks within Ada-LEval. To ensure that models know what to do, we contain the sample input and output format that models need to follow in solving problems.
Location: Evaluation Setups
Exact Quote: We present instructions of both tasks within Ada-LEval. To ensure that models know what to do, we contain the sample input and output format that models need to follow in solving problems.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: This claim is justified as providing clear instructions to models on how to solve problems in Ada-LEval can help improve the quality of results.
Key Limitations: The paper does not provide specific examples of the sample input and output formats.

--------------------------------------------------

Claim 4:
Type: methodology
Statement: Our experiments for open-source LLMs are conducted on NVIDIA A100 80GB GPU. The entire evaluation consumes around 800 GPU-hours.
Location: Evaluation Setups
Exact Quote: Our experiments for open-source LLMs are conducted on NVIDIA A100 80GB GPU. The entire evaluation consumes around 800 GPU-hours.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: This claim is justified as it provides specific details about the computational resources used for the evaluation.
Key Limitations: The paper does not provide a detailed breakdown of the GPU-hours consumed by different models.

--------------------------------------------------

Claim 5:
Type: methodology
Statement: To ensure that evaluation results on 200-testcase subset is valid, Table 12 and Table 13 display results on 200-testcase subset.
Location: Validity of 200-testcase subset
Exact Quote: To ensure that evaluation results on 200-testcase subset is valid, Table 12 and Table 13 display results on 200-testcase subset.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: This claim is justified as providing results on a smaller subset of test cases can help identify any potential issues with the evaluation process.
Key Limitations: The paper does not provide a detailed analysis of the differences between the results on the 200-testcase subset and the full test set.

--------------------------------------------------

