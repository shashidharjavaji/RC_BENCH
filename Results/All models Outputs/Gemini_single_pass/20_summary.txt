Claim 1:
Type: methodology
Statement: Cognitive biases as motivation to hypotheses and experiments that identify erroneous behavior of large language model.
Location: ### Abstract
Exact Quote: To hypothesize and test for such qualitative errors, we draw\ninspiration from human cognitive biases—systematic patterns of deviation from rational judgement. Specifically, we use cognitive biases as motivation to (i) generate\nhypotheses for problems that models may have, and (ii) develop experiments that\nelicit these problems.

Evidence:
- Evidence Text: We use cognitive biases as motivation to generate\nhypotheses for problems that models may have, and (ii) develop experiments that\nelicit these problems.
  Strength: strong
  Location: ### Abstract
  Limitations: None.
  Exact Quote: To hypothesize and test for such qualitative errors, we draw\ninspiration from human cognitive biases—systematic patterns of deviation from rational judgement. Specifically, we use cognitive biases as motivation to (i) generate\nhypotheses for problems that models may have, and (ii) develop experiments that\nelicit these problems.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated in the abstract and supported by the evidence provided.
Key Limitations: None.

--------------------------------------------------

Claim 2:
Type: contribution
Statement: Qualitative and quantitative analysis of error modes in code generation.
Location: ### Abstract
Exact Quote: Using code generation as a case study, we find that OpenAI’s\nCodex errs predictably based on how the input prompt is framed, adjusts outputs\ntowards anchors, and is biased towards outputs that mimic frequent training examples. We then use our framework to elicit high-impact errors such as incorrectly\ndeleting files.

Evidence:
- Evidence Text: Using code generation as a case study, we find that OpenAI’s\nCodex errs predictably based on how the input prompt is framed, adjusts outputs\ntowards anchors, and is biased towards outputs that mimic frequent training examples.
  Strength: moderate
  Location: ### Abstract
  Limitations: The evidence is limited to the findings of the study on code generation.
  Exact Quote: Using code generation as a case study, we find that OpenAI’s\nCodex errs predictably based on how the input prompt is framed, adjusts outputs\ntowards anchors, and is biased towards outputs that mimic frequent training examples.

- Evidence Text: We then use our framework to elicit high-impact errors such as incorrectly\ndeleting files.
  Strength: strong
  Location: ### Abstract
  Limitations: None.
  Exact Quote: We then use our framework to elicit high-impact errors such as incorrectly\ndeleting files.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the evidence, but the evidence is limited to the findings of the study on code generation.
Key Limitations: The findings may not generalize to other types of large language models or to other tasks.

--------------------------------------------------

Claim 3:
Type: contribution
Statement: Experimental methodology from cognitive science can help characterize behavior of machine learning systems.
Location: ### Abstract
Exact Quote: Our results indicate that experimental methodology from cognitive\nscience can help characterize how machine learning systems behave.

Evidence:
- Evidence Text: Our results indicate that experimental methodology from cognitive\nscience can help characterize how machine learning systems behave.
  Strength: strong
  Location: ### Abstract
  Limitations: None.
  Exact Quote: Our results indicate that experimental methodology from cognitive\nscience can help characterize how machine learning systems behave.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated in the abstract and supported by the evidence provided.
Key Limitations: None.

--------------------------------------------------

Claim 4:
Type: methodology
Statement: Four cognitive biases used to hypothesize potential failures of large language models.
Location: ### 1 Introduction
Exact Quote: We draw on four different cognitive biases to hypothesize potential failures of OpenAI’s Codex [Chen\net al., 2021] and Salesforce’s CodeGen [Nijkamp et al., 2022], then apply our framework to each.

Evidence:
- Evidence Text: We draw on four different cognitive biases to hypothesize potential failures of OpenAI’s Codex [Chen\net al., 2021] and Salesforce’s CodeGen [Nijkamp et al., 2022], then apply our framework to each.
  Strength: strong
  Location: ### 1 Introduction
  Limitations: None.
  Exact Quote: We draw on four different cognitive biases to hypothesize potential failures of OpenAI’s Codex [Chen\net al., 2021] and Salesforce’s CodeGen [Nijkamp et al., 2022], then apply our framework to each.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated in the introduction and supported by the evidence provided.
Key Limitations: None.

--------------------------------------------------

Claim 5:
Type: result
Statement: The add-var anchor function consistently lowers functional accuracy.
Location: Part 6, Figure 8
Exact Quote: We measure the functional accuracy of Codex (top) and CodeGen (bottom) without an anchor function (baseline acc), the functional accuracy with an add-var anchor function prepended (anchor acc), and find that the anchor function consistently lowers accuracy.

Evidence:
- Evidence Text: The plot in Figure 8 shows that the functional accuracy of Codex and CodeGen is lower when the add-var anchor function is prepended.
  Strength: strong
  Location: Part 6, Figure 8
  Limitations: None
  Exact Quote: None

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the conclusion that the add-var anchor function lowers functional accuracy.
Key Limitations: None

--------------------------------------------------

Claim 6:
Type: result
Statement: Both Codex and CodeGen adjust their output to related-but-incorrect solutions when the add-var anchor function is used.
Location: Part 6, Figure 8
Exact Quote: Moreover, we see that both models adjust their output to related-but-incorrect solutions; in the same plot, we see that our test for the anchor, the presence of return tmp consistently appears in the generated solutions, while both anchor lines rarely appear together.

Evidence:
- Evidence Text: The plot in Figure 8 shows that the fraction of generated solutions that contain return tmp increases when the add-var anchor function is prepended.
  Strength: strong
  Location: Part 6, Figure 8
  Limitations: None
  Exact Quote: None

- Evidence Text: The plot in Figure 8 shows that the fraction of generated solutions that are exact copies of the anchor function decreases when the add-var anchor function is prepended.
  Strength: strong
  Location: Part 6, Figure 8
  Limitations: None
  Exact Quote: None

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the conclusion that Codex and CodeGen adjust their output to related-but-incorrect solutions when the add-var anchor function is used.
Key Limitations: None

--------------------------------------------------

Claim 7:
Type: result
Statement: The addition of irrelevant variables in the anchor function does not affect the anchoring results.
Location: Part 6, Figure 9 and Figure 10
Exact Quote: Both results are nearly identical to the results where the function name is shared presented in Section 3.3.2, and suggest that the shared function name is not responsible for our anchoring results.

Evidence:
- Evidence Text: The plot in Figure 9 shows that the functional accuracy of Codex when the anchor function name is changed is nearly identical to the functional accuracy when the anchor function name is the same as the function to be completed.
  Strength: strong
  Location: Part 6, Figure 9
  Limitations: None
  Exact Quote: None

- Evidence Text: The plot in Figure 10 shows that the functional accuracy of Codex when the anchor function name is changed is nearly identical to the functional accuracy when the anchor function name is the same as the function to be completed.
  Strength: strong
  Location: Part 6, Figure 10
  Limitations: None
  Exact Quote: None

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the conclusion that the addition of irrelevant variables in the anchor function does not affect the anchoring results.
Key Limitations: None

--------------------------------------------------

