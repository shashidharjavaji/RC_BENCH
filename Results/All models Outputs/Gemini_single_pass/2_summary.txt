Claim 1:
Type: result
Statement: Language models can produce calibrated probabilities for multiple choice questions, when the questions and choices are provided in the right format.
Location: section 3
Exact Quote: We have seen that language models can produce calibrated probabilities for multiple choice questions, at least when the questions and choices are provided in the right format.

Evidence:
- Evidence Text: Figure 5 shows calibration curves for various model sizes on MMLU, TruthfulQA, QuALITY, and LogiQA in the format described in section 2.
  Strength: strong
  Location: section 3
  Limitations: Only tested on a few specific multiple choice question datasets.
  Exact Quote: We show calibration curves for various model sizes on MMLU, TruthfulQA, QuALITY, and LogiQA in the format described in section 2.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence provides strong support for the claim, but it is limited to a few specific datasets.
Key Limitations: Need to test on a wider range of multiple choice question datasets.

--------------------------------------------------

Claim 2:
Type: result
Statement: Calibration improves as we pass from 0-shot to few-shot evaluation.
Location: section 3
Exact Quote: We expect calibration is also easier to achieve with this format because each answer option corresponds to a single token (this isn’t the case in BIG Bench by default, see appendix A.4).

Evidence:
- Evidence Text: Figure 5 shows calibration curves for various model sizes on MMLU, TruthfulQA, QuALITY, and LogiQA in the format described in section 2.
  Strength: strong
  Location: section 3
  Limitations: Only tested on a few specific multiple choice question datasets.
  Exact Quote: We show calibration curves for various model sizes on MMLU, TruthfulQA, QuALITY, and LogiQA in the format described in section 2.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence provides strong support for the claim, but it is limited to a few specific datasets.
Key Limitations: Need to test on a wider range of multiple choice question datasets.

--------------------------------------------------

Claim 3:
Type: methodology
Statement: Calibration is easier to achieve with this format because each answer option corresponds to a single token.
Location: section 3
Exact Quote: We expect calibration is also easier to achieve with this format because each answer option corresponds to a single token (this isn’t the case in BIG Bench by default, see appendix A.4).

Evidence:
- Evidence Text: In almost all cases, calibration improves with model size and capability.
  Strength: weak
  Location: section 3
  Limitations: Does not provide a direct comparison to other formats.
  Exact Quote: We find that in almost all cases, calibration improves with model size and capability.

Evaluation:
Conclusion Justified: No
Robustness: low
Confidence Level: low
Justification: The evidence does not provide strong support for the claim.
Key Limitations: Need to compare calibration performance to other formats.

--------------------------------------------------

Claim 4:
Type: result
Statement: Replacing an option with ‘None of the Above’ harms performance and calibration.
Location: section 3.1
Exact Quote: We have seen that language models can produce calibrated probabilities for multiple choice questions, at least when the questions and choices are provided in the right format.

Evidence:
- Evidence Text: Figure 7 shows accuracy on MMLU in the standard format, and after replacing option (D) with none of the above.
  Strength: strong
  Location: section 3.1
  Limitations: Only tested on one dataset (MMLU).
  Exact Quote: We show accuracy on MMLU in the standard format, and after replacing option (D) with none of the above.

- Evidence Text: Figure 7 shows calibration specifically for the (D) answer option, in the standard form of MMLU and with (D) as none of the above.
  Strength: strong
  Location: section 3.1
  Limitations: Only tested on one dataset (MMLU).
  Exact Quote: We show calibration specifically for the (D) answer option, in the standard form of MMLU and with (D) as none of the above.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence provides strong support for the claim, but it is limited to one dataset.
Key Limitations: Need to test on a wider range of multiple choice question datasets.

--------------------------------------------------

Claim 5:
Type: result
Statement: Models are well-calibrated on True/False tasks.
Location: section 3.2
Exact Quote: We saw in section 3.1 that language models seem to be confused by a “none of the above” option.

Evidence:
- Evidence Text: Figure 8 shows calibration curves for various model sizes on all of the multiple choice tasks in BIG Bench, reformulated as True/False questions on a mix of the correct answers, and randomly chosen incorrect answer options.
  Strength: strong
  Location: section 3.2
  Limitations: Only tested on a few specific multiple choice question datasets.
  Exact Quote: We show calibration curves for various model sizes on all of the multiple choice tasks in BIG Bench, reformulated as True/False questions on a mix of the correct answers, and randomly chosen incorrect answer options.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence provides strong support for the claim, but it is limited to a few specific datasets.
Key Limitations: Need to test on a wider range of multiple choice question datasets.

--------------------------------------------------

Claim 6:
Type: result
Statement: RLHF policy miscalibration can be remedied with a temperature tuning.
Location: section 3.3
Exact Quote: Our focus in this paper is on pure language models, but as a quick experiment we also looked at calibration for a helpful and harmless RLHF policy, trained exactly as in [Bai et al., 2022] using the base language models we are studying here.

Evidence:
- Evidence Text: Figure 9 shows calibration curves for RLHF policies finetuned from our language models.
  Strength: strong
  Location: section 3.3
  Limitations: Only tested on a few specific RLHF policies.
  Exact Quote: We show calibration curves for RLHF policies finetuned from our language models.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence provides strong support for the claim, but it is limited to a few specific RLHF policies.
Key Limitations: Need to test on a wider range of RLHF policies.

--------------------------------------------------

Claim 7:
Type: result
Statement: Showing more of the GSM8k hint results in higher P(IK).
Location: Figure 19
Exact Quote: We see that showing more of the GSM8k hint results in higher P(IK).

Evidence:
- Evidence Text: The P(IK) score increases as the fraction of the GSM8k hint shown increases.
  Strength: strong
  Location: Figure 19
  Limitations: None.
  Exact Quote: We see that showing more of the GSM8k hint results in higher P(IK).

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim that showing more of the GSM8k hint results in higher P(IK).
Key Limitations: None.

--------------------------------------------------

Claim 8:
Type: result
Statement: Good hints that lead to the correct answer result in higher P(IK) scores than bad hints.
Location: Figure 19
Exact Quote: Good hints that lead to the correct answer result in higher P(IK) scores than bad hints.

Evidence:
- Evidence Text: The P(IK) score is higher for good hints (leading to correct answer) than for bad hints (leading to incorrect answer).
  Strength: strong
  Location: Figure 19
  Limitations: None.
  Exact Quote: Good hints that lead to the correct answer result in higher P(IK) scores than bad hints.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim that good hints that lead to the correct answer result in higher P(IK) scores than bad hints.
Key Limitations: None.

--------------------------------------------------

Claim 9:
Type: result
Statement: The P(IK) model that was trained on TriviaQA, LAMBADA, Arithmetic, and Python Function Synthesis performs better and more consistently, especially with partial hints.
Location: Figure 19
Exact Quote: The P(IK) model that was trained on TriviaQA, LAMBADA, Arithmetic, and Python Function Synthesis performs better and more consistently, especially with partial hints.

Evidence:
- Evidence Text: The P(IK) scores are higher and more consistent for the model trained on all 4 tasks than for the model trained on just TriviaQA, especially when partial hints are shown.
  Strength: strong
  Location: Figure 19
  Limitations: None.
  Exact Quote: The P(IK) model that was trained on TriviaQA, LAMBADA, Arithmetic, and Python Function Synthesis performs better and more consistently, especially with partial hints.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim that the P(IK) model trained on all 4 tasks performs better and more consistently than the model trained on just TriviaQA, especially with partial hints.
Key Limitations: None.

--------------------------------------------------

