Claim 1:
Type: methodology
Statement: Integrated gradients satisfy an axiom called completeness that the attributions add up to the difference between the output of F at the input x and the baseline x[′].
Location: Section 2
Exact Quote: (x[′]+∂xα×i(x−x[′]))_ _dα_\n_α=0_\n(1)\n**Axiom: Completeness.** Integrated gradients satisfy an\naxiom called completeness that the attributions add up to\nthe difference between the output of F at the input x and\nthe baseline x[′].

Evidence:
- Evidence Text: Σ[n]i=1[IntegratedGrads]i[(][x][) =][ F] [(][x][)][ −] _[F]_ [(][x][′][)]
  Strength: strong
  Location: Section 2
  Limitations: None
  Exact Quote: Σ[n]i=1[IntegratedGrads]i[(][x][) =][ F] [(][x][)][ −] _[F]_ [(][x][′][)]

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided is a mathematical equation that demonstrates the completeness property of integrated gradients.
Key Limitations: None

--------------------------------------------------

Claim 2:
Type: methodology
Statement: Integrated gradients satisfies Sensitivity(a) because Completeness implies Sensitivity(a) and is thus a strengthening of the Sensitivity(a) axiom.
Location: Section 2
Exact Quote: This is formalized by the proposition below, which instantiates the fundamental theorem of calculus for path integrals.\n\n**Proposition 1. If F : R[n]** _→_ R is differentiable almost\n_everywhere_ [1] _then_\n\nΣ[n]i=1[IntegratedGrads]i[(][x][) =][ F] [(][x][)][ −] _[F]_ [(][x][′][)]\n\nFor most deep networks, it is possible to choose a baseline such that the prediction at the baseline is near zero\n(F (x[′]) 0). (For image models, the black image base_≈_line indeed satisfies this property.) In such cases, there is\nan intepretation of the resulting attributions that ignores the\nbaseline and amounts to distributing the output to the individual input features.\n\n**Remark 2. Integrated gradients satisfies Sensivity(a) be-**\n_cause Completeness implies Sensivity(a) and is thus a_ \n_strengthening of the Sensitivity(a) axiom. This is because_ \n_Sensitivity(a) refers to a case where the baseline and the_ \n_input differ only in one variable, for which Completeness_ \n_asserts that the difference in the two output values is equal_ \n_to the attribution to this variable. Attributions generated_ \n_by integrated gradients satisfy Implementation Invariance_ \n_since they are based only on the gradients of the function_ \n_represented by the network._

Evidence:
- Evidence Text: Sensitivity(a) refers to a case where the baseline and the input differ only in one variable, for which Completeness asserts that the difference in the two output values is equal to the attribution to this variable.
  Strength: strong
  Location: Section 2
  Limitations: None
  Exact Quote: Sensitivity(a) refers to a case where the baseline and the input differ only in one variable, for which Completeness asserts that the difference in the two output values is equal to the attribution to this variable.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided clearly states that Completeness implies Sensitivity(a), making integrated gradients a strengthening of the Sensitivity(a) axiom.
Key Limitations: None

--------------------------------------------------

Claim 3:
Type: methodology
Statement: Integrated gradients is the unique path method that is symmetry-preserving.
Location: Section 4.2
Exact Quote: **Theorem 1. Integrated gradients is the unique path**\n_method that is symmetry-preserving.

Evidence:
- Evidence Text: The proof is provided in Appendix A.
  Strength: strong
  Location: Section 4.2
  Limitations: The proof is not provided in the current text, so its validity cannot be assessed.
  Exact Quote: The proof is provided in Appendix A.

Evaluation:
Conclusion Justified: No
Robustness: low
Confidence Level: low
Justification: While the claim states that integrated gradients is the unique symmetry-preserving path method, the proof is not provided in the current text, making it difficult to assess the validity of this claim.
Key Limitations: Lack of proof in the provided text

--------------------------------------------------

Claim 4:
Type: methodology
Statement: Integrated gradients can be efficiently approximated via a summation.
Location: section/paragraph 5
Exact Quote: The integral of integrated gradients can be efficiently approximated via a summation.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: medium
Confidence Level: low
Justification: The claim is made without providing any supporting evidence or explanation.
Key Limitations: Lack of explanation and evidence.

--------------------------------------------------

Claim 5:
Type: result
Statement: Integrated gradients are better at reflecting distinctive features of the input image than gradients at the actual image.
Location: section/paragraph 6.1
Exact Quote: Notice that the visualizations obtained from integrated gradients are better at reflecting distinctive features of the image.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: medium
Confidence Level: low
Justification: The claim is made without providing any supporting evidence or explanation.
Key Limitations: Lack of explanation and evidence.

--------------------------------------------------

Claim 6:
Type: result
Statement: Integrated gradients are localized to a few pixels that seem to be lesions in the retina.
Location: section/paragraph 6.2
Exact Quote: Notice that integrated gradients are localized to a few pixels that seem to be lesions in the retina.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: medium
Confidence Level: low
Justification: The claim is made without providing any supporting evidence or explanation.
Key Limitations: Lack of explanation and evidence.

--------------------------------------------------

Claim 7:
Type: result
Statement: Attributions from integrated gradients largely agree with commonly used rules for question classification.
Location: section/paragraph 6.3
Exact Quote: Notice that the attributions largely agree with commonly used rules, for e.g., “how many” indicates a numeric seeking question.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: medium
Confidence Level: low
Justification: The claim is made without providing any supporting evidence or explanation.
Key Limitations: Lack of explanation and evidence.

--------------------------------------------------

Claim 8:
Type: result
Statement: Integrated gradients can identify novel question classification rules.
Location: section/paragraph 6.3
Exact Quote: In addition, attributions help identify novel question classification rules, for e.g., questions containing “total number” are seeking numeric answers.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: medium
Confidence Level: low
Justification: The claim is made without providing any supporting evidence or explanation.
Key Limitations: Lack of explanation and evidence.

--------------------------------------------------

Claim 9:
Type: result
Statement: Integrated gradients can identify undesirable correlations in question classification models.
Location: section/paragraph 6.3
Exact Quote: Attributions also point out undesirable correlations, for e.g., “charles” is used as trigger for a yes/no question.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: medium
Confidence Level: low
Justification: The claim is made without providing any supporting evidence or explanation.
Key Limitations: Lack of explanation and evidence.

--------------------------------------------------

Claim 10:
Type: result
Statement: Integrated gradients can be used to align output tokens in a neural machine translation system with input tokens.
Location: section/paragraph 6.4
Exact Quote: We attribute the output probability of every output token (in form of wordpieces) to the input tokens. Such attributions “align” the output sentence with the input sentence.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: medium
Confidence Level: low
Justification: The claim is made without providing any supporting evidence or explanation.
Key Limitations: Lack of explanation and evidence.

--------------------------------------------------

Claim 11:
Type: result
Statement: Integrated gradients can identify the contributions of each feature in a chemistry model.
Location: section/paragraph 6.5
Exact Quote: Since integrated gradients add up to the final prediction score (see Proposition 1), the magnitudes can be use for accounting the contributions of each feature.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: medium
Confidence Level: low
Justification: The claim is made without providing any supporting evidence or explanation.
Key Limitations: Lack of explanation and evidence.

--------------------------------------------------

Claim 12:
Type: result
Statement: Integrated gradients helped identify an anomaly in the W1N2 architecture.
Location: section/paragraph 6.5
Exact Quote: On applying the integrated gradients method to this network, we found that several atoms in the same molecule received identical attribution despite being bonded to different atoms.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: medium
Confidence Level: low
Justification: The claim is made without providing any supporting evidence or explanation.
Key Limitations: Lack of explanation and evidence.

--------------------------------------------------

Claim 13:
Type: methodology
Statement: Our technique can be implemented using a few calls to the gradients operator, can be applied to a variety of deep networks, and has a strong theoretical justification.
Location: Conclusion
Exact Quote: It can be implemented using a few calls to the gradients operator, can be applied to a variety of deep networks, and has a strong theoretical justification.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: low
Confidence Level: low
Justification: The claim is not supported by any evidence in the provided text.
Key Limitations: Lack of experimental results or data to support the claim.

--------------------------------------------------

Claim 14:
Type: methodology
Statement: Without the axiomatic approach it is hard to tell whether the attribution method is affected by data artifacts, network’s artifacts or artifacts of the method.
Location: Conclusion
Exact Quote: Without the axiomatic approach it is hard to tell whether the attribution method is affected by data artifacts, network’s artifacts or artifacts of the method.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the fact that the axiomatic approach can rule out artifacts of the method.
Key Limitations: The claim does not provide specific examples or experimental results to support the statement.

--------------------------------------------------

Claim 15:
Type: methodology
Statement: Integrated gradients is a method that attributes the prediction of a deep network to its inputs.
Location: Conclusion
Exact Quote: Integrated gradients that attributes the prediction of a deep network to its inputs.

Evidence:
Evaluation:
Conclusion Justified: No
Robustness: low
Confidence Level: low
Justification: The claim is not supported by any evidence in the provided text.
Key Limitations: Lack of experimental results or data to support the claim.

--------------------------------------------------

Claim 16:
Type: methodology
Statement: The axiomatic approach can rule out artifacts of the last type.
Location: Conclusion
Exact Quote: The axiomatic approach rules out artifacts of the last type.

Evidence:
Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the fact that the axiomatic approach can rule out artifacts of the method.
Key Limitations: The claim does not provide specific examples or experimental results to support the statement.

--------------------------------------------------

Claim 17:
Type: result
Statement: For a fixed value of x1 greater than 1, the output of the network f decreases linearly as x2 increases from 0 to x1 - 1.
Location: part 5
Exact Quote: For a fixed value of x1 greater than 1, the output decreases linearly as x2 increases from 0 to x1 1.

Evidence:
- Evidence Text: This result is illustrated in Figure 7 of the paper.
  Strength: strong
  Location: part 5
  Limitations: None
  Exact Quote: nsider the network f (x1, x2) from Figure 7.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by strong evidence from the paper.
Key Limitations: None

--------------------------------------------------

Claim 18:
Type: result
Statement: For all inputs, Deconvolutional networks and Guided back-propagation result in zero attribution for x2.
Location: part 5
Exact Quote: Yet, for all inputs, De_−_convolutional networks and Guided back-propagation results in zero attribution for x2.

Evidence:
- Evidence Text: This result is due to the fact that for all inputs, the back-propagated signal received at the node ReLU(x2) is negative and is therefore not back-propagated through the ReLU operation.
  Strength: strong
  Location: part 5
  Limitations: None
  Exact Quote: This happens because for all inputs the back-propagated signal received at the node ReLU(x2) is negative and is therefore not back-propagated through the ReLU operation (per the rules of deconvolution and guided back-propagation; see (Springenberg et al., 2014) for details).

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by strong evidence from the paper.
Key Limitations: None

--------------------------------------------------

Claim 19:
Type: result
Statement: As a result, the feature x2 receives zero attribution, despite the network’s output being sensitive to it.
Location: part 5
Exact Quote: As a result, the feature x2 receives zero attribution despite the network’s output being sensitive to it.

Evidence:
- Evidence Text: This is because the ReLU operation effectively discards all negative values from the back-propagated signal.
  Strength: strong
  Location: part 5
  Limitations: None
  Exact Quote: As a result, the feature x2 receives zero attribution despite the network’s output being sensitive to it.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by strong evidence from the paper.
Key Limitations: None

--------------------------------------------------

