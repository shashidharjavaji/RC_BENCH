=== Paper Analysis Summary ===

Claim 1:
Statement: The Y-NQ dataset is a new comprehensive open-book question-answer dataset that allows comparison between English and Yorùbá
Location: Introduction
Type: Dataset contribution
Quote: For this, we introduce Y-NQ (Yorùbá Natural Questions) a comprehensive open-book question-answer dataset

Evidence:
- Dataset provides parallel documents and comparable responses between English and Yorùbá
  Strength: strong
  Location: Dataset description section
  Limitations: Not fully comparable due to length differences
  Quote: Y-NQ allows for the comparison of LLM results in a reading comprehension task across a high- and a low-resource language

Conclusion:
Justified: True
Robustness: medium
Limitations: Documents are not fully comparable in length and content; Yorùbá documents are significantly shorter
Confidence: medium

==================================================

Claim 2:
Statement: Responses in Yorùbá are more inaccurate than those in English
Location: Introduction
Type: Result finding
Quote: The results and analysis (Section 3) shows that responses in Yorùbá are more inaccurate than those in English.

Evidence:
Conclusion:
Justified: True
Robustness: medium
Limitations: Simpler task for Yorùbá due to shorter documents; potential model bias from pre-training
Confidence: medium

==================================================

Claim 3:
Statement: There are accuracy discrepancies across languages for the same Wikipedia topics
Location: Introduction
Type: Finding
Quote: which confirms the existence of accuracy discrepancies across languages for the same Wikipedia topics

Evidence:
Conclusion:
Justified: True
Robustness: high
Limitations: Limited sample size (26 out of 1,566 questions)
Confidence: high

==================================================

Claim 4:
Statement: The automatic pre-annotation using SONAR embeddings showed low reliability and was abandoned
Location: Dataset creation
Type: Methodology finding
Quote: The analysis shows a low similarity matching rate, which is likely due to the low quality and short length of many Yorùbá articles and/or SONAR embeddings not being suitable for such a task. Given this low reliability, we abandoned this automatic pre-annotation

Evidence:
- SONAR embedding similarity analysis showed low matching rate
  Strength: strong
  Location: Dataset creation section 2.2
  Limitations: Specific threshold values not provided
  Quote: The analysis shows a low similarity matching rate, which is likely due to the low quality and short length of many Yorùbá articles and/or SONAR embeddings not being suitable for such a task. Given this low reliability, we abandoned this automatic pre-annotation

Conclusion:
Justified: True
Robustness: high
Limitations: Specific reasons for low matching rate not fully explained
Confidence: high

==================================================

Claim 5:
Statement: Yorùbá consistently performs worse than English in model evaluations
Location: Experiments
Type: Result finding
Quote: Table 4 reports the results showing that Yorùbá consistently performs worse than English (e.g., losing 0.4 in Rouge-1)

Evidence:
- Rouge scores consistently lower for Yorùbá across all models
  Strength: strong
  Location: Experiments section, Table 4
  Limitations: Limited number of models tested
  Quote: Table 4 reports the results showing that Yorùbá consistently performs worse than English (e.g., losing 0.4 in Rouge-1)

Conclusion:
Justified: True
Robustness: high
Limitations: Task complexity differs due to document length differences; possible model bias
Confidence: high

==================================================

Claim 6:
Statement: Model performance drops significantly for Yorùbá documents over 1,500 words
Location: Length analysis
Type: Result finding
Quote: We can see a drop in performance when the Yorùbá documents reach 1,500 words, which shows the challenges that current models face in long-context understanding of low-resource languages

Evidence:
- Performance drop observed at 1,500 words for Yorùbá
  Strength: moderate
  Location: Length analysis section
  Limitations: Based on Figure 1, specific numbers not provided
  Quote: We can see a drop in performance when the Yorùbá documents reach 1,500 words, which shows the challenges that current models face in long-context understanding of low-resource languages

Conclusion:
Justified: True
Robustness: medium
Limitations: Number of documents in this length category not specified
Confidence: medium

==================================================

Claim 7:
Statement: For comparable length documents, English performance is significantly better (1.58X-2.56X)
Location: Length analysis
Type: Result finding
Quote: For a small portion of long-enough documents of comparable length between English and Yorùbá (only 4 documents that are over 900 words long), English performance demonstrates a significant edge (1.58X-2.56X)

Evidence:
- Comparative performance on long documents
  Strength: moderate
  Location: Length analysis section
  Limitations: Very small sample size (only 4 documents)
  Quote: For a small portion of long-enough documents of comparable length between English and Yorùbá (only 4 documents that are over 900 words long), English performance demonstrates a significant edge (1.58X-2.56X)

Conclusion:
Justified: True
Robustness: low
Limitations: Very small sample size (only 4 documents)
Confidence: low

==================================================

Claim 8:
Statement: The reading comprehension capabilities of current English LLMs do not extend well to Yorùbá
Location: Conclusions
Type: Main conclusion
Quote: Our experiments show that the reading comprehension capabilities of current English LLMs do not extend to Yorùbá

Evidence:
Conclusion:
Justified: True
Robustness: medium
Limitations: Confounding factors like document length differences and potential model bias not fully controlled
Confidence: medium

==================================================

