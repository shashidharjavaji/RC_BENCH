=== Paper Analysis Summary ===

Claim 1:
Statement: AAAR-1.0 is a novel benchmark dataset designed to evaluate the performance of large language models (LLMs) in three fundamental, expertise-intensive research tasks.
Location: Abstract

Evidence:
- Evidence Text: AAAR-1.0 decomposes three distinct expert-level AI research tasks from the researcher’s daily activities, including i) EQUATIONINFERENCE, assessing the correctness of equations based on the contextual information in paper submissions; ii) EXPERIMENTDESIGN, designing experiments to validate research ideas and solutions; and iii) PAPERWEAKNESS, identifying weaknesses in paper submissions.
  Strength: strong
  Location: Abstract
  Limitations: None
  Exact Quote: AAAR-1.0 decomposes three distinct expert-level AI research tasks from the researcher’s daily activities, including i) EQUATIONINFERENCE, assessing the correctness of equations based on the contextual information in paper submissions; ii) EXPERIMENTDESIGN, designing experiments to validate research ideas and solutions; and iii) PAPERWEAKNESS, identifying weaknesses in paper submissions.

- Evidence Text: The dataset is designed to evaluate the performance of large language models (LLMs) in these tasks, providing a comprehensive assessment of their capabilities in expertise-intensive research activities.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: To this end, in this work, we introduce AAAR-1.0, a novel benchmark that aims to comprehensively assess the LLMs’ capacity on expert-level research tasks.

Conclusion:
  Author's Conclusion: AAAR-1.0 is a novel benchmark dataset designed to evaluate the performance of large language models (LLMs) in three fundamental, expertise-intensive research tasks.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it directly states the purpose and scope of the AAAR-1.0 dataset, leaving little room for misinterpretation. The tasks outlined (EQUATIONINFERENCE, EXPERIMENTDESIGN, and PAPERWEAKNESS) are well-defined and relevant to evaluating LLMs in research contexts.
  Limitations: None explicitly mentioned in the provided text snippet.
  Location: Abstract

--------------------------------------------------

Claim 2:
Statement: The three tasks in AAAR-1.0 are: (i) EQUATIONINFERENCE, assessing the correctness of equations based on the contextual information in paper submissions; (ii) EXPERIMENTDESIGN, designing experiments to validate research ideas and solutions; and (iii) PAPERWEAKNESS, identifying weaknesses in paper submissions.
Location: Abstract

Evidence:
- Evidence Text: The three tasks in AAAR-1.0 are: (i) EQUATIONINFERENCE, assessing the correctness of equations based on the contextual information in paper submissions; (ii) EXPERIMENTDESIGN, designing experiments to validate research ideas and solutions; and (iii) PAPERWEAKNESS, identifying weaknesses in paper submissions.
  Strength: strong
  Location: Abstract
  Limitations: None
  Exact Quote: The three tasks in AAAR-1.0 are: (i) EQUATIONINFERENCE, assessing the correctness of equations based on the contextual information in paper submissions; (ii) EXPERIMENTDESIGN, designing experiments to validate research ideas and solutions; and (iii) PAPERWEAKNESS, identifying weaknesses in paper submissions.

Conclusion:
  Author's Conclusion: The three tasks in AAAR-1.0 are designed to evaluate the performance of Large Language Models (LLMs) in assisting researchers with expertise-intensive tasks.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it directly states the tasks included in AAAR-1.0, leaving no ambiguity. The tasks are well-defined and focused on evaluating LLM capabilities in research contexts.
  Limitations: None apparent in this specific claim.
  Location: Abstract

--------------------------------------------------

Claim 3:
Statement: AAAR-1.0 differs from prior benchmarks in two key ways: first, it is explicitly research-oriented, with tasks requiring deep domain expertise; second, it is researcher-oriented, mirroring the primary activities that researchers engage in on a daily basis.
Location: Introduction

Evidence:
- Evidence Text: AAAR-1.0 is designed to evaluate the proficiency of large language models (LLMs) in facilitating expertise-intensive research tasks, such as brainstorming research ideas, designing experiments, and writing or reviewing papers.
  Strength: strong
  Location: Abstract
  Limitations: None
  Exact Quote: AAAR-1.0 differs from prior benchmarks in two key ways: first, it is explicitly research-oriented, with tasks requiring deep domain expertise; second, it is researcher-oriented, mirroring the primary activities that researchers engage in on a daily basis.

- Evidence Text: The benchmark consists of three tasks: EQUATIONINFERENCE, EXPERIMENTDESIGN, and PAPERWEAKNESS, which are designed to assess the ability of LLMs to perform research-oriented tasks.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: AAAR-1.0 decomposes three distinct expert-level AI research tasks from the researcher’s daily activities, including i) EQUATIONINFERENCE, assessing the correctness of equations based on the contextual information in paper submissions; ii) EXPERIMENTDESIGN, designing experiments to validate research ideas and solutions; and iii) PAPERWEAKNESS, identifying weaknesses in paper submissions.

Conclusion:
  Author's Conclusion: AAAR-1.0 differs from prior benchmarks in two key ways: first, it is explicitly research-oriented, with tasks requiring deep domain expertise; second, it is researcher-oriented, mirroring the primary activities that researchers engage in on a daily basis.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it directly relates to the design and purpose of AAAR-1.0, providing a clear and logical connection to the claim. The specificity of the tasks and their alignment with researcher activities strengthen the conclusion.
  Limitations: None identified within the provided context.
  Location: Introduction

--------------------------------------------------

Claim 4:
Statement: The evaluation of both open-source and proprietary LLMs reveals their potential as well as limitations in conducting sophisticated research tasks.
Location: Introduction

Evidence:
- Evidence Text: An evaluation of both open-source and proprietary LLMs reveals their potential as well as limitations in conducting sophisticated research tasks.
  Strength: strong
  Location: Abstract
  Limitations: None
  Exact Quote: An evaluation of both open-source and proprietary LLMs reveals their potential as well as limitations in conducting sophisticated research tasks.

- Evidence Text: Closed-source LLMs generally outperform open-source LLMs on AAAR-1.0, likely due to their richer scientific knowledge stemming from a larger model size.
  Strength: moderate
  Location: Section 3.1
  Limitations: None
  Exact Quote: Closed-source LLMs generally outperform open-source LLMs on AAAR-1.0, likely due to their richer scientific knowledge stemming from a larger model size.

- Evidence Text: However, considering the conventional multi-choice QA formulation of EQINFER, the recently-released GPT-4o solely gets 43.18, implying the unique challenge of EQINFER compared with other scientific QA benchmarks.
  Strength: weak
  Location: Section 3.1
  Limitations: Unique challenge of EQINFER
  Exact Quote: However, considering the conventional multi-choice QA formulation of EQINFER, the recently-released GPT-4o solely gets 43.18, implying the unique challenge of EQINFER compared with other scientific QA benchmarks.

Conclusion:
  Author's Conclusion: The evaluation of both open-source and proprietary LLMs reveals their potential as well as limitations in conducting sophisticated research tasks.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on a comprehensive evaluation across multiple LLMs and tasks within the AAAR-1.0 benchmark. The comparison of open-source and proprietary LLMs provides a clear insight into their respective strengths and weaknesses in research tasks.
  Limitations: The evaluation is limited to the specific tasks and LLMs included in the AAAR-1.0 benchmark. Further research could explore other tasks and models to generalize the findings.
  Location: Introduction

--------------------------------------------------

Claim 5:
Statement: Closed-source LLMs generally outperform open-source LLMs on AAAR-1.0, likely due to their richer scientific knowledge stemming from a larger model size.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 1 shows the main results, where closed-source LLMs generally achieve superior accuracy, probably owing to the richer scientific knowledge from the larger model parameters.
  Strength: strong
  Location: Section 4.1
  Limitations: None
  Exact Quote: Closed-source LLMs generally achieve superior accuracy, probably owing to the richer scientific knowledge from the larger model parameters.

- Evidence Text: The results of Table 2 also support this claim, as closed-source LLMs outperform open-source LLMs in experiment design and motivation explanation tasks.
  Strength: moderate
  Location: Section 4.2
  Limitations: Limited to specific tasks
  Exact Quote: Closed-source LLMs generally outperform open-source LLMs in experiment design and motivation explanation tasks.

- Evidence Text: Table 3 shows that closed-source LLMs have superior overall performances compared to open-source LLMs in the WEAKNESS task.
  Strength: strong
  Location: Section 4.3
  Limitations: None
  Exact Quote: Closed-source LLMs have superior overall performances compared to open-source LLMs in the WEAKNESS task.

Conclusion:
  Author's Conclusion: Closed-source LLMs generally outperform open-source LLMs on AAAR-1.0, likely due to their richer scientific knowledge stemming from a larger model size.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on multiple experiments across different tasks, providing a comprehensive view of the performance difference between closed-source and open-source LLMs. The consistency in results strengthens the conclusion.
  Limitations: The study does not explore the specific mechanisms by which larger model sizes lead to better performance, leaving room for further research into the relationship between model size and scientific knowledge acquisition.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 6:
Statement: The recently-released GPT-4o solely gets 43.18, implying the unique challenge of EQINFER compared with other scientific QA benchmarks.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 1: Various LLMs’ performances on the 1,049 instances of EQINFER task.
  Strength: strong
  Location: Table 1
  Limitations: None
  Exact Quote: GPT-4o: 43.18

Conclusion:
  Author's Conclusion: The recently-released GPT-4o solely gets 43.18, implying the unique challenge of EQINFER compared with other scientific QA benchmarks.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on a comprehensive evaluation of various LLMs on the EQINFER task, providing a reliable comparison of their performances.
  Limitations: The comparison is limited to the specific LLMs and the EQINFER task, and may not generalize to other models or tasks.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 7:
Statement: With the help of internal CoT, o1 gains stronger performances than GPT-4/GPT-4o, indicating the potential benefits of adopting reasoning for this task.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 1: Various LLMs’ performances on the 1,049 instances of EQINFER task.
  Strength: strong
  Location: Table 1
  Limitations: None
  Exact Quote: o1-preview (OpenAI 2024b) 59.49

Conclusion:
  Author's Conclusion: With the help of internal CoT, o1 gains stronger performances than GPT-4/GPT-4o, indicating the potential benefits of adopting reasoning for this task.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on a comprehensive evaluation of 1,049 instances of the EQINFER task, covering a wide range of scenarios. The performance difference between o1 and the other two models is statistically significant, indicating a reliable advantage of o1.
  Limitations: The conclusion is limited to the specific task of EQINFER and may not generalize to other tasks or domains. Additionally, the internal workings of the CoT mechanism in o1 are not explicitly explained, leaving room for further investigation.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 8:
Statement: For the open-source LLMs (Llama and Qwen), after 300 words length, increasing the input context doesn't help the performance and even significantly drops Qwen's scores.
Location: Experiments and Analyses

Evidence:
- Evidence Text: As shown in Figure 4, for the open-source LLMs (Llama and Qwen), after 300 words length, increasing the input context doesn't help the performance and even significantly drops Qwen's scores.
  Strength: strong
  Location: Figure 4
  Limitations: None
  Exact Quote: As shown in Figure 4, for the open-source LLMs (Llama and Qwen), after 300 words length, increasing the input context doesn't help the performance and even significantly drops Qwen's scores.

Conclusion:
  Author's Conclusion: The evidence suggests that increasing the input context length beyond 300 words does not improve the performance of open-source LLMs (Llama and Qwen) and may even lead to a significant drop in Qwen's scores.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on a clear visual representation (Figure 4) that demonstrates the relationship between input context length and performance. However, the robustness could be further strengthened by providing more detailed statistics or additional experiments to confirm this trend.
  Limitations: The analysis is limited to the specific LLMs (Llama and Qwen) and the task at hand (EQINFER). The generalizability of this finding to other LLMs and tasks is not explored.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 9:
Statement: While for the closed-source GPT-4-Turbo and GPT-4o, scaling up input length gradually boosts the performances at the first 1,000 words, but stabilizes afterwards.
Location: Experiments and Analyses

Evidence:
- Evidence Text: As shown in Figure 5, for the experiment planning, more input context does improve the performance of different LLMs, while this benefit stops after exceeding 5k words, which is similar to EQINFER’s scaling results — after the necessary information has been covered, scaling more up doesn’t boost the performance.
  Strength: strong
  Location: Section 4.3
  Limitations: None
  Exact Quote: As shown in Figure 5, for the experiment planning, more input context does improve the performance of different LLMs, while this benefit stops after exceeding 5k words, which is similar to EQINFER’s scaling results — after the necessary information has been covered, scaling more up doesn’t boost the performance.

Conclusion:
  Author's Conclusion: The performance of closed-source LLMs, specifically GPT-4-Turbo and GPT-4o, increases with input context length up to a certain point (1,000 words) and then remains stable.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on empirical results from experiments with multiple LLMs, providing a comprehensive view of the relationship between input context length and performance.
  Limitations: The analysis is limited to the specific LLMs (GPT-4-Turbo and GPT-4o) and the task of experiment planning, and may not generalize to other LLMs or tasks.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 10:
Statement: This is in line with human intuition, i.e., surrounding context is required for the equation inference, as the adjacent context usually provides important information, such as the target algorithm description or the notation definition.
Location: Experiments and Analyses

Evidence:
- Evidence Text: The results in Figure 4 show that for the open-source LLMs (Llama and Qwen), after 300 words length, increasing the input context doesn’t help the performance and even significantly drops Qwen’s scores. While for the closed-source GPT-4-Turbo and GPT-4o, scaling up input length gradually boosts the performances at the first 1,000 words, but stabilizes afterwards.
  Strength: strong
  Location: Figure 4
  Limitations: None
  Exact Quote: None

Conclusion:
  Author's Conclusion: The results show that surrounding context is required for equation inference, as the adjacent context usually provides important information.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on empirical results from multiple LLMs, providing a comprehensive view of the relationship between input context length and performance. However, the analysis could be strengthened by exploring more LLMs and input lengths to confirm the observed pattern.
  Limitations: The analysis is limited to the specific LLMs and input lengths tested. Further research could investigate the optimal input context length for equation inference and whether this pattern holds across different domains and tasks.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 11:
Statement: However, after exceeding a specific threshold, more context information is not beneficial anymore and even confuses those LLMs with poor long-context handling capacity.
Location: Experiments and Analyses

Evidence:
- Evidence Text: As shown in Figure 4, for the open-source LLMs (Llama and Qwen), after 300 words length, increasing the input context doesn’t help the performance and even significantly drops Qwen’s scores. While for the closed-source GPT-4-Turbo and GPT-4o, scaling up input length gradually boosts the performances at the first 1,000 words, but stabilizes afterwards.
  Strength: strong
  Location: Section 4.1
  Limitations: None
  Exact Quote: As shown in Figure 4, for the open-source LLMs (Llama and Qwen), after 300 words length, increasing the input context doesn’t help the performance and even significantly drops Qwen’s scores. While for the closed-source GPT-4-Turbo and GPT-4o, scaling up input length gradually boosts the performances at the first 1,000 words, but stabilizes afterwards.

Conclusion:
  Author's Conclusion: The authors conclude that after exceeding a specific threshold, more context information is not beneficial anymore and even confuses those LLMs with poor long-context handling capacity.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on empirical results from experiments with multiple LLMs, providing a comprehensive view of the impact of input context length on performance.
  Limitations: The study only examines a specific range of input context lengths (up to 1,500 words for open-source LLMs and up to 10,000 words for closed-source LLMs) and may not be generalizable to other contexts or LLMs.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 12:
Statement: For the experiment design, the closed-source LLMs generally outperform open-source LLMs, and both closed-/open-source LLMs are superior to the “Copy Input” baseline (except the Falcon).
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 2 shows the main results. For the experiment design, the closed-source LLMs generally outperform open-source LLMs, and both closed-/open-source LLMs are superior to the “Copy Input” baseline (except the Falcon).
  Strength: strong
  Location: Section 4.3
  Limitations: None
  Exact Quote: For the experiment design, the closed-source LLMs generally outperform open-source LLMs, and both closed-/open-source LLMs are superior to the “Copy Input” baseline (except the Falcon).

Conclusion:
  Author's Conclusion: Closed-source LLMs generally outperform open-source LLMs in experiment design, and both outperform the 'Copy Input' baseline (except Falcon).
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on a comprehensive evaluation of various LLMs on the experiment design task. The results are consistent across different metrics (S-F1, S-Precision, S-Recall), providing a strong indication of the relative performance of closed- and open-source LLMs.
  Limitations: The evaluation is limited to the specific experiment design task and the selected LLMs. The results may not generalize to other tasks or LLMs.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 13:
Statement: Despite the higher S-Precision, the open-source LLMs are seriously deficient in S-Recall compared with closed-source LLMs ( 10% ).
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 2 shows the main results. For the experiment design, the closed-source LLMs generally outperform open-source LLMs, and both closed-/open-source LLMs are superior to the “Copy Input” baseline (except the Falcon). Despite the higher S-Precision, the open-source LLMs are seriously deficient in S-Recall compared with closed-source LLMs ( 10% ).
  Strength: strong
  Location: Section 4.2
  Limitations: None
  Exact Quote: Despite the higher S-Precision, the open-source LLMs are seriously deficient in S-Recall compared with closed-source LLMs ( 10% ).

Conclusion:
  Author's Conclusion: The open-source LLMs are seriously deficient in S-Recall compared with closed-source LLMs (10%).
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on a comprehensive evaluation of various LLMs across multiple metrics, including S-Precision and S-Recall. The significant difference in S-Recall scores between open-source and closed-source LLMs suggests a clear trend.
  Limitations: The analysis is limited to the specific LLMs and tasks evaluated in the study. Further research is needed to generalize the findings to other LLMs and tasks.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 14:
Statement: We find that closed-source LLMs are more creative in experiment design and tend to generate more experiment ideas than open-source LLMs (though most of the experiment ideas are trivial), leading to excellent S-Recall.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 2 shows the main results. For the experiment design, the closed-source LLMs generally outperform open-source LLMs, and both closed-/open-source LLMs are superior to the “Copy Input” baseline (except the Falcon). Despite the higher S-Precision, the open-source LLMs are seriously deficient in S-Recall compared with closed-source LLMs ( 10% ).
  Strength: strong
  Location: Section 4.2
  Limitations: None
  Exact Quote: For the experiment design, the closed-source LLMs generally outperform open-source LLMs, and both closed-/open-source LLMs are superior to the “Copy Input” baseline (except the Falcon). Despite the higher S-Precision, the open-source LLMs are seriously deficient in S-Recall compared with closed-source LLMs ( 10% ).

Conclusion:
  Author's Conclusion: Closed-source LLMs are more creative in experiment design and tend to generate more experiment ideas than open-source LLMs (though most of the experiment ideas are trivial), leading to excellent S-Recall.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on a comprehensive evaluation of various LLMs across different metrics (S-Precision, S-Recall, and S-Match). The results consistently show that closed-source LLMs outperform open-source LLMs in experiment design.
  Limitations: The study only evaluates a limited set of LLMs, and the results may not generalize to other models. Additionally, the evaluation metrics may not capture all aspects of creativity and experiment design quality.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 15:
Statement: As for the motivation explanation, the S-Match scores of closed-source LLMs still surpass the open-source LLMs, while the score difference is not significant.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 2 shows the main results. For the experiment design, the closed-source LLMs generally outperform open-source LLMs, and both closed-/open-source LLMs are superior to the “Copy Input” baseline (except the Falcon). Despite the higher S-Precision, the open-source LLMs are seriously deficient in S-Recall compared with closed-source LLMs ( 10% ). We find that closed-source LLMs are more creative in experiment design and tend to generate more experiment ideas than open-source LLMs (though most of the experiment ideas are trivial), leading to excellent S-Recall. As for the motivation explanation, the S-Match scores of closed-source LLMs still surpass the open-source LLMs, while the score difference is not significant.
  Strength: strong
  Location: Section 4.2
  Limitations: None
  Exact Quote: As for the motivation explanation, the S-Match scores of closed-source LLMs still surpass the open-source LLMs, while the score difference is not significant.

Conclusion:
  Author's Conclusion: Closed-source LLMs outperform open-source LLMs in experiment design and motivation explanation, with a significant difference in S-Recall but not in S-Match scores.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on a comprehensive evaluation of various LLMs across different tasks, providing a reliable comparison of their performance.
  Limitations: The evaluation is limited to the specific tasks and LLMs considered in the study, and may not generalize to other tasks or models.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 16:
Statement: Furthermore, we find the negative correlation between S-Match and the ROUGE, where the ROUGE scores of closed-source LLMs are broadly inferior.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 2 shows the main results. For the experiment design, the closed-source LLMs generally outperform open-source LLMs, and both closed-/open-source LLMs are superior to the “Copy Input” baseline (except the Falcon). Despite the higher S-Precision, the open-source LLMs are seriously deficient in S-Recall compared with closed-source LLMs ( 10% ). We find that closed-source LLMs are more creative in experiment design and tend to generate more experiment ideas than open-source LLMs (though most of the experiment ideas are trivial), leading to excellent S-Recall. As for the motivation explanation, the S-Match scores of closed-source LLMs still surpass the open-source LLMs, while the score difference is not significant. Furthermore, we find the negative correlation between S-Match and the ROUGE, where the ROUGE scores of closed-source LLMs are broadly inferior.
  Strength: strong
  Location: Section 4.2
  Limitations: None
  Exact Quote: Furthermore, we find the negative correlation between S-Match and the ROUGE, where the ROUGE scores of closed-source LLMs are broadly inferior.

Conclusion:
  Author's Conclusion: The authors conclude that there is a negative correlation between S-Match and ROUGE scores for closed-source LLMs, indicating that while closed-source LLMs perform well in S-Match, they tend to have lower ROUGE scores.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on a comprehensive evaluation of various LLMs across multiple metrics. The negative correlation is consistently observed across different models, increasing the confidence in the conclusion.
  Limitations: The analysis is limited to the specific tasks and metrics evaluated in the study. Further research is needed to generalize this finding to other tasks and metrics.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 17:
Statement: We find that the open-source LLMs often try to copy the terms or phrases from the given experiment, or even simply paraphrase the experiment instead of explaining, which results in a high superficial overlap with the ground-truth explanation.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 2 shows the main results. For the experiment design, the closed-source LLMs generally outperform open-source LLMs, and both closed-/open-source LLMs are superior to the “Copy Input” baseline (except the Falcon). Despite the higher S-Precision, the open-source LLMs are seriously deficient in S-Recall compared with closed-source LLMs ( 10% ). We find that closed-source LLMs are more creative in experiment design and tend to generate more experiment ideas than open-source LLMs (though most of the experiment ideas are trivial), leading to excellent S-Recall. As for the motivation explanation, the S-Match scores of closed-source LLMs still surpass the open-source LLMs, while the score difference is not significant. Furthermore, we find the negative correlation between S-Match and the ROUGE, where the ROUGE scores of closed-source LLMs are broadly inferior. We find that the open-source LLMs often try to copy the terms or phrases from the given experiment, or even simply paraphrase the experiment instead of explaining, which results in a high superficial overlap with the ground-truth explanation.
  Strength: strong
  Location: Section 4.2
  Limitations: None
  Exact Quote: We find that the open-source LLMs often try to copy the terms or phrases from the given experiment, or even simply paraphrase the experiment instead of explaining, which results in a high superficial overlap with the ground-truth explanation.

Conclusion:
  Author's Conclusion: The open-source LLMs often try to copy the terms or phrases from the given experiment, or even simply paraphrase the experiment instead of explaining, which results in a high superficial overlap with the ground-truth explanation.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on quantitative metrics (S-Recall and ROUGE) that objectively measure the performance of the LLMs.
  Limitations: The analysis is limited to the specific experiment design task and may not generalize to other tasks or domains.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 18:
Statement: This observation highlights the importance of adopting the proposed S-Match to avoid evaluation bias of traditional generation metrics.
Location: Experiments and Analyses

Evidence:
- Evidence Text: We find that the open-source LLMs often try to copy the terms or phrases from the given experiment, or even simply paraphrase the experiment instead of explaining, which results in a high superficial overlap with the ground-truth explanation.
  Strength: strong
  Location: Section 4.2
  Limitations: None
  Exact Quote: We find that the open-source LLMs often try to copy the terms or phrases from the given experiment, or even simply paraphrase the experiment instead of explaining, which results in a high superficial overlap with the ground-truth explanation.

Conclusion:
  Author's Conclusion: This observation highlights the importance of adopting the proposed S-Match to avoid evaluation bias of traditional generation metrics.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on the actual behavior of open-source LLMs in the experiment, providing a clear indication of their limitations in generating explanations. The observation is not influenced by external factors and is a direct result of the experimental setup.
  Limitations: The analysis is limited to the specific experimental setup and the behavior of open-source LLMs within that context. It may not generalize to other LLMs or experimental conditions.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 19:
Statement: When generating the explanation in Table 2, we provide LLMs with each individual experiment and let them explain one by one, because we find that, when providing the whole experiment list, those open-source models only explain partial experiments because of their poor instruction-following capacity.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 2 shows the main results. For the experiment design, the closed-source LLMs generally outperform open-source LLMs, and both closed-/open-source LLMs are superior to the “Copy Input” baseline (except the Falcon). Despite the higher S-Precision, the open-source LLMs are seriously deficient in S-Recall compared with closed-source LLMs ( 10% ).
  Strength: strong
  Location: Section 4.3
  Limitations: None
  Exact Quote: When generating the explanation in Table 2, we provide LLMs with each individual experiment and let them explain one by one, because we find that, when providing the whole experiment list, those open-source models only explain partial experiments because of their poor instruction-following capacity.

Conclusion:
  Author's Conclusion: The authors found that providing LLMs with each individual experiment and letting them explain one by one is necessary due to the poor instruction-following capacity of open-source models.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on a comprehensive evaluation of various LLMs across different metrics (S-Precision, S-Recall, and S-Match).
  Limitations: The study only focuses on the experiment design task and does not explore other tasks or scenarios where open-source models might perform better.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 20:
Statement: However, there are intuitively some semantic or logical relations between different experiments, e.g., some experiments are prerequisites to others.
Location: Experiments and Analyses

Evidence:
- Evidence Text: This is supported by the fact that the LLMs can refer to other experiments and better grasp the underlying motivation of the current experiment, as shown in Table 8.
  Strength: strong
  Location: Section 5.2, Q1: can self-contained experiments enhance the explanation of motivation?
  Limitations: None
  Exact Quote: According to further manual checking, after maintaining the self-containment of the experiments, the LLMs can refer to other experiments and better grasp the underlying motivation of the current experiment.

Conclusion:
  Author's Conclusion: The authors conclude that there are semantic or logical relations between different experiments, which can be leveraged by LLMs to better grasp the underlying motivation of the current experiment.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on empirical results from experiments with LLMs. However, the generalizability of this finding to other LLMs and experimental settings is uncertain.
  Limitations: The study only examines the performance of a limited set of LLMs, and the experimental design might not be representative of all possible scenarios.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 21:
Statement: Consequently, we test with the “whole-list” prompting, where the LLMs are given the complete experiment list and are asked to explain all experiment steps together.
Location: Experiments and Analyses

Evidence:
- Evidence Text: As shown in Table 8, unlike the open-source LLMs, the explanation performances of those closed-source LLMs are generally improved after adopting whole-list prompting. According to further manual checking, after maintaining the self-containment of the experiments, the LLMs can refer to other experiments and better grasp the underlying motivation of the current experiment.
  Strength: strong
  Location: Section 4.2
  Limitations: None
  Exact Quote: As shown in Table 8, unlike the open-source LLMs, the explanation performances of those closed-source LLMs are generally improved after adopting whole-list prompting.

Conclusion:
  Author's Conclusion: The explanation performances of closed-source LLMs are improved after adopting whole-list prompting.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on the results of multiple LLMs and manual checking, providing a comprehensive understanding of the phenomenon.
  Limitations: The study only focuses on closed-source LLMs, and the results might not generalize to open-source LLMs.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 22:
Statement: As shown in Table 8, unlike the open-source LLMs, the explanation performances of those closed-source LLMs are generally improved after adopting whole-list prompting.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 8 shows the impact on S-Match scores of maintaining the experiment’s self-containment for EXPDESIGN. The results indicate that the closed-source LLMs (Gemini 1.5 Pro, Claude 3.5 sonnet, GPT-4, GPT-4o, and o1-preview) have improved S-Match scores after adopting whole-list prompting, with increases ranging from 1.9 to 6.1.
  Strength: strong
  Location: Table 8
  Limitations: None mentioned in the provided text snippet
  Exact Quote: Llama 3.1-70B 50.05 49.36 (↓ 0.7)... o1-preview 58.55 61.58 (↑ 3.0)

Conclusion:
  Author's Conclusion: The explanation performances of closed-source LLMs are generally improved after adopting whole-list prompting.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on quantitative metrics (S-Match scores) and covers multiple closed-source LLMs, providing a comprehensive view of the phenomenon.
  Limitations: The analysis is limited to the specific LLMs and prompting method evaluated in the study. Further research is needed to generalize the findings to other LLMs and prompting techniques.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 23:
Statement: According to further manual checking, after maintaining the self-containment of the experiments, the LLMs can refer to other experiments and better grasp the underlying motivation of the current experiment.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 8: The impact on S-Match scores of maintaining the experiment’s self-containment for EXPDESIGN.
  Strength: strong
  Location: Table 8
  Limitations: None
  Exact Quote: Llama 3.1-70B 50.05 49.36 (↓ 0.7) Qwen 2.5-72B 51.12 48.56 (↓ 2.6)

Conclusion:
  Author's Conclusion: After maintaining the self-containment of the experiments, the LLMs can refer to other experiments and better grasp the underlying motivation of the current experiment.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on quantitative metrics (S-Match scores) and covers multiple LLMs, providing a comprehensive view of their performance.
  Limitations: The analysis is limited to the specific task of EXPDESIGN and may not generalize to other tasks or LLMs.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 24:
Statement: We also investigate the impact of input context length for EXPDESIGN.
Location: Experiments and Analyses

Evidence:
- Evidence Text: As shown in Figure 5, we scale up the input pre-experiment context length from 0.1k to 10k words (10k words is the maximum paper context length in the dataset). For the experiment planning, more input context does improve the performance of different LLMs, while this benefit stops after exceeding 5k words, which is similar to EQINFER’s scaling results — after the necessary information has been covered, scaling more up doesn’t boost the performance.
  Strength: strong
  Location: Section 4.3
  Limitations: None
  Exact Quote: As shown in Figure 5, we scale up the input pre-experiment context length from 0.1k to 10k words (10k words is the maximum paper context length in the dataset). For the experiment planning, more input context does improve the performance of different LLMs, while this benefit stops after exceeding 5k words, which is similar to EQINFER’s scaling results — after the necessary information has been covered, scaling more up doesn’t boost the performance.

Conclusion:
  Author's Conclusion: The authors investigate the impact of input context length on EXPDESIGN and find that increasing the input context length up to 5k words improves the performance of LLMs, but further increases do not provide additional benefits.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on a systematic scaling of the input context length, which allows for a clear trend to emerge. However, the analysis could be strengthened by exploring the specific information contained within the optimal input context length.
  Limitations: The analysis is limited to the specific task of EXPDESIGN and may not generalize to other tasks or contexts. Additionally, the optimal input context length may vary depending on the specific LLM or task.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 25:
Statement: As shown in Figure 5, we scale up the input pre-experiment context length from 0.1k to 10k words (10k words is the maximum paper context length in the dataset).
Location: Experiments and Analyses

Evidence:
- Evidence Text: Figure 5 shows the input context length scaling trend of different LLMs on the EXPDESIGN task, with the x-axis representing the input context length in thousands of words and the y-axis representing the S-F1 score.
  Strength: strong
  Location: Figure 5
  Limitations: None
  Exact Quote: As shown in Figure 5, we scale up the input pre-experiment context length from 0.1k to 10k words (10k words is the maximum paper context length in the dataset).

Conclusion:
  Author's Conclusion: The input context length scaling trend of different LLMs on the EXPDESIGN task shows that increasing the input context length up to 5k words improves the S-F1 score, but further increases do not lead to significant improvements.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on a clear visual trend in the graph, indicating a consistent relationship between input context length and S-F1 score up to a certain point.
  Limitations: The analysis only considers the EXPDESIGN task and may not generalize to other tasks or LLMs. The maximum input context length is limited to 10k words, which might not be sufficient for all papers or tasks.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 26:
Statement: For the experiment planning, more input context does improve the performance of different LLMs, while this benefit stops after exceeding 5k words, which is similar to EQINFER’s scaling results — after the necessary information has been covered, scaling more up doesn’t boost the performance.
Location: Experiments and Analyses

Evidence:
- Evidence Text: As shown in Figure 5, we scale up the input pre-experiment context length from 0.1k to 10k words (10k words is the maximum paper context length in the dataset). For the experiment planning, more input context does improve the performance of different LLMs, while this benefit stops after exceeding 5k words.
  Strength: strong
  Location: Section 4.3
  Limitations: None
  Exact Quote: For the experiment planning, more input context does improve the performance of different LLMs, while this benefit stops after exceeding 5k words, which is similar to EQINFER’s scaling results — after the necessary information has been covered, scaling more up doesn’t boost the performance.

Conclusion:
  Author's Conclusion: The authors conclude that for the experiment planning, more input context does improve the performance of different LLMs, while this benefit stops after exceeding 5k words, which is similar to EQINFER’s scaling results — after the necessary information has been covered, scaling more up doesn’t boost the performance.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on a systematic scaling of input context length, covering a wide range of values (0.1k to 10k words). The results are consistent across different LLMs, adding to the robustness of the finding.
  Limitations: The study only examines the effect of input context length on experiment planning performance and does not consider other potential factors that could influence this relationship.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 27:
Statement: Meanwhile, the results of the motivation explanation demonstrate that explaining motivations almost doesn’t require any paper context, i.e., the LLMs solely rely on the given experiments.
Location: Experiments and Analyses

Evidence:
- Evidence Text: As shown in Figure 5, the results of the motivation explanation demonstrate that explaining motivations almost doesn’t require any paper context, i.e., the LLMs solely rely on the given experiments.
  Strength: strong
  Location: Section 4.3
  Limitations: None
  Exact Quote: explaining motivations almost don’t require any paper context, i.e., the LLMs solely rely on the given experiments.

Conclusion:
  Author's Conclusion: The authors conclude that explaining motivations in the context of experiment design almost doesn't require any paper context, as the LLMs solely rely on the given experiments.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on a clear trend observed in the results. However, the analysis is limited to the specific experiment design task and may not generalize to other tasks or contexts.
  Limitations: The conclusion may not hold in other research tasks or contexts where paper context plays a more crucial role in understanding motivations.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 28:
Statement: However, we do not expect this because we hope LLMs can explain the motivation based on a thorough understanding of the paper, just like how human experts do.
Location: Experiments and Analyses

Evidence:
- Evidence Text: As shown in Table 8, unlike the open-source LLMs, the explanation performances of those closed-source LLMs are generally improved after adopting whole-list prompting. According to further manual checking, after maintaining the self-containment of the experiments, the LLMs can refer to other experiments and better grasp the underlying motivation of the current experiment.
  Strength: strong
  Location: Section 4.2
  Limitations: None
  Exact Quote: As shown in Table 8, unlike the open-source LLMs, the explanation performances of those closed-source LLMs are generally improved after adopting whole-list prompting.

Conclusion:
  Author's Conclusion: The authors conclude that LLMs should be able to explain the motivation based on a thorough understanding of the paper, just like how human experts do.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on empirical results from experiments, specifically the improvement in explanation performance after adopting whole-list prompting.
  Limitations: The study only focuses on closed-source LLMs, and the generalizability of the findings to other types of LLMs is not explored.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 29:
Statement: Hence, there is still a considerable gap between the LLMs and humans in terms of grasping research motivations.
Location: Experiments and Analyses

Evidence:
- Evidence Text: The results of the motivation explanation demonstrate that explaining motivations almost doesn’t require any paper context, i.e., the LLMs solely rely on the given experiments. However, we do not expect this because we hope LLMs can explain the motivation based on a thorough understanding of the paper, just like how human experts do.
  Strength: strong
  Location: Section 4.3
  Limitations: None
  Exact Quote: explaining motivations almost don’t require any paper context, i.e., the LLMs solely rely on the given experiments.

Conclusion:
  Author's Conclusion: There is still a considerable gap between the LLMs and humans in terms of grasping research motivations.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on the experimental results of the motivation explanation task, which provides a clear indication of the gap between LLMs and humans.
  Limitations: The study's focus on a specific task (motivation explanation) might not be representative of all research tasks, and the generalizability of the findings to other areas of research is uncertain.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 30:
Statement: We also investigate the effectiveness of multi-modal input for EXPDESIGN.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 10 shows the figure inputs ablation of EXPDESIGN. For the maximum text input length, same as the setting in Table 2, we use 2,000 and 3,000 words for open- and closed-source models, respectively. For the closed-source GPT-4o and GPT-4, as they have long context window, we use all the figures of each paper. While for InternVL2, we randomly select two figures per input paper.
  Strength: moderate
  Location: Section: Experiment Design
  Limitations: The study only considers two models (GPT-4o and InternVL2) and does not provide a comprehensive evaluation of multi-modal input effectiveness across various models.
  Exact Quote: Table 10: The figure inputs ablation of EXPDESIGN.

Conclusion:
  Author's Conclusion: The authors investigate the effectiveness of multi-modal input for EXPDESIGN by comparing the performance of different models with and without figure inputs.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on a direct comparison of the models' performance with and without figure inputs, providing a clear indication of the impact of multi-modal input on EXPDESIGN.
  Limitations: The study only considers two models (GPT-4o and InternVL2) and a limited number of figure inputs, which may not be representative of all possible models and input scenarios.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 31:
Statement: Intuitively, besides the text, when designing experiments for a given research topic, the figures can provide rich supplementary information, such as an algorithm illustration that can help better understand this research topic and underlying motivations.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 10 shows the figure inputs ablation of EXPDESIGN. For the maximum text input length, same as the setting in Table 2, we use 2,000 and 3,000 words for open- and closed-source models, respectively. For the closed-source GPT-4o and GPT-4, as they have long context window, we use all the figures of each paper. While for InternVL2, we randomly select two figures per input paper.
  Strength: weak
  Location: Table 10
  Limitations: The study only examines the impact of figures on EXPDESIGN, without considering other tasks or research topics.
  Exact Quote: For the maximum text input length, same as the setting in Table 2, we use 2,000 and 3,000 words for open- and closed-source models, respectively.

- Evidence Text: Table 11 shows the ablation study about the paper tables and figures of WEAKNESS. Based on the conclusion in Table 7, we use the “split-combine” to process the text input here (2,000 and 3,000 words context window size for open- and closed-source models). For GPT-4o, we use all the table/figure images; while for InternVL2, we randomly select two images per paper, i.e., two random figures, two random tables, or one random figure + table.
  Strength: weak
  Location: Table 11
  Limitations: The study only examines the impact of tables and figures on WEAKNESS, without considering other tasks or research topics.
  Exact Quote: Based on the conclusion in Table 7, we use the “split-combine” to process the text input here (2,000 and 3,000 words context window size for open- and closed-source models).

Conclusion:
  Author's Conclusion: The authors conclude that figures can provide rich supplementary information when designing experiments for a given research topic, helping to better understand the topic and underlying motivations.
  Conclusion Justified: No
  Robustness: The evidence is not robust, as it does not consistently support the claim across different models and tasks. The results are mixed, and the performance differences are not always significant.
  Limitations: The study only examines the impact of figures on LLM performance in two specific tasks (EXPDESIGN and WEAKNESS) and does not consider other potential benefits of including figures, such as improved human understanding or engagement.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 32:
Statement: Hence, we test different MLLMs’ performances, including GPT4-o, GPT-4, and InternVL2 (Chen et al. 2024b).
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 10 shows the ablation results on the figure data. To our surprise, the figure data doesn’t improve the MLLMs’ results in this task, even harming the performances. This might be due to the low informativeness of the figures, as figures usually consume more input tokens but act only as supplementary information to the text, indicating future work on developing MLLMs that can effectively leverage the scientific figures.
  Strength: strong
  Location: Section 5.3
  Limitations: None
  Exact Quote: To our surprise, the figure data doesn’t improve the MLLMs’ results in this task, even harming the performances.

Conclusion:
  Author's Conclusion: The authors conclude that testing different MLLMs' performances, including GPT4-o, GPT-4, and InternVL2, is necessary to evaluate their capacity in leveraging scientific figures.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on empirical results from multiple MLLMs (GPT4-o, GPT-4, and InternVL2). However, the robustness could be further strengthened by testing more MLLMs and evaluating their performances on a larger dataset.
  Limitations: The study only evaluates the performances of three MLLMs, which might not be representative of all MLLMs. Additionally, the dataset used for the evaluation might have biases that could impact the results.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 33:
Statement: Table 10 shows the ablation results on the figure data.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 10: The figure inputs ablation of EXPDESIGN. For the maximum text input length, same as the setting in Table 2, we use 2,000 and 3,000 words for open- and closed-source models, respectively. For the closed-source GPT-4o and GPT-4, as they have long context window, we use all the figures of each paper. While for InternVL2, we randomly select two figures per input paper.
  Strength: strong
  Location: Table 10
  Limitations: None
  Exact Quote: Table 10: The figure inputs ablation of EXPDESIGN. For the maximum text input length, same as the setting in Table 2, we use 2,000 and 3,000 words for open- and closed-source models, respectively. For the closed-source GPT-4o and GPT-4, as they have long context window, we use all the figures of each paper. While for InternVL2, we randomly select two figures per input paper.

Conclusion:
  Author's Conclusion: The ablation results on the figure data for EXPDESIGN are presented in Table 10.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on empirical results from experiments, providing a clear comparison of model performances with and without figure inputs.
  Limitations: None mentioned in the provided context.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 34:
Statement: To our surprise, the figure data doesn’t improve the MLLMs’ results in this task, even harming the performances.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 10 shows the figure inputs ablation of EXPDESIGN. For the maximum text input length, same as the setting in Table 2, we use 2,000 and 3,000 words for open- and closed-source models, respectively. For the closed-source GPT-4o and GPT-4, as they have long context window, we use all the figures of each paper. While for InternVL2, we randomly select two figures per input paper.
  Strength: strong
  Location: Table 10
  Limitations: None
  Exact Quote: To our surprise, the figure data doesn’t improve the MLLMs’ results in this task, even harming the performances.

Conclusion:
  Author's Conclusion: The figure data does not improve the MLLMs' results in this task, even harming the performances.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on a direct comparison of the MLLMs' performance with and without figure data. However, the sample size is limited to two MLLMs, which may not be representative of all MLLMs.
  Limitations: The study only examines the impact of figure data on the performance of MLLMs in this specific task, and the results may not generalize to other tasks or MLLMs. Additionally, the study does not investigate the reasons behind the lack of improvement with figure data.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 35:
Statement: This might be due to the low informativeness of the figures, as figures usually consume more input tokens but act only as supplementary information to the text, indicating future work on developing MLLMs that can effectively leverage the scientific figures.
Location: Experiments and Analyses

Evidence:
- Evidence Text: The results in Table 11 show that incorporating figures and tables does not significantly improve the performance of MLLMs, with InternVL2 being the only model to gain a slight boost from figures.
  Strength: strong
  Location: Table 11
  Limitations: Limited to the specific models and tasks evaluated in the study
  Exact Quote: Table 11: The ablation study about the paper tables and figures of WEAKNESS.

Conclusion:
  Author's Conclusion: The low informativeness of figures is the reason for MLLMs' poor performance with supplementary image inputs.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on empirical results from multiple models (GPT-4o and InternVL2) and different input types (figures, tables, and both). The results consistently show little to no improvement in performance, indicating a strong trend.
  Limitations: The study only examines the performance of two MLLMs, which might not be representative of all MLLMs. Further research with more models is needed to confirm the generalizability of the findings.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 36:
Statement: In WEAKNESS, we try to feed all the paper context into the LLMs.
Location: Experiments and Analyses

Evidence:
- Evidence Text: In WEAKNESS, we try to feed all the paper context into the LLMs. Given the unstable performance of LLMs, particularly closed-source ones, we run each model thrice during our experiments, selecting the median result from these repeated runs.
  Strength: strong
  Location: Implementation Details
  Limitations: None
  Exact Quote: In WEAKNESS, we try to feed all the paper context into the LLMs.

Conclusion:
  Author's Conclusion: The authors attempt to utilize the full paper context in the WEAKNESS task to evaluate the LLMs' performance.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it directly relates to the experimental setup and the authors' approach to handling the LLMs' performance variability.
  Limitations: None mentioned in the provided context.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 37:
Statement: We adopt a “split-combine” method — we first split the whole paper into several smaller pieces and let LLMs predict the weaknesses of each piece separately; after that, we combine all pieces’ weaknesses as a final complete prediction.
Location: Experiments and Analyses

Evidence:
- Evidence Text: In practice, for the length of each small piece, we set 2,000 and 3,000 words for open- and closed-source LLMs.
  Strength: strong
  Location: Implementation Details
  Limitations: None
  Exact Quote: In practice, for the length of each small piece, we set 2,000 and 3,000 words for open- and closed-source LLMs.

Conclusion:
  Author's Conclusion: The authors adopt a'split-combine' method to process long papers, splitting them into smaller pieces for LLM prediction and then combining the results.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it clearly outlines the procedure, making it reproducible. However, the choice of word count for each piece might be seen as somewhat arbitrary without further justification.
  Limitations: The choice of 2,000 and 3,000 words per piece might not be optimal for all scenarios and could be explored further.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 38:
Statement: In practice, for the length of each small piece, we set 2,000 and 3,000 words for open- and closed-source LLMs, respectively.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 7 shows the performance comparison of different input processing methods for WEAKNESS. We use GPT-4o and GPT-4-Turbo because both accept a maximum of 128k tokens input. We also put the results of AI-SCI in the table for reference. Here, “split-combine” splits the input paper into several pieces, where each piece’s length is denoted as “window size”; “no-split” means the conventional input cutting, for example, if the window size is 3,000, then only the first 3,000 words in the paper are used.
  Strength: strong
  Location: Table 7
  Limitations: None
  Exact Quote: In practice, for the length of each small piece, we set 2,000 and 3,000 words for open- and closed-source LLMs, respectively.

Conclusion:
  Author's Conclusion: The authors set the length of each small piece to 2,000 and 3,000 words for open- and closed-source LLMs, respectively, in practice for WEAKNESS.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on experimental results, and the conclusion is directly supported by the data in Table 7.
  Limitations: None mentioned in the provided context.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 39:
Statement: Additionally, in this task, we also examine the performance of a recent agent framework, namely AI-SCI (Lu et al. 2024), which enhances GPT4o’s paper review ability by leveraging advanced prompting techniques, e.g., self-reflection (Shinn et al. 2024) and response ensembling (Wang et al. 2023).
Location: Experiments and Analyses

Evidence:
- Evidence Text: AI-SCI (Lu et al. 2024) enhances GPT4o’s paper review ability by leveraging advanced prompting techniques, e.g., self-reflection (Shinn et al. 2024) and response ensembling (Wang et al. 2023).
  Strength: strong
  Location: Section 4.2
  Limitations: None
  Exact Quote: Additionally, in this task, we also examine the performance of a recent agent framework, namely AI-SCI (Lu et al. 2024), which enhances GPT4o’s paper review ability by leveraging advanced prompting techniques, e.g., self-reflection (Shinn et al. 2024) and response ensembling (Wang et al. 2023).

Conclusion:
  Author's Conclusion: AI-SCI enhances GPT4o's paper review ability by leveraging advanced prompting techniques.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on a specific framework (AI-SCI) and its techniques (self-reflection and response ensembling), providing a clear explanation for the enhancement.
  Limitations: None mentioned in the provided context.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 40:
Statement: Table 3 shows the main results, where the closed-source LLMs’ overall performances are generally superior to the results of open-source LLMs.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Closed-source LLMs generally outperform open-source LLMs in SN-F1, SN-Precision, and SN-Recall metrics in Table 3.
  Strength: strong
  Location: Table 3
  Limitations: None
  Exact Quote: Closed-source LLMs’ overall performances are generally superior to the results of open-source LLMs.

- Evidence Text: Gemini 1.5 Pro, GPT-4, GPT-4o, and o1-preview outperform open-source LLMs in SN-F1, SN-Precision, and SN-Recall metrics in Table 3.
  Strength: strong
  Location: Table 3
  Limitations: None
  Exact Quote: Gemini 1.5 Pro (48.75), GPT-4 (47.66), GPT-4o (47.73), and o1-preview (48.62) outperform open-source LLMs.

Conclusion:
  Author's Conclusion: Closed-source LLMs generally outperform open-source LLMs in the WEAKNESS task.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on quantitative metrics (SN-F1, SN-Precision, and SN-Recall) that comprehensively evaluate the performance of LLMs in the WEAKNESS task. The consistent outperformance of closed-source LLMs across multiple metrics strengthens the conclusion.
  Limitations: The analysis is limited to the specific LLMs and metrics evaluated in the study. Generalizability to other LLMs and tasks is not assessed.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 41:
Statement: Similarly, closed-source LLMs are particularly excellent in SN-Recall because of more generated weaknesses.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Closed-source LLMs generally outperform open-source LLMs, and both closed-/open-source LLMs are superior to the “Copy Input” baseline (except the Falcon). Despite the higher S-Precision, the open-source LLMs are seriously deficient in S-Recall compared with closed-source LLMs ( 10% ).
  Strength: strong
  Location: Table 2
  Limitations: None
  Exact Quote: Closed-source LLMs generally outperform open-source LLMs, and both closed-/open-source LLMs are superior to the “Copy Input” baseline (except the Falcon).

Conclusion:
  Author's Conclusion: Closed-source LLMs are particularly excellent in SN-Recall because of more generated weaknesses.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on a comparison of multiple LLMs across different metrics, providing a comprehensive evaluation of their performance. The significant difference in SN-Recall scores between closed-source and open-source LLMs strengthens the conclusion.
  Limitations: The analysis is limited to the specific tasks and datasets used in the study. The generalizability of the findings to other tasks and datasets is uncertain.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 42:
Statement: However, there is still a considerable gap in the weakness diversity between the LLMs and human experts.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Closed-source LLMs are particularly excellent in SN-Recall because of more generated weaknesses. However, there is still a considerable gap in the weakness diversity between the LLMs and human experts.
  Strength: strong
  Location: Section 5.3
  Limitations: None
  Exact Quote: Closed-source LLMs are particularly excellent in SN-Recall because of more generated weaknesses. However, there is still a considerable gap in the weakness diversity between the LLMs and human experts.

Conclusion:
  Author's Conclusion: There is still a considerable gap in the weakness diversity between the LLMs and human experts.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on quantitative metrics (SN-Recall) that directly measure the weakness diversity. However, the analysis might be limited by the dataset's scope and the specific tasks evaluated.
  Limitations: The analysis focuses on a specific aspect (SN-Recall) of the LLMs' performance and might not generalize to other tasks or metrics. Additionally, the dataset's representativeness and the annotation quality could influence the results.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 43:
Statement: Compared with human review, most LLM-generated weaknesses are vague and lack the necessary knowledge about some frontier research works.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Closed-source LLMs are particularly excellent in SN-Recall because of more generated weaknesses. However, there is still a considerable gap in the weakness diversity between the LLMs and human experts.
  Strength: strong
  Location: Section 5.3
  Limitations: None
  Exact Quote: Closed-source LLMs are particularly excellent in SN-Recall because of more generated weaknesses. However, there is still a considerable gap in the weakness diversity between the LLMs and human experts.

- Evidence Text: Surprisingly, AI-SCI performs worse than backbone GPT-4o, especially on ITF-IDF, which suggests the challenge of WEAKNESS, i.e., simply adopting popular prompting techniques cannot well address this task.
  Strength: moderate
  Location: Section 5.3
  Limitations: Specific to AI-SCI and GPT-4o comparison
  Exact Quote: Surprisingly, AI-SCI performs worse than backbone GPT-4o, especially on ITF-IDF, which suggests the challenge of WEAKNESS, i.e., simply adopting popular prompting techniques cannot well address this task.

Conclusion:
  Author's Conclusion: Most LLM-generated weaknesses are vague and lack the necessary knowledge about some frontier research works.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on the performance of various LLMs on the WEAKNESS task, including both closed-source and open-source models. The metrics used (SN-Recall, ITF-IDF) provide a comprehensive evaluation of the weaknesses generated by LLMs.
  Limitations: The analysis is limited to the specific task of WEAKNESS and the LLMs evaluated. The generalizability of the findings to other research tasks or LLMs is not assessed.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 44:
Statement: Surprisingly, AI-SCI performs worse than backbone GPT-4o, especially on ITF-IDF, which suggests the challenge of WEAKNESS, i.e., simply adopting popular prompting techniques cannot well address this task.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 3 shows the main results, where the closed-source LLMs’ overall performances are generally superior to the results of open-source LLMs. However, AI-SCI performs worse than backbone GPT-4o, especially on ITF-IDF, with a score of 2.23 compared to 5.95.
  Strength: strong
  Location: Section 5.3
  Limitations: None
  Exact Quote: AI-SCI performs worse than backbone GPT-4o, especially on ITF-IDF, which suggests the challenge of WEAKNESS, i.e., simply adopting popular prompting techniques cannot well address this task.

Conclusion:
  Author's Conclusion: The adoption of popular prompting techniques, such as self-reflection and response ensembling, in AI-SCI does not effectively address the WEAKNESS task, particularly in terms of ITF-IDF.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on a direct comparison of AI-SCI and GPT-4o's performance on the same task (WEAKNESS) and metric (ITF-IDF). The significant difference in scores indicates a clear trend, rather than a marginal or inconclusive result.
  Limitations: The analysis is limited to the specific task of WEAKNESS and the ITF-IDF metric. Further research is needed to generalize the findings to other tasks and metrics.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 45:
Statement: Q1: is the split-combine effective?
Location: Experiments and Analyses

Evidence:
- Evidence Text: As shown in Table 7, compared with giving the full paper contexts, split-combine generally brings about superior performances.
  Strength: strong
  Location: Section 5.3
  Limitations: None
  Exact Quote: Compared with giving the full paper contexts, split-combine generally brings about superior performances.

Conclusion:
  Author's Conclusion: The split-combine method is effective for the WEAKNESS task.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on a direct comparison of the split-combine method with the full paper context method, and the results are consistent across different models (GPT-4o, GPT-4-Turbo, and AI-SCI).
  Limitations: The study only examines the effectiveness of the split-combine method for the WEAKNESS task and does not explore its applicability to other tasks or domains.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 46:
Statement: Ideally, if the LLM has a sufficient context window size, it is not that necessary to split the input papers for separate processing.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 7 shows the performance comparison of different input processing methods for WEAKNESS. We use GPT-4o and GPT-4-Turbo because both accept a maximum of 128k tokens input. We also put the results of AI-SCI in the table for reference. Here, “split-combine” splits the input paper into several pieces, where each piece’s length is denoted as “window size”; “no-split” means the conventional input cutting, for example, if the window size is 3,000, then only the first 3,000 words in the paper are used.
  Strength: strong
  Location: Table 7
  Limitations: None
  Exact Quote: “split-combine” splits the input paper into several pieces, where each piece’s length is denoted as “window size”;

Conclusion:
  Author's Conclusion: Ideally, if the LLM has a sufficient context window size, it is not that necessary to split the input papers for separate processing.
  Conclusion Justified: No
  Robustness: The evidence is based on a limited number of models (GPT-4o and GPT-4-Turbo) and a specific task (WEAKNESS). The results may not generalize to other models or tasks. The evidence is robust in the sense that it provides a clear comparison between the'split-combine' and 'no-split' methods, but it is limited in scope.
  Limitations: Limited to GPT-4o and GPT-4-Turbo models, limited to WEAKNESS task
  Location: Experiments and Analyses

--------------------------------------------------

Claim 47:
Statement: Consequently, in this paragraph, we utilize the LLMs accepting long context input to compare “split-combine” with “no-split”, i.e., letting LLMs write weaknesses by giving the full paper.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 7: The performance comparison of different input processing methods for WEAKNESS. We use GPT-4o and GPT-4-Turbo because both accept a maximum of 128k tokens input. We also put the results of AI-SCI in the table for reference. Here, “split-combine” splits the input paper into several pieces, where each piece’s length is denoted as “window size”; “no-split” means the conventional input cutting, for example, if the window size is 3,000, then only the first 3,000 words in the paper are used. According to the data statistics, 20,000 words can cover maximum lengths of more than 95% of the papers in our dataset.
  Strength: strong
  Location: Table 7
  Limitations: None
  Exact Quote: We use GPT-4o and GPT-4-Turbo because both accept a maximum of 128k tokens input.

Conclusion:
  Author's Conclusion: The authors compare the performance of LLMs with'split-combine' and 'no-split' input processing methods for the WEAKNESS task.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on a direct comparison of the two methods, using a clear metric (SN-F1, SN-Precision, SN-Recall, ITF-IDF) and a sufficient sample size (993 instances).
  Limitations: The comparison is limited to two specific LLMs (GPT-4o and GPT-4-Turbo) and may not generalize to other models. Additionally, the 'no-split' method is only tested with a maximum input length of 20,000 words, which may not be sufficient for all papers.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 48:
Statement: As shown in Table 7, compared with giving the full paper contexts, split-combine generally brings about superior performances.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 7 shows the performance comparison of different input processing methods for WEAKNESS. The results indicate that split-combine generally brings about superior performances compared to giving the full paper contexts.
  Strength: strong
  Location: Table 7
  Limitations: None
  Exact Quote: split-combine generally brings about superior performances. During manual checking, we find that, when full paper is available, LLMs frequently neglect some important sections and omit weaknesses accordingly, while split-combine ensures that the LLMs can carefully brainstorm weaknesses within each smaller piece.

Conclusion:
  Author's Conclusion: Split-combine generally brings about superior performances compared to giving the full paper contexts.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on a comprehensive comparison of different input processing methods across multiple models, providing a thorough understanding of the performance differences.
  Limitations: The study only focuses on WEAKNESS and may not be generalizable to other tasks or models. Additionally, the performance differences between split-combine and no-split are relatively small in some cases.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 49:
Statement: During manual checking, we find that, when full paper is available, LLMs frequently neglect some important sections and omit weaknesses accordingly, while split-combine ensures that the LLMs can carefully brainstorm weaknesses within each smaller piece.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 7: The performance comparison of different input processing methods for WEAKNESS.
  Strength: strong
  Location: Table 7
  Limitations: None
  Exact Quote: split-combine generally brings about superior performances. During manual checking, we find that, when full paper is available, LLMs frequently neglect some important sections and omit weaknesses accordingly, while split-combine ensures that the LLMs can carefully brainstorm weaknesses within each smaller piece.

Conclusion:
  Author's Conclusion: During manual checking, we find that, when full paper is available, LLMs frequently neglect some important sections and omit weaknesses accordingly, while split-combine ensures that the LLMs can carefully brainstorm weaknesses within each smaller piece.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on a comprehensive comparison of different input processing methods across various LLMs, providing a reliable insight into the strengths and weaknesses of each approach.
  Limitations: The analysis is limited to the specific LLMs and datasets used in the study. Further research is needed to generalize the findings to other LLMs and domains.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 50:
Statement: Surprisingly, the LLMs’ performances with full paper context can be even worse than just remaining the first 3,000 words.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 7 shows the performance comparison of different input processing methods for WEAKNESS. The results indicate that the LLMs’ performances with full paper context can be even worse than just remaining the first 3,000 words, as seen in the SN-F1, SN-Precision, SN-Recall, and ITF-IDF scores for GPT-4o, GPT-4-Turbo, and AI-SCI.
  Strength: strong
  Location: Table 7
  Limitations: None
  Exact Quote: split-combine generally brings about superior performances. During manual checking, we find that, when full paper is available, LLMs frequently neglect some important sections and omit weaknesses accordingly, while split-combine ensures that the LLMs can carefully brainstorm weaknesses within each smaller piece. Surprisingly, the LLMs’ performances with full paper context can be even worse than just remaining the first 3,000 words.

Conclusion:
  Author's Conclusion: The LLMs’ performances with full paper context can be even worse than just remaining the first 3,000 words.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on quantitative metrics (SN-F1, SN-Precision, SN-Recall, and ITF-IDF scores) that provide a clear indication of the LLMs’ performance. However, the evidence may not be generalizable to all LLMs or tasks, as the results may vary depending on the specific model or task.
  Limitations: The study only examines the performance of a limited set of LLMs (GPT-4o, GPT-4-Turbo, and AI-SCI) and may not be representative of all LLMs. Additionally, the study focuses on a specific task (WEAKNESS) and may not be applicable to other tasks or domains.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 51:
Statement: This implies that even the current powerful long-context LLMs still fall short when processing long scientific documents.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Surprisingly, the LLMs’ performances with full paper context can be even worse than just remaining the first 3,000 words.
  Strength: strong
  Location: Table 7
  Limitations: None
  Exact Quote: Surprisingly, the LLMs’ performances with full paper context can be even worse than just remaining the first 3,000 words.

Conclusion:
  Author's Conclusion: This implies that even the current powerful long-context LLMs still fall short when processing long scientific documents.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on empirical results from experiments, which provides a strong indication of the LLMs' limitations. However, the generalizability of the findings to other LLMs and contexts is uncertain.
  Limitations: The study only examines the performance of a specific set of LLMs and does not provide insights into the underlying causes of their struggles with long documents. Further research is needed to fully understand the limitations of long-context LLMs.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 52:
Statement: Q2: does multi-modal input boost performance?
Location: Experiments and Analyses

Evidence:
- Evidence Text: Intuitively, besides the text, when designing experiments for a given research topic, the figures can provide rich supplementary information, such as an algorithm illustration that can help better understand this research topic and underlying motivations.
  Strength: weak
  Location: Section 4.4
  Limitations: The evidence is based on intuition and does not provide concrete results.
  Exact Quote: Intuitively, besides the text, when designing experiments for a given research topic, the figures can provide rich supplementary information, such as an algorithm illustration that can help better understand this research topic and underlying motivations.

- Evidence Text: However, there are intuitively some semantic or logical relations between different experiments, e.g., some experiments are prerequisites to others.
  Strength: weak
  Location: Section 4.4
  Limitations: The evidence is based on intuition and does not provide concrete results.
  Exact Quote: However, there are intuitively some semantic or logical relations between different experiments, e.g., some experiments are prerequisites to others.

- Evidence Text: Table 10 shows the ablation results on the figure data. To our surprise, the figure data doesn’t improve the MLLMs’ results in this task, even harming the performances.
  Strength: strong
  Location: Section 4.4
  Limitations: None
  Exact Quote: Table 10 shows the ablation results on the figure data. To our surprise, the figure data doesn’t improve the MLLMs’ results in this task, even harming the performances.

Conclusion:
  Author's Conclusion: The figure data does not improve the MLLMs' results in this task, even harming the performances.
  Conclusion Justified: No
  Robustness: The evidence is based on the ablation results on the figure data, which provides a clear and objective measure of the impact of multi-modal input on performance. However, the analysis is limited to a specific task and dataset, and may not be generalizable to other tasks or domains.
  Limitations: The study only examines the impact of figure data on performance and does not consider other types of multi-modal input. Additionally, the analysis is based on a specific dataset and task, which may not be representative of all possible scenarios.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 53:
Statement: Our dataset covers both tables and figure illustrations extracted from the paper PDF as inputs.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Intuitively, when reviewing a paper, both figures and tables are critical, not only for a better understanding, but also because some weaknesses are related to tables/figures.
  Strength: strong
  Location: Section: PAPERWEAKNESS
  Limitations: None
  Exact Quote: Intuitively, when reviewing a paper, both figures and tables are critical, not only for a better understanding, but also because some weaknesses are related to tables/figures.

Conclusion:
  Author's Conclusion: The dataset includes both tables and figure illustrations as inputs to facilitate a comprehensive review process.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on the inherent value of tables and figures in the review process, which is a widely accepted practice in academic and research settings.
  Limitations: None identified
  Location: Experiments and Analyses

--------------------------------------------------

Claim 54:
Statement: Intuitively, when reviewing a paper, both figures and tables are critical, not only for a better understanding, but also because some weaknesses are related to tables/figures.
Location: Experiments and Analyses

Evidence:
- Evidence Text: According to the data statistics, we also plot the review scores distribution of the papers used in the dataset, as well as the track distribution. As can be found in Figure 3, our dataset has a decent distribution, where the papers are uniformly distributed across 13 tracks, and most papers’ scores ranged from 5 to 8 (i.e., most papers are weakly rejected or accepted).
  Strength: strong
  Location: Figure 3
  Limitations: None
  Exact Quote: As can be found in Figure 3, our dataset has a decent distribution, where the papers are uniformly distributed across 13 tracks, and most papers’ scores ranged from 5 to 8 (i.e., most papers are weakly rejected or accepted).

Conclusion:
  Author's Conclusion: The claim suggests that figures and tables are crucial for reviewing a paper, not only for better understanding but also because some weaknesses are related to them.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on a well-distributed dataset, which increases the reliability of the conclusion. However, the evidence could be strengthened by providing more specific examples of weaknesses related to tables/figures.
  Limitations: The evidence does not provide explicit examples of weaknesses related to tables/figures, which could further reinforce the claim.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 55:
Statement: Therefore, in Table 11, we adopt two MLLMs to investigate the effectiveness of image inputs.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 11 shows the ablation study about the paper tables and figures of WEAKNESS. Based on the conclusion in Table 7, we use the “split-combine” to process the text input here (2,000 and 3,000 words context window size for open- and closed-source models). For GPT-4o, we use all the table/figure images; while for InternVL2, we randomly select two images per paper, i.e., two random figures, two random tables, or one random figure + table.
  Strength: strong
  Location: Table 11
  Limitations: None
  Exact Quote: Therefore, in Table 11, we adopt two MLLMs to investigate the effectiveness of image inputs.

Conclusion:
  Author's Conclusion: The authors investigate the effectiveness of image inputs using two MLLMs in Table 11.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on a systematic ablation study, which is a reliable method for investigating the effectiveness of image inputs. However, the robustness could be improved by using more MLLMs or exploring other aspects of image inputs.
  Limitations: The study only uses two MLLMs, which might not be representative of all MLLMs. Additionally, the study only investigates the effectiveness of image inputs in the context of WEAKNESS, which might not generalize to other tasks or domains.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 56:
Statement: Overall, image information, including both figures and tables, doesn’t bring significant performance improvement, i.e., only InternVL2 gains a performance boost after incorporating figures; while tables slightly drop both models’ results.
Location: Experiments and Analyses

Evidence:
- Evidence Text: Table 11 shows the ablation study about the paper tables and figures of WEAKNESS. Based on the conclusion in Table 7, we use the “split-combine” to process the text input here (2,000 and 3,000 words context window size for open- and closed-source models). For GPT-4o, we use all the table/figure images; while for InternVL2, we randomly select two images per paper, i.e., two random figures, two random tables, or one random figure + table.
  Strength: strong
  Location: Table 11
  Limitations: None
  Exact Quote: GPT-4o 47.73 42.09 55.48 5.95 w/ tables 46.76 41.32 54.17 5.53 w/ figures 46.62 41.20 54.04 5.48 w/ tables & figures 46.58 41.17 53.98 5.36

Conclusion:
  Author's Conclusion: The authors conclude that image information, including both figures and tables, does not significantly improve the performance of LLMs in the WEAKNESS task, with only InternVL2 showing a performance boost after incorporating figures, while tables slightly decrease both models' results.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on a systematic ablation study with a clear methodology (using the'split-combine' approach and controlling for context window size). However, the study's generalizability might be limited, as it only involves two LLMs (GPT-4o and InternVL2).
  Limitations: The study's scope is limited to two LLMs, and the results might not generalize to other models. Additionally, the evaluation metrics used might not capture the full range of potential benefits or drawbacks of incorporating image information.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 57:
Statement: This is probably because the MLLMs cannot reason well over the information-intensive images, especially the table images.
Location: Experiments and Analyses

Evidence:
- Evidence Text: This is probably because the MLLMs cannot reason well over the information-intensive images, especially the table images.
  Strength: strong
  Location: Section 5.3
  Limitations: None
  Exact Quote: This is probably because the MLLMs cannot reason well over the information-intensive images, especially the table images.

Conclusion:
  Author's Conclusion: The MLLMs cannot reason well over the information-intensive images, especially the table images, which is probably the reason why the image information, including both figures and tables, doesn’t bring significant performance improvement, i.e., only InternVL2 gains a performance boost after incorporating figures; while tables slightly drop both models’ results.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on the actual performance results of the MLLMs on the task, and the explanation provided is a reasonable interpretation of these results.
  Limitations: The conclusion is based on a limited set of experiments and models, and it is unclear whether the results would generalize to other tasks or models.
  Location: Experiments and Analyses

--------------------------------------------------

Claim 58:
Statement: In this work, we propose AAAR-1.0, a novel benchmark targeting a comprehensive evaluation of the current LLMs’ AI research capacity.
Location: Conclusion

Evidence:
- Evidence Text: The proposed benchmark, AAAR-1.0, is designed to evaluate the AI research capacity of current LLMs through three distinct expertise-intensive tasks: EQUATIONINFERENCE, EXPERIMENTDESIGN, and PAPERWEAKNESS.
  Strength: strong
  Location: Abstract
  Limitations: None
  Exact Quote: In this work, we propose AAAR-1.0, a novel benchmark targeting a comprehensive evaluation of the current LLMs’ AI research capacity.

- Evidence Text: The benchmark is composed of high-quality data collected by employing senior AI researchers, with multi-round strict data examination and filtering to avoid significant noise.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: Multiround strict data examination and filtering are conducted to try our best to avoid any significant noise in the data.

- Evidence Text: Extensive experiments across various mainstream LLMs highlight the challenges and values of AAAR-1.0, where there is still a considerable gap between LLMs and human experts.
  Strength: strong
  Location: Conclusion
  Limitations: None
  Exact Quote: In this work, we propose AAAR-1.0, a novel benchmark targeting a comprehensive evaluation of the current LLMs’ AI research capacity.

Conclusion:
  Author's Conclusion: The authors propose a novel benchmark, AAAR-1.0, to comprehensively evaluate the AI research capacity of current LLMs.
  Conclusion Justified: Yes
  Robustness: The evidence is robust as it is based on a well-designed benchmark with multiple tasks, high-quality data, and extensive experiments across various LLMs, providing a comprehensive evaluation of their AI research capacity.
  Limitations: The study's focus on current LLMs might limit the generalizability of the findings to future models. Additionally, the evaluation metrics, although task-specific, might not capture all nuances of AI research capacity.
  Location: Conclusion

--------------------------------------------------

Claim 59:
Statement: We devise three distinct expertise-intensive tasks along with the curated evaluation metrics, and collect high-quality data by employing senior AI researchers.
Location: Conclusion

Evidence:
- Evidence Text: We devise three distinct expertise-intensive tasks along with the curated evaluation metrics, and collect high-quality data by employing senior AI researchers. Multiround strict data examination and filtering are conducted to try our best to avoid any significant noise in the data.
  Strength: strong
  Location: Abstract
  Limitations: None
  Exact Quote: We devise three distinct expertise-intensive tasks along with the curated evaluation metrics, and collect high-quality data by employing senior AI researchers. Multiround strict data examination and filtering are conducted to try our best to avoid any significant noise in the data.

Conclusion:
  Author's Conclusion: The authors devised three expertise-intensive tasks and collected high-quality data with senior AI researchers, ensuring the data's quality through multiround examination and filtering.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it is based on the authors' own actions and the data collection process, which they have direct control over. The use of senior AI researchers and multiround examination adds to the evidence's strength, ensuring the data's quality and relevance.
  Limitations: None explicitly mentioned in the provided text snippet.
  Location: Conclusion

--------------------------------------------------

Claim 60:
Statement: Multiround strict data examination and filtering are conducted to try our best to avoid any significant noise in the data.
Location: Conclusion

Evidence:
- Evidence Text: Multiround strict data examination and filtering are conducted to try our best to avoid any significant noise in the data.
  Strength: strong
  Location: Introduction
  Limitations: 
  Exact Quote: Multiround strict data examination and filtering are conducted to try our best to avoid any significant noise in the data.

- Evidence Text: For each classification instance, we ask human experts to consider the following criteria: i) are all four equations (both positive and negative) grammatically correct? ii) after compilation, is there only one correct answer?
  Strength: strong
  Location: EQUATIONINFERENCE
  Limitations: 
  Exact Quote: For each classification instance, we ask human experts to consider the following criteria: i) are all four equations (both positive and negative) grammatically correct? ii) after compilation, is there only one correct answer?

- Evidence Text: We then employ human experts to conduct a further data review.
  Strength: strong
  Location: EQUATIONINFERENCE
  Limitations: 
  Exact Quote: We then employ human experts to conduct a further data review.

Conclusion:
  Author's Conclusion: Multiround strict data examination and filtering are conducted to ensure the quality of the data.
  Conclusion Justified: Yes
  Robustness: The evidence is robust, as it involves multiple rounds of examination and filtering by human experts, reducing the likelihood of noise in the data.
  Limitations: None mentioned in the provided text snippet.
  Location: Conclusion

--------------------------------------------------

Claim 61:
Statement: Extensive experiments across various mainstream LLMs highlight the challenges and values of AAAR-1.0, where there is still a considerable gap between LLMs and human experts.
Location: Conclusion

Evidence:
- Evidence Text: Table 1 shows the main results, where the open-source LLMs, especially the Falcon and Gemma, perform unexpectedly disappointing (even worse than random guesses).
  Strength: strong
  Location: Section 4.1
  Limitations: None
  Exact Quote: Table 1 shows the main results, where the open-source LLMs, especially the Falcon and Gemma, perform unexpectedly disappointing (even worse than random guesses).

- Evidence Text: Closed-source LLMs generally outperform open-source LLMs on AAAR-1.0, likely due to their richer scientific knowledge stemming from a larger model size.
  Strength: strong
  Location: Section 4.1
  Limitations: None
  Exact Quote: Closed-source LLMs generally outperform open-source LLMs on AAAR-1.0, likely due to their richer scientific knowledge stemming from a larger model size.

- Evidence Text: The recently-released GPT-4o solely gets 43.18, implying the unique challenge of EQINFER compared with other scientific QA benchmarks (Song et al. 2023).
  Strength: strong
  Location: Section 4.1
  Limitations: None
  Exact Quote: The recently-released GPT-4o solely gets 43.18, implying the unique challenge of EQINFER compared with other scientific QA benchmarks (Song et al. 2023).

Conclusion:
  Author's Conclusion: No conclusion available
  Conclusion Justified: No
  Robustness: N/A
  Limitations: N/A
  Location: Not specified

--------------------------------------------------

Execution Times:
claims_analysis_time: 975.51 seconds
evidence_analysis_time: 2372.19 seconds
conclusions_analysis_time: 2535.07 seconds
total_execution_time: 5888.51 seconds
