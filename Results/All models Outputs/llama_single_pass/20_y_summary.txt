Claim 1:
Type: methodology
Statement: A static method for neuron-level knowledge attribution in large language models is proposed.
Location: Section 1: Introduction
Exact Quote: In this paper, we propose a static method for pinpointing significant neurons.

Evidence:
- Evidence Text: Comparison with seven other methods shows superior performance across three metrics.
  Strength: strong
  Location: Section 4.1: Comparison of Attribution Methods
  Limitations: Limited to specific types of knowledge and models (GPT2-large and Llama-7B).
  Exact Quote: Our attribution method (second line) attributes more important neurons, resulting in the most significant reduction across all metrics in both GPT2 and Llama.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The proposed method's effectiveness is well-supported by the comparison with other methods, demonstrating its superiority.
Key Limitations: Generalizability to other knowledge types and models not explored in this study.

--------------------------------------------------

Claim 2:
Type: result
Statement: Both attention and FFN layers have the ability to store knowledge, and all top10 layers are in deep layers.
Location: Section 4.2: Exploration on Different Knowledge
Exact Quote: Both attention and FFN layers have ability to store knowledge, and all the top10 layers are in deep layers.

Evidence:
- Evidence Text: Layer-level knowledge storage analysis in GPT2 and Llama shows top10 layers are in deep layers.
  Strength: strong
  Location: Table 3 and Figure 4-5
  Limitations: Analysis limited to specific models (GPT2 and Llama-7B).
  Exact Quote: Both attention and FFN layers have ability to store knowledge, and all the top10 layers are in deep layers.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence from layer-level analysis directly supports the claim, showing deep layers' involvement in knowledge storage.
Key Limitations: Specifics of knowledge storage in other models not explored.

--------------------------------------------------

Claim 3:
Type: result
Statement: Intervening a few value neurons (300) or query neurons (1000) can significantly influence the final prediction.
Location: Section 4.2: Exploration on Different Knowledge
Exact Quote: When intervening the top200 attention neurons and top100 FFN neurons for each sentence, the MRR and probability decreases significantly.

Evidence:
- Evidence Text: Intervention experiments on top200 attention neurons and top100 FFN neurons show significant MRR and probability decreases.
  Strength: strong
  Location: Table 13
  Limitations: Specific intervention method and neuron selection might not generalize to all scenarios.
  Exact Quote: When intervening the top200 attention neurons and top100 FFN neurons for each sentence, the MRR and probability decreases significantly.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The intervention experiments provide direct evidence for the claim, demonstrating the significant impact of few neurons on predictions.
Key Limitations: Generalizability of intervention effects across different models and scenarios not fully explored.

--------------------------------------------------

