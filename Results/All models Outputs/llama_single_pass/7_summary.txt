Claim 1:
Type: performance
Statement: REALM outperforms all previous approaches by a significant margin on Open-QA tasks.
Location: Section 4.4
Exact Quote: Table 1 shows the accuracy of different approaches on the three Open-QA datasets. REALM outperform all previous approaches by a significant margin.

Evidence:
- Evidence Text: REALM achieves 39.2%, 40.2%, and 46.8% accuracy on NaturalQuestions-Open, WebQuestions, and CuratedTrec, respectively.
  Strength: strong
  Location: Table 1
  Limitations: None mentioned in the paper
  Exact Quote: REALM (X = Wikipedia, Z = Wikipedia) Dense Retr.+Transformer REALM 39.2 40.2 46.8 330m

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim, as REALM's performance is significantly higher than all other approaches on all three Open-QA datasets.
Key Limitations: None mentioned in the paper

--------------------------------------------------

Claim 2:
Type: methodology
Statement: REALM's improvement over ORQA is purely due to better pre-training.
Location: Section 4.4
Exact Quote: The improvement of REALM over ORQA is purely due to better pre-training.

Evidence:
- Evidence Text: REALM and ORQA have identical fine-tuning setups and training data, but REALM's pre-training differs.
  Strength: moderate
  Location: Section 4.4
  Limitations: Only comparing to ORQA, not other approaches
  Exact Quote: Among all systems, the most direct comparison with REALM is ORQA (Lee et al., 2019) where the fine-tuning setup and training data are identical.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence supports the claim, as the comparison to ORQA isolates the effect of pre-training, but the conclusion's robustness is medium due to the specific comparison.
Key Limitations: Comparison limited to ORQA

--------------------------------------------------

Claim 3:
Type: methodology
Statement: REALM can retrieve documents to fill in masked words even though it is trained with unsupervised text only.
Location: Section 4.5, Table 3
Exact Quote: REALM is able to retrieve document to fill in the masked word even though it is trained with unsupervised text only.

Evidence:
- Evidence Text: Example in Table 3 shows REALM retrieving a document with a related fact to predict a masked word.
  Strength: strong
  Location: Section 4.5, Table 3
  Limitations: Single example, not comprehensive evaluation
  Exact Quote: Table 3: An example where REALM utilizes retrieved documents to better predict masked tokens.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim, as the example demonstrates REALM's capability, but the conclusion's robustness is high due to the specific, illustrative nature of the evidence.
Key Limitations: Single example

--------------------------------------------------

