{
    "paper_analysis": [],
    "raw_claims": " For example:\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"The audio-visual video parsing task aims to parse a video into modality- and category-aware temporal segments.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Method\",\n            \"exact_quote\": \"The audio-visual video parsing task aims to parse a video into modality- and category-aware temporal segments.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"Previous work mainly focuses on weakly-supervised approaches, which learn from video-level event labels.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Method\",\n            \"exact_quote\": \"Previous work mainly focuses on weakly-supervised approaches, which learn from video-level event labels.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"The proposed Multi-modal Grouping Network (MGN) achieves improving results against previous baselines on weakly-supervised audiovisual video parsing.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed Multi-modal Grouping Network (MGN) achieves improving results against previous baselines on weakly-supervised audiovisual video parsing.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines (17 MB vs. 36 MB).\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines (17 MB vs. 36 MB).\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"The audio-visual video parsing task aims to parse a video into temporal event segments and predict the audible, visible, or audio-visible event categories.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Method\",\n            \"exact_quote\": \"The audio-visual video parsing task aims to parse a video into temporal event segments and predict the audible, visible, or audio-visible event categories.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"Existing approaches usually focus on learning to leverage the unimodal and cross-modal temporal contexts from weak supervisions.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Method\",\n            \"exact_quote\": \"Existing approaches usually focus on learning to leverage the unimodal and cross-modal temporal contexts from weak supervisions.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"The key motivation is to learn compact and discriminative audio and visual representations by explicit multi-modal grouping for mitigating the modality and temporal uncertainties in the weakly-supervised audio-visual video parsing problem.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Method\",\n            \"exact_quote\": \"The key motivation is to learn compact and discriminative audio and visual representations by explicit multi-modal grouping for mitigating the modality and temporal uncertainties in the weakly-supervised audio-visual video parsing problem.\"\n        },\n        {\n            \"claim_id\": 8,\n            \"claim_text\": \"The proposed Multi-modal Grouping Network (MGN) enables explicit grouping in a multi-modal network to learn compact and discriminative audio and visual embeddings.\",\n            \"location\": \"2 Related Work\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"The proposed Multi-modal Grouping Network (MGN) enables explicit grouping in a multi-modal network to learn compact and discriminative audio and visual embeddings.\"\n        },\n        {\n            \"claim_id\": 9,\n            \"claim_text\": \"The proposed MGN introduces class-aware unimodal grouping and modality-aware cross-modal grouping modules to aggregate multi-modal temporal contexts.\",\n            \"location\": \"2 Related Work\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"The proposed MGN introduces class-aware unimodal grouping and modality-aware cross-modal grouping modules to aggregate multi-modal temporal contexts.\"\n        },\n        {\n            \"claim_id\": 10,\n            \"claim_text\": \"The experiments can demonstrate the superiority of our MGN over state-of-the-art AVVP approaches and its generalizability to contrastive learning and label refinement.\",\n            \"location\": \"2 Related Work\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"The experiments can demonstrate the superiority of our MGN over state-of-the-art AVVP approaches and its generalizability to contrastive learning and label refinement.\"\n        },\n        {\n            \"claim_id\": 11,\n            \"claim_text\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\"\n        },\n        {\n            \"claim_id\": 12,\n            \"claim_text\": \"The proposed MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.\"\n        },\n        {\n            \"claim_id\": 13,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\"\n        },\n        {\n            \"claim_id\": 14,\n            \"claim_text\": \"Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual.\"\n        },\n        {\n            \"claim_id\": 15,\n            \"claim_text\": \"With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.\"\n        },\n        {\n            \"claim_id\": 16,\n            \"claim_text\": \"The proposed MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 17,\n            \"claim_text\": \"The proposed MGN achieves superior results against previous state-of-the-art methods [1, 2, 3, 4].\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves superior results against previous state-of-the-art methods [1, 2, 3, 4].\"\n        },\n        {\n            \"claim_id\": 18,\n            \"claim_text\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\"\n        },\n        {\n            \"claim_id\": 19,\n            \"claim_text\": \"The proposed MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.\"\n        },\n        {\n            \"claim_id\": 20,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\"\n        },\n        {\n            \"claim_id\": 21,\n            \"claim_text\": \"Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual.\"\n        },\n        {\n            \"claim_id\": 22,\n            \"claim_text\": \"With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.\"\n        },\n        {\n            \"claim_id\": 23,\n            \"claim_text\": \"The proposed MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 24,\n            \"claim_text\": \"The proposed MGN achieves superior results against previous state-of-the-art methods [1, 2, 3, 4].\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves superior results against previous state-of-the-art methods [1, 2, 3, 4].\"\n        },\n        {\n            \"claim_id\": 25,\n            \"claim_text\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\"\n        },\n        {\n            \"claim_id\": 26,\n            \"claim_text\": \"The proposed MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.\"\n        },\n        {\n            \"claim_id\": 27,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\"\n        },\n        {\n            \"claim_id\": 28,\n            \"claim_text\": \"Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual.\"\n        },\n        {\n            \"claim_id\": 29,\n            \"claim_text\": \"With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.\"\n        },\n        {\n            \"claim_id\": 30,\n            \"claim_text\": \"The proposed MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 31,\n            \"claim_text\": \"The proposed MGN achieves superior results against previous state-of-the-art methods [1, 2, 3, 4].\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves superior results against previous state-of-the-art methods [1, 2, 3, 4].\"\n        },\n        {\n            \"claim_id\": 32,\n            \"claim_text\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\"\n        },\n        {\n            \"claim_id\": 33,\n            \"claim_text\": \"The proposed MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.\"\n        },\n        {\n            \"claim_id\": 34,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\"\n        },\n        {\n            \"claim_id\": 35,\n            \"claim_text\": \"Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual.\"\n        },\n        {\n            \"claim_id\": 36,\n            \"claim_text\": \"With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.\"\n        },\n        {\n            \"claim_id\": 37,\n            \"claim_text\": \"The proposed MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 38,\n            \"claim_text\": \"The proposed MGN achieves superior results against previous state-of-the-art methods [1, 2, 3, 4].\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves superior results against previous state-of-the-art methods [1, 2, 3, 4].\"\n        },\n        {\n            \"claim_id\": 39,\n            \"claim_text\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\"\n        },\n        {\n            \"claim_id\": 40,\n            \"claim_text\": \"The proposed MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.\"\n        },\n        {\n            \"claim_id\": 41,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\"\n        },\n        {\n            \"claim_id\": 42,\n            \"claim_text\": \"Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual.\"\n        },\n        {\n            \"claim_id\": 43,\n            \"claim_text\": \"With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.\"\n        },\n        {\n            \"claim_id\": 44,\n            \"claim_text\": \"The proposed MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 45,\n            \"claim_text\": \"The proposed MGN achieves superior results against previous state-of-the-art methods [1, 2, 3, 4].\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves superior results against previous state-of-the-art methods [1, 2, 3, 4].\"\n        },\n        {\n            \"claim_id\": 46,\n            \"claim_text\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\"\n        },\n        {\n            \"claim_id\": 47,\n            \"claim_text\": \"The proposed MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.\"\n        },\n        {\n            \"claim_id\": 48,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\"\n        },\n        {\n            \"claim_id\": 49,\n            \"claim_text\": \"Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual.\"\n        },\n        {\n            \"claim_id\": 50,\n            \"claim_text\": \"With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.\"\n        },\n        {\n            \"claim_id\": 51,\n            \"claim_text\": \"The proposed MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 52,\n            \"claim_text\": \"The proposed MGN achieves superior results against previous state-of-the-art methods [1, 2, 3, 4].\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves superior results against previous state-of-the-art methods [1, 2, 3, 4].\"\n        },\n        {\n            \"claim_id\": 53,\n            \"claim_text\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\"\n        },\n        {\n            \"claim_id\": 54,\n            \"claim_text\": \"The proposed MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.\"\n        },\n        {\n            \"claim_id\": 55,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\"\n        },\n        {\n            \"claim_id\": 56,\n            \"claim_text\": \"Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual.\"\n        },\n        {\n            \"claim_id\": 57,\n            \"claim_text\": \"With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.\"\n        },\n        {\n            \"claim_id\": 58,\n            \"claim_text\": \"The proposed MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 59,\n            \"claim_text\": \"The proposed MGN achieves superior results against previous state-of-the-art methods [1, 2, 3, 4].\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves superior results against previous state-of-the-art methods [1, 2, 3, 4].\"\n        },\n        {\n            \"claim_id\": 60,\n            \"claim_text\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\"\n        },\n        {\n            \"claim_id\": 61,\n            \"claim_text\": \"The proposed MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.\"\n        },\n        {\n            \"claim_id\": 62,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\"\n        },\n        {\n            \"claim_id\": 63,\n            \"claim_text\": \"Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual.\"\n        },\n        {\n            \"claim_id\": 64,\n            \"claim_text\": \"With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.\"\n        },\n        {\n            \"claim_id\": 65,\n            \"claim_text\": \"The proposed MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 66,\n            \"claim_text\": \"The proposed MGN achieves superior results against previous state-of-the-art methods [1, 2, 3, 4].\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves superior results against previous state-of-the-art methods [1, 2, 3, 4].\"\n        },\n        {\n            \"claim_id\": 67,\n            \"claim_text\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\"\n        },\n        {\n            \"claim_id\": 68,\n            \"claim_text\": \"The proposed MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.\"\n        },\n        {\n            \"claim_id\": 69,\n            \"claim_text\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.\"\n        },\n        {\n            \"claim_id\": 70,\n            \"claim_text\": \"Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual.\"\n        },\n        {\n            \"claim_id\": 71,\n            \"claim_text\": \"With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.\"\n        },\n        {\n            \"claim_id\": 72,\n            \"claim_text\": \"The proposed MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.\"\n        },\n        {\n            \"claim_id\": 73,\n            \"claim_text\": \"The proposed MGN achieves superior results against previous state-of-the-art methods [1, 2, 3, 4].\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"The proposed MGN achieves superior results against previous state-of-the-art methods [1, 2, 3, 4].\"\n        },\n        {\n            \"claim_id\": 74,\n            \"claim_text\": \"The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics.\",\n            \"location\": \"4.2 Comparison to Prior Work\",\n            \"claim_type\": \"Result\",\n            \"exact",
    "raw_evidence": "",
    "raw_conclusions": "",
    "execution_times": {
        "claims_analysis_time": "310.34 seconds",
        "evidence_analysis_time": "1.28 seconds",
        "conclusions_analysis_time": "1.28 seconds",
        "total_execution_time": "322.64 seconds"
    }
}