{
    "paper_analysis": [],
    "raw_claims": " For example:\n        ```json\n        {\n            \"claims\": [\n                {\n                    \"claim_id\": 1,\n                    \"claim_text\": \"Example claim text\",\n                    \"location\": \"Section X\",\n                    \"claim_type\": \"Example type\",\n                    \"exact_quote\": \"Exact quote from the paper\"\n                }\n            ]\n        }\n        ```\n\n        Analyze the research paper and extract ALL possible claims made by the authors.\n\n        Paper text: # In-Context Retrieval-Augmented Language Models\n\n## Ori Ram[\u2217] Yoav Levine[\u2217] Itay Dalmedigos Dor Muhlgay Amnon Shashua Kevin Leyton-Brown Yoav Shoham\nAI21 Labs, Israel\n[orir,yoavl,itayd,dorm,amnons,kevinlb,yoavs](mailto:orir@ai21.com) @ai21.com\n_{_ _}_\n\n\n## Abstract\n\n\nRetrieval-Augmented Language Modeling\n(RALM) methods, which condition a language\nmodel (LM) on relevant documents from a\ngrounding corpus during generation, were\nshown to significantly improve language\nmodeling performance. In addition, they\ncan mitigate the problem of factually inaccurate text generation and provide natural\nsource attribution mechanism. Existing\nRALM approaches focus on modifying\nthe LM architecture in order to facilitate\nthe incorporation of external information,\nsignificantly complicating deployment. This\npaper considers a simple alternative, which\nwe dub In-Context RALM: leaving the LM\narchitecture unchanged and prepending\ngrounding documents to the input, without\n_any further training of the LM. We show that_\nIn-Context RALM that builds on off-the-shelf\ngeneral purpose retrievers provides surprisingly large LM gains across model sizes and\ndiverse corpora. We also demonstrate that the\ndocument retrieval and ranking mechanism\ncan be specialized to the RALM setting to\nfurther boost performance. We conclude that\nIn-Context RALM has considerable potential\nto increase the prevalence of LM grounding,\nparticularly in settings where a pretrained LM\nmust be used without modification or even\nvia API access.[1]\n\n## 1 Introduction\n\n\nRecent advances in language models (LMs)\nhave dramatically increased the usefulness of\nmachine-generated text across a wide range of\nuse-cases and domains (Brown et al., 2020). However, the mainstream paradigm of generating text\nwith LMs bears inherent limitations in access\nto external knowledge. First, LMs are not coupled with any source attribution, and must be\n\n_\u2217Equal contribution._\n[1Our code is available at https://github.com](https://github.com/AI21Labs/in-context-ralm)\n[/AI21Labs/in-context-ralm.](https://github.com/AI21Labs/in-context-ralm)\n\n\ntrained in order to incorporate up-to-date information that was not seen during training. More\nimportantly, they tend to produce factual inaccuracies and errors (Lin et al., 2022; Maynez\net al., 2020; Huang et al., 2020). This problem is\npresent in any LM generation scenario, and is exacerbated when generation is made in uncommon\ndomains or private data. A promising approach\nfor addressing the above is Retrieval-Augmented\nLanguage Modeling (RALM), grounding the LM\nduring generation by conditioning on relevant\ndocuments retrieved from an external knowledge\nsource. RALM systems include two high level\ncomponents: (i) document selection, selecting the\nset of documents upon which to condition; and\n(ii) document reading, determining how to incorporate the selected documents into the LM\ngeneration process.\nLeading RALM systems introduced recently\ntend to be focused on altering the language model\narchitecture (Khandelwal et al., 2020; Borgeaud\net al., 2022; Zhong et al., 2022; Levine et al.,\n2022c; Li et al., 2022). Notably, Borgeaud et al.\n(2022) introduced RETRO, featuring document\nreading via nontrivial modifications that require\nfurther training to the LM architecture, while using an off-the-shelf frozen BERT retriever for\ndocument selection. Although the paper\u2019s experimental findings showed impressive performance\ngains, the need for changes in architecture and dedicated retraining has hindered the wide adoption\nof such models.\nIn this paper, we show that a very simple document reading mechanism can have a large impact,\nand that substantial gains can also be made by\nadapting the document selection mechanism to the\ntask of language modeling. Thus, we show that\nmany of the benefits of RALM can be achieved\nwhile working with off-the-shelf LMs, even via\nAPI access. Specifically, we consider a simple but\npowerful RALM framework, dubbed In-Context\n\n\n1316\n\n\n-----\n\nFigure 1: An example of In-Context RALM: We simply prepend the retrieved document before the input prefix.\n\n\n_RALM (presented in Section 3), which employs_\na zero-effort document reading mechanism: We\nsimply prepend the selected documents to the\nLM\u2019s input text (Figure 1).\nSection 4 describes our experimental setup.\nTo show the wide applicability of our framework, we performed LM experiments on a suite\nof five diverse corpora: WikiText-103 (Merity\net al., 2016), RealNews (Zellers et al., 2019), and\nthree datasets from The Pile (Gao et al., 2021):\nArXiv, Stack Exchange, and FreeLaw. We use\nopen-source LMs ranging from 110M to 66B parameters (from the GPT-2, GPT-Neo, OPT, and\nLLaMA model families).\nIn Section 5 we evaluate the application of\noff-the-shelf retrievers to our framework. In this\nminimal-effort setting, we found that In-Context\nRALM led to LM performance gains equivalent\nto increasing the LM\u2019s number of parameters by\n2\u20133 across all of the text corpora we exam_\u00d7_\nined. In Section 6 we investigate methods for\nadapting document ranking to the LM task, a relatively under-explored RALM degree of freedom.\nOur adaptation methods range from using a small\nLM to perform zero-shot ranking of the retrieved\ndocuments, up to training a dedicated bidirectional reranker by employing self-supervision\n_from the LM signal. These methods lead to fur-_\nther gains in the LM task corresponding to an\nadditional size increase of 2 in the LM archi_\u00d7_\ntecture. As a concrete example of the gains, a\n345M parameter GPT-2 enhanced by In-Context\nRALM outperforms a 762M parameter GPT-2\nwhen employing an off-the-shelf BM25 retriever\n(Robertson and Zaragoza, 2009), and outperforms\na 1.5B parameter GPT-2 when employing our\ntrained LM-oriented reranker (see Figure 2). For\nlarge model sizes, our method is even more effective: In-Context RALM with an off-the-shelf\nretriever improved the performance of a 6.7B\nparameter OPT model to match that of a 66B\nparameter parameter OPT model (see Figure 4).\n\n\nFigure 2: Our framework, dubbed In-Context RALM,\nprovides large language modeling gains on the test set\nof WikiText-103, without modifying the LM. Adapting\nthe use of a BM25 retriever (Robertson and Zaragoza,\n2009) to the LM task ( 5) yields significant gains, and\n_\u00a7_\nchoosing the grounding documents via our new class\nof Predictive Rerankers ( 6) provides a further boost.\n_\u00a7_\nSee Table 1 for the full results on five diverse corpora.\n\nIn Section 7 we demonstrate the applicability of\nIn-Context RALM to downstream open-domain\nODQA tasks.\nIn a concurrent work, Shi et al. (2023) also suggest to augment off-the-shelf LMs with retrieved\ntexts by prepending them to the input. Their results are based on training a dedicated retriever for\nlanguage modeling. In contrast, we focus on the\ngains achievable in using off-the-shelf retrievers\nfor this task. We show strong gains of this simpler\nsetting by investigating: (1) which off-the-shelf\nretriever is best suited for language modeling, (2)\nthe frequency of retrieval operations, and (3) the\noptimal query length. In addition, we boost the\noff-the-shelf retrieval performance by introducing\ntwo reranking methods that demonstrate further\ngains in perplexity.\nWe believe that In-Context RALM can play\ntwo important roles in making RALM systems\nmore powerful and more prevalent. First, given\nits simple reading mechanism, In-Context RALM\ncan serve as a clean probe for developing document retrieval methods that are specialized for the\nLM task. These in turn can be used to improve\nboth In-Context RALM and other more elaborate\n\n\n1317\n\n\n-----\n\nRALM methods that currently leverage general\npurpose retrievers. Second, due to its compatibility with off-the-shelf LMs, In-Context RALM can\nhelp drive wider deployment of RALM systems.\n\n## 2 Related Work\n\n\nRALM approaches can be roughly divided into\ntwo families of models: (i) nearest-neighbor lan_guage models (also called kNN-LM), and (ii)_\n_retrieve and read models. Our work belongs to_\nthe second family, but is distinct in that it involves\nno further training of the LM.\n\nNearest Neighbor Language Models The\n_kNN-LM_ approach was first introduced in\nKhandelwal et al. (2020). The authors suggest a\nsimple inference-time model that interpolates between two next-token distributions: one induced\nby the LM itself, and one induced by the k neighbors from the retrieval corpus that are closest to the\nquery token in the LM embedding space. Zhong\net al. (2022) suggest a framework for training\nthese models. While they showed significant gains\nfrom kNN-LM, the approach requires storing the\nrepresentations for each token in the corpus, an\nexpensive requirement even for a small corpus\nlike Wikipedia. Although numerous approaches\nhave been suggested for alleviating this issue (He\net al., 2021; Alon et al., 2022), scaling any of them\nto large corpora remains an open challenge.\n\nRetrieve and Read Models This family of\nRALMs creates a clear division between docu_ment selection and document reading components._\nAll prior work involves training the LM. We\nbegin by describing works that use this approach for tackling downstream tasks, and then\nmention works oriented towards RALM. Lewis\net al. (2020) and Izacard and Grave (2021) fine\ntuned encoder\u2013decoder architectures for downstream knowledge-intensive tasks. Izacard et al.\n(2022b) explored different ways of pretraining such models, while Levine et al. (2022c)\npretrained an autoregressive LM on clusters\nof nearest neighbors in sentence embedding\nspace. Levine et al. (2022a) showed competitive\nopen domain question-answering performance by\nprompt-tuning a frozen LM as a reader. Guu\net al. (2020) pretrained REALM, a retrieval augmented bidirectional, masked LM, later fine-tuned\nfor open-domain question answering. The work\nclosest to this paper\u2014with a focus on the\n\n\nlanguage modeling task\u2014is RETRO (Borgeaud\net al., 2022), which modifies an autoregressive\nLM to attend to relevant documents via chunked\ncross-attention, thus introducing new parameters\nto the model. Our In-Context RALM differs from\nprior work in this family of models in two key\naspects:\n\nWe use off-the-shelf LMs for document\n\n_\u2022_\nreading without any further training of\n_LM._\n\nWe focus on how to choose documents for\n\n_\u2022_\n_improved LM performance._\n\n\n## 3 Our Framework\n\n3.1 In-Context RALM\n\n\nLanguage models define probability distributions\nover sequences of tokens. Given such a sequence\n_x1,..., xn, the standard way to model its probabil-_\nity is via next-token prediction: p(x1,..., xn) =\n\ufffdn\n_i=1_ _[p][(][x][i][|][x][<i][)][, where][ x][<i][ :=][ x][1][,..., x][i][\u2212][1][ is the]_\nsequence of tokens preceding xi, also referred to\nas its prefix. This autoregressive model is usually\nimplemented via a learned transformer network\n(Vaswani et al., 2017) parameterized by the set of\nparameters \u03b8:\n\n\n_p(x1,..., xn) =_\n\n\n_n_\n\ufffd\n\n_p\u03b8(xi|x<i),_ (1)\n_i=1_\n\n\nwhere the conditional probabilities are modeled by\nemploying a causal self-attention mask (Radford\net al., 2018). Notably, leading LMs such as GPT-2\n(Radford et al., 2019), GPT-3 (Brown et al., 2020),\nOPT (Zhang et al., 2022), or Jurassic-1 (Lieber\net al., 2021) follow this simple parameterization.\nRetrieval augmented language models\n(RALMs) add an operation that retrieves one or\nmore documents from an external corpus, and\n_C_\ncondition the above LM predictions on these documents. Specifically, for predicting xi, the retrieval\noperation from C depends on its prefix: RC(x<i),\nso the most general RALM decomposition is:\n_p(x1,..., xn) =_ [\ufffd]i[n]=1 _[p][(][x][i][|][x][<i][,][ R][C][(][x][<i][))][. In]_\norder to condition the LM generation on the\nretrieved document, previous RALM approaches\nused specialized architectures or algorithms\n(see 2). Inspired by the success of In-Context\n_\u00a7_\nLearning (Brown et al., 2020; Dong et al., 2023),\n_In-Context RALM refers to the following specific,_\nsimple method of concatenating the retrieved\n\n\n1318\n\n\n-----\n\ndocuments[2] within the Transformer\u2019s input prior\nto the prefix (see Figure 1), which does not\n_involve altering the LM weights \u03b8:_\n\n_p(x1,..., xn) =_\n\n\n_n_\n\n(2)\n\n\ufffd\n\n_p\u03b8 (xi| [RC(x<i); x<i]),_\n_i=1_\n\nwhere [a; b] denotes the concatenation of strings a\nand b.\nSince common Transformer-based LM implementations support limited length input sequences,\nwhen the concatenation of the document and the\ninput sequence exceed this limit we remove tokens from the beginning of x until the overall input\nlength equals that allowed by the model. Because\nour retrieved documents are passages of limited\nlength, we always have enough context left from\n_x (see_ 4.3).\n_\u00a7_\n\n\nRetrieval Query Length While the retrieval\nquery above in principle depends on all prefix\ntokens x\u2264s\u00b7j, the information at the very end\nof the prefix is typically the most relevant to\nthe generated tokens. If the retrieval query is\ntoo long then this information can be diluted.\nTo avoid this, we restrict the retrieval query at\nstride j to the last \u2113 tokens of the prefix, i.e.,\nwe use qj[s,\u2113] := xs\u00b7j\u2212\u2113+1,..., xs\u00b7j. We refer to\n_\u2113_ as the retrieval query length. Note that prior\nRALM work couples the retrieval stride s and the\nretrieval query length \u2113 (Borgeaud et al., 2022).\nIn 5, we show that enforcing s = \u2113 degrades LM\n_\u00a7_\nperformance. Integrating these hyper-parameters\ninto the In-Context RALM formulation gives\n\n_p(x1,..., xn) =_\n\n\n_s_\n\ufffd \ufffd \ufffd \ufffd\ufffd\n\n_p\u03b8_ _xs\u00b7j+i|_ _RC(qj[s,\u2113][);][ x][<][(][s][\u00b7][j][+][i][)]_ _._\n_i=1_\n\n(4)\n\n\n3.2 RALM Design Choices\n\nWe detail below two practical design choices often\nmade in RALM systems. In 5, we investigate the\n_\u00a7_\neffect of these in the setting of In-Context RALM.\n\nRetrieval Stride While in the above formulation a retrieval operation can occur at each\ngeneration step, we might want to perform retrieval only once every s > 1 tokens due to the\ncost of calling the retriever, and the need to replace\nthe documents in the LM prefix during generation.\nWe refer to s as the retrieval stride. This gives rise\nto the following In-Context RALM formulation\n(which reduces back to Eq. (2) for s = 1):\n\n\n_ns\u22121_\n\ufffd\n\n_j=0_\n\n\n## 4 Experimental Details\n\nWe now describe our experimental setup, including all models we use and their implementation\ndetails.\n\n\n4.1 Datasets\n\nWe evaluated the effectiveness of In-Context\nRALM across five diverse language modeling\ndatasets and two common open-domain question\nanswering datasets.\n\n\n_p(x1,..., xn) =_\n\n\n_ns\u22121_ _s_\n\ufffd \ufffd _p\u03b8_ \ufffdxs\u00b7j+i| \ufffdRC(x\u2264s\u00b7j); x<(s\u00b7j+i)\ufffd\ufffd _,_\n\n_j=0_ _i=1_\n\n(3)\nwhere ns = n/s is the number of retrieval strides.\nNotably, in this framework the runtime costs\nof each retrieval operation is composed of (a)\napplying the retriever itself, and (b) recomputing\nthe embeddings of the prefix. In 5.2 we show\n_\u00a7_\nthat using smaller retrieval strides, i.e., retrieving\nas often as possible, is superior to using larger\nones (though In-Context RALM with larger strides\nalready provides large gains over vanilla LM).\nThus, choosing the retrieval stride is ultimately a\ntradeoff between runtime and performance.\n\n2We always use a single document, but it is conceptually\nsimple to support multiple documents as well.\n\n\nLanguage Modeling The first LM dataset is\n_WikiText-103 (Merity et al., 2016), which has been_\nextensively used to evaluate RALMs (Khandelwal\net al., 2020; He et al., 2021; Borgeaud et al., 2022;\nAlon et al., 2022; Zhong et al., 2022). Second,\nwe chose three datasets spanning diverse subjects\nfrom The Pile (Gao et al., 2021): ArXiv, Stack\n_Exchange, and FreeLaw. Finally, we also investi-_\ngated RealNews (Zellers et al., 2019), since The\nPile lacks a corpus focused only on news (which\nis by nature a knowledge-intensive domain).\n\nOpen-Domain Question Answering In order\nto evaluate In-Context RALM on downstream\ntasks as well, we use the Natural Questions (NQ;\nKwiatkowski et al. 2019) and TriviaQA (Joshi\net al., 2017) open-domain question answering\ndatasets.\n\n\n1319\n\n\n-----\n\n4.2 Models\n\nLanguage Models We performed our experiments using the four models of GPT-2\n(110M\u20131.5B; Radford et al., 2019), three models\nof GPT-Neo and GPT-J (1.3B\u20136B; Black et al.,\n2021; Wang and Komatsuzaki, 2021), eight models of OPT (125M\u201366B; Zhang et al. 2022), and\nthree models of LLaMA (7B\u201333B; Touvron et al.,\n2023). All models are open source and publicly\navailable.[3]\n\nWe elected to study these particular models\nfor the following reasons. The first four (GPT-2)\nmodels were trained on WebText (Radford et al.,\n2019), with Wikipedia documents excluded from\ntheir training datasets. We were thus able to evaluate our method\u2019s \u2018\u2018zero-shot\u2019\u2019 performance when\nretrieving from a novel corpus (for WikiText-103).\nThe rest of the models brought two further benefits. First, they allowed us to investigate how\nour methods scale to models larger than GPT-2.\nSecond, the fact that Wikipedia was part of their\ntraining data allowed us to investigate the usefulness of In-Context RALM for corpora seen\nduring training. The helpfulness of such retrieval\nhas been demonstrated for previous RALM methods (Khandelwal et al., 2020) and has also been\njustified theoretically by Levine et al. (2022c).\nWe ran all models with a maximum sequence\nlength of 1,024, even though GPT-Neo, OPT,\nand LLaMA models support a sequence length of\n2,048.[4]\n\n\n4.3 Implementation Details\n\nWe implemented our code base using the Transformers library (Wolf et al., 2020). We based\nour dense retrieval code on the DPR repository\n(Karpukhin et al., 2020).\n\n\nRetrieval Corpora For WikiText-103 and\nODQA datasets, we used the Wikipedia corpus\nfrom Dec. 20, 2018, standardized by Karpukhin\net al. (2020) using the preprocessing from Chen\net al. (2017). To avoid contamination, we found\nand removed all 120 articles of the development\nand test set of WikiText-103 from the corpus.\nFor the remaining datasets, we used their training\ndata as the retrieval corpus. Similar to Karpukhin\net al. (2020), our retrieval corpora consist of\nnon-overlapping passages of 100 words (which\ntranslate to less than 150 tokens for the vast\nmajority of passages). Thus, we truncate our\nretrieved passages at 256 tokens when input to\nthe models, but they are usually much smaller.\n\nRetrieval For sparse retrieval, we used the\nPyserini library (Lin et al., 2021). For dense\nretrieval, we applied exact search using FAISS\n(Johnson et al., 2021).\n\n\nRetrievers We experimented with both sparse\n(word-based) and dense (neural) retrievers. We\nused BM25 (Robertson and Zaragoza, 2009) as our\nsparse model. For dense models, we experimented\nwith (i) a frozen BERT-base (Devlin et al., 2019)\nfollowed by mean pooling, similar to Borgeaud\net al. (2022); and (ii) the Contriever (Izacard\net al., 2022a) and Spider (Ram et al., 2022) models,\nwhich are dense retrievers that were trained in\nunsupervised manners.\n\nReranking When training rerankers (Section 6.2), we initialized from RoBERTa-base (Liu\net al., 2019).\n\n[3All models are available for use use via https://](https://huggingface.co/)\n[huggingface.co/.](https://huggingface.co/)\n4In preliminary experiments, we observed similar improvements from In-Context RALM when using a sequence\nlength of 2,048. We used a sequence length of 1,024 in order\nto facilitate a direct comparison between all models.\n\n\n## 5 The Effectiveness of In-Context RALM with Off-the-Shelf Retrievers\n\nWe now empirically show that despite its simple\ndocument reading mechanism, In-Context RALM\nleads to substantial LM gains across our diverse\nevaluation suite. We begin in this section by\ninvestigating the effectiveness of off-the-shelf retrievers for In-Context RALM; we go on in 6\n_\u00a7_\nto show that further LM gains can be made by\ntailoring document rankingfunctionstotheLMtask.\nThe experiments in this section provided us\nwith a recommended configuration for applying\nIn-Context RALM: applying a sparse BM25 retriever that receives \u2113 = 32 query tokens and\nis applied as frequently as possible. Practically,\nwe retrieve every s = 4 tokens (\u2113 and s are defined in 3). Table 1 shows for the GPT-2 models\n_\u00a7_\nthat across all the examined corpora, employing\nIn-Context RALM with an off-the-shelf retriever\nimproved LM perplexity to a sufficient extent that\nit matched that of a 2\u20133 larger model. Figure 4\n_\u00d7_\nand Tables 2 and 5 show that this trend holds\nacross model sizes up to 66B parameters, for both\nWikiText-103 and RealNews.\n\n\n1320\n\n\n-----\n\nWikiText-103 RealNews ArXiv Stack Exch. FreeLaw\nModel Retrieval Reranking\n\nword ppl token ppl token ppl token ppl token ppl\n\n\nGPT-2 S\n\nGPT-2 M\n\n\nGPT-2 L\n\nGPT-2 XL\n\n\n\u2013 \u2013 37.5 21.3 12.0 12.8 13.0\nBM25 5 \u2013 29.6 16.1 10.9 11.3 9.6\n_\u00a7_\nBM25 Zero-shot 6.1 28.6 15.5 10.1 10.6 8.8\n_\u00a7_\nBM25 Predictive 6.2 26.8 \u2013 \u2013 \u2013 \u2013\n_\u00a7_\n\n\u2013 \u2013 26.3 15.7 9.3 8.8 9.6\nBM25 5 \u2013 21.5 12.4 8.6 8.1 7.4\n_\u00a7_\nBM25 Zero-shot 6.1 20.8 12.0 8.0 7.7 6.9\n_\u00a7_\nBM25 Predictive 6.2 19.7 \u2013 \u2013 \u2013 \u2013\n_\u00a7_\n\n\u2013 \u2013 22.0 13.6 8.4 8.5 8.7\nBM25 5 \u2013 18.1 10.9 7.8 7.8 6.8\n_\u00a7_\nBM25 Zero-shot 6.1 17.6 10.6 7.3 7.4 6.4\n_\u00a7_\nBM25 Predictive 6.2 16.6 \u2013 \u2013 \u2013 \u2013\n_\u00a7_\n\n\u2013 \u2013 20.0 12.4 7.8 8.0 8.0\nBM25 5 \u2013 16.6 10.1 7.2 7.4 6.4\n_\u00a7_\nBM25 Zero-shot 6.1 16.1 9.8 6.8 7.1 6.0\n_\u00a7_\nBM25 Predictive 6.2 15.4 \u2013 \u2013 \u2013 \u2013\n_\u00a7_\n\n\nTable 1: Perplexity on the test set of WikiText-103, RealNews and three datasets from the Pile. For\neach LM, we report: (a) its performance without retrieval, (b) its performance when fed the top-scored\npassage by BM25 ( 5), and (c) its performance when applied on the top-scored passage of each of our\n_\u00a7_\ntwo suggested rerankers ( 6). All models share the same vocabulary, thus token-level perplexity (token\n_\u00a7_\n_ppl) numbers are comparable. For WikiText we follow prior work and report word-level perplexity_\n(word ppl).\n\nWikiText-103\nModel Retrieval\n\nword ppl\n\n\n\u2013 9.9\nLLaMA-7B\nBM25, 5 8.8\n_\u00a7_\n\n\u2013 8.5\nLLaMA-13B\nBM25, 5 7.6\n_\u00a7_\n\n\u2013 6.3\nLLaMA-33B\nBM25, 5 6.1\n_\u00a7_\n\nTable 2: The performance of models from the\nLLaMA family, measured by word-level perplexity on the test set of WikiText-103.\n\n\n5.1 BM25 Outperforms Off-the-Shelf Neural\nRetrievers in Language Modeling\n\nWe experimented with different off-the-shelf\ngeneral purpose retrievers, and found that the\nsparse (lexical) BM25 retriever (Robertson and\nZaragoza, 2009) outperformed three popular dense\n(neural) retrievers: the self-supervised retrievers\nContriever (Izacard et al., 2022a) and Spider (Ram\net al., 2022), as well as a retriever based on the average pooling of BERT embeddings that was used\nin the RETRO system (Borgeaud et al., 2022).\n\n\nFigure 3: The performance of four off-the-shelf retrievers used for In-Context RALM on the development set\nof WikiText-103. All RALMs are run with s = 4\n(i.e., retrieval is applied every four tokens). For each\nRALM, we report the result of the best query length \u2113\n(see Figures 6, 9, 10).\n\nWe conducted a minimal hyper-parameter search\non the query length \u2113 for each of the retrievers,\nand found that \u2113 = 32 was optimal for BM25\n(Figure 6), and \u2113 = 64 worked best for dense\nretrievers (Figures 9, 10).\nFigure 3 compares the performance gains of InContext RALM with these four general-purpose\nretrievers. The BM25 retriever clearly outperformed all dense retrievers. This outcome is\nconsistent with prior work showing that BM25\n\n\n1321\n\n\n-----\n\nFigure 4: Results of OPT models (Zhang et al., 2022) on the test set of WikiText-103 (word-level perplexity) and\nthe development set of RealNews (token-level perplexity). In-Context RALM models use a BM25 retriever with\n_s = 4 (i.e., the retriever is called every four tokens) and \u2113_ = 32 (i.e., the retriever query is comprised of the last\n32 tokens of the prefix). In-Context RALM with an off-the-shelf retriever improved the performance of a 6.7B\n_parameter OPT model to match that of a 66B parameter OPT model._\n\n\nFigure 5: An analysis of perplexity as a function of s,\nthe retrieval stride, i.e., the number of tokens between\nconsecutive retrieval operations, on the development\nset of WikiText-103. Throughout the paper, we use\n_s = 4 to balance perplexity and runtime._\n\noutperforms neural retrievers across a wide\narray of tasks, when applied in zero-shot settings (Thakur et al., 2021). This result renders\nIn-Context RALM even more appealing since applying a BM25 retriever is significantly cheaper\nthan the neural alternatives.\n\n\nLM performance improved as the retrieval operation became more frequent. This supports the\nintuition that retrieved documents become more\nrelevant the closer the retrieval query becomes\nto the generated tokens. Of course, each retrieval\noperation imposes a runtime cost. To balance\nperformance and runtime, we used s = 4 in\nour experiments. For comparison, RETRO employed a retrieval frequency of s = 64 (Borgeaud\net al., 2022), which leads to large degradation in\nperplexity. Intuitively, retrieving with high frequency (low retrieval stride) allows to ground the\nLM in higher resolution.\n\n5.3 A Contextualization vs. Recency\nTradeoff in Query Length\n\n\n5.2 Frequent Retrieval Improves\nLanguage Modeling\n\nWe investigated the effect of varying the retrieval\nstride s (i.e., the number of tokens between consecutive retrieval operations). Figure 5 shows that\n\n\nWe also investigated the effect of varying \u2113, the\nlength of the retrieval query for BM25. Figure 6\nreveals an interesting tradeoff and a sweet spot\naround a query length of 32 tokens. Similar experiments for dense retrievers are given in Appendix A.\nWe conjecture that when the retriever query is\ntoo short, it does not include enough of the input context, decreasing the retrieved document\u2019s\nrelevance. Conversely, excessively growing the\nretriever query deemphasizes the tokens at the very\nend of the prefix, diluting the query\u2019s relevance to\nthe LM task.\n\n\n1322\n\n\n-----\n\nFigure 6: An analysis of perplexity as a function of\n_the number of tokens in the query \u2113_ for BM25 on the\ndevelopment set of WikiText-103. In the appendix,\nwe show similar trade-offs for dense retrievers within\nWikiText-103. Throughout the paper, we use a query\nlength of \u2113 = 32 tokens.\n\n## 6 Improving In-Context RALM with LM-Oriented Reranking\n\n\nFigure 7: Potential for gains from reranking. Perplexity improvement (on the development set of\nWikiText-103) from an oracle that takes the best of\nthe top-16 documents retrieved by BM25 rather than\nthe first.\n\nprovide further LM gains (results in forth row for\neach of the models in Table 1).\n\n\nSince In-Context RALM uses a fixed document\nreading component by definition, it is natural to\nask whether performance can be improved by\nspecializing its document retrieval mechanism to\nthe LM task. Indeed, there is considerable scope\nfor improvement: the previous section considered\nconditioning the model only on the first document\nretrieved by the BM25 retriever. This permits very\nlimited semantic understanding of the query, since\nBM25 is based only on the bag of words signal.\nMoreover, it offers no way to accord different\ndegrees of importance to different retrieval query\ntokens, such as recognizing that later query tokens\nare more relevant to the generated text.\nIn this section, we focus on choosing which\ndocument to present to the model, by reranking the\ntop-k documents returned by the BM25 retriever.[5]\n\nWe use Figure 7 as motivation: It shows the\nlarge potential for improvement among the top-16\ndocuments returned by the BM25 retriever. We\nact upon this motivation by using two rerankers.\nSpecifically, in 6.1 we show performance gains\n_\u00a7_\nacross our evaluation suite obtained by using an\nLM to perform zero-shot reranking of the top-k\nBM25 retrieved documents (results in third row\nfor each of the models in Table 1). Then, in 6.2\n_\u00a7_\nwe show that training a specialized bidirectional\nreranker of the top-k BM25 retrieved documents\nin a self-supervised manner via the LM signal can\n\n5In both \u00a76.1 and \u00a76.2 we use k = 16.\n\n\n6.1 LMs as Zero-Shot Rerankers\n\nFirst, we used off-the-shelf language models as\ndocument rerankers for the In-Context RALM\nsetting. Formally, for a query q consisting of the\nlast \u2113 tokens in the prefix of the LM input x, let\n_{d1,..., dk} be the top-k documents returned by_\nBM25. For retrieval iteration j, let the text for\ngeneration be y := xs\u00b7j+1,..., xs\u00b7j+s. Ideally, we\nwould like to find the document di[\u2217] that maximizes\nthe probability of the text for generation, i.e.,\n\n\n_i[\u2217]_ = arg max (5)\n_i\u2208[k]_",
    "raw_evidence": "",
    "raw_conclusions": "",
    "execution_times": {
        "claims_analysis_time": "333.32 seconds",
        "evidence_analysis_time": "1.84 seconds",
        "conclusions_analysis_time": "1.84 seconds",
        "total_execution_time": "344.80 seconds"
    }
}