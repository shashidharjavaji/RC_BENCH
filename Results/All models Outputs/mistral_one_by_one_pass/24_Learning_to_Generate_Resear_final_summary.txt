=== Paper Analysis Summary ===

Raw Claims:
 For example:
        ```json
        {
            "claims": [
                {
                    "claim_id": 1,
                    "claim_text": "Example claim text",
                    "location": "Section X",
                    "claim_type": "Example type",
                    "exact_quote": "Exact quote from the paper"
                }
            ]
        }
        ```

        Paper text: # Learning to Generate Research Idea with Dynamic Control

## Abstract

The rapid advancements in large language models (LLMs)
have demonstrated their potential to accelerate scientific discovery, particularly in automating the process of research
ideation. LLM-based systems have shown promise in generating hypotheses and research ideas. However, current approaches predominantly rely on prompting-based pre-trained
models, limiting their ability to optimize generated content
effectively. Moreover, they also lack the capability to deal with
the complex interdependence and inherent restrictions among
novelty, feasibility, and effectiveness, which remains challenging due to the inherent trade-offs among these dimensions,
such as the innovation-feasibility conflict. To address these
limitations, we propose a novel framework that employs a
two-stage approach combining Supervised Fine-Tuning (SFT)
and controllable Reinforcement Learning (RL). In the SFT
stage, the model learns foundational patterns from pairs of
research papers and follow-up ideas. In the RL stage, multidimensional reward modeling, guided by fine-grained feedback, evaluates and optimizes the generated ideas across key
metrics. Dimensional controllers enable dynamic adjustment
of generation, while a sentence-level decoder ensures contextaware emphasis during inference. Our framework provides a
balanced approach to research ideation, achieving high-quality
outcomes by dynamically navigating the trade-offs among
novelty, feasibility, and effectiveness.

## Introduction

In recent years, advances in LLM have been made in their
capacity to accelerate scientific discovery. Specifically, LLMs
like GPT-4 and LLama have demonstrated their capability to
produce coherent and contextually relevant text across diverse
applications, including sentiment analysis (Zhang et al. 2023;
Yang et al. 2024a; Zhu et al. 2024), question answering (spr
2024), and summarization (Authors 2023). Moreover, their
remarkable performance in multi-step reasoning and complex
decision-making (Zhou et al. 2023; Park et al. 2023) has
underscored their potential in the field of research ideation.
Typically, a well-developed research idea (or hypothesis)[1] is
established with a methodology and an experiment plan, as
shown in Figure 1.
By automating the research ideation process, these research idea generation systems can swiftly synthesize vast

1In this paper, research idea and hypothesis are used interchangeably.


Figure 1: Research ideas generation from research papers.
Each idea is measured across dimensions of novelty, feasibility, and effectiveness.

data and insights, uncovering novel connections that might
elude human researchers. This capability is evidenced by the
growing number of studies employing research agents to autonomously generate and validate new ideas (Wang and Zhou
2023; Du, Kim, and Park 2023; Bornstein and Singh 2024).
However, despite notable progress in LLM-based research
ideation as demonstrated in prior work, these efforts primarily rely on pre-trained models with no task-specific learning.
Such reliance restricts the full exploitation of optimizing the
generated content, underscoring the urgent need for further
refinement and development in this area.
The quality assessment of a research idea involves multiple aspects, centered around key metrics in three dimensions: (1) Novelty, which assesses how unique or original
the ideas generated by the system are, distinguishing them
from existing ideas; (2) Feasibility evaluates how practical
or implementable the ideas are given current resources and
constraints and (3) Effectiveness, which measures how likely
the generated ideas will achieve their intended outcomes or
solve identified problems. These metrics can serve as optimization objectives to guide the research ideation process, by
leveraging techniques such as reinforcement learning (RL)().
Recent studies have explored Reinforcement Learning from
Human Feedback (RLHF) to benefit LLM training (). However, existing techniques lack of capability to deal with the
complex interdependence and inherent restrictions among
the metrics used for assessing research idea quality. For instance, a recent study highlights these challenges by revealing
the inevitable innovation-feasibility trade-off: highly novel
hypotheses may lack feasibility, while overly feasible ideas
often limit the scope for groundbreaking discoveries. (Chen
et al. 2024b). How to optimize research ideation towards
each of the key metrics while balancing them with satisfying
trade-offs remains a critical, unresolved question.
To address this issue, we propose a novel research ideation
framework designed to dynamically control the emphasis
on key assessment metrics through a two-stage approach:
SFT and controllable RL. In the SFT stage, the idea generator learns foundational patterns by training on pairs of
research papers and corresponding follow-up ideas. In the
RL stage, we employ multi-dimensional reward modeling as
a real-world assessment approximation (Wu et al. 2023). Reward models, trained on fine-grained feedback from review
data, score each metric—novelty, feasibility, and effectiveness—providing detailed guidance for model refinement. To
enable precise and adaptive control, we introduce dimensional controllers, trained alongside the RL process, which
adjusts the generation style to prioritize specific metric dimensions when necessary. This is complemented at inference
time by a sentence-level decoder that dynamically adjusts
the weights of controllers, ensuring context-aware emphasis such as prioritizing novelty in the method section and feasibility in the experiment planning. Together, these mechanisms,
guided by feedback signals from the reward models, result in
more balanced and high-quality idea generation.
Our contributions are summarized as follows:

 - We propose a research ideation framework to dynamically
control the optimization of the generated idea towards
novelty, feasibility, and effectiveness.

 - We first introduce dynamic decoding into the RL-based
supervised fine-tuning framework, achieving satisfying
performance with a balanced trade-off among different
assessment metrics of research ideation.

 - We train our reward models using collected real-world
datasets, enabling research idea scoring in a fine-grained
manner.

 - We conduct a comprehensive evaluation with a human
study, demonstrating the effectiveness of our proposed
method for optimized, controllable research ideation.


## Related Work

**NLP for scientific discovery NLP techniques have signifi-**
cantly advanced scientific discovery by enabling researchers
to manage extensive literature, identify knowledge gaps, and
analyze trends effectively (Raghu and Schmidt 2020; Hope
et al. 2021). Models such as SciBERT (Beltagy, Lo, and Cohan 2019) and BioBERT (Lee et al. 2020) pre-trained on
scientific materials have enhanced these abilities by improving performance on fundamental tasks. Recent developments
in LLMs have extended their utility to creative and generative tasks in scientific research. For example, LLMs have
been employed to formulate research questions, generate
hypotheses, draft research proposals, and even outline experimental designs (Brown et al. 2020; Zhong et al. 2023; Qi
et al. 2023; Yang et al. 2023; Wang et al. 2024a). Several
prior works have specifically explored methods to enhance
idea generation. Approaches such as iterative novelty boosting (Wang et al. 2024b), multi-agent collaboration (Baek
et al. 2024), and multi-module retrieval and revision (Yang
et al. 2024b) have been proposed to advance ideation capabilities beyond baseline prompting methods. Beyond ideation,
another branch of research leverages LLMs for automating
experimental workflows. Works like MLAgent (Huang et al.
2024) and SciCode (Tian et al. 2024) have used LLMs to generate code for executing research experiments, while systems
such as AI Scientist (Lu et al. 2024) and MLR-Copilot (Li
et al. 2024) combine idea generation with code implementation to directly test AI-generated concepts. However, these
approaches are often limited to constrained problem spaces
or rely on proxy metrics for evaluation, such as LLM-based
scoring, which can be inconsistent and unreliable.

**Fine-tuning LLM with RL** RLHF has shown success in diverse NLP tasks (Christiano et al. 2017; Stiennon et al. 2020;
Ouyang et al. 2022), including text summarization (Ziegler
et al. 2019), instruction following (Ouyang et al. 2022), and
question answering (Nakano et al. 2021). While most works
focus on optimizing a single holistic reward combining multiple objectives, recent efforts have explored fine-grained
rewards for specific attributes, such as reasoning or ethical
considerations (Glaese et al. 2022; Uesato et al. 2022).
Additionally, non-RL methods have leveraged feedback to
improve model outputs. For example, supervised fine-tuning
has been used with high-scoring samples selected by reward
models (Rafailov et al. 2023). Conversational models have
incorporated binary user satisfaction signals to enhance response generation (Askell et al. 2021), while natural language feedback has been stored in memory banks and retrieved during task execution (Madaan et al. 2022). Some
approaches refine outputs conditioned on human feedback
and subsequently use reward models to select the best refinements (Scheurer et al. 2022; Menick et al. 2022).

## Method

We introduce a scientific idea proposer with multi-dimension
feedback, which consists of two stages: supervised finetuning stage, and reinforcement learning stage that has three
components: reward modeling, multi-dimension reward augmented controllable reinforcement learning, and decoding.


-----

### Overview

Suppose we have a training set D = {Xi, Yi}i[N]=1[, where]
_Xi and Yi are research paper and idea, respectively. Then_
we fine-tune the language model M with the training
set. Thereafter, we collect a reward training set Dr =
_{(Xi[r][, Y][ n]i_ _[, Y]i[ f]_ _[, Y][ e]i_ [)]i[N]=1[}][, where][ X][i][ include the textual con-]
tent of research paper and research idea, and Yi[n][, Y][ f]i _[, Y][ e]i_ [are]
the labels which show the scores of novelty, feasibility, and
effectiveness of research idea. We could utilize this training
set to train three reward models as follows,
Fn = Rn(Xi[r][, Y]i[ n][|][Θ][n][)][,]

_Ff = Rf_ (Xi[r][, Y][ f]i _[|][Θ][f]_ [)][,] (1)

Fe = Re(Xi[r][, Y][ e]i _[|][Θ][e][)]_

where Θn/f/e is the parameters of the reward model Rn/f/e.
denotes reward models that aim to score the novelty,
_R[n/f/e]_
feasibility, and effectiveness of the research idea. Fn/f/e is
reward values from reward models. Then, we use a set of
_Nf research papers {Pi}i[N]=1[f]_ [as input to the language model]
to generate research ideas, which are assessed with reward
models based on three criteria: novelty, feasibility, and effectiveness. Finally, we conduct reinforcement learning on the
language model as,
_H = M(P_ _|Θm, Θn, Θf_ _, Θe),_ (2)
where Θm is final optimized parameters of the language
model. During which the dimensional controllers are
_M_
jointly trained to improve its ability to generate high-quality
research ideas with fine-grained control at inference time.
During this process, three dimensional controllers are trained
jointly with the language model to enable fine-grained control
at inference time.

### Supervised Fine-Tuning

To make the model training more stable in reinforcement
learning (Chen et al. 2024a), we also introduce the supervised
fin-tuning stage.
**Data Collection. To conduct a Supervised Fine-Tuning**
stage, we first collect papers from the ICLR 2023 and 2024.
We selected papers from ICLR as training data due to its
prestigious standing as a top-tier conference in the field of
machine learning, offering cutting-edge research and highquality technical discussions. We sample 1,000 instances
_P =_ _p_ for training. We utilize the LLaMA with a prompt
_{_ _}_
(detailed in appendix ) to extract the research idea y from
the sampled paper p as the golden output. To extract the one corresponding input paper x for each output, we select
the one most significant supporting paper from all related
works ˆx1, ˆx2..., ˆxn by prompting LLaMA of the abstract and
introduction section of p, together with the citation counts of
_xˆ1, ˆx2..., ˆxn within the sampled paper p._
**Fine-Tuning. Based on the collected training set, we fine-**
tune the language model as follows,
_M_

_Lsup = CE(Y,_ _Y[ˆ] )_ (3)

where CE( ) denotes the cross-entropy loss and _Y[ˆ] is the_

_·_
predicted research idea from, formulated as _Y[ˆ] =_ (X).
_M_ _M_
_X is the research paper and Y is the research idea._


### Reward Modeling

Researchers mainly consider three aspects when they devise a research idea: novelty, feasibility, and effectiveness.
Therefore, we train three distinct reward models to score the
generated idea in reinforcement learning, each corresponding
to one of the quality dimensions.
**Multi-dimension Feedback Collection. To train reward**
models, we need to collect three kinds of feedback. Similar
to the supervised fine-tuning stage, we also use the papers
from ICLR[2]. Specifically, we collect the review data from
OpenReview platform[3], and we also get the research idea by
prompting the language model. For the Novelty score of
_M_
the research idea in ICLR 2023, we could use the novelty
score from the review directly. As for ICLR 2024, we prompt
the LLM to get novelty scores since they don’t provide direct
ratings (see Appendix for prompt). Similarly, since there is
no feasibility score or effectiveness score in the review, we
prompt the LLM to get scores for every research idea. Feasibility score is mainly based on the experiment setup and
method sections, taking into account factors such as dataset
size, model complexity, and relevant review comments, while
Effectiveness score is derived primarily from the experimental results and corresponding review comments. The detailed
Scoring Criteria for Novelty, Feasibility, and Effectiveness
are outlined in Appendix.
Notably, all the collected novelty, feasibility, and effectiveness are subsequently normalized to a 0-1 scale for training.
**Reward Model Training. We select an LLM as the back-**
bone of reward models. To make the model predict the score
for each dimension, we add a Multi-Layer Perceptron as
follows,
�Fn/f/e = An/f/e (X _r),_

(4)
_Fˆn/f/e = Cn/f/e(Fn/f/e),_

where Cn/f/e are MLPs which can output score for each dimension. An/f/e is the LLM backbone. Each reward model
takes the generated idea as input and outputs a score Fn/f/e
between 0 and 1, representing its evaluation of novelty, feasibility, or effectiveness. To optimize the reward models, we
utilize cross-entropy loss as follows,

_Ln/f/e = CE( F[ˆ]n/f/e, Fn/f/e),_ (5)

where Fn/f/e is the ground-truth label.

### Multi-dimension Reward Augmented Controllable Reinforcement Learning

In this stage, we fine-tune the research idea proposer with
controllable steering through reinforcement learning??, refining the model based on feedback across three dimensions:
novelty, feasibility, and effectiveness.
**Dimensional Controllers Inspired by the existing work**
(Han et al. 2024), we introduce the dimensional controllers
of the novelty, feasibility, and effectiveness of the generated
idea, as these dimensions often exhibit interdependency and

2https://iclr.cc/
3https://docs.openreview.net/reference/api-v2.


-----

**Research** **Proposer** **Controller** novelty Method: We apply semantic Novelty

**Papers** score = 0.5 divergence minimized RM

prompt [...] and prioritize

Feasibility candidate concepts with Feasibility

score = 0.6 semantic similarity to reduce RM

hallucinations [...]

Effectiveness Effective
Experiment Plan: [....]

score = 0.4 ness RM

Figure 2: The learning framework with dynamic control across 3 dimensions. Generated research ideas are assessed by
corresponding reward models, which provide scores for each dimension. These scores guide the fine-tuning process during
reinforcement learning, optimizing both the idea proposer and the corresponding dimensional control parameters to enhance the
quality of idea generation.


trade-offs. We achieve this by adding additional control parameters (i.e. the steers) as follows,
M[l]n [=][ M][l] [+][ ϵ][n][W][n][M][l][,]


**M[l]f** [=][ M][l] [+][ ϵ][f] **[W][f]** **[M][l][,]** (6)

M[l]e [=][ M][l] [+][ ϵ][e][W][e][M][l][,]

where Ml represents the output of l-th layer in the LLM.
_ϵn, ϵf_, and ϵe are the hyper-parameters for controlling novelty, feasibility, and effectiveness. Wn, Wf, and We are
learnable parameters. In the training stage, we set all ϵn,
_ϵf_, and ϵe as 1. By this, we use M[l]n/f/e [to replace the]
original output of the l-th layer. We denote the parameters for each resulting model as Θn = {ΘLLM _, ΘϵnWnMl_ _},_
Θf = {ΘLLM _, Θϵf Wf Ml_ _} and Θe = {ΘLLM_ _, ΘϵeWeMl_ _}._
**Reward. Specifically, we get all three kinds of rewards for**
each research idea based on the well-trained reward model.
We define rn, rf, and re as the novelty, feasibility, and effectiveness rewards for the research idea. Then we have a
reward function for each dimension of the research idea at
timestep t as follows,

 _t_
rt[n] [=][ −]��i=1t I(i = K)wlrn,

_rt[f]_ [=][ −] I(i = K)wlrf _,_ (7)

rt[e] [=][ −]�i=1t I(i = K)wlre,

_i=1_

where K is the token length of the research idea. t is the
timestep. I(·) is the indicator function. wl is a weight assigned to rewards. Thereafter, we utilize the PPO algorithm
(Schulman et al. 2017) to train the model following the existing work (Jing and Du 2024). More details of PPO algorithm
can be found in Appendix.

### Decoding

In this part, we devise two decoding methods for the inference
stage.


Figure 3: Decoding RNN dynamically steers the dimensions
for a balanced and context-aware generation. It starts with
(ϵ[0]n[, ϵ][0]f _[, ϵ]e[0][)][, and predicts the control parameter weights for]_
the next sentence, based on the context generated by the
combined proposer and controller.

**Naive Static Decoding. In this decoding method, we set**
_ϵn, ϵf_, and ϵe as fixed values for the steers. To achieve a high
score over novelty, feasibility, and effectiveness, we set all
_ϵn, ϵf_, and ϵe as 1, because we set them as 1 in the training
stage for maximum novelty, feasibility, and effectiveness.
**Goal-driven Dynamic Decoding. The goal of achieving a**
good research idea is not only to blindly improve the result
of a certain dimension but also to consider the overall quality.
For example, too high a degree of novelty may result in a
low effectiveness (Si, Yang, and Hashimoto 2024a), while
different parts of a research idea, such as the method and
experiment planning, may require varying levels of focus on
novelty and feasibility. Therefore, how to balance novelty,
feasibility, and effectiveness in the inference stage is important for generating a good idea. To achieve this, we utilize an


-----

RNN (Sherstinsky 2020) to predict the steer value ϵn, ϵf, and
_ϵe, because RNN is good at sequence-level prediction (Figure_
3).
To optimize the RNN for steer values prediction, we first
collect 1,000 high-quality research ideas generated with Idea
Proposer (above 8 in overall score). hereafter, we get the corresponding controller weights using our three reward models
for each sentence of the high-quality research idea. Specifically, we feed each sentence in the research idea into our reward models to get the rewards as ˆrn, ˆrf, ˆre. Furthermore, we
normalize the reward and get the corresponding steer values
of each sentence as ˆϵn/f/e = (ˆrn/f/e _sn/f/e)/(an/f/e_
_−_ _−_
_sn/f/e) × w[′], where sn/f/e and an/f/e are the minimum_
value and maximum value for all rewards and w[′] is the maximal controller weight, which is 5 in our case. This reflects
the controller-weight ratios between 3 controllers, as well as
the absolute scale of each controller weight from 0-5. After
the data collection, we can use the pair (S[t], s[t]n/f/e[+1] [)][ to train]
the model as follows,

_Lrnn = CE(RNN_ (S[<t]), s[t]n/f/e[)][,] (8)

where S[<t] is the previous t 1 sentences in the research
_−_
idea and s[t]n/f/e [is steer values][ ϵ][n][,][ ϵ][f] [, and][ ϵ][e][ of][ t][-th sentence.
Therefore, we can use the well-trained RNN to predict the
controller weights of the next sentence based on the current
generated sentence in the inference phrase.

## Experiment

### Dataset and Analysis

We collect a dataset of 6,765 usable research papers in total submitted to ICLR[4]and NeurIPS[5] in the years 2023 and
2024, including both accepted and rejected submissions and
filtered 5,687 usable data. 4,271 of them are used for training,
and 500 are sampled for evaluation. Each paper contains its
abstract, methodology, and experiment sections. Additionally, review data from OpenReview[6] provides human ratings
for overall quality as well as the review contents and key
sub-dimensions - novelty, feasibility, and effectiveness. Paper
content is scraped with title from the Semantic Scholar[7] and
arXiv APIs[8] and then cleaned up with regular expression
to extract corresponding sections. These papers and ratings
are used to: 1. Derive ground-truth ideas for supervised finetuning. 2. Train reward models for the key dimensions. 3.
Optimize idea generation using reinforcement learning with
multi-dimensional steering.
The dataset is split into the following subsets:

1. Supervised Fine-Tuning split.: We use 1,000 papers from
only ICLR to derive the golden generated idea, paired with
the most supporting related work idea as input to fine-tune
the model.

4https://iclr.cc/
5https://neurips.cc/
6https://docs.openreview.net/reference/api-v2.
7https://www.semanticscholar.org/product/api.
8https://arxiv.org/help/api.


2. Reinforcement Learning split.: 3,271 research papers
from both ICLR and NeurIPS with detailed reviews are
used to train three distinct reward models for novelty,
feasibility, and effectiveness, each capturing expert evaluations for further reinforcement learning.

3. Evaluation split.: 500 research papers from both ICLR
and NeurIPS are sampled for evaluation, of which 30 are
randomly selected for manual expert evaluation.
Figures 4 and 5 provide an overview of the dataset distribution and top keywords.

Figure 4: Rating distribution statistics of our dataset.

Figure 5: Top 10 topic distribution of our dataset.

**Evaluation** The evaluation is performed on two datasets:
500 papers of the evaluation split for automatic evaluation,
and a subset of 30 papers are selected for manual expert evaluation. We measure performance across three core metrics
(details in Appendix):

 - Novelty: Evaluates how original and creative the generated ideas are, compared to existing works.

 - Feasibility: Assesses the practical implementation and
the likelihood that the idea can be executed within typical
resource constraints.

 - Effectiveness: Measures the potential improvement or
impact of the generated idea when compared to baseline
models.
We split our evaluation into two types:

1. Automatic Evaluation: For automatic evaluation, we
evaluate novelty, feasibility, and effectiveness of the generated ideas with prompt-based method. We adopt GPT-4
as our reviewing agent.


-----

|Model|Novelty Feasibility Effectiveness|Overall|
|---|---|---|


_T5-SFT_ 3.3 5.1 4.2 4.2
_T5-RLHF_ 3.9 5.3 4.9 4.7
_LLaMA2-SFT_ 4.8 5.9 5.2 5.3
_LLaMA2-RLHF_ 5.5 6.1 5.6 5.8

_LLaMA2-RLHF + Novelty Ctrl_ 6.4 5.9 5.5 6.0
_LLaMA2-RLHF + Feasibility Ctrl_ 5.3 7.2 5.2 5.6
_LLaMA2-RLHF + Effectiveness Ctrl_ 5.6 6.0 6.4 5.9

_LLaMA2-RLHF + All Ctrls (Static)_ 5.8 6.0 5.5 5.9
_LLaMA2-RLHF + All Ctrls (Dynamic)_ 6.0 6.1 5.8 6.2

Table 1: Experiment Results with Novelty (N), Feasibility (F), Effectiveness (E), and Overall Scores. N/F/E Ctrl (abbrev. for
_Control) represents that only 1 corresponding controller is enabled, whereas All Ctrl activate all the 3 controllers. Static and_
dynamic denote different decoding strategies.

|T5-SFT T5-RLHF LLaMA2-SFT LLaMA2-RLHF|3.3 5.1 4.2 3.9 5.3 4.9 4.8 5.9 5.2 5.5 6.1 5.6|4.2 4.7 5.3 5.8|
|---|---|---|

|LLaMA2-RLHF + Novelty Ctrl LLaMA2-RLHF + Feasibility Ctrl LLaMA2-RLHF + Effectiveness Ctrl|6.4 5.9 5.5 5.3 7.2 5.2 5.6 6.0 6.4|6.0 5.6 5.9|
|---|---|---|

|LLaMA2-RLHF + All Ctrls (Static) LLaMA2-RLHF + All Ctrls (Dynamic)|5.8 6.0 5.5 6.0 6.1 5.8|5.9 6.2|
|---|---|---|


2. Manual Evaluation: For manual evaluation, we select 30
papers and have domain experts assess the quality of the
generated ideas of the selected model (SFT, RLHF and
RLHF with Dynamic Controls), providing human scores
for novelty, feasibility, and effectiveness. These scores are
then compared with the scores generated by our automatic
reviewing agent to measure the alignment between human
judgment and the agent’s reviews.

### Main Experiments

**Baselines and Setups** We establish several baselines to
evaluate the effectiveness of different control strategies applied to the LLaMA2-RLHF model. The baselines include
T5-SFT, T5-RLHF, and LLaMA2-SFT, representing varying
levels of model capacity and reinforcement learning application. These baselines are chosen to compare the impact
of applying reinforcement learning fine-tuning (RLHF) and
enabling targeted controls for Novelty, Feasibility, and Effective.

 - T5-SFT: A version of the T5 model trained using SFT
on 1,000 examples, without reinforcement learning or
control strategies, in which ideas are generated based on
the prompt structure, serving as the simplest baseline.

 - These baselines allow for a comprehensive comparison,
highlighting the incremental improvements brought by
RLHF, control strategies, and advanced decoding techniques.
For the RL for idea proper and dimensional controllers
training, we use The RL split to optimize our model using PPO and multi-dimensional reward augmentation. We
incorporate the three distinct reward models for novelty,
feasibility, and effectiveness, allowing for controllable
generation combined with 3 control parameters, and experiment with different decoding stategies.

## Main Results

 Main Results and Statistical Analysis

Table 1 presents the experimental results for Novelty (N),
_Feasibility (F), Effectiveness (E), and Overall metrics._

The baseline models establish foundational performance
levels, with T5-SFT and T5-RLHF showing modest improvements in Feasibility and Effectiveness due to reinforcement learning, though their Novelty scores remain
limited by the lack of mechanisms to encourage innovation. In contrast, LLaMA2-SFT achieves higher overall
scores, benefiting from larger model capacity and superior pretraining, yet its reliance on supervised fine-tuning
leaves room for enhancement through reinforcement learning and control strategies.
Adding targeted controls to LLaMA2-RLHF demonstrates
the potential for metric-specific optimizations. For instance, introducing Novelty Control significantly boosts
creativity while maintaining balanced practicality and
performance, highlighting the feasibility of improving
originality without major trade-offs. Similarly, Feasibility
_Control achieves the highest observed feasibility, albeit_
with minor reductions in novelty and effectiveness, showcasing its focus on practicality. The Effectiveness Control,
on the other hand, enhances impact without compromising
the balance across dimensions.
When all controls are combined, Static Decoding provides
reliable, balanced performance, but its fixed nature limits adaptability. In contrast, Dynamic Decoding emerges
as the most effective approach, leveraging contextual dynamic strategy to balance creativity, practicality, and impact, ultimately producing higher-quality ideas.
These results show the importance of rl and dynamic control in tailoring model behavior to complex requirements,
while also illustrating trade-offs inherent in single-metric
optimizations.
To validate the observed improvements, we conducted
paired t-tests to evaluate statistical significance. Results
show that LLaMA2-RLHF + Novelty Ctrl achieved a statistically significant improvement in Novelty (p-value <
0.01) compared to LLaMA2-RLHF without controls. Similarly, Feasibility Ctrl significantly enhanced Feasibility
(p-value < 0.05), while Effectiveness Ctrl showed a notable gain in Effectiveness (p-value < 0.05). Furthermore,
_Dynamic Decoding demonstrated statistically significant_
improvements across all metrics (p-value < 0.01) com

-----

|Model|Novelty Feasibility Effectiveness|Overall|
|---|---|---|


_LLaMA2-SFT_ 4.3 5.6 4.8 4.6
_LLaMA2-RLHF_ 4.9 6.2 5.2 5.3
_LLaMA2-RLHF + Dynamic_ 5.5 6.4 5.1 5.5

Table 2: Human evaluation results, LLaMA2-RLHF + Dynamic denotes the Dynamic decoding with all the 3 controllers enabled.

pared to the static approach, validating its superior adapt- **Novelty Weight** **Novelty Score** **Feasibility Score**
ability and performance.

1.0 6.4 6.1
2.0 6.7 5.8

### Human Evaluation

3.0 7.0 5.3
4.0 7.3 4.9

Table 4: Novelty and Feasibility trade-off by increasing the
novelty controller weight.

Figure 6: Human Evaluation Results

**Metrics** **Novelty** **Feasibility** **Effectiveness** **Overall**

_Pearson (r)_ 0.995 0.972 0.839 0.970
_Spearman (p)_ 1.000 0.866 0.500 1.000

Table 3: Correlation Coefficients (Pearson and Spearman)
between human and reviewing agent scores.

|LLaMA2-SFT LLaMA2-RLHF LLaMA2-RLHF + Dynamic|4.3 5.6 4.8 4.9 6.2 5.2 5.5 6.4 5.1|4.6 5.3 5.5|
|---|---|---|

|Novelty Weight|Novelty Score Feasibility Score|
|---|---|

|1.0 2.0 3.0 4.0|6.4 6.1 6.7 5.8 7.0 5.3 7.3 4.9|
|---|---|

|Metrics|Novelty Feasibility Effectiveness|Overall|
|---|---|---|
|Pearson (r) Spearman (p)|0.995 0.972 0.839 1.000 0.866 0.500|0.970 1.000|


Domain experts validated the effectiveness of our framework of the generated idea as shown in Table 2 and bar
plot in Figure 6, with human scores showing a strong correlation with the automatic scores produced by our reward
models. The Correlation Coefficients computed with both
Pearson and Spearman between human and reviewing
agent scores are shown in table 3.
Experts also highlighted the trade-off between novelty and
feasibility, noting that the fine-tuned model with novelty
steering produced more creative, though sometimes less
practical, ideas compared to the equal-weighted model.

## Analysis

### Novelty and Feasibility Trade-off

We learn from (Si, Yang, and Hashimoto 2024b) that
increasing novelty will likely reduce the feasibility of an
idea. To test this idea, we controlled the weight of the
novelty steer in the RLHF + Steer1 setup and observed its
impact on both novelty and feasibility scores. The results
are shown in Table 4.


Figure 7: Novelty and Feasibility control analysis

As expected, increasing the novelty steer weight led to
higher novelty scores but lower feasibility scores. This
demonstrates the trade-off between generating highly creative ideas and ensuring their practical feasibility.

### Decoding Strategy Motivation

Raw Evidence:


Raw Conclusions:


Execution Times:
claims_analysis_time: 318.79 seconds
evidence_analysis_time: 1.58 seconds
conclusions_analysis_time: 1.57 seconds
total_execution_time: 324.84 seconds
