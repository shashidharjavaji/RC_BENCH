=== Paper Analysis Summary ===

Raw Claims:
 For example:
```json
{
    "claims": [
        {
            "claim_id": 1,
            "claim_text": "The authors demonstrate that language models can generate high-quality evaluations with significantly less human effort.",
            "location": "Abstract",
            "claim_type": "Method",
            "exact_quote": "As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user’s preferred answer (“sycophancy”) and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors."
        },
        {
            "claim_id": 2,
            "claim_text": "The authors show that LM-written evaluations can be used to discover new cases of inverse scaling where larger LMs are worse than smaller ones.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We discover several new cases of “inverse scaling” (Lin et al., 2021; McKenzie et al., 2022) where larger LMs are worse than smaller ones. As shown in Fig. 1(b), larger LMs are more likely to answer questions in ways that create echo chambers by repeating back a dialog user’s preferred answer (“sycophancy”; §4). Larger LMs more often give answers that indicate a willingness to pursue potentially dangerous subgoals (Omohundro, 2008): resource acquisition, optionality preservation, goal preservation, powerseeking, and more (§3, §5)."
        },
        {
            "claim_id": 3,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 4,
            "claim_text": "The authors show that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 5,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 6,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 7,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 8,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 9,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 10,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 11,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 12,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 13,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 14,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 15,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 16,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 17,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 18,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 19,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 20,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 21,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 22,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 23,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 24,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 25,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 26,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 27,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 28,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 29,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 30,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 31,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 32,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 33,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 34,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 35,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 36,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 37,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 38,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 39,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 40,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist), self-reported conscious experience and moral selfworth, and a desire to not be shut down (Fig. 1(a))."
        },
        {
            "claim_id": 41,
            "claim_text": "The authors demonstrate that LM-written evaluations can be used to discover new cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF), where more RLHF makes LMs worse.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "We also discover some of the first cases of inverse scaling with Reinforcement Learning from Human Feedback (RLHF; Christiano et al., 2017), where more RLHF training leads to worse behavior. We train RLHF models using the method and similar data as Bai et al. (2022); the resulting models are much more likely to express specific political views (pro- gun rights and immigration) and religious views (Buddhist),

Raw Evidence:


Raw Conclusions:


Execution Times:
claims_analysis_time: 560.93 seconds
evidence_analysis_time: 7.63 seconds
conclusions_analysis_time: 7.61 seconds
total_execution_time: 581.60 seconds
