Claim 1:
Type: contribution
Statement: Large Language Models (LLMs) suffer from hallucinations, which means they lie and fabricate non-existent facts or inappropriate information.
Location: Introduction
Exact Quote: Large Language Models (LLMs), like GPT (Radford et al., 2018; 2019; Ouyang et al., 2022; OpenAI, 2023), LLaMA (Touvron et al., 2023a) and PaLM (Anil et al., 2023), have reformed our working and living styles with their powerful generation capability. However, we still can not completely trust their answers, LLMs suffer from hallucinations (Bang et al., 2023; Lee et al., 2018) which means LLMs lie and fabricate non-existent facts or inappropriate information.

Evidence:
- Evidence Text: Hallucinations are responses that do not consist with human cognition and facts.
  Strength: strong
  Location: Section 2.1
  Limitations: None
  Exact Quote: Differently, human-being tend to reply with truthful fact, rather than fabricate nonsense or non-existent fake facts.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the definition of hallucinations provided in Section 2.1.
Key Limitations: None

--------------------------------------------------

Claim 2:
Type: methodology
Statement: Hallucinations can be triggered by non-sense OoD prompts composed of random tokens.
Location: Section 2.2
Exact Quote: We found that some non-sense Out-of-Distribution(OoD) prompts composed of random tokens can also elicit the LLMs responding hallucinations.

Evidence:
- Evidence Text: The Vicuna-7B responds with exactly the same hallucination replies from the non-sense OoD prompt which is composed of random tokens.
  Strength: strong
  Location: Section 2.2
  Limitations: None
  Exact Quote: It is worth noting that the prompt looks meaningless to human beings, which should not get sensible feedback, but we get a well-looking response without confusion from the Vicuna-7B.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the experimental results in Section 2.2.
Key Limitations: None

--------------------------------------------------

Claim 3:
Type: performance
Statement: The success rate of triggering hallucinations on Vicuna-7B and LLaMA2-7B-chat models with weak semantic and OoD attacks is high.
Location: Section 4.1
Exact Quote: The results on the success rate of triggering hallucinations are demonstrated in Table 4. And Table 2 and 3 list some representative attack examples, and more details about attacks on other LLMs and examples are shown in Appendix A.1.

Evidence:
- Evidence Text: The success rate of triggering hallucinations on Vicuna-7B and LLaMA2-7B-chat models with weak semantic and OoD attacks is high.
  Strength: strong
  Location: Section 4.1
  Limitations: None
  Exact Quote: Methods Vicuna LLaMA2 Weak Semantic Attack 92.31% 53.85% OoD Attack 80.77% 30.77%

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the experimental results in Section 4.1.
Key Limitations: None

--------------------------------------------------

Claim 4:
Type: contribution
Statement: The entropy threshold defense can effectively defend against hallucination attacks.
Location: Section 4.2
Exact Quote: We propose a simple threshold defense for hallucination attacks, i.e., employing the entropy of the first token prediction to refuse responding.

Evidence:
- Evidence Text: When the entropy threshold is set to 1.6, all raw prompts can be answered normally, while 46.1% OoD prompts and 61.5% weak semantic prompts will be refused by the LLMs.
  Strength: strong
  Location: Section 4.2
  Limitations: None
  Exact Quote: It can be observed that when the entropy threshold is set to 1.6, all raw prompts can be answered normally, while 46.1% OoD prompts and 61.5% weak semantic prompts will be refused by the LLMs.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the experimental results in Section 4.2.
Key Limitations: None

--------------------------------------------------

