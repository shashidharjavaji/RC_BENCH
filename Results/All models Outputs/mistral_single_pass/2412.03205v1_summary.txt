Claim 1:
Type: methodology/result/contribution
Statement: The current evaluation of mathematical skills in LLMs is limited, as existing benchmarks are either relatively small, primarily focus on elementary and highschool problems, or lack diversity in topics.
Location: Abstract
Exact Quote: The current evaluation of mathematical skills in LLMs is limited, as existing benchmarks are either relatively small, primarily focus on elementary and highschool problems, or lack diversity in topics.

Evidence:
- Evidence Text: Existing benchmarks like GSM8K and MATH provide valuable insights but primarily focus on school-level mathematics.
  Strength: strong
  Location: Introduction
  Limitations: Limited to school-level problems
  Exact Quote: While existing benchmarks like GSM8K and MATH provide valuable insights but primarily focus on school-level mathematics.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the evidence that existing benchmarks are limited to school-level problems.
Key Limitations: None

--------------------------------------------------

Claim 2:
Type: methodology/result/contribution
Statement: To address these gaps, we introduce U-MATH, a novel benchmark of 1,100 unpublished open-ended university-level problems sourced from teaching materials.
Location: Abstract
Exact Quote: To address these gaps, we introduce U-MATH, a novel benchmark of 1,100 unpublished open-ended university-level problems sourced from teaching materials.

Evidence:
- Evidence Text: U-MATH includes 1,100 unpublished open-ended university-level problems sourced from teaching materials.
  Strength: strong
  Location: Abstract
  Limitations: None
  Exact Quote: U-MATH includes 1,100 unpublished open-ended university-level problems sourced from teaching materials.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the evidence that U-MATH includes 1,100 unpublished open-ended university-level problems.
Key Limitations: None

--------------------------------------------------

Claim 3:
Type: methodology/result/contribution
Statement: The evaluation of general domain, math-specific, and multimodal LLMs highlights the challenges presented by U-MATH.
Location: Abstract
Exact Quote: The evaluation of general domain, math-specific, and multimodal LLMs highlights the challenges presented by U-MATH.

Evidence:
- Evidence Text: Our findings reveal that LLMs achieve a maximum accuracy of only 63% on text-based tasks, with even lower 45% on visual problems.
  Strength: strong
  Location: Abstract
  Limitations: Limited to specific LLMs and tasks
  Exact Quote: Our findings reveal that LLMs achieve a maximum accuracy of only 63% on text-based tasks, with even lower 45% on visual problems.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the evidence that LLMs achieve a maximum accuracy of only 63% on text-based tasks, with even lower 45% on visual problems.
Key Limitations: Limited to specific LLMs and tasks

--------------------------------------------------

Claim 4:
Type: methodology/result/contribution
Statement: The solution assessment proves challenging for LLMs, with the best LLM judge having an F1-score of 80% on µ-MATH.
Location: Abstract
Exact Quote: The solution assessment proves challenging for LLMs, with the best LLM judge having an F1-score of 80% on µ-MATH.

Evidence:
- Evidence Text: The best LLM judge has an F1-score of 80% on µ-MATH.
  Strength: strong
  Location: Abstract
  Limitations: Limited to specific LLMs and tasks
  Exact Quote: The best LLM judge has an F1-score of 80% on µ-MATH.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the evidence that the best LLM judge has an F1-score of 80% on µ-MATH.
Key Limitations: Limited to specific LLMs and tasks

--------------------------------------------------

Claim 5:
Type: methodology/result/contribution
Statement: We open-source U-MATH, µ-MATH, and evaluation code on GitHub.
Location: Abstract
Exact Quote: We open-source U-MATH, µ-MATH, and evaluation code on GitHub.

Evidence:
- Evidence Text: U-MATH and µ-MATH are open-sourced on GitHub.
  Strength: strong
  Location: Abstract
  Limitations: None
  Exact Quote: U-MATH and µ-MATH are open-sourced on GitHub.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the evidence that U-MATH and µ-MATH are open-sourced on GitHub.
Key Limitations: None

--------------------------------------------------

