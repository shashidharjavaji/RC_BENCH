Claim 1:
Type: methodology/result/contribution/performance
Statement: The fashion domain has a variety of vision-and-language (V+L) tasks, including cross-modal retrieval, text-guided image retrieval, multi-modal classification, and image captioning.
Location: Introduction
Exact Quote: In the fashion domain, there exists a variety of vision-and-language (V+L) tasks, including cross-modal retrieval, text-guided image retrieval, multi-modal classification, and image captioning.

Evidence:
- Evidence Text: The fashion domain has a variety of vision-and-language (V+L) tasks, including cross-modal retrieval, text-guided image retrieval, multi-modal classification, and image captioning.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: In the fashion domain, there exists a variety of vision-and-language (V+L) tasks, including cross-modal retrieval, text-guided image retrieval, multi-modal classification, and image captioning.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the introduction section.
Key Limitations: None

--------------------------------------------------

Claim 2:
Type: methodology/result/contribution/performance
Statement: Existing methods typically take a pre-trained generic V+L model and fine-tune it on every single task independently.
Location: Introduction
Exact Quote: Existing methods [21, 24, 33, 87, 94] typically take a pre-trained generic V+L model [7, 38, 41–43, 49, 60, 67, 72, 79] and fine-tune it on every single task independently.

Evidence:
- Evidence Text: Existing methods [21, 24, 33, 87, 94] typically take a pre-trained generic V+L model [7, 38, 41–43, 49, 60, 67, 72, 79] and fine-tune it on every single task independently.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: Existing methods [21, 24, 33, 87, 94] typically take a pre-trained generic V+L model [7, 38, 41–43, 49, 60, 67, 72, 79] and fine-tune it on every single task independently.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the introduction section.
Key Limitations: None

--------------------------------------------------

Claim 3:
Type: methodology/result/contribution/performance
Statement: The FAME-ViL model is designed to address the heterogeneous nature of fashion tasks.
Location: Introduction
Exact Quote: In this work, we introduce a novel FAshion-focused Multi-task Efficient learning method for various Vision- and-Language based fashion tasks, dubbed as FAME-ViL.

Evidence:
- Evidence Text: In this work, we introduce a novel FAshion-focused Multi-task Efficient learning method for various Vision- and-Language based fashion tasks, dubbed as FAME-ViL.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: In this work, we introduce a novel FAshion-focused Multi-task Efficient learning method for various Vision- and-Language based fashion tasks, dubbed as FAME-ViL.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the introduction section.
Key Limitations: None

--------------------------------------------------

Claim 4:
Type: methodology/result/contribution/performance
Statement: The FAME-ViL model achieves superior performance across a set of diverse fashion tasks with much fewer parameters.
Location: Introduction
Exact Quote: Specifically, we design a task-versatile architecture on top of a pre-trained generic V+L model (i.e., CLIP [60]). To adapt the simple two-stream architecture of CLIP to various fashion tasks, we introduce a lightweight Cross-Attention Adapter (XAA) to enable the cross-modality interaction between the two streams.

Evidence:
- Evidence Text: Specifically, we design a task-versatile architecture on top of a pre-trained generic V+L model (i.e., CLIP [60]). To adapt the simple two-stream architecture of CLIP to various fashion tasks, we introduce a lightweight Cross-Attention Adapter (XAA) to enable the cross-modality interaction between the two streams.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: Specifically, we design a task-versatile architecture on top of a pre-trained generic V+L model (i.e., CLIP [60]). To adapt the simple two-stream architecture of CLIP to various fashion tasks, we introduce a lightweight Cross-Attention Adapter (XAA) to enable the cross-modality interaction between the two streams.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the introduction section.
Key Limitations: None

--------------------------------------------------

Claim 5:
Type: methodology/result/contribution/performance
Statement: The FAME-ViL model is enabled by two novel components: (1) a task-versatile architecture with cross-attention adapters and task-specific adapters integrated into a unified V+L model, and (2) a stable and effective multi-task training strategy that supports learning from heterogeneous data and prevents negative transfer.
Location: Introduction
Exact Quote: It is enabled by two novel components: (1) a task-versatile architecture with cross-attention adapters and task-specific adapters integrated into a unified V+L model, and (2) a stable and effective multi-task training strategy that supports learning from heterogeneous data and prevents negative transfer.

Evidence:
- Evidence Text: It is enabled by two novel components: (1) a task-versatile architecture with cross-attention adapters and task-specific adapters integrated into a unified V+L model, and (2) a stable and effective multi-task training strategy that supports learning from heterogeneous data and prevents negative transfer.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: It is enabled by two novel components: (1) a task-versatile architecture with cross-attention adapters and task-specific adapters integrated into a unified V+L model, and (2) a stable and effective multi-task training strategy that supports learning from heterogeneous data and prevents negative transfer.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the introduction section.
Key Limitations: None

--------------------------------------------------

Claim 6:
Type: methodology/result/contribution/performance
Statement: The FAME-ViL model can switch among three operational modes to flexibly support various fashion tasks.
Location: Introduction
Exact Quote: Our FAME-ViL can switch among three operational modes to flexibly support various fashion tasks.

Evidence:
- Evidence Text: Our FAME-ViL can switch among three operational modes to flexibly support various fashion tasks.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: Our FAME-ViL can switch among three operational modes to flexibly support various fashion tasks.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the introduction section.
Key Limitations: None

--------------------------------------------------

Claim 7:
Type: methodology/result/contribution/performance
Statement: The FAME-ViL model can significantly improve parameter efficiency while boosting the model performance per task over existing independently fine-tuned single-task models.
Location: Introduction
Exact Quote: Note, each axis is normalized according to the respective maximum value for easier visualization.

Evidence:
- Evidence Text: Note, each axis is normalized according to the respective maximum value for easier visualization.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: Note, each axis is normalized according to the respective maximum value for easier visualization.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the introduction section.
Key Limitations: None

--------------------------------------------------

Claim 8:
Type: methodology/result/contribution/performance
Statement: The FAME-ViL model can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.
Location: Introduction
Exact Quote: Code is available at https://github.com/BrandonHanx/FAME-ViL.

Evidence:
- Evidence Text: Code is available at https://github.com/BrandonHanx/FAME-ViL.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: Code is available at https://github.com/BrandonHanx/FAME-ViL.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the introduction section.
Key Limitations: None

--------------------------------------------------

Claim 9:
Type: methodology/result/contribution/performance
Statement: The FAME-ViL model is designed to address the heterogeneous nature of fashion tasks.
Location: Introduction
Exact Quote: In this work, we introduce a novel FAshion-focused Multi-task Efficient learning method for various Vision- and-Language based fashion tasks, dubbed as FAME-ViL.

Evidence:
- Evidence Text: In this work, we introduce a novel FAshion-focused Multi-task Efficient learning method for various Vision- and-Language based fashion tasks, dubbed as FAME-ViL.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: In this work, we introduce a novel FAshion-focused Multi-task Efficient learning method for various Vision- and-Language based fashion tasks, dubbed as FAME-ViL.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the introduction section.
Key Limitations: None

--------------------------------------------------

Claim 10:
Type: methodology/result/contribution/performance
Statement: The FAME-ViL model is enabled by two novel components: (1) a task-versatile architecture with cross-attention adapters and task-specific adapters integrated into a unified V+L model, and (2) a stable and effective multi-task training strategy that supports learning from heterogeneous data and prevents negative transfer.
Location: Introduction
Exact Quote: It is enabled by two novel components: (1) a task-versatile architecture with cross-attention adapters and task-specific adapters integrated into a unified V+L model, and (2) a stable and effective multi-task training strategy that supports learning from heterogeneous data and prevents negative transfer.

Evidence:
- Evidence Text: It is enabled by two novel components: (1) a task-versatile architecture with cross-attention adapters and task-specific adapters integrated into a unified V+L model, and (2) a stable and effective multi-task training strategy that supports learning from heterogeneous data and prevents negative transfer.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: It is enabled by two novel components: (1) a task-versatile architecture with cross-attention adapters and task-specific adapters integrated into a unified V+L model, and (2) a stable and effective multi-task training strategy that supports learning from heterogeneous data and prevents negative transfer.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the introduction section.
Key Limitations: None

--------------------------------------------------

Claim 11:
Type: methodology/result/contribution/performance
Statement: The FAME-ViL model can switch among three operational modes to flexibly support various fashion tasks.
Location: Introduction
Exact Quote: Our FAME-ViL can switch among three operational modes to flexibly support various fashion tasks.

Evidence:
- Evidence Text: Our FAME-ViL can switch among three operational modes to flexibly support various fashion tasks.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: Our FAME-ViL can switch among three operational modes to flexibly support various fashion tasks.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the introduction section.
Key Limitations: None

--------------------------------------------------

Claim 12:
Type: methodology/result/contribution/performance
Statement: The FAME-ViL model can significantly improve parameter efficiency while boosting the model performance per task over existing independently fine-tuned single-task models.
Location: Introduction
Exact Quote: Note, each axis is normalized according to the respective maximum value for easier visualization.

Evidence:
- Evidence Text: Note, each axis is normalized according to the respective maximum value for easier visualization.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: Note, each axis is normalized according to the respective maximum value for easier visualization.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the introduction section.
Key Limitations: None

--------------------------------------------------

Claim 13:
Type: methodology/result/contribution/performance
Statement: The FAME-ViL model can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.
Location: Introduction
Exact Quote: Code is available at https://github.com/BrandonHanx/FAME-ViL.

Evidence:
- Evidence Text: Code is available at https://github.com/BrandonHanx/FAME-ViL.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: Code is available at https://github.com/BrandonHanx/FAME-ViL.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is clearly stated and supported by the introduction section.
Key Limitations: None

--------------------------------------------------

