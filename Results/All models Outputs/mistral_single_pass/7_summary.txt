Claim 1:
Type: methodology
Statement: REALM augments language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference.
Location: Abstract
Exact Quote: REALM augments language model pre-training with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference.

Evidence:
- Evidence Text: The model uses the retriever to retrieve documents from a large corpus such as Wikipedia, and then attends over those documents to help inform its prediction.
  Strength: strong
  Location: Abstract
  Limitations: None
  Exact Quote: The model uses the retriever to retrieve documents from a large corpus such as Wikipedia, and then attends over those documents to help inform its prediction.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is directly stated in the abstract and is supported by the methodology described.
Key Limitations: None

--------------------------------------------------

Claim 2:
Type: performance
Statement: REALM achieves new state-of-the-art results on all three benchmarks, significantly outperforming all previous systems by 4-16% absolute accuracy.
Location: Abstract
Exact Quote: REALM achieves new state-of-the-art results on all three benchmarks, significantly outperforming all previous systems by 4-16% absolute accuracy.

Evidence:
- Evidence Text: REALM outperforms all previous systems by 4-16% absolute accuracy on three popular Open-QA benchmarks.
  Strength: strong
  Location: Abstract
  Limitations: None
  Exact Quote: REALM achieves new state-of-the-art results on all three benchmarks, significantly outperforming all previous systems by 4-16% absolute accuracy.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is directly stated in the abstract and is supported by the experimental results.
Key Limitations: None

--------------------------------------------------

Claim 3:
Type: methodology
Statement: REALM decomposes p(y _x) into two steps: retrieve, then predict. Given an input x, we first retrieve possibly helpful documents z from a knowledge corpus. We model this as a sample from the distribution p(z _x). Then, we condition on both the retrieved z and the original input x to generate the output y—modeled as p(y _z, x). To obtain the overall likelihood of generating y, we treat z as a latent variable and marginalize over all possible documents z, yielding _p(y_ _x) =_ _p(y_ _z, x) p(z_ _x)._ (1)
Location: Section 3.1
Exact Quote: REALM decomposes p(y _x) into two steps: retrieve, then predict. Given an input x, we first retrieve possibly helpful documents z from a knowledge corpus. We model this as a sample from the distribution p(z _x). Then, we condition on both the retrieved z and the original input x to generate the output y—modeled as p(y _z, x). To obtain the overall likelihood of generating y, we treat z as a latent variable and marginalize over all possible documents z, yielding _p(y_ _x) =_ _p(y_ _z, x) p(z_ _x)._ (1)

Evidence:
- Evidence Text: The generative process is described in detail, including the retrieval and prediction steps.
  Strength: strong
  Location: Section 3.1
  Limitations: None
  Exact Quote: REALM decomposes p(y _x) into two steps: retrieve, then predict. Given an input x, we first retrieve possibly helpful documents z from a knowledge corpus. We model this as a sample from the distribution p(z _x). Then, we condition on both the retrieved z and the original input x to generate the output y—modeled as p(y _z, x). To obtain the overall likelihood of generating y, we treat z as a latent variable and marginalize over all possible documents z, yielding _p(y_ _x) =_ _p(y_ _z, x) p(z_ _x)._ (1)

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is directly stated in the methodology section and is supported by the detailed description of the generative process.
Key Limitations: None

--------------------------------------------------

Claim 4:
Type: methodology
Statement: REALM retriever is defined using a dense inner product model: exp f (x, z) _p(z_ _x) =_ _|_ � _z[′][ exp][ f]_ [(][x, z][′][)] _[,] _f_ (x, z) = Embedinput(x)[⊤]Embeddoc(z),
Location: Section 3.2
Exact Quote: REALM retriever is defined using a dense inner product model: exp f (x, z) _p(z_ _x) =_ _|_ � _z[′][ exp][ f]_ [(][x, z][′][)] _[,] _f_ (x, z) = Embedinput(x)[⊤]Embeddoc(z),

Evidence:
- Evidence Text: The retriever is defined using a dense inner product model, with the relevance score f (x, z) between x and z defined as the inner product of the vector embeddings.
  Strength: strong
  Location: Section 3.2
  Limitations: None
  Exact Quote: REALM retriever is defined using a dense inner product model: exp f (x, z) _p(z_ _x) =_ _|_ � _z[′][ exp][ f]_ [(][x, z][′][)] _[,] _f_ (x, z) = Embedinput(x)[⊤]Embeddoc(z),

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is directly stated in the methodology section and is supported by the detailed description of the retriever.
Key Limitations: None

--------------------------------------------------

Claim 5:
Type: methodology
Statement: REALM retriever is trained using a performance-based signal from unsupervised text: a retrieval that improves the language model’s perplexity is helpful and should be rewarded, while an uninformative retrieval should be penalized.
Location: Section 3.3
Exact Quote: REALM retriever is trained using a performance-based signal from unsupervised text: a retrieval that improves the language model’s perplexity is helpful and should be rewarded, while an uninformative retrieval should be penalized.

Evidence:
- Evidence Text: The retriever is trained using a performance-based signal from unsupervised text, with a retrieval that improves the language model’s perplexity being rewarded and an uninformative retrieval being penalized.
  Strength: strong
  Location: Section 3.3
  Limitations: None
  Exact Quote: REALM retriever is trained using a performance-based signal from unsupervised text: a retrieval that improves the language model’s perplexity is helpful and should be rewarded, while an uninformative retrieval should be penalized.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is directly stated in the methodology section and is supported by the detailed description of the training process.
Key Limitations: None

--------------------------------------------------

Claim 6:
Type: performance
Statement: REALM achieves new state-of-the-art results on all three benchmarks, significantly outperforming all previous systems by 4-16% absolute accuracy.
Location: Section 4.4
Exact Quote: REALM achieves new state-of-the-art results on all three benchmarks, significantly outperforming all previous systems by 4-16% absolute accuracy.

Evidence:
- Evidence Text: REALM outperforms all previous systems by 4-16% absolute accuracy on three popular Open-QA benchmarks.
  Strength: strong
  Location: Section 4.4
  Limitations: None
  Exact Quote: REALM achieves new state-of-the-art results on all three benchmarks, significantly outperforming all previous systems by 4-16% absolute accuracy.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is directly stated in the results section and is supported by the experimental results.
Key Limitations: None

--------------------------------------------------

Claim 7:
Type: methodology
Statement: REALM retriever is designed to transfer to other tasks, and the retrieval is just text, not a labeled example.
Location: Section 3.4
Exact Quote: REALM retriever is designed to transfer to other tasks, and the retrieval is just text, not a labeled example.

Evidence:
- Evidence Text: The retriever is designed to transfer to other tasks, and the retrieval is just text, not a labeled example.
  Strength: strong
  Location: Section 3.4
  Limitations: None
  Exact Quote: REALM retriever is designed to transfer to other tasks, and the retrieval is just text, not a labeled example.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is directly stated in the methodology section and is supported by the detailed description of the retriever.
Key Limitations: None

--------------------------------------------------

