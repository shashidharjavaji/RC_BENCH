Claim 1:
Type: contribution
Statement: Integrated Gradients is a new attribution method that satisfies Sensitivity and Implementation Invariance.
Location: Section 3
Exact Quote: We use the axioms to identify a new method, called integrated gradients.

Evidence:
- Evidence Text: Integrated Gradients is defined as the path integral of the gradients along the straightline path from the baseline to the input.
  Strength: strong
  Location: Section 3
  Limitations: None
  Exact Quote: IntegratedGrads[approx]i (x) ::= (3) ∂F (x[′]+[k] (xi − x[′]i[)][ ×][ Σ][m]k=1 m∂x[×]i[(][x][−][x][′][)))] × m[1]

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The definition of Integrated Gradients is mathematically sound and aligns with the proposed axioms.
Key Limitations: None

--------------------------------------------------

Claim 2:
Type: contribution
Statement: Integrated Gradients is the unique path method that is symmetry-preserving.
Location: Section 4.2
Exact Quote: Integrated gradients is the unique path method that is symmetry-preserving.

Evidence:
- Evidence Text: The proof shows that for any non-straightline path, the attributions for symmetric variables will not be equal.
  Strength: strong
  Location: Section 4.2
  Limitations: None
  Exact Quote: Consider a non-straightline path γ : [0, 1] R[n] → from baseline to input. W.l.o.g., there exists t0 ∈ [0, 1] such that for two dimensions i, j, γi(t0) > γj(t0).

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The proof logically demonstrates that Integrated Gradients is the only method that preserves symmetry.
Key Limitations: None

--------------------------------------------------

Claim 3:
Type: contribution
Statement: Integrated Gradients can be applied to various deep networks including image, text, and chemistry models.
Location: Section 6
Exact Quote: The integrated gradients technique is applicable to a variety of deep networks.

Evidence:
- Evidence Text: The paper demonstrates the application of Integrated Gradients to image recognition, diabetic retinopathy prediction, question classification, neural machine translation, and chemistry models.
  Strength: strong
  Location: Section 6
  Limitations: None
  Exact Quote: We apply it to two image models, two natural language models, and a chemistry model.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The practical applications across different types of models validate the versatility of Integrated Gradients.
Key Limitations: None

--------------------------------------------------

Claim 4:
Type: contribution
Statement: Integrated Gradients satisfies the axiom of Completeness.
Location: Section 3
Exact Quote: Integrated gradients satisfy an axiom called completeness that the attributions add up to the difference between the output of F at the input x and the baseline x[′].

Evidence:
- Evidence Text: The proposition states that Integrated Gradients satisfy Completeness.
  Strength: strong
  Location: Section 3
  Limitations: None
  Exact Quote: Proposition 1. If F : R[n] → R is differentiable almost everywhere [1] then Σ[n]i=1[IntegratedGrads]i[(][x][) =][ F] [(][x][)][ −] [F] [(][x][′][)]

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The mathematical proof and proposition support the claim that Integrated Gradients satisfy Completeness.
Key Limitations: None

--------------------------------------------------

