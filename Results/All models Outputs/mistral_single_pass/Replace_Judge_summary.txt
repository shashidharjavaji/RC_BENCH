Claim 1:
Type: contribution
Statement: Using a Panel of LLm evaluators (PoLL) composed of a larger number of smaller models outperforms a single large judge, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive.
Location: Abstract
Exact Quote: Using a Panel of LLm evaluators (PoLL) composed of a larger number of smaller models outperforms a single large judge, exhibits less intra-model bias due to its composition of disjoint model families, and does so while being over seven times less expensive.

Evidence:
- Evidence Text: The cost of running our specific instance of PoLL is $1.25/input + $4.25/output, whereas the cost of running GPT-4 Turbo is $10/input + $30/output. Depending on the ratio of input-to-output tokens in a given task, running the entire three model PoLL is seven to eight times less expensive than running a single GPT-4 judge.
  Strength: strong
  Location: Section 4.5
  Limitations: Costs are specific to the instance and may vary with different implementations.
  Exact Quote: The cost of running our specific instance of PoLL is $1.25/input + $4.25/output, whereas the cost of running GPT-4 Turbo is $10/input + $30/output. Depending on the ratio of input-to-output tokens in a given task, running the entire three model PoLL is seven to eight times less expensive than running a single GPT-4 judge.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The cost comparison is directly stated and supported by specific cost figures.
Key Limitations: Costs are specific to the instance and may vary with different implementations.

--------------------------------------------------

Claim 2:
Type: result
Statement: Using an instantiation of PoLL correlates better with human judgements compared to a single large judge (GPT-4), while being over seven times cheaper.
Location: Section 4.1
Exact Quote: Using an instantiation of PoLL correlates better with human judgements compared to a single large judge (GPT-4), while being over seven times cheaper.

Evidence:
- Evidence Text: Table 1 shows how the ratings from different evaluator judges, on different single-hop QA datasets from KILT, correlate with human judgements as measured by κ. We see that overall, PoLL has the strongest correlation across various tasks, while GPT-4 is one of the weaker evaluators on this particular task setup.
  Strength: strong
  Location: Section 4.1
  Limitations: The correlation is specific to the datasets and models used in the study.
  Exact Quote: Table 1 shows how the ratings from different evaluator judges, on different single-hop QA datasets from KILT, correlate with human judgements as measured by κ. We see that overall, PoLL has the strongest correlation across various tasks, while GPT-4 is one of the weaker evaluators on this particular task setup.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The correlation is directly stated and supported by specific correlation values.
Key Limitations: The correlation is specific to the datasets and models used in the study.

--------------------------------------------------

Claim 3:
Type: performance
Statement: In some scenarios, GPT-4 is a relatively weak judge, exhibiting high variance with minor changes to the prompt.
Location: Section 4.3
Exact Quote: In some scenarios, GPT-4 is a relatively weak judge, exhibiting high variance with minor changes to the prompt.

Evidence:
- Evidence Text: Table 3 shows how the correlation between GPT-4 and human annotators varies as the prompt changes. In all cases, having in-context examples improves the performance over zero-shot and the most effective strategy is an explicit instruction to the model not to 'overthink' and not to concern itself with the wider factuality of the answers with respect to the outside world.
  Strength: strong
  Location: Section 4.3
  Limitations: The performance is specific to the datasets and models used in the study.
  Exact Quote: Table 3 shows how the correlation between GPT-4 and human annotators varies as the prompt changes. In all cases, having in-context examples improves the performance over zero-shot and the most effective strategy is an explicit instruction to the model not to 'overthink' and not to concern itself with the wider factuality of the answers with respect to the outside world.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The performance is directly stated and supported by specific correlation values.
Key Limitations: The performance is specific to the datasets and models used in the study.

--------------------------------------------------

Claim 4:
Type: contribution
Statement: Intra-model scoring bias is reduced by pooling judgements across a panel of heterogeneous evaluator models.
Location: Section 4.4
Exact Quote: Intra-model scoring bias is reduced by pooling judgements across a panel of heterogeneous evaluator models.

Evidence:
- Evidence Text: Figures 3 and 4 show results on HotPotQA and Bamboogle. We can see how the different judges score different models and how far those predictions deviate from human annotator decisions (the dotted line at 0). We observe that overall, PoLL has the smallest spread in scores, with a standard deviation of 2.2, compared to EM and individual judges. GPT-3.5 has the highest spread, with a standard deviation of 6.1. We also see in Figure 4 that the highest positive delta for each individual model being scored occurs when it is judged by itself.
  Strength: strong
  Location: Section 4.4
  Limitations: The bias reduction is specific to the datasets and models used in the study.
  Exact Quote: Figures 3 and 4 show results on HotPotQA and Bamboogle. We can see how the different judges score different models and how far those predictions deviate from human annotator decisions (the dotted line at 0). We observe that overall, PoLL has the smallest spread in scores, with a standard deviation of 2.2, compared to EM and individual judges. GPT-3.5 has the highest spread, with a standard deviation of 6.1. We also see in Figure 4 that the highest positive delta for each individual model being scored occurs when it is judged by itself.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The bias reduction is directly stated and supported by specific standard deviation values.
Key Limitations: The bias reduction is specific to the datasets and models used in the study.

--------------------------------------------------

