{
    "paper_analysis": [],
    "raw_claims": "\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Improvement claim\",\n            \"exact_quote\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"PromptBERT outperforms SimCSE and BERT-flow in sentence embeddings performance.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Compared to SimCSE, PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"PromptBERT's code is available on GitHub.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Availability claim\",\n            \"exact_quote\": \"Our code is available at https://github.com/kongds/Prompt-BERT.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"Original BERT layers fail to improve the performance of sentence embeddings.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"We firstly analyze the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers.\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"Static token embeddings are biased by token frequency, subword, and case.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Bias claim\",\n            \"exact_quote\": \"We find the distribution of token embeddings is not only biased by frequency, but also case sensitive and subword in WordPiece.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"Removing biased tokens can improve the performance of sentence embeddings.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Improvement claim\",\n            \"exact_quote\": \"Simply removing a set of tokens, the result can be improved by 9.22, 7.08 and 11.76 respectively.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"Prompt-based methods can avoid embedding bias and utilize the original BERT layers.\",\n            \"location\": \"Section 4\",\n            \"claim_type\": \"Method claim\",\n            \"exact_quote\": \"Prompt-based method can avoid embedding bias and utilize the original BERT layers.\"\n        },\n        {\n            \"claim_id\": 8,\n            \"claim_text\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between supervised and unsupervised performance.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between the unsupervised and supervised performance.\"\n        },\n        {\n            \"claim_id\": 9,\n            \"claim_text\": \"PromptBERT outperforms SimCSE and BERT-flow in sentence embeddings performance.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Our method still outperforms them.\"\n        },\n        {\n            \"claim_id\": 10,\n            \"claim_text\": \"PromptBERT achieves state-of-the-art results in both unsupervised and supervised settings.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n        },\n        {\n            \"claim_id\": 11,\n            \"claim_text\": \"Template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.\",\n            \"location\": \"Section 6\",\n            \"claim_type\": \"Method claim\",\n            \"exact_quote\": \"We find the template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.\"\n        },\n        {\n            \"claim_id\": 12,\n            \"claim_text\": \"Prompt-based contrastive learning method with template denoising is more stable than SimCSE in unsupervised contrastive learning.\",\n            \"location\": \"Section 6\",\n            \"claim_type\": \"Stability claim\",\n            \"exact_quote\": \"The difference between the best and worst results can be up to 3.14% in SimCSE. However, the gap in our method is only 0.53.\"\n        },\n        {\n            \"claim_id\": 13,\n            \"claim_text\": \"PromptBERT achieves state-of-the-art results on transfer tasks.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"We also report the result of SimCSE with 10 random seeds. Compared to SimCSE, our method shows more stable results than it.\"\n        }\n    ]\n}\n```\n\n\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Improvement claim\",\n            \"exact_quote\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"PromptBERT outperforms SimCSE and BERT-flow in sentence embeddings performance.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Compared to SimCSE, PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"PromptBERT's code is available on GitHub.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Availability claim\",\n            \"exact_quote\": \"Our code is available at https://github.com/kongds/Prompt-BERT.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"Original BERT layers fail to improve the performance of sentence embeddings.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"We firstly analyze the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers.\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"Static token embeddings are biased by token frequency, subword, and case.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Bias claim\",\n            \"exact_quote\": \"We find the distribution of token embeddings is not only biased by frequency, but also case sensitive and subword in WordPiece.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"Removing biased tokens can improve the performance of sentence embeddings.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Improvement claim\",\n            \"exact_quote\": \"Simply removing a set of tokens, the result can be improved by 9.22, 7.08 and 11.76 respectively.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"Prompt-based methods can avoid embedding bias and utilize the original BERT layers.\",\n            \"location\": \"Section 4\",\n            \"claim_type\": \"Method claim\",\n            \"exact_quote\": \"Prompt-based method can avoid embedding bias and utilize the original BERT layers.\"\n        },\n        {\n            \"claim_id\": 8,\n            \"claim_text\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between supervised and unsupervised performance.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between the unsupervised and supervised performance.\"\n        },\n        {\n            \"claim_id\": 9,\n            \"claim_text\": \"PromptBERT outperforms SimCSE and BERT-flow in sentence embeddings performance.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Our method still outperforms them.\"\n        },\n        {\n            \"claim_id\": 10,\n            \"claim_text\": \"PromptBERT achieves state-of-the-art results in both unsupervised and supervised settings.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n        },\n        {\n            \"claim_id\": 11,\n            \"claim_text\": \"Template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.\",\n            \"location\": \"Section 6\",\n            \"claim_type\": \"Method claim\",\n            \"exact_quote\": \"We find the template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.\"\n        },\n        {\n            \"claim_id\": 12,\n            \"claim_text\": \"Prompt-based contrastive learning method with template denoising is more stable than SimCSE in unsupervised contrastive learning.\",\n            \"location\": \"Section 6\",\n            \"claim_type\": \"Stability claim\",\n            \"exact_quote\": \"The difference between the best and worst results can be up to 3.14% in SimCSE. However, the gap in our method is only 0.53.\"\n        },\n        {\n            \"claim_id\": 13,\n            \"claim_text\": \"PromptBERT achieves state-of-the-art results on transfer tasks.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"We also report the result of SimCSE with 10 random seeds. Compared to SimCSE, our method shows more stable results than it.\"\n        }\n    ]\n}\n```\n\n\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Improvement claim\",\n            \"exact_quote\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"PromptBERT outperforms SimCSE and BERT-flow in sentence embeddings performance.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Compared to SimCSE, PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"PromptBERT's code is available on GitHub.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Availability claim\",\n            \"exact_quote\": \"Our code is available at https://github.com/kongds/Prompt-BERT.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"Original BERT layers fail to improve the performance of sentence embeddings.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"We firstly analyze the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers.\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"Static token embeddings are biased by token frequency, subword, and case.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Bias claim\",\n            \"exact_quote\": \"We find the distribution of token embeddings is not only biased by frequency, but also case sensitive and subword in WordPiece.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"Removing biased tokens can improve the performance of sentence embeddings.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Improvement claim\",\n            \"exact_quote\": \"Simply removing a set of tokens, the result can be improved by 9.22, 7.08 and 11.76 respectively.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"Prompt-based methods can avoid embedding bias and utilize the original BERT layers.\",\n            \"location\": \"Section 4\",\n            \"claim_type\": \"Method claim\",\n            \"exact_quote\": \"Prompt-based method can avoid embedding bias and utilize the original BERT layers.\"\n        },\n        {\n            \"claim_id\": 8,\n            \"claim_text\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between supervised and unsupervised performance.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between the unsupervised and supervised performance.\"\n        },\n        {\n            \"claim_id\": 9,\n            \"claim_text\": \"PromptBERT outperforms SimCSE and BERT-flow in sentence embeddings performance.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Our method still outperforms them.\"\n        },\n        {\n            \"claim_id\": 10,\n            \"claim_text\": \"PromptBERT achieves state-of-the-art results in both unsupervised and supervised settings.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n        },\n        {\n            \"claim_id\": 11,\n            \"claim_text\": \"Template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.\",\n            \"location\": \"Section 6\",\n            \"claim_type\": \"Method claim\",\n            \"exact_quote\": \"We find the template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.\"\n        },\n        {\n            \"claim_id\": 12,\n            \"claim_text\": \"Prompt-based contrastive learning method with template denoising is more stable than SimCSE in unsupervised contrastive learning.\",\n            \"location\": \"Section 6\",\n            \"claim_type\": \"Stability claim\",\n            \"exact_quote\": \"The difference between the best and worst results can be up to 3.14% in SimCSE. However, the gap in our method is only 0.53.\"\n        },\n        {\n            \"claim_id\": 13,\n            \"claim_text\": \"PromptBERT achieves state-of-the-art results on transfer tasks.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"We also report the result of SimCSE with 10 random seeds. Compared to SimCSE, our method shows more stable results than it.\"\n        }\n    ]\n}\n```\n\n\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Improvement claim\",\n            \"exact_quote\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"PromptBERT outperforms SimCSE and BERT-flow in sentence embeddings performance.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Compared to SimCSE, PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"PromptBERT's code is available on GitHub.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Availability claim\",\n            \"exact_quote\": \"Our code is available at https://github.com/kongds/Prompt-BERT.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"Original BERT layers fail to improve the performance of sentence embeddings.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"We firstly analyze the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers.\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"Static token embeddings are biased by token frequency, subword, and case.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Bias claim\",\n            \"exact_quote\": \"We find the distribution of token embeddings is not only biased by frequency, but also case sensitive and subword in WordPiece.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"Removing biased tokens can improve the performance of sentence embeddings.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Improvement claim\",\n            \"exact_quote\": \"Simply removing a set of tokens, the result can be improved by 9.22, 7.08 and 11.76 respectively.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"Prompt-based methods can avoid embedding bias and utilize the original BERT layers.\",\n            \"location\": \"Section 4\",\n            \"claim_type\": \"Method claim\",\n            \"exact_quote\": \"Prompt-based method can avoid embedding bias and utilize the original BERT layers.\"\n        },\n        {\n            \"claim_id\": 8,\n            \"claim_text\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between supervised and unsupervised performance.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between the unsupervised and supervised performance.\"\n        },\n        {\n            \"claim_id\": 9,\n            \"claim_text\": \"PromptBERT outperforms SimCSE and BERT-flow in sentence embeddings performance.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Our method still outperforms them.\"\n        },\n        {\n            \"claim_id\": 10,\n            \"claim_text\": \"PromptBERT achieves state-of-the-art results in both unsupervised and supervised settings.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n        },\n        {\n            \"claim_id\": 11,\n            \"claim_text\": \"Template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.\",\n            \"location\": \"Section 6\",\n            \"claim_type\": \"Method claim\",\n            \"exact_quote\": \"We find the template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.\"\n        },\n        {\n            \"claim_id\": 12,\n            \"claim_text\": \"Prompt-based contrastive learning method with template denoising is more stable than SimCSE in unsupervised contrastive learning.\",\n            \"location\": \"Section 6\",\n            \"claim_type\": \"Stability claim\",\n            \"exact_quote\": \"The difference between the best and worst results can be up to 3.14% in SimCSE. However, the gap in our method is only 0.53.\"\n        },\n        {\n            \"claim_id\": 13,\n            \"claim_text\": \"PromptBERT achieves state-of-the-art results on transfer tasks.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"We also report the result of SimCSE with 10 random seeds. Compared to SimCSE, our method shows more stable results than it.\"\n        }\n    ]\n}\n```\n\n\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Improvement claim\",\n            \"exact_quote\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"PromptBERT outperforms SimCSE and BERT-flow in sentence embeddings performance.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Compared to SimCSE, PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"PromptBERT's code is available on GitHub.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Availability claim\",\n            \"exact_quote\": \"Our code is available at https://github.com/kongds/Prompt-BERT.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"Original BERT layers fail to improve the performance of sentence embeddings.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"We firstly analyze the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers.\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"Static token embeddings are biased by token frequency, subword, and case.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Bias claim\",\n            \"exact_quote\": \"We find the distribution of token embeddings is not only biased by frequency, but also case sensitive and subword in WordPiece.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"Removing biased tokens can improve the performance of sentence embeddings.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Improvement claim\",\n            \"exact_quote\": \"Simply removing a set of tokens, the result can be improved by 9.22, 7.08 and 11.76 respectively.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"Prompt-based methods can avoid embedding bias and utilize the original BERT layers.\",\n            \"location\": \"Section 4\",\n            \"claim_type\": \"Method claim\",\n            \"exact_quote\": \"Prompt-based method can avoid embedding bias and utilize the original BERT layers.\"\n        },\n        {\n            \"claim_id\": 8,\n            \"claim_text\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between supervised and unsupervised performance.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between the unsupervised and supervised performance.\"\n        },\n        {\n            \"claim_id\": 9,\n            \"claim_text\": \"PromptBERT outperforms SimCSE and BERT-flow in sentence embeddings performance.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Our method still outperforms them.\"\n        },\n        {\n            \"claim_id\": 10,\n            \"claim_text\": \"PromptBERT achieves state-of-the-art results in both unsupervised and supervised settings.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n        },\n        {\n            \"claim_id\": 11,\n            \"claim_text\": \"Template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.\",\n            \"location\": \"Section 6\",\n            \"claim_type\": \"Method claim\",\n            \"exact_quote\": \"We find the template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.\"\n        },\n        {\n            \"claim_id\": 12,\n            \"claim_text\": \"Prompt-based contrastive learning method with template denoising is more stable than SimCSE in unsupervised contrastive learning.\",\n            \"location\": \"Section 6\",\n            \"claim_type\": \"Stability claim\",\n            \"exact_quote\": \"The difference between the best and worst results can be up to 3.14% in SimCSE. However, the gap in our method is only 0.53.\"\n        },\n        {\n            \"claim_id\": 13,\n            \"claim_text\": \"PromptBERT achieves state-of-the-art results on transfer tasks.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"We also report the result of SimCSE with 10 random seeds. Compared to SimCSE, our method shows more stable results than it.\"\n        }\n    ]\n}\n```\n\n\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Improvement claim\",\n            \"exact_quote\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"PromptBERT outperforms SimCSE and BERT-flow in sentence embeddings performance.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Compared to SimCSE, PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"PromptBERT's code is available on GitHub.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Availability claim\",\n            \"exact_quote\": \"Our code is available at https://github.com/kongds/Prompt-BERT.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"Original BERT layers fail to improve the performance of sentence embeddings.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"We firstly analyze the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers.\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"Static token embeddings are biased by token frequency, subword, and case.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Bias claim\",\n            \"exact_quote\": \"We find the distribution of token embeddings is not only biased by frequency, but also case sensitive and subword in WordPiece.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"Removing biased tokens can improve the performance of sentence embeddings.\",\n            \"location\": \"Section 3\",\n            \"claim_type\": \"Improvement claim\",\n            \"exact_quote\": \"Simply removing a set of tokens, the result can be improved by 9.22, 7.08 and 11.76 respectively.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"Prompt-based methods can avoid embedding bias and utilize the original BERT layers.\",\n            \"location\": \"Section 4\",\n            \"claim_type\": \"Method claim\",\n            \"exact_quote\": \"Prompt-based method can avoid embedding bias and utilize the original BERT layers.\"\n        },\n        {\n            \"claim_id\": 8,\n            \"claim_text\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between supervised and unsupervised performance.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between the unsupervised and supervised performance.\"\n        },\n        {\n            \"claim_id\": 9,\n            \"claim_text\": \"PromptBERT outperforms SimCSE and BERT-flow in sentence embeddings performance.\",\n            \"location\": \"Section 5\",\n            \"claim_type\": \"Performance claim\",\n            \"exact_quote\": \"Our method still outperforms them.\"\n        },\n        {\n            \"claim_id\": 10,\n            \"claim_text\": \"PromptBERT achieves",
    "raw_evidence": "\n\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n                },\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 2,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"Compared to SimCSE, PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n                },\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our method still outperforms them.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 3,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"Our code is available at https://github.com/kongds/Prompt-BERT.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 4,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"We firstly analyze the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 5,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"We find the distribution of token embeddings is not only biased by frequency, but also case sensitive and subword in WordPiece.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 6,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"Simply removing a set of tokens, the result can be improved by 9.22, 7.08 and 11.76 respectively.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 7,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 4\",\n                    \"text\": \"Prompt-based method can avoid embedding bias and utilize the original BERT layers.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 8,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between the unsupervised and supervised performance.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 9,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our method still outperforms them.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 10,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 11,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 6\",\n                    \"text\": \"We find the template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 12,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 6\",\n                    \"text\": \"The difference between the best and worst results can be up to 3.14% in SimCSE. However, the gap in our method is only 0.53.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 13,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"We also report the result of SimCSE with 10 random seeds. Compared to SimCSE, our method shows more stable results than it.\"\n                }\n            ]\n        }\n    ]\n}\n```\n\n\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n                },\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 2,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"Compared to SimCSE, PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n                },\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our method still outperforms them.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 3,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"Our code is available at https://github.com/kongds/Prompt-BERT.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 4,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"We firstly analyze the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 5,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"We find the distribution of token embeddings is not only biased by frequency, but also case sensitive and subword in WordPiece.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 6,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"Simply removing a set of tokens, the result can be improved by 9.22, 7.08 and 11.76 respectively.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 7,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 4\",\n                    \"text\": \"Prompt-based method can avoid embedding bias and utilize the original BERT layers.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 8,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between the unsupervised and supervised performance.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 9,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our method still outperforms them.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 10,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 11,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 6\",\n                    \"text\": \"We find the template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 12,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 6\",\n                    \"text\": \"The difference between the best and worst results can be up to 3.14% in SimCSE. However, the gap in our method is only 0.53.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 13,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"We also report the result of SimCSE with 10 random seeds. Compared to SimCSE, our method shows more stable results than it.\"\n                }\n            ]\n        }\n    ]\n}\n```\n\n\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n                },\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 2,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"Compared to SimCSE, PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n                },\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our method still outperforms them.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 3,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"Our code is available at https://github.com/kongds/Prompt-BERT.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 4,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"We firstly analyze the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 5,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"We find the distribution of token embeddings is not only biased by frequency, but also case sensitive and subword in WordPiece.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 6,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"Simply removing a set of tokens, the result can be improved by 9.22, 7.08 and 11.76 respectively.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 7,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 4\",\n                    \"text\": \"Prompt-based method can avoid embedding bias and utilize the original BERT layers.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 8,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between the unsupervised and supervised performance.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 9,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our method still outperforms them.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 10,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 11,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 6\",\n                    \"text\": \"We find the template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 12,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 6\",\n                    \"text\": \"The difference between the best and worst results can be up to 3.14% in SimCSE. However, the gap in our method is only 0.53.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 13,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"We also report the result of SimCSE with 10 random seeds. Compared to SimCSE, our method shows more stable results than it.\"\n                }\n            ]\n        }\n    ]\n}\n```\n\n\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n                },\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 2,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"Compared to SimCSE, PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n                },\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our method still outperforms them.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 3,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"Our code is available at https://github.com/kongds/Prompt-BERT.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 4,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"We firstly analyze the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 5,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"We find the distribution of token embeddings is not only biased by frequency, but also case sensitive and subword in WordPiece.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 6,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"Simply removing a set of tokens, the result can be improved by 9.22, 7.08 and 11.76 respectively.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 7,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 4\",\n                    \"text\": \"Prompt-based method can avoid embedding bias and utilize the original BERT layers.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 8,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between the unsupervised and supervised performance.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 9,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our method still outperforms them.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 10,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 11,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 6\",\n                    \"text\": \"We find the template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 12,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 6\",\n                    \"text\": \"The difference between the best and worst results can be up to 3.14% in SimCSE. However, the gap in our method is only 0.53.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 13,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"We also report the result of SimCSE with 10 random seeds. Compared to SimCSE, our method shows more stable results than it.\"\n                }\n            ]\n        }\n    ]\n}\n```\n\n\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n                },\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 2,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"Compared to SimCSE, PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n                },\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our method still outperforms them.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 3,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"Our code is available at https://github.com/kongds/Prompt-BERT.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 4,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"We firstly analyze the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 5,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"We find the distribution of token embeddings is not only biased by frequency, but also case sensitive and subword in WordPiece.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 6,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"Simply removing a set of tokens, the result can be improved by 9.22, 7.08 and 11.76 respectively.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 7,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 4\",\n                    \"text\": \"Prompt-based method can avoid embedding bias and utilize the original BERT layers.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 8,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between the unsupervised and supervised performance.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 9,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our method still outperforms them.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 10,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 11,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 6\",\n                    \"text\": \"We find the template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 12,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 6\",\n                    \"text\": \"The difference between the best and worst results can be up to 3.14% in SimCSE. However, the gap in our method is only 0.53.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 13,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"We also report the result of SimCSE with 10 random seeds. Compared to SimCSE, our method shows more stable results than it.\"\n                }\n            ]\n        }\n    ]\n}\n```\n\n\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n                },\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 2,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"Compared to SimCSE, PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n                },\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our method still outperforms them.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 3,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"Our code is available at https://github.com/kongds/Prompt-BERT.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 4,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"We firstly analyze the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 5,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"We find the distribution of token embeddings is not only biased by frequency, but also case sensitive and subword in WordPiece.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 6,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"Simply removing a set of tokens, the result can be improved by 9.22, 7.08 and 11.76 respectively.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 7,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 4\",\n                    \"text\": \"Prompt-based method can avoid embedding bias and utilize the original BERT layers.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 8,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between the unsupervised and supervised performance.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 9,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our method still outperforms them.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 10,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 11,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 6\",\n                    \"text\": \"We find the template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 12,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 6\",\n                    \"text\": \"The difference between the best and worst results can be up to 3.14% in SimCSE. However, the gap in our method is only 0.53.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 13,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"We also report the result of SimCSE with 10 random seeds. Compared to SimCSE, our method shows more stable results than it.\"\n                }\n            ]\n        }\n    ]\n}\n```\n\n\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n                },\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 2,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"Compared to SimCSE, PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n                },\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our method still outperforms them.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 3,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"Our code is available at https://github.com/kongds/Prompt-BERT.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 4,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"We firstly analyze the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 5,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"We find the distribution of token embeddings is not only biased by frequency, but also case sensitive and subword in WordPiece.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 6,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 3\",\n                    \"text\": \"Simply removing a set of tokens, the result can be improved by 9.22, 7.08 and 11.76 respectively.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 7,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 4\",\n                    \"text\": \"Prompt-based method can avoid embedding bias and utilize the original BERT layers.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 8,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Prompt-based contrastive learning method with template denoising significantly shortens the gap between the unsupervised and supervised performance.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 9,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our method still outperforms them.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 10,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 11,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 6\",\n                    \"text\": \"We find the template denoising efficiently removes the bias from templates and improves the quality of top-k tokens predicted by MLM head in original BERT.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 12,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 6\",\n                    \"text\": \"The difference between the best and worst results can be up to 3.14% in SimCSE. However, the gap in our method is only 0.53.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 13,\n            \"evidence\": [\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"We also report the result of SimCSE with 10 random seeds. Compared to SimCSE, our method shows more stable results than it.\"\n                }\n            ]\n        }\n    ]\n}\n```\n\n\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n                },\n                {\n                    \"section\": \"Section 5\",\n                    \"text\": \"Our extensive experiments demonstrate the efficiency of our method on STS tasks and transfer tasks.\"\n                }\n            ]\n        },\n        {\n            \"claim_id\": 2,\n            \"evidence\": [\n                {\n                    \"section\": \"Abstract\",\n                    \"text\": \"Compared to SimCSE, PromptBert achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.\"\n                },\n                {\n                   ",
    "raw_conclusions": "",
    "execution_times": {
        "claims_analysis_time": "719.90 seconds",
        "evidence_analysis_time": "781.22 seconds",
        "conclusions_analysis_time": "847.22 seconds",
        "total_execution_time": "2350.52 seconds"
    }
}