Claim 1:
Type: performance
Statement: RETRO obtains comparable performance to GPT-3 and Jurassic-1 on the Pile with 25 fewer parameters.
Location: Abstract
Exact Quote: our Retrieval-Enhanced Transformer (RETRO) obtains comparable performance to GPT-3 and Jurassic-1 on the Pile, despite using 25 fewer parameters.

Evidence:
- Evidence Text: RETRO performance translates to downstream knowledge-intensive tasks such as question answering.
  Strength: moderate
  Location: Abstract
  Limitations: performance on specific tasks not directly compared to GPT-3 and Jurassic-1
  Exact Quote: RETRO performance translates to downstream knowledge-intensive tasks such as question answering.

- Evidence Text: Figure 4 shows RETRO outperforming Jurassic-1 on a majority of the test sets on the Pile.
  Strength: strong
  Location: 4. Results
  Limitations: performance on specific subsets of the Pile may vary
  Exact Quote: RETRO outperforms the baseline on all test sets and outperforms Jurassic-1 on a majority of them, despite being over an order of magnitude smaller.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided shows that RETRO performs comparably to larger models on a variety of tasks and datasets.
Key Limitations: Specific comparisons to GPT-3 are not provided.

--------------------------------------------------

Claim 2:
Type: methodology
Statement: RETRO can be trained from scratch or RETROfit pre-trained transformers with retrieval.
Location: Abstract
Exact Quote: We typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance.

Evidence:
- Evidence Text: RETRO models are trained by freezing the pre-trained weights and training only chunked cross-attention and retrieval encoder weights.
  Strength: strong
  Location: 2. Method
  Limitations: The claim does not specify the conditions under which RETROfit is more effective.
  Exact Quote: we typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance.

- Evidence Text: RETROfit models are trained by freezing the pre-trained weights and training only chunked cross-attention and retrieval encoder weights.
  Strength: strong
  Location: 2. Method
  Limitations: The claim does not specify the conditions under which RETROfit is more effective.
  Exact Quote: we typically train RETRO from scratch, yet can also rapidly RETROfit pre-trained transformers with retrieval and still achieve good performance.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided supports the claim that RETRO can be trained from scratch or RETROfit pre-trained transformers with retrieval.
Key Limitations: The claim does not specify the conditions under which RETROfit is more effective.

--------------------------------------------------

Claim 3:
Type: result
Statement: RETRO's performance scales well with model size and database size.
Location: 2. Method
Exact Quote: RETRO provides a constant gain for models ranging from 150M to 7B parameters; it can be improved at evaluation time by increasing the database size and the number of retrieved neighbours.

Evidence:
- Evidence Text: Figure 1 shows the performance gain of RETRO models remaining constant with model scale.
  Strength: strong
  Location: 2. Method
  Limitations: The claim is based on a single dataset (C4 validation set) and may not generalize to other datasets.
  Exact Quote: RETRO provides a constant gain for models ranging from 150M to 7B parameters; it can be improved at evaluation time by increasing the database size and the number of retrieved neighbours.

- Evidence Text: Figure 1 also shows that performance increases with the size of the retrieval database and the number of retrieved neighbours.
  Strength: strong
  Location: 2. Method
  Limitations: The claim is based on a single dataset (C4 validation set) and may not generalize to other datasets.
  Exact Quote: The performance gain of our retrieval models remains constant with model scale (left), and is comparable to multiplying the parameteric model size by ∼ 10×. The gain increases with the size of the retrieval database (middle) and the number of retrieved neighbours (right) on the C4 validation set, when using up to 40 neighbours.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided supports the claim that RETRO's performance scales well with model size and database size.
Key Limitations: The claim is based on a single dataset (C4 validation set) and may not generalize to other datasets.

--------------------------------------------------

Claim 4:
Type: result
Statement: RETRO's performance on question answering is competitive.
Location: 4. Results
Exact Quote: RETRO achieves competitive performance on question answering (4.3).

Evidence:
- Evidence Text: RETRO achieves a top-1 accuracy of 45.5% on Natural Questions, outperforming the baseline 7B model.
  Strength: moderate
  Location: 4. Results
  Limitations: The claim is based on a single dataset (Natural Questions) and may not generalize to other question answering datasets.
  Exact Quote: RETRO 7.5B (DPR retrieval) 45.5

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided supports the claim that RETRO's performance on question answering is competitive.
Key Limitations: The claim is based on a single dataset (Natural Questions) and may not generalize to other question answering datasets.

--------------------------------------------------

Claim 5:
Type: result
Statement: RETRO's performance on language modeling is dataset dependent.
Location: 4. Results
Exact Quote: The performance is dataset dependent, with the largest gains on Wikitext103 and C4.

Evidence:
- Evidence Text: Figure 3 shows that RETRO outperforms the baseline on all datasets, with the largest gains on Wikitext103 and C4.
  Strength: strong
  Location: 4. Results
  Limitations: The claim is based on a single dataset (C4 validation set) and may not generalize to other datasets.
  Exact Quote: The performance is dataset dependent, with the largest gains on Wikitext103 and C4.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided supports the claim that RETRO's performance on language modeling is dataset dependent.
Key Limitations: The claim is based on a single dataset (C4 validation set) and may not generalize to other datasets.

--------------------------------------------------

Claim 6:
Type: result
Statement: RETRO's performance on language modeling is not significantly affected by disabling retrieval at evaluation.
Location: 4. Results
Exact Quote: Disabling retrieval at evaluation (RETRO [OFF]) brings little degradation compared to a standard transformer.

Evidence:
- Evidence Text: Figure 4 shows that the relative bits-per-byte improvement over the baseline is similar for RETRO with and without retrieval.
  Strength: moderate
  Location: 4. Results
  Limitations: The claim is based on a single dataset (C4 validation set) and may not generalize to other datasets.
  Exact Quote: Relative bits-per-byte improvement over our 7B standard Transformer

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided supports the claim that RETRO's performance on language modeling is not significantly affected by disabling retrieval at evaluation.
Key Limitations: The claim is based on a single dataset (C4 validation set) and may not generalize to other datasets.

--------------------------------------------------

Claim 7:
Type: result
Statement: RETRO's performance on language modeling is not significantly affected by the number of retrieved neighbours.
Location: 4. Results
Exact Quote: We observe consistent improvements for all models when the number of neighbours is increased from 1 to 10.

Evidence:
- Evidence Text: Figure 1 shows that performance increases with the number of retrieved neighbours.
  Strength: moderate
  Location: 4. Results
  Limitations: The claim is based on a single dataset (C4 validation set) and may not generalize to other datasets.
  Exact Quote: The performance gain of our retrieval models remains constant with model scale (left), and is comparable to multiplying the parameteric model size by ∼ 10×. The gain increases with the size of the retrieval database (middle) and the number of retrieved neighbours (right) on the C4 validation set, when using up to 40 neighbours.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided supports the claim that RETRO's performance on language modeling is not significantly affected by the number of retrieved neighbours.
Key Limitations: The claim is based on a single dataset (C4 validation set) and may not generalize to other datasets.

--------------------------------------------------

Claim 8:
Type: result
Statement: RETRO's performance on language modeling is not significantly affected by the depth of the retrieval encoder.
Location: E. Model ablations
Exact Quote: Using a 6-layer encoder results in a tiny decrease in loss compared to the default 12-layer encoder.

Evidence:
- Evidence Text: Figure 7 shows that using a 6-layer encoder results in a tiny decrease in loss compared to the default 12-layer encoder.
  Strength: weak
  Location: E. Model ablations
  Limitations: The claim is based on a single dataset (C4 validation set) and may not generalize to other datasets.
  Exact Quote: Using a 6-layer encoder results in a tiny decrease in loss compared to the default 12-layer encoder.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence provided supports the claim that RETRO's performance on language modeling is not significantly affected by the depth of the retrieval encoder.
Key Limitations: The claim is based on a single dataset (C4 validation set) and may not generalize to other datasets.

--------------------------------------------------

Claim 9:
Type: result
Statement: RETRO's performance on language modeling is not significantly affected by the number of retrieved neighbours.
Location: E. Model ablations
Exact Quote: Training on a single neighbour results in a large decrease in performance, while training on 4 neighbours does not give substantial performance improvement.

Evidence:
- Evidence Text: Figure 7 shows that training on a single neighbour results in a large decrease in performance, while training on 4 neighbours does not give substantial performance improvement.
  Strength: strong
  Location: E. Model ablations
  Limitations: The claim is based on a single dataset (C4 validation set) and may not generalize to other datasets.
  Exact Quote: Training on a single neighbour results in a large decrease in performance, while training on 4 neighbours does not give substantial performance improvement.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided supports the claim that RETRO's performance on language modeling is not significantly affected by the number of retrieved neighbours.
Key Limitations: The claim is based on a single dataset (C4 validation set) and may not generalize to other datasets.

--------------------------------------------------

Claim 10:
Type: result
Statement: RETRO's performance on language modeling is not significantly affected by the frequency of cross-attention in the decoder.
Location: E. Model ablations
Exact Quote: Attending only once at the top or the bottom layer is a bad choice, while attending once on a mid-depth layer is relatively sound.

Evidence:
- Evidence Text: Figure 7 shows that attending only once at the top or the bottom layer is a bad choice, while attending once on a mid-depth layer is relatively sound.
  Strength: moderate
  Location: E. Model ablations
  Limitations: The claim is based on a single dataset (C4 validation set) and may not generalize to other datasets.
  Exact Quote: Attending only once at the top or the bottom layer is a bad choice, while attending once on a mid-depth layer is relatively sound.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided supports the claim that RETRO's performance on language modeling is not significantly affected by the frequency of cross-attention in the decoder.
Key Limitations: The claim is based on a single dataset (C4 validation set) and may not generalize to other datasets.

--------------------------------------------------

