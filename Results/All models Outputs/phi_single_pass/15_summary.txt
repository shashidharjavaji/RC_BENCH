Claim 1:
Type: result
Statement: Pre-trained LLMs can reason by simple decoding changes, without the use of prompting.
Location: Introduction
Exact Quote: We find that, perhaps surprisingly, there exists a task-agnostic way to elicit CoT reasoning from pre-trained LLMs by simply altering the decoding procedure.

Evidence:
- Evidence Text: Examples of greedy decoded paths and alternative top-ùëò paths over the PaLM-2 Large model.
  Strength: moderate
  Location: 2.1 Pre-trained Language Models Can Reason without Prompting
  Limitations: Limited to specific models and tasks.
  Exact Quote: Table 1 | Examples of greedy decoded paths and alternative top-ùëò paths over the PaLM-2 Large model.

- Evidence Text: CoT-decoding is the only decoding strategy that effectively enables language models to reason.
  Strength: strong
  Location: 3.1 CoT-decoding Effectively Elicits Reasoning from Language Models
  Limitations: Limited to specific models and tasks.
  Exact Quote: Table 4 | CoT-decoding is the only decoding strategy that can significantly enhance language models‚Äô reasoning.

- Evidence Text: CoT-decoding enables a pre-trained model to achieve a similar performance of an instruction-tuned model.
  Strength: moderate
  Location: 3.2 CoT-decoding Enables a Better Understanding of Model‚Äôs Intrinsic Reasoning Abilities
  Limitations: Limited to specific models and tasks.
  Exact Quote: CoT-decoding achieves 63.2% accuracy on the pre-trained PaLM-2 Large model, close to the performance of the instruction-tuned model of the same scale at 67.8%.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided shows that CoT-decoding can effectively elicit reasoning from pre-trained LLMs without the use of prompting, and can even achieve similar performance to instruction-tuned models.
Key Limitations: The evidence is limited to specific models and tasks.

--------------------------------------------------

Claim 2:
Type: result
Statement: The presence of a CoT in the decoding path correlates with a higher confidence in the model‚Äôs decoded answer.
Location: Introduction
Exact Quote: Moreover, we observe that the presence of a CoT in the decoding path correlates with a higher confidence in the model‚Äôs decoded answer.

Evidence:
- Evidence Text: The model demonstrates increased confidence in the final answer when a CoT is present in its decoding process.
  Strength: strong
  Location: 2.2 CoT-Decoding for Extracting CoT Paths
  Limitations: Limited to specific models and tasks.
  Exact Quote: This decoding modification bypasses the confounders of prompting and is entirely unsupervised without the need for model tuning.

- Evidence Text: CoT-decoding reliably extracts the CoT-paths compared to other methods.
  Strength: strong
  Location: 2.2 CoT-Decoding for Extracting CoT Paths
  Limitations: Limited to specific models and tasks.
  Exact Quote: Table 2 | CoT-decoding reliably extracts the CoT-paths compared to other methods (on PaLM-2 L).

- Evidence Text: The model‚Äôs confidence in its final answers increases when a CoT is present in its decoding path.
  Strength: strong
  Location: 2.2 CoT-Decoding for Extracting CoT Paths
  Limitations: Limited to specific models and tasks.
  Exact Quote: Table 2 | CoT-decoding reliably extracts the CoT-paths compared to other methods (on PaLM-2 L).

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided shows that the presence of a CoT in the decoding path correlates with a higher confidence in the model‚Äôs decoded answer.
Key Limitations: The evidence is limited to specific models and tasks.

--------------------------------------------------

Claim 3:
Type: result
Statement: CoT-decoding effectively elicits reasoning across multiple language model families.
Location: 3.1 CoT-decoding Effectively Elicits Reasoning from Language Models
Exact Quote: In Figure 3, we show that across three language model families, PaLM-2, Mistral and Gemma, CoT-decoding effectively elicits model‚Äôs reasoning, yielding consistent accuracy gains over both math and commonsense reasoning tasks, sometimes doubling or even tripling the performance compared to greedy decoding.

Evidence:
- Evidence Text: CoT-decoding consistently yields +10-30% absolute accuracy gains on GSM8K.
  Strength: strong
  Location: 3.1 CoT-decoding Effectively Elicits Reasoning from Language Models
  Limitations: Limited to specific models and tasks.
  Exact Quote: Figure 4 | CoT-decoding reliably improves reasoning performance across model scales (PaLM-2), even when the task does not naturally improve by scaling up only (e.g., year parity).

- Evidence Text: CoT-decoding achieves 63.2% accuracy on the pre-trained PaLM-2 Large model, close to the performance of the instruction-tuned model of the same scale at 67.8%.
  Strength: moderate
  Location: 3.2 CoT-decoding Enables a Better Understanding of Model‚Äôs Intrinsic Reasoning Abilities
  Limitations: Limited to specific models and tasks.
  Exact Quote: CoT-decoding achieves 63.2% accuracy on the pre-trained PaLM-2 Large model, close to the performance of the instruction-tuned model of the same scale at 67.8%.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided shows that CoT-decoding can effectively elicit reasoning across multiple language model families.
Key Limitations: The evidence is limited to specific models and tasks.

--------------------------------------------------

Claim 4:
Type: result
Statement: CoT-decoding can be combined with CoT-prompting to further boost reasoning performance.
Location: 3.3 Combining CoT-decoding with CoT-Prompting
Exact Quote: Adding CoT-decoding on top of zero-shot CoT-prompting can further boost the reasoning performance on both models.

Evidence:
- Evidence Text: CoT-decoding maintains a strong performance compared to self-consistency when both are combined with CoT-prompts.
  Strength: strong
  Location: 3.3 Combining CoT-decoding with CoT-Prompting
  Limitations: Limited to specific models and tasks.
  Exact Quote: Table 7 | Adding CoT-decoding on top of zero-shot CoT-prompting can further boost the reasoning performance on both models.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided shows that CoT-decoding can be combined with CoT-prompting to further boost reasoning performance.
Key Limitations: The evidence is limited to specific models and tasks.

--------------------------------------------------

Claim 5:
Type: result
Statement: CoT-decoding partially closes the reasoning gap between pre-trained and instruction-tuned models.
Location: 3.2 CoT-decoding Enables a Better Understanding of Model‚Äôs Intrinsic Reasoning Abilities
Exact Quote: CoT-decoding enables a pre-trained model to achieve a similar performance of an instruction-tuned model: in Figure 4 (left), CoT-decoding achieves 63.2% accuracy on the pre-trained PaLM-2 Large model, close to the performance of the instruction-tuned model of the same scale at 67.8%.

Evidence:
- Evidence Text: CoT-decoding achieves 63.2% accuracy on the pre-trained PaLM-2 Large model, close to the performance of the instruction-tuned model of the same scale at 67.8%.
  Strength: moderate
  Location: 3.2 CoT-decoding Enables a Better Understanding of Model‚Äôs Intrinsic Reasoning Abilities
  Limitations: Limited to specific models and tasks.
  Exact Quote: CoT-decoding achieves 63.2% accuracy on the pre-trained PaLM-2 Large model, close to the performance of the instruction-tuned model of the same scale at 67.8%.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided shows that CoT-decoding can partially close the reasoning gap between pre-trained and instruction-tuned models.
Key Limitations: The evidence is limited to specific models and tasks.

--------------------------------------------------

Claim 6:
Type: result
Statement: The choice of ùëò affects the overall accuracy.
Location: 3.2 CoT-decoding Enables a Better Understanding of Model‚Äôs Intrinsic Reasoning Abilities
Exact Quote: Overall we found that higher values of ùëò typically result in improved model performance.

Evidence:
- Evidence Text: Figure 5 | The effect of ùëò on reasoning accuracy w.r.t. PaLM-2 model scales and task difficulty.
  Strength: moderate
  Location: 3.2 CoT-decoding Enables a Better Understanding of Model‚Äôs Intrinsic Reasoning Abilities
  Limitations: Limited to specific models and tasks.
  Exact Quote: Figure 5 | The effect of ùëò on reasoning accuracy w.r.t. PaLM-2 model scales and task difficulty.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided shows that the choice of ùëò affects the overall accuracy.
Key Limitations: The evidence is limited to specific models and tasks.

--------------------------------------------------

