Claim 1:
Type: contribution
Statement: Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs’ long context capabilities.
Location: Abstract
Exact Quote: Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs’ long context capabilities.

Evidence:
- Evidence Text: The paper introduces two tasks, TSort and BestAnswer, with detailed descriptions of their design and purpose.
  Strength: strong
  Location: Section 3.1
  Limitations: None mentioned
  Exact Quote: The task definition section provides a detailed explanation of TSort and BestAnswer tasks.

- Evidence Text: The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings.
  Strength: moderate
  Location: Section 5
  Limitations: The paper does not provide a comprehensive comparison with all existing benchmarks.
  Exact Quote: The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The tasks are designed to specifically test long-context capabilities, and the evaluation results show limitations in current models.
Key Limitations: None mentioned

--------------------------------------------------

Claim 2:
Type: result
Statement: The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings.
Location: Section 5
Exact Quote: The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings.

Evidence:
- Evidence Text: The performance of all open-source models rapidly deteriorates to random guess level when the input length scales to 4,000 tokens.
  Strength: strong
  Location: Section 5
  Limitations: The evaluation is limited to a subset of models and may not generalize to all LLMs.
  Exact Quote: When the input length scales to 4,000 tokens, most open-source models rapidly deteriorates to random guess level.

- Evidence Text: No proprietary model notably outperforms the random baseline in the ultra-long setting (32,000+ tokens).
  Strength: strong
  Location: Section 5
  Limitations: The evaluation is limited to a subset of models and may not generalize to all LLMs.
  Exact Quote: When it comes to the ultra-long setting (32,000+ tokens), no proprietary model notably outperforms the random baseline.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evaluation results show a clear decline in performance for both open-source and proprietary models in ultra-long-context settings.
Key Limitations: The evaluation is limited to a subset of models and may not generalize to all LLMs.

--------------------------------------------------

Claim 3:
Type: result
Statement: Scalable position embeddings do improve the long-context modeling capability.
Location: Section 4.5.3
Exact Quote: Our findings indicate that scalable position embeddings do im**prove the long-context modeling capability.

Evidence:
- Evidence Text: All methods enhance the accuracy under the 8k setting, which is beyond the original context window.
  Strength: moderate
  Location: Section 4.5.3
  Limitations: The advantage of these methods is more obvious on Vicuna-13b-v1.5.
  Exact Quote: All methods enhance the accuracy under the 8k setting, which is beyond the original context window.

- Evidence Text: The advantage of these methods is more obvious on Vicuna-13b-v1.5.
  Strength: moderate
  Location: Section 4.5.3
  Limitations: The evaluation is limited to a subset of models and may not generalize to all LLMs.
  Exact Quote: The advantage of these methods is more obvious on Vicuna-13b-v1.5.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The results show that scalable position embeddings improve performance in ultra-long-context settings.
Key Limitations: The evaluation is limited to a subset of models and may not generalize to all LLMs.

--------------------------------------------------

Claim 4:
Type: contribution
Statement: Ada-LEval is the first benchmark that evaluates LLMs under the ultra-long setting.
Location: Section 5
Exact Quote: Ada-LEval is the first benchmark that evaluates LLMs under the ultra-long setting.

Evidence:
- Evidence Text: The paper introduces Ada-LEval as the first benchmark for ultra-long-context evaluation.
  Strength: strong
  Location: Section 5
  Limitations: None mentioned
  Exact Quote: Ada-LEval is the first benchmark that evaluates LLMs under the ultra-long setting.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the introduction of Ada-LEval.
Key Limitations: None mentioned

--------------------------------------------------

