Claim 1:
Type: result
Statement: Three sources of bad annotations can corrupt the reliability of open leaderboard rankings.
Location: Introduction
Exact Quote: we demonstrate that three sources of bad annotations, both malicious and otherwise, can corrupt the reliability of open leaderboard rankings.

Evidence:
- Evidence Text: Experiments showing that 10% of poor quality votes can change model rankings by up to 5 places.
  Strength: strong
  Location: Section 3
  Limitations: The experiments are limited to the Chatbot Arena platform and may not generalize to other platforms.
  Exact Quote: only 10% of poor quality votes by apathetic or adversarial annotators can change the rankings of models by up to 5 places on the leaderboard.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The experiments provide clear evidence of the impact of poor quality annotations on leaderboard rankings.
Key Limitations: The experiments are limited to the Chatbot Arena platform and may not generalize to other platforms.

--------------------------------------------------

Claim 2:
Type: result
Statement: Apathetic or adversarial voting are not easy to detect in a post-hoc manner.
Location: Section 3.1
Exact Quote: However, poor annotations from either apathetic or adversarial voting are not easy to detect in a post-hoc manner.

Evidence:
- Evidence Text: Experiments showing that apathetic and adversarial votes are indistinguishable from arbitrary votes.
  Strength: moderate
  Location: Section 3.1
  Limitations: The experiments are limited to the Chatbot Arena platform and may not generalize to other platforms.
  Exact Quote: A major challenge in detecting apathetic votes is that they are often indistinguishable from arbitrary votes.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The experiments provide evidence that apathetic and adversarial votes are difficult to detect, but the results may not generalize to other platforms.
Key Limitations: The experiments are limited to the Chatbot Arena platform and may not generalize to other platforms.

--------------------------------------------------

Claim 3:
Type: result
Statement: Adversarial attacks can substantially change leaderboard rankings if adversaries get to contribute 10% votes for their model.
Location: Section 3.2
Exact Quote: Across all models, we show that adversarial attacks can substantially change leaderboard rankings if adversaries get to contribute 10% votes for their model.

Evidence:
- Evidence Text: Experiments showing that adversarial attacks can change the rank of all systems by more than 4 places.
  Strength: strong
  Location: Section 3.2
  Limitations: The experiments are limited to the Chatbot Arena platform and may not generalize to other platforms.
  Exact Quote: we find that adversarial attacks can substantially change leaderboard rankings if adversaries get to contribute 10% votes for their model.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The experiments provide clear evidence of the impact of adversarial voting on leaderboard rankings.
Key Limitations: The experiments are limited to the Chatbot Arena platform and may not generalize to other platforms.

--------------------------------------------------

Claim 4:
Type: result
Statement: Arbitrary votes are not 'noise' and provide useful signals about models’ relative performance.
Location: Section 3.3
Exact Quote: Arbitrary votes are not 'noise' and provide useful signals about models’ relative performance.

Evidence:
- Evidence Text: Low agreement between annotators on subjective questions.
  Strength: moderate
  Location: Section 3.3
  Limitations: The study is limited to a small-scale annotation study with only four annotators.
  Exact Quote: Overall, we find very low agreement between these well-intentioned annotators with clear guidelines, irrespective of the performance difference between the model pairs.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The study provides evidence that arbitrary votes can provide useful signals about models' relative performance, but the results may not generalize to larger-scale studies.
Key Limitations: The study is limited to a small-scale annotation study with only four annotators.

--------------------------------------------------

Claim 5:
Type: contribution
Statement: Rich feedback and stronger guardrails are needed to ensure high-quality human annotations.
Location: Section 4
Exact Quote: Our experiments in Section 3 lay a convincing case for the need for stronger guardrails in open community-driven platforms.

Evidence:
- Evidence Text: Discussion of potential solutions such as soliciting fine-grained annotations, rationales, reputation-based systems, CAPTCHA, machine learning based anomaly detection, and behavior traces.
  Strength: moderate
  Location: Section 4
  Limitations: The paper does not provide empirical evidence for the effectiveness of these solutions.
  Exact Quote: We encourage the community to explore ideas from past research, such as soliciting fine-grained annotations (Krishna et al., 2023; Goyal et al., 2022b) or rationales (McDonnell et al., 2016).

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The paper provides a strong argument for the need for stronger guardrails, but empirical evidence for the effectiveness of proposed solutions is lacking.
Key Limitations: The paper does not provide empirical evidence for the effectiveness of proposed solutions.

--------------------------------------------------

