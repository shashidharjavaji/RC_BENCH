Claim 1:
Type: result
Statement: Fine-tuning LLMs on a corpus of planning instances does not lead to robust planning skills.
Location: Introduction
Exact Quote: we find that merely fine-tuning LLMs on a corpus of planning instances does not lead to robust planning skills, as indicated by poor performance on out-of-distribution test sets.

Evidence:
- Evidence Text: poor performance on out-of-distribution test sets
  Strength: strong
  Location: Introduction
  Limitations: performance on in-distribution test sets not discussed
  Exact Quote: poor performance on out-of-distribution test sets

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the experimental results showing poor performance on out-of-distribution test sets, which is a strong indicator of the lack of robustness in planning skills.
Key Limitations: The evaluation does not discuss performance on in-distribution test sets, which could provide additional context.

--------------------------------------------------

Claim 2:
Type: result
Statement: Various strategies, including chain_of_thought, enhance the probability of a plan being executable.
Location: Introduction
Exact Quote: we find that various strategies, including chain_of_thought, do enhance the probability of a plan being exe-cutable.

Evidence:
- Evidence Text: enhance the probability of a plan being exe-cutable
  Strength: moderate
  Location: Introduction
  Limitations: does not directly enhance the final validity rate
  Exact Quote: do enhance the probability of a plan being exe-cutable

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the experimental results showing an increase in plan executability, but it is noted that this does not directly translate to an increase in the final validity rate.
Key Limitations: The claim does not address the impact on plan validity.

--------------------------------------------------

Claim 3:
Type: result
Statement: Reinforcement learning with the 'Longest Contiguous Common Subsequence' reward is the most effective strategy.
Location: Introduction
Exact Quote: Reinforcement learning with our novel ‘Longest Contiguous Common Subsequence’ reward emerged as the most effective.

Evidence:
- Evidence Text: improves plan validity by 7% and executability by 9% in longer planning problems
  Strength: strong
  Location: Introduction
  Limitations: improvement is specific to longer planning problems
  Exact Quote: improves plan validity by 7% and executability by 9% in longer planning problems

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the experimental results showing significant improvements in both plan validity and executability for longer planning problems.
Key Limitations: The effectiveness is specific to longer planning problems and may not generalize to other scenarios.

--------------------------------------------------

Claim 4:
Type: result
Statement: Fine-tuning LLMs on datasets containing problem contexts and reference plans does not acquire robust planning skills.
Location: Introduction
Exact Quote: We challenge the claim that fine-tuning LLMs simply on datasets containing problem contexts and reference plans acquire robust planning skills.

Evidence:
- Evidence Text: failure on OOD cases
  Strength: strong
  Location: Introduction
  Limitations: does not discuss performance on in-distribution test sets
  Exact Quote: failure on OOD cases

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the experimental results showing failure on out-of-distribution test sets, which is a strong indicator of the lack of robustness in planning skills.
Key Limitations: The evaluation does not discuss performance on in-distribution test sets, which could provide additional context.

--------------------------------------------------

Claim 5:
Type: result
Statement: Strategies like CoT lead to incremental improvements in plan quality by enhancing plan executability.
Location: Introduction
Exact Quote: strategies like CoT lead to incremental improvements in plan quality by enhancing plan executability

Evidence:
- Evidence Text: enhance plan executability
  Strength: moderate
  Location: Introduction
  Limitations: does not directly increase validity rates
  Exact Quote: enhancing plan executability

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the experimental results showing an increase in plan executability, but it is noted that this does not directly translate to an increase in the final validity rate.
Key Limitations: The claim does not address the impact on plan validity.

--------------------------------------------------

Claim 6:
Type: result
Statement: RL with the proposed 'LCCS' reward improves validity and executability in longer planning problems.
Location: Introduction
Exact Quote: RL with our proposed ‘LCCS’ reward emerges as the most effective strategy. In particular, it improves plan validity by 7% and executability by 9% in longer planning problems.

Evidence:
- Evidence Text: improves plan validity by 7% and executability by 9% in longer planning problems
  Strength: strong
  Location: Introduction
  Limitations: effectiveness is specific to longer planning problems
  Exact Quote: improves plan validity by 7% and executability by 9% in longer planning problems

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the experimental results showing significant improvements in both plan validity and executability for longer planning problems.
Key Limitations: The effectiveness is specific to longer planning problems and may not generalize to other scenarios.

--------------------------------------------------

Claim 7:
Type: result
Statement: Permutation augmentation does not significantly improve the validity rate but enhances the executability rate.
Location: 4.2 Ablation Study on Strategy Effectiveness in Planning
Exact Quote: Permutation, CoT, and Self-Correct show no significant validity. improvements but enhance executability in ‘long’ and other OOD scenarios.

Evidence:
- Evidence Text: enhance executability in ‘long’ and other OOD scenarios
  Strength: moderate
  Location: 4.2 Ablation Study on Strategy Effectiveness in Planning
  Limitations: does not significantly improve the validity rate
  Exact Quote: enhance executability in ‘long’ and other OOD scenarios

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the experimental results showing an increase in executability, but it is noted that there is no significant improvement in the validity rate.
Key Limitations: The claim does not address the impact on plan validity.

--------------------------------------------------

Claim 8:
Type: result
Statement: Goal CoT hinders planning performance among OOD cases.
Location: 4.3 Goal CoT: The Complexity Paradox and Overfitting Issue
Exact Quote: Goal CoT is the only strategy that hinders planning perfor-mance among OOD cases, showing no improvement whatsoever.

Evidence:
- Evidence Text: no improvement whatsoever
  Strength: strong
  Location: 4.3 Goal CoT: The Complexity Paradox and Overfitting Issue
  Limitations: complexity paradox and poor generalization
  Exact Quote: no improvement whatsoever

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the experimental results showing no improvement in planning performance among OOD cases.
Key Limitations: The claim does not address the impact on plan validity.

--------------------------------------------------

Claim 9:
Type: result
Statement: Self-correction learning does not improve validity rates.
Location: 4.4 LLMs Recognize Mistakes But Fail to Correct Them
Exact Quote: RL notably improves the performance under our end-to-end planning paradigm, especially on longer problems.

Evidence:
- Evidence Text: RL boosted the validity rate on the ‘long’ test set from 34.8% to 41.5%
  Strength: moderate
  Location: 4.7 RL Enhances Model Performance
  Limitations: limited training data and suboptimal rewards
  Exact Quote: RL boosted the validity rate on the ‘long’ test set from 34.8% to 41.5%

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the experimental results showing an increase in validity rate, but it is noted that the training data and rewards were limited.
Key Limitations: The claim does not address the impact on plan executability.

--------------------------------------------------

Claim 10:
Type: result
Statement: State CoT boosts executability with a caveat: efficacy limited to short problems.
Location: 4.6 Familiar-Length Plan Continuation Experiments Reveal CoT’s Potential
Exact Quote: State CoT boosts executability with a caveat: efficacy limited to short problems.

Evidence:
- Evidence Text: efficacy limited to short problems
  Strength: moderate
  Location: 4.6 Familiar-Length Plan Continuation Experiments Reveal CoT’s Potential
  Limitations: does not improve executability in 'long' test set
  Exact Quote: efficacy limited to short problems

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the experimental results showing an increase in executability for short problems, but it is noted that there is no improvement in the 'long' test set.
Key Limitations: The claim does not address the impact on plan validity.

--------------------------------------------------

