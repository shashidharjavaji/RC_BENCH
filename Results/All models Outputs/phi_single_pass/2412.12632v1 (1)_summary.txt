Claim 1:
Type: performance
Statement: CoE enhances LLMs through more accurate generation
Location: Section 5. Effectiveness Assessment
Exact Quote: External knowledge equipped with CoE can more effectively (than Non-CoE) help LLMs generate correct answers in context rich with irrelevant information.

Evidence:
- Evidence Text: CoE achieves an average accuracy of 92.0% across five LLMs and two datasets, outperforming Non-CoE variants SenP and WordP by 22.5% and 16.3%, respectively.
  Strength: strong
  Location: Section 5. Effectiveness Assessment
  Limitations: assumes relevance and interconnectivity are the only factors influencing LLM performance
  Exact Quote: CoE achieves an average accuracy of 92.0% across five LLMs and two datasets, outperforming Non-CoE variants SenP and WordP by 22.5% and 16.3%, respectively.

- Evidence Text: LLMs exhibit higher faithfulness to the answer implicated in CoE (than Non-CoE), even when CoE contains factual errors.
  Strength: strong
  Location: Section 6. Faithfulness Assessment
  Limitations: assumes faithfulness is solely determined by CoE presence
  Exact Quote: LLMs exhibit higher faithfulness to the answer implicated in CoE (than Non-CoE), even when CoE contains factual errors.

- Evidence Text: LLMs exhibit higher robustness against knowledge conflict (than Non-CoE) if the external knowledge is equipped with CoE.
  Strength: strong
  Location: Section 7. Robustness Assessment
  Limitations: assumes robustness is solely determined by CoE presence
  Exact Quote: LLMs augmented with CoE ex-hibit higher robustness against knowledge con-flict than Non-CoE.

- Evidence Text: CoE-guided retrieval strategy can effectively improve LLM’s accuracy after substituting the reranking component in the naive RAG framework.
  Strength: moderate
  Location: Section 8. Usability Assessment
  Limitations: assumes RAG framework is the only applicable scenario
  Exact Quote: CoE-guided retrieval strategy can effectively improve LLMs’ accuracy in the naive RAG framework.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided demonstrates a consistent pattern of improved performance across multiple LLMs and datasets when CoE is present.
Key Limitations: The study assumes that relevance and interconnectivity are the only factors influencing LLM performance, which may not account for other factors such as model architecture or training data.

--------------------------------------------------

Claim 2:
Type: methodology
Statement: LLMs prefer knowledge that forms CoE
Location: Section 3. CoE Discrimination Approach
Exact Quote: We assume that LLMs prefer knowledge that forms CoE.

Evidence:
- Evidence Text: We characterize three features: Intent, Keywords, and Relations.
  Strength: moderate
  Location: Section 3. CoE Discrimination Approach
  Limitations: assumes these three features are the only factors determining CoE
  Exact Quote: We characterize three features: Intent, Keywords, and Relations.

- Evidence Text: External knowledge containing CoE is more effective than Non-CoE in various scenarios.
  Strength: strong
  Location: Section 5. Effectiveness Assessment
  Limitations: assumes CoE is the only factor determining LLM performance
  Exact Quote: External knowledge equipped with CoE can more effectively (than Non-CoE) help LLMs generate correct answers in context rich with irrelevant information.

- Evidence Text: LLMs exhibit higher faithfulness to the answer implicated in CoE (than Non-CoE), even when CoE contains factual errors.
  Strength: strong
  Location: Section 6. Faithfulness Assessment
  Limitations: assumes faithfulness is solely determined by CoE presence
  Exact Quote: LLMs exhibit higher faithfulness to the answer implicated in CoE (than Non-CoE), even when CoE contains factual errors.

- Evidence Text: LLMs exhibit higher robustness against knowledge conflict (than Non-CoE) if the external knowledge is equipped with CoE.
  Strength: strong
  Location: Section 7. Robustness Assessment
  Limitations: assumes robustness is solely determined by CoE presence
  Exact Quote: LLMs exhibit higher robustness against knowledge conflict (than Non-CoE) if the external knowledge is equipped with CoE.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided demonstrates a consistent pattern of improved performance across multiple LLMs and datasets when CoE is present.
Key Limitations: The study assumes that relevance and interconnectivity are the only factors determining LLM performance, which may not account for other factors such as model architecture or training data.

--------------------------------------------------

Claim 3:
Type: performance
Statement: CoE-guided retrieval strategy can improve LLM performance in RAG framework
Location: Section 8. Usability Assessment
Exact Quote: CoE-guided retrieval strategy can effectively improve LLMs’ accuracy in the naive RAG framework.

Evidence:
- Evidence Text: CoE-guided retrieval strategy achieves average ACC of 77.8% and 81.6% on HotpotQA and 2WikiMultihopQA respectively, outperforming RAG by 10.4% and 28.7%.
  Strength: moderate
  Location: Section 8. Usability Assessment
  Limitations: assumes RAG framework is the only applicable scenario
  Exact Quote: CoE-guided retrieval strategy achieves average ACC of 77.8% and 81.6% on HotpotQA and 2WikiMultihopQA respectively, outperforming RAG by 10.4% and 28.7%.

Evaluation:
Conclusion Justified: Yes
Robustness: moderate
Confidence Level: medium
Justification: The evidence provided demonstrates a consistent pattern of improved performance when CoE is used in the retrieval strategy.
Key Limitations: The study assumes that RAG framework is the only applicable scenario, which may not account for other factors such as model architecture or training data.

--------------------------------------------------

