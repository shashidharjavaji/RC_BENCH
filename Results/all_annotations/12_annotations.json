[
    {
        "claim_id": 1,
        "claim_text": "We demonstrate the effectiveness of DocPrompting on two NL→code benchmarks and tasks, across two programming languages, and using several base models: GPT-Neo (Black et al., 2021), T5 (Raffel et al., 2020), CodeT5 (Wang et al., 2021), Fusion-in-Decoder (Izacard and Grave, 2021)), and Codex (Chen et al., 2021).",
        "evidence_text": "We evaluate DocPrompting on two NL→code tasks: shell scripting (§4.1), in which we generate complex shell commands given an intent, and Python programming (§4.2), where we generate answers in Python for NL questions. In this section, we first introduce a newly curated benchmark tldr; we then describe our re-split of the popular CoNaLa benchmark (Yin et al., 2018).; CoNaLa (Yin et al., 2018) is a popular benchmark for NL→Python generation. NL intents are StackOverflow questions, and code snippets are their answers. Both intents and code snippets are rewritten by human annotators. We re-split the dataset to test models’ generalization to unseen Python functions. In our re-split, we verifed that every example in the development or the test set uses at least one Python function (e.g., plt.plot) that was not seen in the training data. In addition, we make sure that the examples from the same StackOverflow posts are in the same set to prevent leakage.; Results for tldr are shown in Table 1. DocPrompting consistently improves the base models. For example, T5+DocPrompting achieves more than twice higher accuracy in predicting the command name, more than 16 charBLEU points on the entire prediction, and almost 9% of absolute exact match gain, compared to the vanilla T5. In the few-shot learning setting with Codex, DocPrompting brings gains of 6.7 charBLEU points, and consistent improvement across all metrics over the baseline that observes only NL-code pairs in its prompt. These results show that retrieving documentation also benefits strong models such as Codex, and with only few examples in the context.; Table 3 shows the results on CoNaLa. CodeT5+DocPrompting yields a 1.65 BLEU improvement over the state-of-the-art baseline that was initialized with CodeT5.4 When measuring the recall of the generated function names, the benefit of DocPrompting is especially higher for unseen functions (recallunseen). For example, DocPrompting achieves 18.30 compared to only 9.03 of the base CodeT5 in unseen functions. Additionally, DocPrompting improves in-context learning setting with Codex.",
        "justification_conclusion": "The authors added two new benchmark datasets for Python and Bash and evaluated the performance of the purposed approach on these datasets with different base models."
    },
    {
        "claim_id": 2,
        "claim_text": "Further, we experiment with both sparse retrievers such as BM25 (Robertson and Jones, 1976) and dense retrieval models such as SimCSE (Gao et al., 2021)",
        "evidence_text": "We compared different configurations of the retriever, to gather more insights for effective DocPrompting. Table 4 shows a comparison between different retrievers and their setups. First, the performance of BM25 varies among datasets: In tldr, BM25 matches the recall of trained dense retrievers; however in CoNaLa, BM25 achieves only recall@10 of 9.73%, and strong dense retrievers such as the encoder of CodeT5 achieve recall@10 of 55.81. We hypothesize that this difference between datasets stems from the ways these datasets were created: tldr intents were written based on existing Bash commands and manuals; while CoNaLa examples were mined from StackOverflow posts, where users ask questions with limited or no context. Thus, NL intents in CoNaLa require a better semantic alignment with the documents, and thus benefit from dense retrievers. The gap resulting from different data curation processes was also observed by Rodriguez and Boyd-Graber (2021) in open-domain question answering (QA).  Second, retrievers that were pretrained on the target programming language are generally stronger. For example in CoNaLa, CodeT5 which was pretrained on Python, is both a better off-the-shelf retriever and a better finetuned-retriever than RoBERTa, which was pretrained mainly on text. In contrast, tldr is based on Bash, which neither CodeT5 nor RoBERTa were explicitly pretrained on. Thus, tldr benefits mostly from BM25 and RoBERTa rather than CodeT5 as retrievers.  Finally, training the retriever using weak supervision on the documentation pool (Section 3.1) dramatically improves the retriever. The recall of the best retrievers of each dataset without this corpus is shown in the last column of Table 4 (“Best w/o weak sup.”). On CoNaLa, removing this corpus results in severe performance degradation. One possible explanation is that this weak supervision helps the retriever perform domain adaptation more effectively.",
        "justification_conclusion": "The authors provided a detailed comparison between different retriever methods."
    },
    {
        "claim_id": 3,
        "claim_text": "Retrieving the documentation yields better performance than retrieving examples.",
        "evidence_text": "We finetuned the best retriever RoBERTa and two generators, and retrieved the top-30 NL-code pairs for every example. As shown in Table 2, retrieving documentation (DocPrompting) provides much higher gains than retrieving examples (ExPrompting).",
        "justification_conclusion": "As the excrement results suggested that retrieving documentation yields better performance than retrieving examples."
    },
    {
        "claim_id": 4,
        "claim_text": "The documentation help generating more accurate code because it eases the mapping between NL intents and code.",
        "evidence_text": "We calculated the n-gram overlap between the NL intents and their corresponding code snippets (NL←→code), and the overlap between the NL intents with their top-10 retrieved documents and their code snippets ((NL+docs)←→code). As shown in Figure 4, adding documentation significantly increases the overlap across n-grams, and increase, for example, the unigram overlap from 12% to 24% in tldr. That is, one of the reasons that retrieving documentation helps generating accurate code is that documentation bridges the gap between the “intent terminology” and the “code terminology”.",
        "justification_conclusion": "The n-gram analysis supports that documentation helps bridge the gap between the NL intents and code."
    }
]