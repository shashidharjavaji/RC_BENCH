[
    {
        "claim_id": 1,
        "claim_text": "We propose hallucination-augmented cross-modal contrastive learning (HACL), which enhances the alignment between visual and textual representations to alleviate hallucinations.",
        "evidence_text": "Specifically, we introduce contrastive learning into MLLMs and use text with hallucination as hard negative examples, naturally bringing representations of non-hallucinative text and visual samples closer while pushing away representations of non-hallucinative and hallucinative text.",
        "justification_conclusion": "True. This claim introduces the core method proposed in the paper, supported by experimental results showing reduced hallucination occurrences and improved performance across benchmarks."
    },
    {
        "claim_id": 2,
        "claim_text": "Our experiments show that equipping MLLMs with HACL not only mitigates hallucinations but also effectively improves the performance across multiple benchmark evaluations.",
        "evidence_text": "As shown in Subfigure 1(c), when equipped with HACL, LLaVA achieves a 29% increase in the overall score on the MMhal-Bench benchmark, as well as an 11% improvement on the MME benchmark.",
        "justification_conclusion": "True. The quantitative results on MMhal-Bench and MME benchmarks validate the effectiveness of HACL in reducing hallucinations and enhancing model performance."
    },
    {
        "claim_id": 3,
        "claim_text": "We underline a significant cross-modal semantic gap between visual and textual representations and an unexpected representation tangling among text containing and not containing hallucinations in MLLMs.",
        "evidence_text": "Our analysis of representation distribution revealed that visual and textual representations remain misaligned, and representations of hallucinating text are entangled with non-hallucinating text.",
        "justification_conclusion": "True. This claim is based on a detailed analysis of token representations, highlighting the need for improved cross-modal alignment, which motivates the proposed method."
    },
    {
        "claim_id": 4,
        "claim_text": "HACL forces the visual representation closer to the text representation and makes the correct and hallucinated text representations more distinguishable.",
        "evidence_text": "In Subfigure 1(b), introducing HACL into LLaVA reduces the modality gap and improves the separation between hallucinating and non-hallucinating text representations.",
        "justification_conclusion": "True. Visualization of representation distributions demonstrates the improved alignment and separability achieved with HACL, addressing key challenges in representation learning."
    },
    {
        "claim_id": 5,
        "claim_text": "Our method achieves significant improvements over state-of-the-art models such as MiniGPT-4 and LLaVA on hallucination-related benchmarks.",
        "evidence_text": "Compared to MiniGPT-4 and LLaVA, our method achieves a 34.66% improvement on MMhal-Bench and a 29.5% improvement on MME.",
        "justification_conclusion": "True. This claim is supported by benchmark results showing consistent and significant performance gains over state-of-the-art models."
    }
]
