[
    {
        "claim_id": 1,
        "claim_text": "Achieves human-level performance on reward design across a diverse suite of 29 open-sourced RLenvironments that include 10 distinct robot morphologies, including quadruped, quadcopter, biped, manipulator, as well as several dexterous hands",
        "evidence_text": "In Figure 4, we report the aggregate results on Dexterity and Isaac. Notably, EUREKA exceeds or performs on par to human level on all Isaac tasks and 15 out of 20 tasks on Dexterity (see App. F for a per-task breakdown).",
        "justification_conclusion": "The EUREKA achieves human-level performance on reward design tasks."
    },
    {
        "claim_id": 2,
        "claim_text": "Solves dexterous manipulation tasks that were previously not feasible by manual reward engineering. We consider pen spinning, in which a five-finger hand needs to rapidly rotate a pen in pre-defined spinning configurations for as many cycles as possible. Combining EUREKA with curriculum learning, we demonstrate for the first time rapid pen spinning maneuvers on a simulated anthropomorphic Shadow Hand.",
        "evidence_text": "Finally, we investigate whether EUREKA can be used to solve a truly novel and challenging dexterous task. To this end, we propose pen spinning as a test bed. This task is highly dynamic and requires a Shadow Hand to continuously rotate a pen to achieve some pre-defined spinning patterns for as many cycles as possible; we implement this task on top of the original Shadow Hand environment in Isaac Gym without changes to any physics parameter, ensuring physical realism. We consider a curriculum learning (Bengio et al., 2009) approach to break down the task into manageable components that can be independently solved by EUREKA. Specifically, we first use EUREKA to generate a reward for the task of re-orienting the pen to random target configurations and train a policy using the final EUREKA reward. Then, using this pre-trained policy (Pre-Trained), we fine-tune it using the same EUREKA reward to reach the sequence of pen-spinning configurations (Fine-Tuned).",
        "justification_conclusion": "With the pre-training and fine-tuning approach based on theory of curriculum learning. The EUREKA can solve the pen spinning task."
    },
    {
        "claim_id": 3,
        "claim_text": "Enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF) that cangenerate more performant and human-aligned reward functions based on various forms of human inputs without model updating. We demonstrate that EUREKA can readily benefit from and improve upon existing human reward functions.",
        "evidence_text": "We study whether starting with a human reward function initialization, a common scenario in real-world RL applications, is advantageous for EUREKA. Importantly, incorporating human initialization requires no modification to EUREKA– we can simply substitute the raw human reward function as the output of the first EUREKA iteration. To investigate this, we select several tasks from Dexterity that differ in the relative performances between the original EUREKA and human rewards. The full results are shown in Figure 8. Asshown, regardless of the quality of the human rewards, EUREKA improves and benefits from human rewards as EUREKA (Human Init.) is uniformly better than both EUREKA and Human on all tasks. This suggests that EUREKA’s in-context reward improvement capability is largely independent of the quality of the base reward. Furthermore, the fact that EUREKA can significantly improve over human rewards even when they are highly sub-optimal hints towards an interesting hypothesis: human designers are generally knowledgeable about relevant state variables but are less proficient at designing rewards using them. This makes intuitive sense as identifying relevant state variables that should Figure 8: EUREKA effectively improves and benefits from human reward initialization. be included in the reward function involves mostly common sense reasoning, but reward design requires specialized knowledge and experience in RL. Together, these results demonstrate EUREKA’s reward assistant capability, perfectly complementing human designers’ knowledge about useful state variables and making up for their less proficiency on how to design rewards using them. In App. G.3, we provide several examples of EUREKA (Human Init.) steps.",
        "justification_conclusion": "The EUREKA is able to improve the input human reward function."
    }
]