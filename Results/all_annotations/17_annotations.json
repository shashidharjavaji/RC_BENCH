[
    {
        "claim_id": 1,
        "claim_text": "Our goal for this work is to create cost-effective autonomous intelligent agents, and we do so by adopting design principles of the animal brain and other non-LLM agents. With this approach, we developed agents that cost about 30-100 times less than that of Park et al. (2023).",
        "evidence_text": "Autonomous agents are inherently more expensive than their non-autonomous counterparts. Consider a typical chat-bot, it will not initiate a conversation with the human users, let alone converse with other bots. Autonomous chat-bots, however, might continuously engage in dialogues, leading to potentially unbounded costs. This challenge becomes even more daunting when we need the autonomous agents to provide low-latency responses for real-time human interactions. Low-latency implies that agents can have fast response to each other as well, potentially leading to rapid-paced back-and-forth conversations between agents that get expensive very quickly. Much of our work presented here is aimed to tackle these challenges. As a result, Lyfe Agents achieve a rather low cost of 0.5 US dollar per agent per human hour (Fig. 6) (See Appendix F for more discussions).",
        "justification_conclusion": "The framework's cost is significantly lower than the one of Park et al. (2023)."
    },
    {
        "claim_id": 2,
        "claim_text": "we use a hierarchical action selection mechanism to guide agents’ high-level decisions with minimal reliance on the LLM",
        "evidence_text": "In Lyfe Agents, a cognitive controller module (like HRL’s manager) selects options, inspired by the brain’s prefrontal cortex (Miller & Cohen, 2001). More specifically, the cognitive controller takes in the agent’s goal along with other relevant internal states. Using an LLM call, it then outputs an option along with a subgoal (Fig. 2b). Since the agent’s goal may be too abstract or long-term to justify the choice of an option, the subgoal serves to orient the agent’s actions at an intermediate level between low-level actions and the high-level goal. Once an option is selected, actions are chosen within that option over subsequent steps until a termination condition is met. For example, a selected option may be to talk, then at each step, the specific action of what to actually say is determined by an LLM call. Important for cost-reduction, the termination condition for an option is checked by fast, non-LLM methods, such as time-based triggers or, for agents in conversations, repetition detection which exits conversations that start to lack semantic novelty after some point.",
        "justification_conclusion": "The framework adopts a hierarchical action selection mechanism. The agent will continue the current state until the certain termination condition is met, so the agent would not need to call the LLM on every action."
    },
    {
        "claim_id": 3,
        "claim_text": "we introduce a self-monitoring process that facilitates self-consistency by maintaining a summary of relevant recent events in parallel with the goal-driven, executive decision-making.",
        "evidence_text": "The self-monitoring module provides agents with better context-awareness by distilling goalrelevant content from a stream of disparate and unorganized information. This coherent and focused narrative is then used in downstream processes like action selection. In contrast, passing an unfocused collection of disparate information directly for downstream LLM calls severely impacts performance (see Section 4.1.2). Another advantage of maintaining a self-monitoring summary is to preserve information longer term if it is highly relevant to an agent’s goal. Without this summary, we observed that agents frequently forgot their ongoing tasks or actions. The self-monitoring summary helps agents have actions that are more coherent and adhering to their goals.",
        "justification_conclusion": "The self-monitoring helps agents to maintain a coherent recent event summary and make sure the agents is adhering to their goals."
    },
    {
        "claim_id": 4,
        "claim_text": "we devise a hierarchical memory architecture and introduce a Summarize-and-Forget (SaF) method that improves the quality of memory storage and retrieval.",
        "evidence_text": "Standard memory systems typically struggle with the unfiltered accumulation of recent information, resulting in clutter and inefficiency. Addressing this, we introduce a dual-memory architecture: recentmem for immediate summaries and longmem for enduring storage, modeled after the complementary roles of the hippocampus and neocortex in the brain’s memory systems (McClelland et al., 1995). In particular, recentmem is dedicated to capturing immediate self-monitoring summaries. Upon reaching a specified capacity, these memories are transitioned to longmem. Having a dual memory system allows for intelligent transition methods to ensure that only the most salient memories find their way into long-term storage (Fig. 2d). Our approach to transitioning memories uses a cluster-then-summarize technique. Memories are clustered based on similarity before being refined into high-level summaries using an LLM (Appendix A.3). This ensures that the stored content is not just raw data but possesses semantic richness, enhancing the quality of memories for downstream processes. Addressing the challenge of memory redundancy, our architecture integrates a new forgetting algorithm inspired by the brain (Brown & Lewandowsky, 2010; Georgiou et al., 2021). Rather than merely trimming data, this algorithm assesses and removes older memories that closely resemble new ones (determined by embedding similarities). This mechanism ensures that memories securing their place in recentmem or longmem are not just redundant repetitions, but unique and relevant, granting agents access to a multifaceted information spectrum.",
        "justification_conclusion": "The authors purpose a dual memory structure for intermediate and long-term memory storage. Also, the redundant (based on similarity metric) memories are removed."
    }
]