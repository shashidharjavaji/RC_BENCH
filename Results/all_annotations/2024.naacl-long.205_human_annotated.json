{
    "annotations": [
        {
            "claim": "In this paper, we intro duce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs.",
            "evidences": [
                "TSort. TSort provides LLMs with N shuffled text segments, extracted from contiguous chapters of a long book. The task for models is to sort these seg- ments into their original sequence. A response is regarded accurate only if it precisely reinstates the segments’ initial order. To simplify the challenge and minimize possible confusion, we supply LLMs with adjacent paragraphs from before and after the specified chapters to serve as contextual hints. BestAnswer. Each test case in BestAnswer con- tains one question and a large amount of possible answers to this question. We consider the answer designated by the original inquirer as the most help- ful answer, while LLMs are required to identify this optimal answer among all possible candidates."
            ]
        },
        {
            "claim": "These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens.",
            "evidences": [
                "Under ultra-long-context settings, we build test cases with 32k, 64k, and 128k tokens for both tasks. The construction paradigm is similar to the long- context setting",
                "Under long-context settings, TSort cases span test cases with 2k, 4k, 8k, and 16k tokens. For each length, we fix the segment number N=4 and the length upper limit for each text segment and adjacent paragraphs before and after these contigu- ous chapters. We ensure that each text segment contains complete paragraphs thus no paragraph is sliced in the middle. To build test cases with different contents, we set stride between beginning paragraphs of test cases during construction. After prepending the instructions, we further filter test cases that exceed the token upper bound. For BestAnswer, we generate test cases with 1k, 2k, 4k, 6k, 8k, 12k, and 16k tokens under long- context settings. Test cases contain the distractor answers under corresponding question and adapt- able number of distractor answers from other sim- ilar questions under each length setting. To make evaluation results directly comparable across dif- ferent length settings in long context scenarios, we ensure that the questions within the BestAnswer benchmark remain unchanged, regardless of the case length."
            ]
        },
        {
            "claim": "We evaluate 4 state-of- the-art closed-source API models and 6 opensource models with Ada-LEval.",
            "evidences": [
                "We evaluate the following LLMs under long- context settings: 4 proprietary models: (1) GPT-4- Turbo-0125, (2) GPT-4-Turbo-1106 (3) GPT-3.5- Turbo-1106, (4) Claude-2; and 6 open-source mod- els: (5) LongChat-7b-v1.5-32k(Zheng et al., 2023), (6) ChatGLM2-6B-32k(Zeng et al., 2022), (7) ChatGLM3-6B-32k(Zeng et al., 2022), (8) Vicuna- 7b-v1.5-16k(Zheng et al., 2023), (9) Vicuna-13b- v1.5-16k(Zheng et al., 2023), (10) InternLM2- 7b(Cai et al., 2024). Due to the inferior perfor- mance of open-source LLMs under long-context settings, only models with good performance (GPT- 4-Turbo, Claude-2, etc.) are evaluated under ultra- long-context settings.",
                "4.2 Long-Context Evaluation Results TSort 2k 4k 8k 16k GPT-4-Turbo-0125 15.5 16.5 8.5 5.5 GPT-4-Turbo-1106 18.5 15.5 7.5 3.5 GPT-3.5-Turbo-1106 4.0 4.5 4.5 5.5 Claude-2 5.0 5.0 4.5 3.0 LongChat-7b-v1.5-32k 5.3 5.0 3.1 2.5 ChatGLM2-6B-32k 0.9 0.7 0.2 0.9 ChatGLM3-6B-32k 2.3 2.4 2.0 0.7 Vicuna-7b-v1.5-16k 5.3 2.2 2.3 1.7 Vicuna-13b-v1.5-16k 5.4 5.0 2.4 3.1 InternLM2-7b 5.1 3.9 5.1 4.3 Random Guess 4.2 4.2 4.2 4.2 Table 2: TSort results under long-context settings. We fix the number of segments N = 4 for TSort evaluation, thus random guess accuracy is roughly 4.2% (1 / 24)."
            ]
        },
        {
            "claim": "The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings.",
            "evidences": [
                "4.2 Long-Context Evaluation Results TSort 2k 4k 8k 16k GPT-4-Turbo-0125 15.5 16.5 8.5 5.5 GPT-4-Turbo-1106 18.5 15.5 7.5 3.5 GPT-3.5-Turbo-1106 4.0 4.5 4.5 5.5 Claude-2 5.0 5.0 4.5 3.0 LongChat-7b-v1.5-32k 5.3 5.0 3.1 2.5 ChatGLM2-6B-32k 0.9 0.7 0.2 0.9 ChatGLM3-6B-32k 2.3 2.4 2.0 0.7 Vicuna-7b-v1.5-16k 5.3 2.2 2.3 1.7 Vicuna-13b-v1.5-16k 5.4 5.0 2.4 3.1 InternLM2-7b 5.1 3.9 5.1 4.3 Random Guess 4.2 4.2 4.2 4.2 Table 2: TSort results under long-context settings. We fix the number of segments N = 4 for TSort evaluation, thus random guess accuracy is roughly 4.2% (1 / 24).",
                "We further analyze the error instances on TSort and BestAnswer, and find that most errors can be attributed to two categories: 1. The LLM fails to follow the provided instruction and does not output a valid answer4; 2. The LLM does output a valid answer. However, it simply copies the example answer we provide in the in-context example.",
                "We evaluate the following proprietary models un- der ultra-long-context settings. (1) GPT-4-Turbo- 0125 (2) GPT-4-Turbo-1106 (3) Claude-2. (4) Claude-2.1. We also evaluate InternLM2-7b on BestAnswer benchmark under ultra-long-context settings. Due to high API calling expense, we test 50 samples under each ultra-long context setting. Table 6 demonstrates the result. Though the evaluated models claim that they can understand long text up to 100,000+ tokens (a whole book with hundreds of pages, e.g.), they suffer from a dramatic decline on their performance under ultra-long-context settings, comparing to their long-context performance. For the TSort task GPT-4-Turbo is able to achieve a random guess level accuracy, while Claude fails to give any cor rect answers. For BestAnswer, the performance of all three models fall sharply from 16k to 32k text length. Meanwhile, they can not give any correct answer when the text length is greater than 32k.",
                "Benchmark Model 32k 64k 128k GPT-4-Turbo-0125 2.0 4.0 2.0 GPT-4-Turbo-1106 6.0 6.0 6.0 TSort Claude-2 0.0 0.0 / Claude-2.1 0.0 0.0 0.0 Random Guess 4.2 4.2 4.2 GPT-4-Turbo-0125 30.0 0.0 0.0 GPT-4-Turbo-1106 16.0 0.0 0.0 BestAnswer Claude-2 4.0 0.0 / Claude-2.1 4.0 0.0 0.0 InternLM2-7b 0.5 0.5 0.0 Random Guess 0.6 0.3 0.1 Table 6: Results of LLMs on TSort and BestAnswer benchmarks in ultra-long context settings."
            ]
        },
        {
            "claim": "Furthermore, our ablation study uncovers several shortcomings in current LLMs, including limited instruction fol- lowing over extended texts and pronounced input order bias.",
            "evidences": [
                "All models demonstrate significant po- sition bias in choosing the most helpful answer. Most models achieve much better accuracy when the most helpful answer presents at the beginning. Claude-2 has some unique behaviors. It performs the best when the groundtruth is positioned at the rear across 4 of 5 different settings. As the input length increases, the position bias becomes more obvious. For instance, Vicuna-7b-v1.5-16k demon- strates relatively uniform accuracy under the 1k setting. However, when the input length extends to 16k tokens, the model’s performance remains stable only when the best answer is at the front. BestAnswer Pos 1k 2k 4k 8k 16k front 76.5 82.5 86.5 90.0 82.0 GPT-4-Turbo-1106 mid 74.5 68.0 60.0 38.0 38.5 rear 57.5 46.6 44.0 40.5 26.5 front 77.0 80.5 77.0 46.5 2.5 GPT-3.5-Turbo-1106 mid 64.5 48.5 32.0 9.5 0.5 rear 37.5 19.0 8.5 6.0 3.5 front 34.0 19.0 14.5 50.0 6.0 Claude-2 mid 49.0 35.5 21.5 13.0 5.0 rear 59.0 36.5 26.0 11.0 9.5 front 24.1 5.0 12.1 33.6 29.0 LongChat-7b-v1.5-32k mid 32.7 13.6 0.2 0.2 0.0 rear 29.8 1.9 0.0 0.1 0.1 front 30.0 31.5 46.2 10.5 0.5 ChatGLM2-6B-32k mid 27.7 10.4 1.0 0.1 0.1 rear 28.5 12.4 2.6 4.1 0.0 front 48.9 34.3 37.6 35.8 19.0 ChatGLM3-6B-32k mid 41.9 22.3 5.3 0.9 0.1 rear 28.8 5.4 3.7 8.8 2.9 front 29.3 8.9 14.0 37.6 25.4 Vicuna-7b-v1.5-16k mid 32.8 13.6 0.0 0.0 0.2 rear 34.2 2.1 0.0 0.0 0.7 front 52.5 51.4 58.6 81.7 11.8 Vicuna-13b-v1.5-16k mid 64.5 29.2 1.5 0.5 0.3 rear 34.2 2.4 0.0 0.0 13.4 Table 8: Results of LLMs on BestAnswer where the best answer is set at the front, in the middle and at the rear of all answers. Pos denotes the position of the best answer."
            ]
        },
        {
            "claim": "Additionally, we explore various scal- able position embedding techniques aimed at en- larging the context window of LLMs. Our find- ings indicate that models equipped with those tech- niques show improved performance over the stan- dard models, and the performance is comparable to their counterparts trained on longer contexts.",
            "evidences": [
                " Our findings indicate that scalable position embeddings do im- prove the long-context modeling capability. All methods enhance the accuracy under the 8k set- ting, which is beyond the original context window. Concurrently, the model performance under short settings (1k, e.g.) is basically retained. NTK-aware Scaled RoPE diminishes performance on 1k con- text length, but outperforms other two methods on longer context. The advantage of these methods is more obvious on Vicuna-13b-v1.5. Moreover, com- paring to their 16k versions, which utilize Flash Attention and are further trained on high-quality 16k length conversation data, advanced scalable position embeddings still achieve comparable per- formance. Vicuna-7b-v1.5 1k 2k 4k 8k ReRoPE 39.6/39.6 11.6/11.6 4.7/5.4 2.3/3.2 Leaky ReRoPE 39.9/39.9 11.2/11.2 5.1/5.7 1.3/2.0 NTK 32.5/32.5 10.7/10.7 5.8/5.8 3.9/3.9 Original(4k) 39.5/39.5 9.8/11.0 4.2/5.5 0.0/0.0 Original(16k) 37.0/39.5 11.1/11.1 5.8/5.8 2.5/2.7 Vicuna-13b-v1.5 1k 2k 4k 8k ReRoPE 49.2/49.2 22.5/22.5 9.2/10.0 1.5/2.8 Leaky ReRoPE 49.3/49.3 23.8/23.8 8.7/9.8 1.3/2.6 NTK 43.8/43.8 23.0/23.0 11.1/11.1 2.3/2.3 Original(4k) 49.1/49.1 17.7/17.7 5.9/5.9 0.1/1.0 Original(16k) 53.4/53.4 29.2/29.2 13.1/13.5 2.6/2.7 Table 9: Results of Vicuna-v1.5 with different context window extrapolation methods on BestAnswer. ‘Orig- inal (4k) / (16k)’ denotes the original Vicuna model trained with 4k / 16k context lengths. In the reported ‘X/Y’, X indicates the accuracy while Y indicates the accuracy which cases failed to follow the instruction are excluded."
            ]
        }
    ]
}