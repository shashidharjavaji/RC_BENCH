{
    "annotations": [
        {
            "claim": "we develop two modes of evaluation for natural language explanations that claim individual neurons represent a con- cept in a text input.",
            "evidences": [
                "The observational mode only evaluates whether a concept is encoded, as opposed to used (Antverg and Belinkov, 2022). Thus, we propose an interven- tion mode to evaluate the claim that a is a causally active representation of the concept denoted by E. We construct next token prediction tasks that hinge on the concept and intervene on the neuron a to study whether the neuron is a causal mediator of concepts picked out by E.",
                "In the observational mode, we evaluate the claim that a neuron a activates on all and only input strings that refer to a concept picked out by the proposed explanation E. Relative to a set of inputs, we can then use the error rates to assess the quality of E for a."
            ]
        },
        {
            "claim": "We apply our frame- work to the GPT-4-generated explanations of GPT-2 XL neurons of Bills et al. (2023) and show that even the most confident explanations have high error rates and little to no causal effi- cacy.",
            "evidences": [
                "Does GPT-4 produce causal explanations? GPT-4 generated explanations have similar causal effects as the random baseline on most tasks. The only exception is the explanation for neurons re- lated to numerical expressions, which has higher IIA than the random baseline, but still far below the token-activation correlation baseline. In other words, if we were using GPT-4 gen- erated explanation to inform us which weights to modify in a model editing task, we would have sim- ilar performance as randomly selecting neurons to edit. This finding is worrisome but not surprising given low precision and recall values we obtained in our observational evaluation (Section 3).",
                "Results over 300 neuron explanations are shown in Table 1. For single neuron without probing, the GPT-4 explanations have a mean F1 score of 0.56 (with a precision of 0.64 and a recall of 0.50), whereas the random baseline has a F1 score of zero. With learned probes, the F1 score of GPT-4 explanations is 0.60. The F1-score has a correlation coefficient of âˆ’0.1 with the GPT-4 score. Withmore neurons, F1 scores increase while the margin over the random baseline decreases, suggesting that most semantically relevant neurons have already been sampled. Examples of error cases are shown in Table 2, with analysis in Appendix B.",
                "The GPT-4 score is computed on a set of 10 ex- amples from the GPT-2 XL training corpus, 5 con- taining tokens with top activations and 5 randomly sampled. We now show that an unfaithful explana- tion with a precision of 0.50 can still have a perfect GPT-4 score with high probability. ",
                "No Probe With Probe N=1 N=1 N=2 N=4 N=16 Random 0.00 0.29 0.44 0.54 0.69 GPT-4 0.56 0.60 0.64 0.67 0.73 Table 1: F1 scores measure how well randomly selected explanations and GPT-4 generated explanations predict neuron activations, averaged over 300 explanations with a GPT-4 score of at least 0.8. For each explanation to evaluate, we either randomly select N neurons or select N neurons whose explanations are semantically most similar to the given explanation."
            ]
        }
    ]
}