{
    "annotations": [
        {
            "claim": "We created 500 chatbots with distinct personality designs and evaluated the validity of self-reported personality scales in LLM-based chatbot’s personality evaluation.",
            "evidences": [
                "2.1 Chatbot Design In this study, we first created a set of LLM-based chatbots with various personality designs. Those chatbots were created for tasks in which personal- ity design is critical in user experience. Our chat- bot framework consists of two main components: a personality module (§2.1.1) and a task module (§2.1.2). The personality module shapes the chat- bot’s behavior with predefined personality traits, while the task module provides role-specific in- structions and task outlines to complete the task. The complete prompts are in Appendix A.",
                "2.1.1 Personality Module The personality design of our chatbot is based on the Big Five model (John et al., 1999), a founda- tional framework in personality research that de- scribes personality across five key domains: ex- traversion (EXT), agreeableness (AGR), consci- entiousness (CON), neuroticism (NEU), and open- ness to experience (OPE). We adopted the shape ap- proach from Serapio-García et al. (2023) to shape a chatbot’s personality in each domain. This method extends Goldberg (1992) list of 70 bipolar adjec- tives to 104 personality trait descriptors, which are mapped to different personality facets within each domain. For example, adjectives such as ‘unen- ergetic’ and ‘energetic’ represent the lower and higher levels of extraversion, respectively. These adjectives were then paired with linguistic quali- fiers from Likert-type scales (Likert, 1932) to cre- ate personality profiles at varying levels. For our high-level or low-level adjectives from the same personality domain to generate prompts."
            ]
        },
        {
            "claim": "Our findings indicate that the chatbot’s answers on human personality scales exhibit weak correlations with both user per- ception and interaction quality, which raises both criterion and predictive validity concerns of such a method.",
            "evidences": [
                "Task Ext Agr Con Neu Ope Job Interview 0.19 0.54 0.28 0.31 0.19 Public Service 0.40 0.58 0.44 0.30 0.23 Social Support 0.03 0.58 -0.03 0.13 0.15 Travel Planning 0.28 0.60 0.39 0.49 0.33 Guided Learning 0.05 0.56 0.03 0.34 0.14 Absolute Mean 0.19 0.57 0.23 0.31 0.21 Table 5: Average correlation between self-reported and human-perceived personality scores across tasks. Note: n = 100 for each task. Ext = Extraversion; Agr = Agreeableness; Con = Conscientiousness; Neu = Neu- roticism; Ope = Openness.",
                " Task Ext Agr Con Neu Ope Job Interview 0.26 0.37 0.49 -0.40 0.48 Public Service 0.14 0.32 0.41 -0.22 0.28 Social Support -0.05 0.17 0.19 -0.15 0.25 Travel Planning 0.28 0.71 0.76 -0.55 0.52 Guided Learning 0.15 0.14 0.24 -0.09 0.16 Table 6: Correlation between aggregated UEQ scores and human-perceived personality scores across tasks. Note: n = 433 in total. Ext = Extraversion; Agr = Agree- ableness; Con = Conscientiousness; Neu = Neuroticism; Ope = Openness.",
                "Task Ext Agr Con Neu Ope Job Interview 0.12 0.15 0.13 -0.09 0.03 Public Service 0.03 0.04 0.17 -0.16 -0.03 Social Support 0.10 0.10 0.08 -0.10 0.02 Travel Planning 0.03 0.08 0.04 -0.17 -0.09 Guided Learning -0.06 -0.04 -0.07 0.02 -0.01 Table 7: Correlation between aggregated UEQ scores and self-reported personality scores across tasks. Note: n = 433. Ext = Extraversion; Agr = Agreeable- ness; Con = Conscientiousness; Neu = Neuroticism; Ope = Openness. Task Ext Agr Con Neu Ope Job Interview 0.12 0.15 0.13 -0.09 0.03 Public Service 0.03 0.04 0.17 -0.16 -0.03 Social Support 0.10 0.10 0.08 -0.10 0.02 Travel Planning 0.03 0.08 0.04 -0.17 -0.09 Guided Learning -0.06 -0.04 -0.07 0.02 -0.01 Table 7: Correlation between aggregated UEQ scores and self-reported personality scores across tasks. Note: n = 433. Ext = Extraversion; Agr = Agreeable- ness; Con = Conscientiousness; Neu = Neuroticism; Ope = Openness."
            ]
        },
        {
            "claim": "we evaluated two sets of validity: Convergent and Discriminant validity, where we examined the in- trinsic structure, and Criterion and Predictive valid- ity, where we examined the extrinsic relationship with external variables. ",
            "evidences": [
                "3.2 Convergent and Discriminant Validity Table 2 presents the correlation between the chat- bot’s self-reported scores across different scales. It is evident that, regardless of the dimension, the cor- relations across scales show a high degree of con- sistency, with an average correlation coefficient of 0.85. This result indicates that the chatbot demon- strates a high level of stability in its self-reports across different personality questionnaires, suggest- ing consistency within self-report methods. Table 3 shows the correlations of human- perceived and self-report personality scores across different personality traits. The results further demonstrate alignment across the self-report meth- ods, both in terms of the magnitude and direction of correlations between each pair of traits, as well as the absolute mean values."
            ]
        },
        {
            "claim": "• Through an empirical study with 500 partici- pants, we unveil the validity concerns of using self-report personality scales for evaluating LLM-based chatbot’s personality design.",
            "evidences": [
                "3.4 Predictive Validity (Interaction Quality) Tables 6 and 7 detail correlation analyses between aggregated UEQ scores and personality scores, both human-perceived and self-reported, across five tasks. This analysis evaluates the predictive validity of these personality assessments in relation to interaction quality with chatbots, which is the ultimate goal of building such chatbots. Table 6 demonstrates significant correlations between per- ceived agreeableness and conscientiousness and user experience, notably in the travel planning task, with correlations of 0.71 and 0.76, respectively. These findings suggest that chatbots perceived as agreeable and conscientious markedly enhance user experience. In contrast, neurotic traits show consis- tently negative correlations across all tasks, most pronounced with a correlation of -0.55 in travel planning, indicating that these traits detract from usability. The data illustrates that positive personal- ity traits are predictive of enhanced usability, while negative traits hinder it. This consists with research indicating the crucial role of personality design in shaping user experience among the selected tasks. Conversely, Table 7 reveals discrepancies be- tween self-reported personality scores and user ex- perience, characterized by low and inconsistent correlations. For instance, conscientiousness ex- hibits a modest positive correlation of 0.17 in the public service task, the highest observed among the traits. However, the overall pattern of weak and occasionally near zero correlations, such as -0.01 for openness in guided learning, indicates that self-reported traits are unreliable predictors of user experience. This disparity likely arises because self-report measures fail to capture the dynamic impacts of interaction and task setting, crucial in shaping user perceptions during real-time chatboti engagements. ",
                "In summary, self-report personality scales failed to correlate with interaction quality, which indi- cates a disconnect between the model’s response to personality items and how their behaviors manifest during real interaction. It highlights the complexity of user interaction and the challenges in evaluating personality design in LLM-based chatbots. As the ultimate goal of personality design is to improve in- teraction, such a validity link is crucially important. The validity issues of the “self-report” evaluation method may misguide chatbot development."
            ]
        },
        {
            "claim": "• We present a dataset containing a rich log of human interactions with 500 chatbots, each with distinct personality designs, along with human perceptions of their personalities, facil- itating the development of novel interaction- based personality evaluation methods.",
            "evidences": [
                "2.2.2 Human-perceived Personality We conducted a human study to gather participants’ perceptions of the chatbot’s personality. Each par- ticipant was randomly assigned to interact with one chatbot and engaged in multi-turn, text-based con- versations to complete the designed task. A user interface example is shown in Appendix C. After completing the task, participants were asked to rate their perceived personality of the chat- bot using the BFI-2-XS (Soto and John, 2017b). The BFI-2-XS consists of 15 items, each represent- ing a distinct facet of one of the Big Five person- ality domains, thus preserving the scale’s compre- hensive descriptive and predictive capabilities.",
                "2.3 Study Procedure To gather the human-perceived personality of each chatbot, we recruited English-speaking participants from Prolific1, with each participant assessing one chatbot. At the beginning of the survey, partici- pants were given the task’s objective, along with clear instructions on how to start the conversation, including an example to clarify how the participant might approach the task. A privacy reminder is included to ensure no private information is shared. After interacting with the chatbot, each participant filled out the chatbot personality evaluation ques- tionnaire and completed a separate User Experi- ence Questionnaire (UEQ) (Laugwitz et al., 2008). The entire process was designed to take 10 to 15 minutes. Appendix F presents detailed participant statistics and their demographic information."
            ]
        }
    ]
}