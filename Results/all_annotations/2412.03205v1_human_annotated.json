{
    "annotations": [
        {
            "claim": "we introduce U-MATH, a novel benchmark of 1,100 unpublished open-ended university-level problems sourced from teaching materials.",
            "evidences": [
                "To create a benchmark that authentically reflects university-level mathematics, we collaborate with Gradarius, a platform providing learning content and software for top US universities specialized in mathematics. The problems are sourced from ongoing courses across various institutions currently run on the Gradarius platform. Problems and solutions are crafted by subject matter experts and represent real-world academic standards. These samples are unpublished and have not been exposed to any external sources. Thus, the dataset could not be leaked to current LLMs. We employ a multi-stage filtering process to select challenging problems from tens of thousands of available samples. First, we filter out problems with short solutions (< 100 characters) and problems in multiple-choice format. As LLMs are not designed to perform arithmetic calculations and are prone to errors (Hendrycks et al., 2021; Lewkowycz et al., 2022), we focus on testing mathematical reasoning rather than calculations. We filter out problems marked as allowing calculator usage. As for the visual problems selection, we chose to keep problems with a single image for convenience.",
                "The U-MATH benchmark comprises 1,100 carefully curated and validated mathematical problems. These problems are distributed across 6 core subjects with about 20% of the tasks incorporating visual elements, such as graphs, tables, and geometric figures, mirroring the multi-modal nature of real- world mathematical problems: Precalculus (Review), Algebra, Differential Calculus (+Differential Equations), Integral Calculus, Multivariable Calculus, and Sequences & Series.",
                "Math Subject #Textual #Visual Avg. Questions Avg. Answers Algebra 150 30 1.93 1.28 Differential Calculus 150 70 2.37 1.15 Integral Calculus 150 58 1.09 1.01 Multivariable Calculus 150 28 1.74 1.09 Precalculus 150 10 1.51 1.23 Sequences and Series 150 4 1.36 1.00 All 900 200 1.66 1.12 Table 2: Average number of questions per problem and answers per question in U-MATH."
            ]
        },
        {
            "claim": "pi g g Our findings reveal that LLMs achieve a maximum accuracy of only 63% on text-based tasks, with even lower 45% on visual problems. The solution assessment proves challenging for LLMs, with the best LLM judge having an F1-score of 80% on µ-MATH",
            "evidences": [
                "Among text-only models, the math-specific model Qwen2.5-Math-72B achieves the highest overall accuracy at 50.2%, showcasing strong mathematical reasoning capabilities. In the multi-modal model group, Gemini-1.5-pro-002 leads with an overall accuracy of 60.1%, highlighting the advantages of integrating visual processing. In contrast, best open-weights model Qwen2-VL-72B lacks mathematical abilities in visual and textual tasks with 31.2% on a U-MATH benchmark. Building on these results, several key trends emerge: • Model Size vs. Specialization: Larger models expectedly outperform smaller ones. However, the small specialized model Qwen2.5-Math-7B surpasses or performs on par with 10 times larger models like Qwen2.5-72B or LLaMA-3.1-70B and almost reaching leading Gemeni-1.5-Pro level. On the other hand, Pixtral-12B performs consistently worse than minor Qwen2-VL-7B, indicating a lack of university-level data in training.",
                "U-MATH Algebra Diff. C. Integral C. Multivar C. Precalculus Seq.& Series Model U-MATH T V T V T V T V T V* T V* T V* 900 200 150 30 150 70 150 58 150 28 150 10 150 4 Text-only models Mathstral 7B 18.0 20.7 6.0 51.3 6.7 4.0 10.0 1.3 1.7 8.0 3.6 48.7 10.0 10.7 0.0 NuminaMath 7B 19.2 22.8 3.0 62.7 0.0 4.0 7.1 1.3 0.0 6.0 3.6 51.3 0.0 11.3 0.0 Llama-3.1 8B 22.3 26.1 5.0 59.3 3.3 6.7 5.7 9.3 3.4 11.3 3.6 54.7 10.0 15.3 25.0 Qwen2.5 7B 33.8 40.0 6.0 86.0 10.0 12.7 1.4 10.0 12.1 26.7 3.6 75.3 0.0 29.3 0.0 Qwen2.5-Math 7B 38.4 45.2 7.5 87.3 6.7 18.7 5.7 8.0 10.3 36.0 10.7 80.7 0.0 40.7 0.0 NuminaMath 72B 25.0 29.7 4.0 74.7 3.3 6.7 4.3 4.0 3.4 11.3 3.6 62.7 10.0 18.7 0.0 Llama-3.1 70B 28.5 33.7 5.0 82.0 3.3 10.7 5.7 4.0 5.2 14.0 3.6 64.0 0.0 27.3 25.0 Llama-3.1 Nemotron 70B 31.4 37.4 4.0 84.0 0.0 14.7 2.9 4.0 3.4 25.3 7.1 64.0 20.0 32.7 0.0 Qwen2.5 72B 41.0 48.6 7.0 88.7 6.7 22.7 4.3 12.0 6.9 40.0 17.9 83.3 0.0 44.7 0.0 Athene-V2 72B Chat 46.2 54.6 8.5 88.7 3.3 34.0 4.3 16.0 6.9 50.7 21.4 88.7 10.0 49.3 50.0 Qwen2.5-Math 72B 50.2 59.0 10.5 92.7 6.7 35.3 7.1 20.7 17.2 58.0 7.1 90.0 0.0 57.3 50.0 Multimodal models Pixtral 12B 15.5 15.6 15.5 44.7 23.3 1.3 34.3 0.7 0.0 3.3 0.0 32.0 0.0 11.3 0.0 Llama-3.2 11B Vision 17.0 18.6 10.0 54.0 10.0 1.3 20.0 1.3 1.7 4.7 3.6 43.3 10.0 6.7 0.0 LLaVA-OV Qwen2-7B 17.7 20.7 4.5 60.7 6.7 4.0 5.7 1.3 1.7 5.3 3.6 43.3 10.0 9.3 0.0 Qwen2-VL 7B 20.4 21.4 15.5 62.7 10.0 4.7 32.9 0.7 5.2 6.7 7.1 45.3 0.0 8.7 0.0 Qwen2-VL 72B 31.2 32.2 26.5 80.7 26.7 9.3 40.0 2.0 13.8 14.7 28.6 65.3 10.0 21.3 0.0 Llama-3.2 90B Vision 32.6 36.3 16.0 85.3 26.7 10.7 25.7 2.7 1.7 22.7 7.1 65.3 20.0 31.3 25.0 Claude Sonnet 3.5 35.1 36.1 30.5 76.0 33.3 12.0 41.4 7.3 17.2 21.3 28.6 65.3 30.0 34.7 25.0 GPT-4o-mini 37.2 40.3 23.0 88.0 16.7 16.7 31.4 4.0 10.3 24.0 35.7 77.3 20.0 32.0 25.0 GPT-4o 43.5 46.4 30.0 91.3 30.0 18.7 32.9 10.0 20.7 41.3 42.9 79.3 30.0 38.0 25.0 Gemini 1.5 Flash 51.3 53.8 40.0 91.3 50.0 36.0 45.7 14.0 24.1 44.0 50.0 80.7 30.0 56.7 50.0 Gemini 1.5 Pro 60.1 63.4 45.0 91.3 60.0 50.7 47.1 27.3 24.1 60.7 57.1 87.3 70.0 63.3 50.0",
                "µ-MATH µ-MATHQwen µ-MATHLlama µ-MATHGPT µ-MATHGemini Model U-MATHText F1CoT / F1AutoCoT TPR TNR PPV NPV F1CoT / F1AutoCoT F1CoT / F1AutoCoT F1CoT / F1AutoCoT F1CoT / F1AutoCoT Llama-3.1 8B 26.1 52.0 / 53.1 48.7 55.9 56.0 48.5 48.7 / 49.6 49.2 / 51.2 51.2 / 57.6 55.5 / 50.5 Qwen2.5 7B 40.0 69.3 / 67.0 78.7 59.8 69.3 70.8 62.4 / 60.5 72.3 / 72.4 68.3 / 66.4 69.1 / 65.0 Qwen2.5-Math 7B 45.2 61.9 / 61.2 76.6 47.9 62.9 63.9 59.7 / 56.7 63.8 / 64.0 57.2 / 58.5 63.8 / 61.2 GPT-4o-mini 40.3 72.3 / 69.2 59.0 88.1 85.1 65.1 69.3 / 61.7 76.2 / 78.5 70.4 / 69.8 69.6 / 64.3 Gemini 1.5 Flash 53.8 74.8 / 65.3 63.3 88.3 86.2 67.6 71.2 / 61.9 80.6 / 70.8 70.1 / 65.3 73.9 / 59.7 Llama-3.1-70B 33.7 61.0 / 68.2 62.5 59.6 64.1 57.9 56.0 / 63.8 57.0 / 70.2 69.4 / 69.8 58.8 / 64.4 Qwen2.5 72B 48.6 75.6 / 75.1 77.1 74.2 77.5 73.7 70.5 / 68.9 79.3 / 80.1 73.7 / 73.4 74.2 / 73.8 Qwen2.5-Math 72B 59.0 74.0 / 75.5 80.9 66.8 73.8 75.2 69.3 / 68.8 77.3 / 79.8 68.2 / 69.2 76.8 / 80.4 Claude 3.5 Sonnet 36.1 74.8 / 68.1 62.5 89.5 87.3 67.4 70.8 / 64.1 77.9 / 71.8 72.2 / 68.1 73.8 / 63.4 GPT-4o 46.4 77.4 / 74.2 70.1 85.9 85.1 71.3 74.2 / 68.2 81.8 / 78.9 77.5 / 75.8 72.6 / 70.5 Gemini 1.5 Pro 63.4 80.7 / 69.1 77.5 84.5 85.2 76.4 77.7 / 64.6 83.6 / 74.8 78.2 / 68.6 79.5 / 64.9 Table 5: Comparison of model’s ability to judge on µ-MATH benchmark using CoT prompting; Macro F1-score (F1), True Positive Rate (TPR), True Negative Rate (TNR), Positive Predictive Value (PPV) and Negative Predictive Value (NPV) are presented, with F1 as the primary one. The second number within each F1 column written in gray represents the F1-score under AutoCoT prompting. µ-MATH columns represent integral scores over the entire benchmark, while µ-MATH <model> columns denote subsets with solutions generated by specific author models. U-MATHText accuracy is added for comparison of each model’s performance as a math solver vs as a math judge. Bold indicates the best result in each column. Full expanded tables are presented in Appendix K."
            ]
        },
        {
            "claim": "1. U-MATH Benchmark (Section 3): We open-source a set of 1,100 of university-level problems collected from actual coursework with final answers and solutions. About 20% of problems require image understanding to be solved. The text-only part of the benchmark is balanced across 6 key subjects: Precalculus, Algebra, Differential Calculus, Integral Calculus, Multivariable Calculus, and Sequences&Series.",
            "evidences": [
                "b The U-MATH benchmark comprises 1,100 carefully curated and validated mathematical problems. These problems are distributed across 6 core subjects with about 20% of the tasks incorporating visual elements, such as graphs, tables, and geometric figures, mirroring the multi-modal nature of real- world mathematical problems: Precalculus (Review), Algebra, Differential Calculus (+Differential Equations), Integral Calculus, Multivariable Calculus, and Sequences & Series."
            ]
        },
        {
            "claim": "2. µ-MATH Meta-Evaluation Benchmark (Section 3.3): Additionally, we introduce a set of 1084 meta-evaluation tasks sourced from U-MATH problems and designed to rigorously assess the quality of LLM judges. We manually select approximately 25% of the U-MATH problem statements and golden answers, supplying each with four solutions produced by different top-performing LLMs, and label them based on whether the generated solutions are correct or not. The benchmark is designed to be challenging for LLM judges yet representative of the typical university-level math grading tasks.",
            "evidences": [
                "q p We selected 271 U-MATH problems (around 25%) based on their assessment difficulty to create a challenging meta-evaluation set. This subset does not aim to reflect the overall U-MATH distribution but rather to provide a robust test for LLM judges. We focused on text-only problems, excluding those needing images, due to the limited size of the labeled U-MATH subset. Four solutions have been generated for each of the selected problems — using Qwen2.5-72B, Llama3.1-8B, GPT-4o and Gemini-1.5-Pro models — 1084 samples in total."
            ]
        },
        {
            "claim": "3. Comparison of Models (Section 4): We conduct a comparative analysis of various open- source and proprietary LLMs on U-MATH. Our analysis highlights the high performance of specialized models in text-only problems and the superiority of proprietary models in visual tasks with the best U-MATH accuracy of 49%. Additionally, we examine several popular LLMs on µ-MATH to assess their ability to judge free-form mathematical problems. Our results show the best model achieving the macro F1-score of 80%.",
            "evidences": [
                "Among text-only models, the math-specific model Qwen2.5-Math-72B achieves the highest overall accuracy at 50.2%, showcasing strong mathematical reasoning capabilities. In the multi-modal model group, Gemini-1.5-pro-002 leads with an overall accuracy of 60.1%, highlighting the advantages of integrating visual processing. In contrast, best open-weights model Qwen2-VL-72B lacks mathematical abilities in visual and textual tasks with 31.2% on a U-MATH benchmark. Building on these results, several key trends emerge: • Model Size vs. Specialization: Larger models expectedly outperform smaller ones. However, the small specialized model Qwen2.5-Math-7B surpasses or performs on par with 10 times larger models like Qwen2.5-72B or LLaMA-3.1-70B and almost reaching leading Gemeni-1.5-Pro level. On the other hand, Pixtral-12B performs consistently worse than minor Qwen2-VL-7B, indicating a lack of university-level data in training.",
                "U-MATH Algebra Diff. C. Integral C. Multivar C. Precalculus Seq.& Series Model U-MATH T V T V T V T V T V* T V* T V* 900 200 150 30 150 70 150 58 150 28 150 10 150 4 Text-only models Mathstral 7B 18.0 20.7 6.0 51.3 6.7 4.0 10.0 1.3 1.7 8.0 3.6 48.7 10.0 10.7 0.0 NuminaMath 7B 19.2 22.8 3.0 62.7 0.0 4.0 7.1 1.3 0.0 6.0 3.6 51.3 0.0 11.3 0.0 Llama-3.1 8B 22.3 26.1 5.0 59.3 3.3 6.7 5.7 9.3 3.4 11.3 3.6 54.7 10.0 15.3 25.0 Qwen2.5 7B 33.8 40.0 6.0 86.0 10.0 12.7 1.4 10.0 12.1 26.7 3.6 75.3 0.0 29.3 0.0 Qwen2.5-Math 7B 38.4 45.2 7.5 87.3 6.7 18.7 5.7 8.0 10.3 36.0 10.7 80.7 0.0 40.7 0.0 NuminaMath 72B 25.0 29.7 4.0 74.7 3.3 6.7 4.3 4.0 3.4 11.3 3.6 62.7 10.0 18.7 0.0 Llama-3.1 70B 28.5 33.7 5.0 82.0 3.3 10.7 5.7 4.0 5.2 14.0 3.6 64.0 0.0 27.3 25.0 Llama-3.1 Nemotron 70B 31.4 37.4 4.0 84.0 0.0 14.7 2.9 4.0 3.4 25.3 7.1 64.0 20.0 32.7 0.0 Qwen2.5 72B 41.0 48.6 7.0 88.7 6.7 22.7 4.3 12.0 6.9 40.0 17.9 83.3 0.0 44.7 0.0 Athene-V2 72B Chat 46.2 54.6 8.5 88.7 3.3 34.0 4.3 16.0 6.9 50.7 21.4 88.7 10.0 49.3 50.0 Qwen2.5-Math 72B 50.2 59.0 10.5 92.7 6.7 35.3 7.1 20.7 17.2 58.0 7.1 90.0 0.0 57.3 50.0 Multimodal models Pixtral 12B 15.5 15.6 15.5 44.7 23.3 1.3 34.3 0.7 0.0 3.3 0.0 32.0 0.0 11.3 0.0 Llama-3.2 11B Vision 17.0 18.6 10.0 54.0 10.0 1.3 20.0 1.3 1.7 4.7 3.6 43.3 10.0 6.7 0.0 LLaVA-OV Qwen2-7B 17.7 20.7 4.5 60.7 6.7 4.0 5.7 1.3 1.7 5.3 3.6 43.3 10.0 9.3 0.0 Qwen2-VL 7B 20.4 21.4 15.5 62.7 10.0 4.7 32.9 0.7 5.2 6.7 7.1 45.3 0.0 8.7 0.0 Qwen2-VL 72B 31.2 32.2 26.5 80.7 26.7 9.3 40.0 2.0 13.8 14.7 28.6 65.3 10.0 21.3 0.0 Llama-3.2 90B Vision 32.6 36.3 16.0 85.3 26.7 10.7 25.7 2.7 1.7 22.7 7.1 65.3 20.0 31.3 25.0 Claude Sonnet 3.5 35.1 36.1 30.5 76.0 33.3 12.0 41.4 7.3 17.2 21.3 28.6 65.3 30.0 34.7 25.0 GPT-4o-mini 37.2 40.3 23.0 88.0 16.7 16.7 31.4 4.0 10.3 24.0 35.7 77.3 20.0 32.0 25.0 GPT-4o 43.5 46.4 30.0 91.3 30.0 18.7 32.9 10.0 20.7 41.3 42.9 79.3 30.0 38.0 25.0 Gemini 1.5 Flash 51.3 53.8 40.0 91.3 50.0 36.0 45.7 14.0 24.1 44.0 50.0 80.7 30.0 56.7 50.0 Gemini 1.5 Pro 60.1 63.4 45.0 91.3 60.0 50.7 47.1 27.3 24.1 60.7 57.1 87.3 70.0 63.3 50.0",
                "• Proprietary vs. Open-weights model: Proprietary models like Gemini still offer top or competi- tive performance but lack transparency and flexibility. At the moment, the gap is evident in visual comprehension, with 18.5% difference on U-MATHVisual between top-1 and best open-weight model. However, open-weight models like Qwen-Math is a big step toward top performance. • Continuous Finetuning: Additional tuning significantly enhances performance, with LLaMA-3.1 70B ⇒LLaMA-3.1 Nemotron 70B and Qwen2.5-72B ⇒Athene-V2 72B achieving 2.9% and 5.2% higher U-MATH accuracy, respectively. This reinforces the idea that models are not fully optimized for their size and require high-quality data for further improvements.",
                "µ-MATH µ-MATHQwen µ-MATHLlama µ-MATHGPT µ-MATHGemini Model U-MATHText F1CoT / F1AutoCoT TPR TNR PPV NPV F1CoT / F1AutoCoT F1CoT / F1AutoCoT F1CoT / F1AutoCoT F1CoT / F1AutoCoT Llama-3.1 8B 26.1 52.0 / 53.1 48.7 55.9 56.0 48.5 48.7 / 49.6 49.2 / 51.2 51.2 / 57.6 55.5 / 50.5 Qwen2.5 7B 40.0 69.3 / 67.0 78.7 59.8 69.3 70.8 62.4 / 60.5 72.3 / 72.4 68.3 / 66.4 69.1 / 65.0 Qwen2.5-Math 7B 45.2 61.9 / 61.2 76.6 47.9 62.9 63.9 59.7 / 56.7 63.8 / 64.0 57.2 / 58.5 63.8 / 61.2 GPT-4o-mini 40.3 72.3 / 69.2 59.0 88.1 85.1 65.1 69.3 / 61.7 76.2 / 78.5 70.4 / 69.8 69.6 / 64.3 Gemini 1.5 Flash 53.8 74.8 / 65.3 63.3 88.3 86.2 67.6 71.2 / 61.9 80.6 / 70.8 70.1 / 65.3 73.9 / 59.7 Llama-3.1-70B 33.7 61.0 / 68.2 62.5 59.6 64.1 57.9 56.0 / 63.8 57.0 / 70.2 69.4 / 69.8 58.8 / 64.4 Qwen2.5 72B 48.6 75.6 / 75.1 77.1 74.2 77.5 73.7 70.5 / 68.9 79.3 / 80.1 73.7 / 73.4 74.2 / 73.8 Qwen2.5-Math 72B 59.0 74.0 / 75.5 80.9 66.8 73.8 75.2 69.3 / 68.8 77.3 / 79.8 68.2 / 69.2 76.8 / 80.4 Claude 3.5 Sonnet 36.1 74.8 / 68.1 62.5 89.5 87.3 67.4 70.8 / 64.1 77.9 / 71.8 72.2 / 68.1 73.8 / 63.4 GPT-4o 46.4 77.4 / 74.2 70.1 85.9 85.1 71.3 74.2 / 68.2 81.8 / 78.9 77.5 / 75.8 72.6 / 70.5 Gemini 1.5 Pro 63.4 80.7 / 69.1 77.5 84.5 85.2 76.4 77.7 / 64.6 83.6 / 74.8 78.2 / 68.6 79.5 / 64.9 Table 5: Comparison of model’s ability to judge on µ-MATH benchmark using CoT prompting; Macro F1-score (F1), True Positive Rate (TPR), True Negative Rate (TNR), Positive Predictive Value (PPV) and Negative Predictive Value (NPV) are presented, with F1 as the primary one. The second number within each F1 column written in gray represents the F1-score under AutoCoT prompting. µ-MATH columns represent integral scores over the entire benchmark, while µ-MATH <model> columns denote subsets with solutions generated by specific author models. U-MATHText accuracy is added for comparison of each model’s performance as a math solver vs as a math judge. Bold indicates the best result in each column. Full expanded tables are presented in Appendix K."
            ]
        }
    ]
}