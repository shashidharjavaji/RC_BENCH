{
    "annotations": [
        {
            "claim": "Experiments show a consistent disparity in performance between the two languages, with Yorùbá falling behind English for automatic metrics even if documents are much shorter for this language.",
            "evidences": [
                "LAN R-1 R-2 R-L GPT4O ENG 0.39 0.23 0.30 YOR 0.34 0.19 0.27 O1MINI ENG 0.45 0.22 0.30 YOR 0.30 0.14 0.22 LLAMA ENG 0.31 0.18 0.23 YOR 0.20 0.15 0.18 Table 4 Results for 3 LLM in terms of Rouge computed for the entire set of questions. Human Score is computed on 358 questions.",
                "Automatic metrics. Table 4 reports the results showing that Yorùbá consistently performs worse than English (e.g., losing 0.4 in Rouge-1). However, the Yorùbá task is much easier because the documents are much shorter, which means that answering the question becomes an easier task. Even if we prompt the model to only answer based on the in-context document, we can not discard the idea that English may get better results due to using the internal knowledge from the model."
            ]
        },
        {
            "claim": " For a small set of documents with comparable length, performance of Yorùbá drops by x2.5 times.",
            "evidences": [
                "AVG W. R-1 R-2 R-L ENG 3299 0.45 0.23 0.30 YOR 3070 0.32 0.09 0.19 Table 5 Results for six comparable English and Yorùbá documents",
                "when the Yorùbá documents reach 1,500 words, which shows the challenges that current models face in long-context understanding of low-resource languages. For a small portion of long-enough documents of comparable length between English and Yorùbá (only 4 documents that are over 900 words long), English performance demonstrates a significant edge (1.58X-2.56X), see Table 5."
            ]
        },
        {
            "claim": "When analyzing performance by length, we observe that Yorùbá decreases performance dramatically for documents that reach 1500 words while English performance is barely affected at that length.",
            "evidences": [
                "when the Yorùbá documents reach 1,500 words, which shows the challenges that current models face in long-context understanding of low-resource languages. For a small portion of long-enough documents of comparable length between English and Yorùbá (only 4 documents that are over 900 words long), English performance demonstrates a significant edge (1.58X-2.56X), see Table 5. AVG W. R-1 R-2 R-L ENG 3299 0.45 0.23 0.30 YOR 3070 0.32 0.09 0.19 Table 5 Results for six comparable English and Yorùbá documents"
            ]
        },
        {
            "claim": "Our data set is benchmarked against state-of-the-art Large Language Models (LLMs).",
            "evidences": [
                "Baselines We evaluate our dataset with GPT-4o3 (et al., 2024b), o1-mini4, and LlaMA-3.1-8b (et al., 2024a), therevy covering both open and closed models, as well as models of different sizes. For each Y-NQ entry, we prompt the models with the following formatted instructions."
            ]
        }
    ]
}