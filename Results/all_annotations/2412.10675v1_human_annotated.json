{
    "annotations": [
        {
            "claim": "We find that merely fine-tuning LLMs on a corpus of planning in- stances does not lead to robust planning skills, as indicated by poor performance on out-of-distribution test sets. ",
            "evidences": [
                "Table 1: Performance of the fine-tuned LLM across various test sets with no additional strategies applied. Although the LLM performs well on in-distribution, it struggles to gener- alize to OOD cases. In-Distrib. Long Domain valid. rate exec. rate valid. rate exec. rate BLOCKSWORLD 98.5% 98.5% 13.5% 23.5% LOGISTICS 100% 100% 14.0% 20.5% BARMAN 100% 100% 25.0% 43.5% CHILDSNACK 100% 100% 66.0% 67.0% DEPOTS 98.5% 98.5% 31.0% 37.0% DRIVERLOG 100% 100% 31.0% 50.7% GRIPPERS 99.0% 99.0% 50.5% 76.0% SATELLITE 99.0% 99.2% 51.5% 53.0% Unseen Domain valid. rate exec. rate HANOI 0% 35% STORAGE 0% 1% Obfuscated Domain valid. rate exec. rate BLOCKSWORLD 0% 0% LOGISTICS 0% 0%"
            ]
        },
        {
            "claim": "At the same time, we find that various strategies, including chain- of-thought, do enhance the probability of a plan being exe- cutable.",
            "evidences": [
                "Table 2: Ablation Study on Strategy Effectiveness in Planning. Validity rates (valid.) and the Executability rate (exec.) are analyzed. Strategies such as Permutation, CoT, and Self-Correct show no significant validity. improvements but enhance executability in ‘long’ and other OOD scenarios. Notably, ‘Goal CoT’ appears to hinder performance. We attribute this to the dual duties of generating plans and accurately estimating the heuristics of the state, which increases overall complexity and hinders the model’s ability to effectively learn both aspects. RL emerges as the only strategy that enhances validity in OOD scenarios. Improvements of statistical sig- nificance are highlighted in green, while significant declines are highlighted in red. Strategies In-Distrib. Long Unseen Obfuscated Label Goal State Self Perm. RL valid. exec. valid. exec. valid. exec. valid. exec. CoT CoT Correct 1 99.3% 99.8% 34.8% 42.3% 0% 20.1% 0% 0% 2 ✓ 99.5% 99.8% 35.0% 48.3% 0% 75.5% 0% 0% 3 ✓ ✓ 96.8% 98.5% 12.1% 18.7% 5.5% 53.4% 0% 0% 4 ✓ ✓ 98.9% 99.5% 29.5% 43.0% 0% 100% 0% 0% 5 ✓ ✓ ✓ 98.7% 99.0% 23.8% 39.3% 0% 90.8% 0% 0% 6 ✓ ✓ 99.7% 99.9% 32.6% 50.6% 0% 70.9% 0% 0% 7 ✓ ✓ ✓ 97.3% 98.6% 14.9% 25.6% 0% 38.7% 0% 0% 8 ✓ ✓ ✓ 98.1% 99.3% 27.5% 49.4% 0% 94.5% 0% 0% 9 ✓ ✓ ✓ ✓ 98.3% 99.1% 25.9% 30.4% 0% 90.6% 0% 0% 10⋆ ✓ 99.2% 99.6% 41.5% 51.3% 12.5% 23.0% 0% 0% 11⋆ ✓ ✓ 99.7% 100% 36.3% 53.6% 0% 71.5% 0% 0%",
                "4.5 State CoT Boosts Executability with a Caveat: Efficacy Limited to Short Problems We observed that State CoT does not improve plan exe- cutability within the ‘long’ test set, yet it significantly en- hances performance within the ‘unseen’ test set (e.g., 100% in row 4). Importantly, the ‘unseen’ test set retains the same plan length distribution as the training set. Thus, we posit that the State CoT’s ability to enhance the model’s under- standing of state transition dynamics may likely be limited to the plan length distribution it encountered during train- ing. Consequently, we do not observe an improvement in the ‘long’ test set. This also rationalizes why the State CoT demonstrates efficacy in other reasoning tasks (Wei et al. 2022; Yao et al. 2024), where these tasks typically require solution steps that align with the training data distribution. We shall verify this hypothesis in the next section through a ‘plan continuation’ experiment.",
                "provided, predicting the world state remains challenging. Interestingly, the model employing CoT (Goal + State) demonstrates the highest performance gain when provided with the hints. Its validity rate improves dramatically from the lowest (23.8%) to the highest (54.2%) among the com- pared strategies. This finding suggests that when CoT op- erates within its “comfort zone” (i.e., in-distribution sce- narios), it begins to show its effectiveness in enhancing the model’s planning, supporting the hypothesis presented in § 4.5. While this performance boost is encouraging, it also highlights a limitation: CoT appears to be overfit to in- distribution inference. This aligns with our earlier observa- tion that the model faces difficulty estimating the goal dis- tance that is not within the training distribution."
            ]
        },
        {
            "claim": "Among the strategies we evaluated, reinforcement learning with our novel ‘Longest Contiguous Common Subsequence’ reward emerged as the most effective, contributing to both plan ex- ecutability and validity. Overall",
            "evidences": [
                "4.7 RL Enhances Model Performance RL notably improves the performance under our end-to-end planning paradigm, especially on longer problems. Note that the model was trained on 10% of the ‘long’ test set with the proposed LCCS-based reward model, and evaluated on the 90% of the ‘long’ test set and other OOD test sets. Despite the limited training data and suboptimal rewards achieved on this subset, RL boosted the validity rate on the ‘long’ test set from 34.8% to 41.5% (a 6.7% increase) and",
                "the executability rate from 42.3% to 53.6% (9.0%) (see Ta- ble 2, row 10). Interestingly, it also enabled the model to solve problems in the ‘unseen’ test set, achieving a 12.5% where it previously failed to generate any valid plans. The updated model does not exhibit overfitting, as indicated by the LCCS reward signal not reaching a perfect score of 1.0. Instead, the model has developed general planning strategies effective in unseen scenarios. To confirm that this improve- ment is not due to the additional training data, we also con- ducted supervised fine-tuning as described in Section 4.1 us- ing the same training data. However, the outcomes were not as promising as those achieved with RL, as demonstrated in Figure 6. These results suggest that RL fosters more compre- hensive planning skills compared to supervised fine-tuning (SFT), aligning with the findings of Liu et al. (2024). Applying RL to the vanilla model led to faster conver- gence and improved results compared to its application to the model with self-correction skills, as illustrated in Fig- ure 8 and row 10 and 11 in Table 2. We hypothesize that the self-correction strategy, by permitting repeated attempts at actions, effectively broadens the model’s state space and thus poses a greater challenge to explore a valid solution. 5 Di i"
            ]
        }
    ]
}