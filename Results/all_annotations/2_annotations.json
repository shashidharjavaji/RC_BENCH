[
    {
        "claim_id": 1,
        "claim_text": "We show that when we use a format with visible lettered answer options, large models are very well calibrated on diverse multiple choice questions",
        "evidence_text": "We show the calibration chart for all multiple choice BIG Bench tasks, in this format, in Figure 4. As can be seen in Figure 6, models are well-calibrated (in this format) even for somewhat adversarial datasets like TruthfulQA3. It is crucial that the model gets to see the answer choices explicitly before choosing amongst them; without this, we would not expect a calibrated response, due to ambiguities and degeneracies among possible paraphrases and specializations of any given answer option. As can be seen in figure 5, task formatting is important for achieving excellent calibration, and calibration improves as we pass from 0-shot to 5-shot evaluation.",
        "justification_conclusion": "With the visible lettered answer options, large models are very well calibrated on diverse multiple choice questions."
    },
    {
        "claim_id": 2,
        "claim_text": "Replacing an option with ‘none of the above’ reduces accuracy and calibration significantly with our models.",
        "evidence_text": "We are more interested in whether the model actually knows whether each of the answer options is correct, when judged independently. To probe this question, we modified our multiple choice evaluations by replacing their final option with “none of the above”. This procedure can make questions that do not actually have a unique correct answer ambiguous or impossible, but for many tasks it should result in a well-defined new evaluation. In particular this procedure appears to be sensible for the vast majority of questions in MMLU. Concretely this means that we took questions such as the example in section 2 and replaced them with: Question: Who was the first president of the United States? Choices: (A) Barack Obama (B) George Washington (C) none of the above Answer: We found that this procedure degraded performance very significantly on evaluations; results for MMLU are shown in Figure 36 in the appendix. Furthermore, adding 'none of the above' also harms calibration, as can be seen in Figures 5 and 7. It seems that even the 52B model is biased against using the “none of the above” option and failed to use it with appropriate frequency. This is particularly surprising for 5-shot evaluations; we also tried evaluating 20-shot and this also did not improve performance.",
        "justification_conclusion": "Replacing an option with ‘none of the above’ reduces accuracy and calibration significantly."
    },
    {
        "claim_id": 3,
        "claim_text": "Models can self-evaluate whether their own samples are True or False, though this tends to be a more challenging task (since models tend to find their own samples more plausible).",
        "evidence_text": "As a first test of this approach, we can use answer options from existing multiple choice tasks. For this purpose, we take the correct answer and a randomly chosen incorrect answer, and create a new evaluation set with twice as many problems in the format above, asking models to determine if each answer is correct. In Figure 8 we show the calibration results from this method on BIG Bench. We see that the 52B model is quite well-calibrated in this context. ",
        "justification_conclusion": "From the results, we can find that the models are well calibrated on True/False distinctions."
    },
    {
        "claim_id": 4,
        "claim_text": "Models can self-evaluate whether their own samples are True or False, though this tends to be a more challenging task.",
        "evidence_text": "In almost all cases self-evaluation performance improves with model size, and for our 52B models answers labeled with P(True) > 50% are far more likely to be correct as compared to generic responses.",
        "justification_conclusion": "The models are capable of whether they know the answer to the questions compared to generic responses."
    },
    {
        "claim_id": 5,
        "claim_text": "We train models with a value head to predict the probability that they can answer a question correctly, which we refer to as P(IK). We find that models trained on TriviaQA have significant power to differentiate between math, lambada, and code questions that they can answer;",
        "evidence_text": "Because we’re adding a new untrained head, language models do not perform well zero or few-shot at predicting P(IK), so we need to finetune them. Since TriviaQA has a large training set, we explore generalization by finetuning P(IK) only on trivia questions. In order to obtain ground-truth P(IK) scores that we use for training, we sample 30 answers (at temperature = 1) from each TriviaQA question with a 10-shot prompt constructed from other random TriviaQA questions and answers. We use a 10-shot prompt simply to ensure that the models almost always output answers in the correct format, which is important because correctness on TriviaQA questions is judged based on a string comparison.  Our dataset for training and evaluating the classifier then contains datapoints of the form (Few-Shot Prompt + Question, Ground Truth Label). During P(IK) training, we finetune the entire model along with the value head. In Figure 12, we then evaluate this classifier on a held-out set of TriviaQA test questions, and see that the model is able to separate the questions it got correct and incorrect quite well. In particular, P(IK) is very well calibrated on TriviaQA. As a note, in a later section we train P(IK) on Lambada, Arithmetic, and Python Function Synthesis problems as well. We used a 10-shot prompt for both Lambada and Arithmetic, while Python Function Synthesis was done 0-shot; Figure 14 gives an overview of generalization performance for P(IK) classifiers that are only trained on TriviaQA. Specifically, we see that generalization gets better as model size increases. Figures 39, 40, and 41 show details of generalization to Mixed-Arithmetic, Python Function Synthesis, and Lambada. We see that there is a general trend where the AUROC of P(IK) increases with model size, and calibration gets better with model size. However, when testing on Lambada, calibration was terrible, because the model produces uniformly low P(IK) scores. However, training on all 4 tasks resolves this issue, as shown on the right side of Figure 41.  Table 1 gives an overview comparing generalization to in-distribution performance of P(IK) scores. Figure 16 gives a more detailed view of how the distributions of P(IK) changes depending on training data on the 52B model. We see that training on specific P(IK) distributions helps performance, indicating that there is a significant generalization gap to fill.",
        "justification_conclusion": "The evidence shows that the ability to predict the probability of answering a question correctly can be generalized across datasets."
    }
]