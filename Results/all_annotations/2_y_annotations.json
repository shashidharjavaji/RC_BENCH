[
    
    {
        "claim_id": 1,
        "claim_text": "DynMM strikes a good balance between computational efficiency and learning performance. For instance, for RGB-D semantic segmentation tasks, DynMM achieves a +0.7% mIoU improvement with over 21% reductions in multiply-add operations (MAdds) for the depth  encoder when compared against [35].",
        "evidence_text": "We provide three DynMM networks trained with different . Compared with the best performing static network (i.e., Late Fusion), DynMM-a can reduce computations by 46.5% with a slightly decreased accuracy (i.e., -0.47%). By allowing more computation, DynMM-b improves both inference efficiency (i.e., reduce MAdds by 17.8%) and prediction accuracy. Finally, DynMM-c further improves the accuracy by trading off some computation; it achieves best accuracy and smallest mean absolute error with reduced computation cost. These results demonstrate the great advantages of dynamic multimodal fusion. Since multimodal data naturally brings redundancy, we observe that many computations can be reduced without loss in accuracy.",
        "justification_conclusion": "True. The claim is supported by the evidence. The evidence records the performance of three DynMM networks trained with different . The results show that DynMM networks can reduce computations while maintaining accuracy. The evidence also explains the reason why DynMM networks can reduce computations without loss in accuracy."
    },
    {
        "claim_id": 2,
        "claim_text": "we find that DynMM yields better predictions than static fusion networks when the input modality is perturbed by noise; this suggests possible use of DynMM to improve the multimodal robustness.",
        "evidence_text": "Finally, we conduct experiments to demonstrate the improved robustness of DynMM compared to ESANet. We consider three settings by injecting random Gaussian noise with probability 1/3 to (1) RGB modality; (2) depth modality and (3) both modalities. We experiment with different degrees of random Gaussian noise and plot the performance degradation of two approaches in Figure 6. From the figure, we observe that the performance gap between DynMM and ESANet becomes larger when the noise level of depth images increases; This demonstrates another advantage of DynMM in reducing data noise and improving robustness. Figure 7 shows some qualitative segmentation results. While ESANet generates reasonable predictions in the normal setting (i.e., first and third row), its performance becomes significantly worse when multimodal data is perturbed by noise (i.e., the second and fourth row). On the contrary, our DynMM is robust to noise and provides a good prediction for both scenarios. These results suggest the potential of a dynamic neural network architecture for improving robustness of multimodal fusion.",
        "justification_conclusion": "True. The claim is supported by the evidence. The evidence records the performance of DynMM and ESANet under different noise settings. The results show that DynMM outperforms ESANet when the noise level of depth images increases. The evidence also provides qualitative segmentation results to demonstrate the robustness of DynMM to noise."
    }
]