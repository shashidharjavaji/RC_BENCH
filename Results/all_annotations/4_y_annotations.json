[
    
    {
        "claim_id": 1,
        "claim_text": "our key insight is that off-the-shelf unimodal encoders that have been pre-trained on large amounts of unimodal data already encode rich semantics that should provide an effective bootstrap for multimodal fusion.",
        "evidence_text": "",
        "justification_conclusion": "False. No direct evidence is provided to support the claim. "
    },
    {
        "claim_id": 2,
        "claim_text": "We show that by aligning the latent spaces of existing pre-trained unimodal encoders using FuseMix, we obtain highly competitive fused multimodal models, which in certain cases even outperform state-of-the-art methods in both image-text and audio-text retrieval tasks, all while using orders of magnitude less compute and data.",
        "evidence_text": "For image-text retrieval, we highlight that our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion. Moreover, we find that the combination of two of the most recent models, DINOv2+BGE, achieves the highest performance, highlighting the benefits of a plug-and-play approach that can leverage the most recent advancements. We also note that when our method and CLIP [91] are both only trained on pairs from Conceptual Captions 3M, we outperform CLIP by a notable margin, demonstrating that FuseMix is an effective strategy for fusion on lower data regimes. Similarly, for audio-text retrieval we outperform all other methods trained on similar data, and can compete with methods that use orders of magnitude more paired data.",
        "justification_conclusion": "True. The claim is supported by the evidence. The evidence records the performance of FuseMix on image-text and audio-text retrieval tasks. The results show that FuseMix is highly competitive and sometimes able to outperform various state-of-the-art methods. The evidence also shows that FuseMix can achieve the highest performance by combining two of the most recent models, DINOv2+BGE. The results also demonstrate that FuseMix is an effective strategy for fusion on lower data regimes."
    },
    {
        "claim_id": 3,
        "claim_text": "we further demonstrate the applicability of our FuseMix fusion framework for audio-to-image generation",
        "evidence_text": "While we omit quantitative analysis for this task due to a lack of suitable metrics, we provide a qualitative comparison of each sample with a corresponding sample generated from the original text-conditioned GLIDE using a text prompt that is semantically equivalent to the audio input. For example, for the sound of a cat meowing, we compare with the text prompt of “a photo of a cat meowing”. We find it noteworthy that conditioning GLIDE on audio inputs using FuseMix can produce samples of similar quality and fidelity as conditioning on text inputs, even though GLIDE itself was never trained with audio data.",
        "justification_conclusion": "Natural."
    }
   
]