[
    
    {
        "claim_id": 1,
        "claim_text": "We thoroughly evaluate the main models for tabular DL on a diverse set of tasks to investigate their relative performance.",
        "evidence_text": "We use a diverse set of eleven public datasets (see supplementary for the detailed description). For each dataset, there is exactly one train-validation-test split, so all algorithms use the same splits. ",
        "justification_conclusion": "True. The claim is supported by the evidence. The experiment setup section recorded that the main models are evaluated on eleven public datasets."
    },
    {
        "claim_id": 2,
        "claim_text": "a simple ResNet-like architecture is an effective baseline for tabular DL, which was overlooked by existing literature.",
        "evidence_text": "ResNet turns out to be an effective baseline that none of the competitors can consistently outperform. Tuning makes simple models such as MLP and ResNet competitive, so we recommend tuning baselines when possible.",
        "justification_conclusion": "True. The claim is supported by the evidence. The results show that ResNet is an effective baseline that none of the competitors can consistently outperform."
    },
    {
        "claim_id": 3,
        "claim_text": "We reveal that there is still no universally superior solution among GBDT and deep models.",
        "evidence_text": "In those cases, the gaps are significant enough to conclude that DL models do not universally outperform GBDT. Importantly, the fact that DL models outperform GBDT on most of the tasks does not mean that DL solutions are “better” in any sense. In fact, it only means that the constructed benchmark is slightly biased towards “DL-friendly” problems. Admittedly, GBDT remains an unsuitable solution to multiclass problems with a large number of classes. Depending on the number of classes, GBDT can demonstrate unsatisfactory performance (Helena) or even be untunable due to extremely slow training (ALOI).",
        "justification_conclusion": "True. The claim is supported by the evidence. The results show that there is still no universally superior solution among GBDT and deep models."
    }
   
]