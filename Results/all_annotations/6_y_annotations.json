[
    {
        "claim_id": 1,
        "claim_text": "We propose a novel and effective visual grounding framework by combining early joint representation and deep cross-modal interaction.",
        "evidence_text": "The framework of the proposed JMRI is shown in Fig. 2. It combines early joint representation using a multimodal embedding space and deep cross-modal interaction via attention mechanisms to achieve accurate reasoning and localization.",
        "justification_conclusion": "True. This claim is supported by detailed descriptions of the architecture, including experimental results that demonstrate improved accuracy compared to state-of-the-art methods."
    },
    {
        "claim_id": 2,
        "claim_text": "We propose to use CLIP to extract and align visual and linguistic features, ensuring that the resulting features are in the same semantic space, which is beneficial for the subsequent cross-modal fusion.",
        "evidence_text": "CLIP jointly trains the dual-encoder and projection layer to maximize the cosine similarity between the embeddings of the N real pairs while minimizing the cosine similarity between the embeddings of the N² − N incorrect pairs. Consequently, the early fusion between vision and language is achieved by the dot product in a learned contrastive embedding space.",
        "justification_conclusion": "True. The alignment of multimodal features in a common semantic space enhances the accuracy of grounding, as evidenced by higher Top-1 accuracy in experiments."
    },
    {
        "claim_id": 3,
        "claim_text": "By freezing the pretrained CLIP and updating the other modules, we achieve the best performance with the least training budget and deployment cost.",
        "evidence_text": "We utilize the pretrained CLIP model to initialize the early joint representation module and freeze its parameters during training, and the other modules are optimized with AdamW optimizer. The initial learning rate is set to 10^-4.",
        "justification_conclusion": "True. This approach allows for significant savings in training cost while still delivering competitive performance on multiple benchmarks."
    },
    {
        "claim_id": 4,
        "claim_text": "The experimental results prove that the cross-modal interaction plays a more critical role than the IMI for grounding, and also demonstrate the necessity of combining early alignment and deep fusion.",
        "evidence_text": "Using only IMI improves the performance from 82.09% to 83.41%, while using only CMI improves performance from 82.09% to 85.95%. The combination of early alignment and deep fusion achieves the highest accuracy.",
        "justification_conclusion": "True. Ablation studies demonstrate the key contribution of cross-modal interaction to grounding performance."
    },
    {
        "claim_id": 5,
        "claim_text": "The results show that the proposed model can perform zero-shot grounding on certain new visual concepts in the open world.",
        "evidence_text": "The results demonstrate zero-shot grounding capabilities on new visual concepts such as 'Sun Wukong,' 'white dragon,' and 'mountain wall.'",
        "justification_conclusion": "True. These results showcase the flexibility of the JMRI framework, attributed to the pretrained CLIP model's robust generalization."
    }
]