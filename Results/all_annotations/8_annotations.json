[
    {
        "claim_id": 1,
        "claim_text": "We introduce REPLUG (§3), the first retrieval augmented language modeling framework for enhancing black-box LMs with retrieval. Unlike previous methods that require updating the LM’s parameters, REPLUG could be easily plugged into any existing LM without additional finetuning.",
        "evidence_text": "We introduce REPLUG (Retrieve and Plug), a new  retrieval-augmented LM paradigm where the LM is treated as black box and the retrieval component is added as a potentially tuneable module.  As shown in Figure 2, given an input context, REPLUG first retrieves a small set of relevant documents from an external corpus using a retriever (§3.1). Then we pass the concatenation of each retrieved document with the input context through the LM in parallel, and ensemble the predicted probabilities.",
        "justification_conclusion": "The evidence text gives a overview of the underlying architecture of the proposed framework. Also, notice the a black-box LM is acceptable, so the framework can be easily plugged into existing LM."
    },
    {
        "claim_id": 2,
        "claim_text": "We propose a training scheme (§4) to further adapt an off-the-shelf retrieval model to the LM, using the language modeling scores as supervision signals, resulting in improved retrieval quality.",
        "evidence_text": "Inspired by Sachan et al. (2023), our approach can be seen as adjusting the probabilities of the retrieved documents to match the probabilities of the output sequence perplexities of the language model. In other words, we would like the retriever to find documents that result in lower perplexity scores. As shown in Figure 3, our training algorithm consists of the four steps: (1) retrieving documents and computing the retrieval likelihood (§4.1), (2) scoring the retrieved documents by the language model (§4.2), (3) updating the retrieval model parameters by minimizing the KL divergence between the retrieval likelihood and the LM’s score distribution (§4.3), and (4) asynchronous update of the datastore index (§4.4).",
        "justification_conclusion": "The evidence text gives an overview of the supervising fine-tuning methods."
    },
    {
        "claim_id": 3,
        "claim_text": "We are the first to demonstrate that retrieval can benefit large-scale, state-of-the-art LMs on language modeling (§6) and in-context learning tasks. Evaluations show that REPLUG can improve the performance of various language models such as GPT, OPT and BLOOM, including very large models with up to 175B parameters.",
        "evidence_text": "Table 1 reports the results of the original baselines, baselines augmented with the REPLUG, and baselines augmented with the REPLUG LSR. We observe that both REPLUG and REPLUG LSR significantly outperform the baselines. This demonstrates that simply adding a retrieval module to a frozen language model (i.e., the black-box setting) is effective at improving the performance of different sized language models on language modeling tasks. Furthermore, REPLUG LSR consistently performs better than REPLUG by a large margin. Specifically, REPLUG LSR results in 7.7% improvement over baselines compared to 4.7% improvement of REPLUG averaged over the 8 models. This indicates that further adapting the retriever to the target LM is beneficial.; As shown in Table 3, REPLUG LSR significantly improves the performance of the original Codex by 12.0% on NQ and 5.0% on TQA. It outperforms the previous best model, Atlas, which was fine-tuned with 64 training examples, achieving a new state-of-the-art in the few-shot setting. However, this result still lags behind the performance of retrieval-augmented language models fine-tuned on the full training data.; Here we further study whether REPLUG could enhance diverse language model families that have been pre-trained using different data and methods. Specifically, we focus on three groups of language models with varying sizes: GPT-2 (117M, 345M, 774M, 1.5B parameters) (Brown et al., 2020), OPT (125M, 350M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B) (Zhang et al., 2022) and BLOOM (560M, 1.1B, 1.7B, 3B and 7B) (Scao et al., 2022). We evaluate each model on Wikitext-103 (Merity et al., 2017) test data and report its perplexity. For comparison, we augment each model with REPLUG that adopts the ensemble method to incorporate top 10 retrieved documents. Following prior work (Khandelwal et al., 2020), we use Wikitext103 training data as the retrieval corpus. Figure 4 shows the performance of differentsized LMs with and without REPLUG. We observe that the performance gain brought by REPLUG stays consistent with model size. For example, OPT-125M achieves 6.9% perplexity improvement, while OPT-66B achieves 5.6% perplexity improvement. Additionally, REPLUG improves the perplexity of all the model families, which indicates that REPLUG is applicable to diverse language models with different sizes.",
        "justification_conclusion": "The evidence demonstrates the effectiveness of the proposed framework can improve the performance of the freezed black-box LMs, and REPLUG LSR can further improve the performance. Also, the proposed framework is effective across a wide range of language models."
    }
]