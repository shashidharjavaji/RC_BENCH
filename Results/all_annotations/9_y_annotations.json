[
    {
        "claim_id": 1,
        "claim_text": "Soft-prompt inputs to the language model do not map onto interpretable tokens in the output vocabulary.",
        "evidence_text": "Image prompts cast into the transformer embedding space do not encode interpretable semantics. Translation between modalities occurs inside the transformer.",
        "justification_conclusion": "True. Analysis shows that semantic alignment happens deeper within the transformer layers, as soft-prompts lack direct interpretability."
    },
    {
        "claim_id": 2,
        "claim_text": "Multimodal neurons causally affect output: modulating them can remove concepts from image captions.",
        "evidence_text": "Ablating multimodal neurons decreases the probability of token predictions by 80% on average and changes the semantics of generated captions.",
        "justification_conclusion": "True. The causal role of multimodal neurons in transforming image representations into specific textual concepts is demonstrated through ablation studies."
    },
    {
        "claim_id": 3,
        "claim_text": "Receptive fields of multimodal neurons better segment specific image concepts compared to randomly sampled neurons.",
        "evidence_text": "Across all categories, the receptive fields of multimodal neurons better segment the object in each image than randomly sampled neurons from the same layers.",
        "justification_conclusion": "True. This claim is supported by quantitative comparisons of Intersection over Union (IoU) metrics for segmentation tasks."
    }
]
