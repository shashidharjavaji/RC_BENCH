[
    
    {
        "claim_id": 1,
        "claim_text": "we are among the first to study the impact of both verbal and vocal features on financial markets, specifically, stock volatility.",
        "evidence_text": "When multimodal verbal and vocal features are available, it is also important to capture the dependency between different modalities, as the vocal cues either affirm or discredit the verbal message. For example, if a CEO says “we are confident about the future product sales” with a voice that is different from the CEO’s base vocal cues, such as increased pitch or pauses, we may infer that the CEO is not as confident as he claims. In fact, existing research (Jiang and Pell, 2017) in speech communication has shown that voice (vocal cues) plays a critical role in verbal communication. If we ignore the voice patterns that are accompanied with the verbal language, we may misinterpret the CEO’s statement. Especially in financial markets where CEO’s word and voice are closely examined by professional analysts and investors, it is plausible that market reacts to both verbal and vocal signals. Therefore, we present a deep model to capture context-dependent unimodal features and fuse multimodal features for the regression task. The high-level idea behind the design is to use contextual BiLSTM to extract context-dependent unimodal features separately corresponding to each sentence, and then use a BiLSTM to fuse multimodalities and extract the inter-dependencies between different modalities. The details of our model is described below.",
        "justification_conclusion": "Not Enough Info. The evidence does not directly support the claim. The evidence explains the importance of vocal cues in verbal communication, but does not mention the claim that the authors are among the first to study the impact of both verbal and vocal features on financial markets."
    },
    {
        "claim_id": 2,
        "claim_text": "we empirically show that multimodal learning with audio and text can indeed reduce prediction error, compared to previous work that relies on text only.",
        "evidence_text": "Both modalities are helpful. We can also conclude from the results that multimodal features are more helpful than unimodal features (either text or audio) alone. When we predict the stock volatility 3-days following the conference call, multimodal (1.371) outperform unimodal (1.431) by 4.2%. As shown in Table 1, MDRM (text+audio) significantly outperforms MDRM (text only) and MDRM (audio-only) model for 3-days, 7-days and 15 days stock volatility prediction. The improvement is not statistically significant for 30days prediction, which we will explain the possible reasons later. In addition to reduced prediction error, fusing both modalities can mitigate potential overfitting problem. We find that training a deep LSTM network with audio data only can result in overfitting very quickly. In our experiment, the audio-only deep network shows a trend of over-fitting in 10 epochs. Therefore, the result that audio-only MDRM performs better than textonly MDRM (1.412 vs. 1.431) may need careful interpretation as we have to stop audio-only model training early to prevent overfitting. However, using both audio features and text features, the model usually converges in 20 epochs without over-fitting.",
        "justification_conclusion": "True. The claim is supported by the evidence. The experiment results show that multimodal learning with audio and text can indeed reduce prediction error, compared to previous work that relies on text only. The evidence records the prediction error of multimodal learning and unimodal learning, and the comparison between them. The evidence also explains the reason why multimodal learning can reduce prediction error."
    },
    {
        "claim_id": 3,
        "claim_text": "The interesting finding that vocal cues play a role in stock volatility is worth further exploring.",
        "evidence_text": "Some Individual Vocal Features are Important. We also design another experiment to investigate the importance of different vocal features. We examine whether the left-out of individual vocal features can affect prediction results. We follow the prior research (Jiang and Pell, 2017) to select five representative vocal features including mean pitch, standard deviation of pitch, mean intensity, number of pulses and mean HNR (Harmonic-to-Noise Ratio). Our experiment results show that without mean pitch feature, the MSE of our model increases 0.7%. The left-out of standard deviation of pitch also raises MSE by 0.65%. For mean intensity and number of pulses, MSE increases by 0.63% and 0.56% respectively. However, MSE is not changed with mean HNR being left-out.This finding is consistent with prior research in speech communication that pitch and intensity are important features when detecting a speaker’s confident and doubt.",
        "justification_conclusion": "Ture. The claim is supported by the evidence. The experiment highlights the importance of vocal features in stock volatility prediction. The evidence records the experiment results of the importance of different vocal features, and the comparison between them."
    },
    {
        "claim_id": 4,
        "claim_text": "We construct a unique dataset containing conference call audio and text data of S&P 500 companies in recent years.",
        "evidence_text": "We build our dataset by acquiring all S&P 500 companies’ quarterly earnings conference calls in 2017. We choose S&P 500 constituent firms as the target for volatility prediction for reasons of importance and tractability. Firms in the S&P 500 index encompass roughly three-quarters of the total U.S. market capitalization. A total of 2,243 earnings conference calls are downloaded from Seeking Alpha and EarningsCast. We discard conference calls which text-audio alignment is not done properly, using the abovementioned data processing method. The final dataset consists of 576 conference calls, with a total number of 88,829 sentences. It can be seen that we discard a large proportion of raw data because the audio-text alignment is very noisy and is prone to errors. We release our processed earnings conference calls dataset6 (text and audio) for readers who are interested in reproducing the results.",
        "justification_conclusion": "True. The claim is supported by the evidence. The evidence record the source of the data set, the general construction process, and the statistics of the data set."
    }

]