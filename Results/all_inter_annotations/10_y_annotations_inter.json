[
    {
        "claim_id": 1,
        "claim_text": "Our method achieves new state-of-the-art performance on the ScienceQA benchmark, outperforming GPT-3.5 by 16% and even surpassing human performance.",
        "evidence_text": [
            "With vision features, the RougeL score of the rationale generation has boosted to 93.46% (QCM→R), which correspondingly contributes to better answer accuracy of 85.31% (QCMR→A)."
          ],
          "justification_conclusion": "True"
    },
    {
        "claim_id": 2,
        "claim_text": "Using vision features significantly improves rationale quality and reduces hallucinated rationales that mislead answer inference.",
        "evidence_text": [
            "Then, we randomly sample 50 error cases and find that the model tends to generate hallucinated rationales that mislead the answer inference... such mistakes occur at a ratio of 56% among the error cases (Figure 3(a)).",
            "With those effective rationales, the phenomenon of hallucination is mitigated — 60.7% hallucination mistakes in Section 3.2 have been corrected (Figure 3(b)), as an example shown in Figure 2 (right part)."
          ],
          "justification_conclusion": "True"
    },
    {
        "claim_id": 3,
        "claim_text": "The two-stage Multimodal-CoT framework outperforms one-stage methods by generating better rationales and leveraging multimodal information.",
        "evidence_text": "The two-stage method achieves 84.91% accuracy, compared to the one-stage method’s 80.40%. Vision features help generate more effective rationales and improve answer inference.",
        "justification_conclusion": "True"
    }

]
