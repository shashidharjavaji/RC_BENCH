[
    {
        "claim_id": 1,
        "claim_text": "We propose hallucination-augmented cross-modal contrastive learning (HACL), which enhances the alignment between visual and textual representations to alleviate hallucinations.",
        "evidence_text": [
            "In Figure 4 (c), with the application of hallucination augmentation in contrast learning not only did the modal gap decrease, but the hallucination sample distribution was also significantly distanced."
        ],
        "justification_conclusion": "True"
    },
    {
        "claim_id": 2,
        "claim_text": "Our experiments show that equipping MLLMs with HACL not only mitigates hallucinations but also effectively improves the performance across multiple benchmark evaluations.",
        "evidence_text": [
            "We discovered that after implementing HACL, all three models exhibited improvements across multiple benchmarks [referring to VQA and MLLM-oriented benchmarks]. Notably, for LLaVA and MiniGPT-4, the enhancement was particularly evident on the MME [12] benchmark."
        ],
        "justification_conclusion": "True"
    },
    {
        "claim_id": 3,
        "claim_text": "We underline a significant cross-modal semantic gap between visual and textual representations and an unexpected representation tangling among text containing and not containing hallucinations in MLLMs.",
        "evidence_text": [
            "As illustrated in Figure 4 (a), a substantial modality gap is observable in the data distribution without contrast learning."
        ],
        "justification_conclusion": "True"
    },
    {
        "claim_id": 4,
        "claim_text": "HACL forces the visual representation closer to the text representation and makes the correct and hallucinated text representations more distinguishable.",
        "evidence_text": [
            "In Figure 4 (c), with the application of hallucination augmentation in contrast learning not only did the modal gap decrease, but the hallucination sample distribution was also significantly distanced."
        ],
        "justification_conclusion": "True"
    },
    {
        "claim_id": 5,
        "claim_text": "Our method achieves significant improvements over state-of-the-art models such as MiniGPT-4 and LLaVA on hallucination-related benchmarks.",
        "evidence_text": [
            "Table 2 shows that miniGPT-4-HACL and LLAVA-HACL both demonstrated significant improvements compared to the original model. Of particular note, the average F1 score of LLaVA-HACL increased by 17.8% compared to LLaVA [32], while the Yes ratio decreased from 99.55 to 48.25."
        ],
        "justification_conclusion": "True"
    }
]