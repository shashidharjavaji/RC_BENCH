[
    {
        "claim_id": 1,
        "claim_text": "The LMs are susceptible to framing effect bias: Using the framing effect as inspiration, we hypothesize that code generation models may generate solutions exclusively from irrelevant information in the prompt.",
        "evidence_text": [
            "Moreover, both models frequently generate rate the framing line: 81% of the time for Codex and 70.7% of time for CodeGen, compared to only 4.5% and 0.0% over untransformed prompts respectively."
        ],
        "justification_conclusion": ""
    },
    {
        "claim_id": 2,
        "claim_text": "Using the availability heuristic as motivation, we hypothesize that code generation models may err by outputting solutions to related prompts that appear more frequently in the training set.",
        "evidence_text": [
            "Among combinations where flipping the order leads to error, we find that 75% of the binary-first outputs are the unary-first solution."
        ],
        "justification_conclusion": ""
    },
    {
        "claim_id": 3,
        "claim_text": "Using attribute substitution as inspiration, we hypothesize that Codex may use simple-but-incorrect heuristics to generate solutions.",
        "evidence_text": [
             "Moreover, for between 52% and 80% of prompts, Codex responds with the function specified in the function name."
        ],
        "justification_conclusion": ""
    },
    {
        "claim_id": 4,
        "claim_text": "Adding irrelevant preceding functions significantly lowers functional accuracy for both Codex and CodeGen.",
        "evidence_text": [
            "We find that adding irrelevant preceding functions consistently lowers functional accuracy, by between 22.3 and 30.5 points for Codex, across the different framing lines we tested."
        ],
        "justification_conclusion": ""
    },
    {
        "claim_id": 5,
        "claim_text": "Models adjust their output towards anchor functions included in the prompt, even incorporating parts of the anchor into otherwise correct solutions.",
        "evidence_text": [
             "Both models sometimes even incorporate the anchor lines into correct solutions; on Codex, the for var loop is used in a correct solution for 3%-11% of all outputs, while print (var) is used in a correct solution for 1%-9% of outputs."
        ],
        "justification_conclusion": ""
    },
    {
        "claim_id": 6,
        "claim_text": "Codex errs by generating code matching a simpler, conflicting function name rather than the complex docstring description.",
        "evidence_text": [
            "When we request a conflicting function name, Codex's accuracy drops from 100% to only 4.4%-4.6%."
        ],
        "justification_conclusion": ""
    }
]