[
    {
        "claim_id": 1,
        "claim_text": "we introduce U-MATH, a novel benchmark of 1,100 unpublished open-ended university-level problems sourced from teaching materials.",
        "evidence_text": [
            "The U-MATH benchmark comprises 1,100 carefully curated and validated mathematical problems."
        ],
        "justification_conclusion": "True"
    },
    {
        "claim_id": 2,
        "claim_text": "Our findings reveal that LLMs achieve a maximum accuracy of only 63% on text-based tasks, with even lower 45% on visual problems. The solution assessment proves challenging for LLMs, with the best LLM judge having an F1-score of 80% on µ-MATH",
        "evidence_text": [
            "In the multi-modal model group, Gemini-1.5-pro-002 leads with an overall accuracy of 60.1% [Table 4 details: 63.4% on Text, 45.0% on Visual]... This [Gemini 1.5 Pro] constitutes a considerable deficiency in the context of judgment, because judges' error rates directly limit the precision of capability evaluations... the best attainable F1 score is only 80.7% [Table 5]."
        ],
        "justification_conclusion": "True"
    },
    {
        "claim_id": 3,
        "claim_text": "2. µ-MATH Meta-Evaluation Benchmark (Section 3.3): Additionally, we introduce a set of 1084 meta-evaluation tasks sourced from U-MATH problems and designed to rigorously assess the quality of LLM judges. We manually select approximately 25% of the U-MATH problem statements and golden answers, supplying each with four solutions produced by different top-performing LLMs, and label them based on whether the generated solutions are correct or not. The benchmark is designed to be challenging for LLM judges yet representative of the typical university-level math grading tasks.",
        "evidence_text": [
            "We selected 271 U-MATH problems (around 25%) based on their assessment difficulty to create a challenging meta-evaluation set... Four solutions have been generated for each of the selected problems using Qwen2.5-72B, Llama3.1-8B, GPT-40 and Gemini-1.5-Pro models 1084 samples in total."
        ],
        "justification_conclusion": "True"
    },
    {
        "claim_id": 4,
        "claim_text": "3. Comparison of Models (Section 4): We conduct a comparative analysis of various open- source and proprietary LLMs on U-MATH. Our analysis highlights the high performance of specialized models in text-only problems and the superiority of proprietary models in visual tasks with the best U-MATH accuracy of 49%. Additionally, we examine several popular LLMs on µ-MATH to assess their ability to judge free-form mathematical problems. Our results show the best model achieving the macro F1-score of 80%.",
        "evidence_text": [
            "Among text-only models, the math-specific model Qwen2.5-Math-72B achieves the highest overall accuracy at 50.2%... In the multi-modal model group, Gemini-1.5-pro-002 leads with an overall accuracy of 60.1% [details in Table 4: 45% on Visual]... [in µ-MATH evaluation, Table 5 shows Gemini 1.5 Pro top F1 80.7%]."
        ],
        "justification_conclusion": "True"
    },
    {
        "claim_id": 5,
        "claim_text": "Math-specific LLMs demonstrate superior performance on text-only university-level math problems compared to larger general-purpose models.",
        "evidence_text": [
            "However, the small specialized model Qwen2.5-Math-7B surpasses or performs on par with 10 times larger models like Qwen2.5-72B or LLaMA-3.1-70B and almost reaching leading Gemeni-1.5-Pro level."
        ],
        "justification_conclusion": "True"
    },
     {
        "claim_id": 6,
        "claim_text": "There is a significant performance gap between proprietary and open-weights models on visual mathematical reasoning tasks at the university level.",
        "evidence_text": [
             "At the moment, the gap is evident in visual comprehension, with 18.5% difference on $U-MATH_{Visual}$ between top-1 [Gemini 1.5 Pro, 45.0%] and best open-weight model [Qwen2-VL-72B, 26.5%]."
        ],
        "justification_conclusion": "True"
    }

]