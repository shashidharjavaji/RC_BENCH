[
    {
        "claim_id": 1,
        "claim_text": "We find that intermediate layers often yield more informative representations for downstream tasks than the final layers.",
        "evidence_text": [
            "Our findings indicate that intermediate layers consistently outperform the final layer across all three architectures (Table 1). Selecting the best-performing intermediate layer yields at least a 2% improvement in average accuracy compared to using the last layer."
        ],
        "justification_conclusion": ""
    },
    {
        "claim_id": 2,
        "claim_text": "To measure the representation quality, we adapt and apply a suite of metrics—such as prompt entropy, curvature, and augmentation-invariance—originally proposed in other contexts.",
        "evidence_text": [
            "Figure 1 compares entropy, InfoNCE, LiDAR, and DiME metrics as a function of model depth, normalized to allow fair comparisons across models with different numbers of layers."
        ],
        "justification_conclusion": ""
    },
    {
        "claim_id": 3,
        "claim_text": "Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer.",
        "evidence_text": [
            "The results [in Figure 2] show that the most significant changes occur in the intermediate layers. As training progresses, prompt entropy in these layers decreases, indicating that the model is learning to compress and abstract input information more efficiently."
        ],
        "justification_conclusion": ""
    },

    {
        "claim_id": 4,
        "claim_text": "Intermediate layers consistently outperform final layers on MTEB downstream tasks across Transformer and SSM architectures.",
        "evidence_text": [
            "Our findings indicate that intermediate layers consistently outperform the final layer across all three architectures (Table 1)."
        ],
        "justification_conclusion": ""
    },
    {
        "claim_id": 5,
        "claim_text": "Transformer (Pythia) representations show greater variability and compression across layers compared to SSMs (Mamba).",
        "evidence_text": [
            "For entropy and LiDAR, Pythia shows a pronounced decrease at intermediate layers, suggesting greater information compression and consolidation. In contrast, Mamba maintains more stable values, indicating less compression in its intermediate representations."
        ],
        "justification_conclusion": ""
    },
    {
        "claim_id": 6,
        "claim_text": "Representation quality, especially in intermediate layers, evolves significantly during training.",
        "evidence_text": [
            "The results [in Figure 2] show that the most significant changes occur in the intermediate layers."
        ],
        "justification_conclusion": ""
    }
]