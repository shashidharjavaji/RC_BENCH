[
    {
        "claim_id": 1,
        "claim_text": "Accordingly, we propose an automated CoE dis- crimination approach and explore LLMs’ pref- erences from their effectiveness, faithfulness and robustness, as well as CoE’s usability in a naive Retrieval-Augmented Generation (RAG)i case.",
        "evidence_text": [
            "Based on the characterized features, we design an approach to discriminate whether external knowl- edge qualifies as CoE, as illustrated in Figure 3... After that, we investigate the LLMs' preference towards CoE from four aspects below. • Effectiveness... • Faithfulness... • Robustness... • Usability..."
        ],
        "justification_conclusion": "True"
    },
    {
        "claim_id": 2,
        "claim_text": "The evaluation on five LLMs reveals that CoE enhances LLMs through more accurate generation, stronger answer faithfulness, better robustness against knowledge conflict, and improved performance in a popular RAG case.",
        "evidence_text": [
            "Generally, ex- perimental results show that CoE achieves an av- erage accuracy of 92.0% across five LLMs and two datasets, outperforming Non-CoE variants SenP and WordP by 22.5% and 16.3%, respec- tively."
        ],
        "justification_conclusion": "True"
    },

    {
        "claim_id": 3,
        "claim_text": "External knowledge equipped with CoE helps LLMs generate correct answers more effectively than Non-CoE in noisy contexts.",
        "evidence_text": [
            "Generally, ex- perimental results show that CoE achieves an av- erage accuracy of 92.0% across five LLMs and two datasets, outperforming Non-CoE variants SenP and WordP by 22.5% and 16.3%, respec- tively."
        ],
        "justification_conclusion": "True"
    },
    {
        "claim_id": 4,
        "claim_text": "LLMs exhibit higher faithfulness to answers supported by CoE, even when the CoE contains factual errors, compared to Non-CoE.",
        "evidence_text": [
            "The results show that under CoE, the average FR [Following Rate] reaches 85.4%, which is 20.6% and 16.2% higher than the SenP and WordP types under Non-CoE respectively."
        ],
        "justification_conclusion": "True"
    },
    {
        "claim_id": 5,
        "claim_text": "LLMs augmented with CoE demonstrate greater robustness against conflicting misinformation than those using Non-CoE.",
        "evidence_text": [
            "The results show that under CoE, the average ACC [Accuracy] of LLMs reaches 84.1%, which is 21.4% and 15.3% higher than the SenP and WordP types under Non-CoE respectively."
        ],
        "justification_conclusion": "True"
    },
    {
        "claim_id": 6,
        "claim_text": "A CoE-guided retrieval strategy (ScopeCoE) improves LLM accuracy in a naive RAG framework.",
        "evidence_text": [
            "The results show that RAG+ScopeCoE achieves average ACC of 77.8% and 81.6% on Hot- potQA and 2WikiMultihopQA respectively, outper- forming RAG by 10.4% and 28.7%."
        ],
        "justification_conclusion": "True"
    }
]