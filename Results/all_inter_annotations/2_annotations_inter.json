[
    {
        "claim_id": 1,
        "claim_text": "We show that when we use a format with visible lettered answer options, large models are very well calibrated on diverse multiple choice questions",
        "evidence_text": [
            "We find that when multiple choice problems are formatted in this way... our largest models tend to produce a well- calibrated probability distribution among the available options."
        ],
        "justification_conclusion": ""
    },
    {
        "claim_id": 2,
        "claim_text": "Replacing an option with ‘none of the above’ reduces accuracy and calibration significantly with our models.",
        "evidence_text": [
            "Furthermore, adding 'none of the above' also harms calibration, as can be seen in Figures 5 and 7. It seems that even the 52B model is biased against using the 'none of the above' option and failed to use it with appropriate frequency."
        ],
        "justification_conclusion": ""
    },
    {
        "claim_id": 3,
        "claim_text": "Models can self-evaluate whether their own samples are True or False, though this tends to be a more challenging task (since models tend to find their own samples more plausible).",
        "evidence_text": [
             "This True/False self-evaluation may be considerably more difficult than the tasks we studied in section 3.2, because there the model was presented with human-written possibilities, whereas here the model is forced to evaluate its own samples."
        ],
        "justification_conclusion": ""
    },
    {
        "claim_id": 4,
        "claim_text": "Models can self-evaluate whether their own samples are True or False, though this tends to be a more challenging task.",
        "evidence_text": [
             "This True/False self-evaluation may be considerably more difficult than the tasks we studied in section 3.2, because there the model was presented with human-written possibilities, whereas here the model is forced to evaluate its own samples."
        ],
        "justification_conclusion": ""
    },

     {
        "claim_id": 5,
        "claim_text": "Models are well-calibrated on True/False tasks, with performance improving with model size.",
        "evidence_text": [
             "In Figure 8 we show the calibration results from this method on BIG Bench. We see that the 52B model is quite well-calibrated in this context."
        ],
        "justification_conclusion": ""
    },
     {
        "claim_id": 6,
        "claim_text": "Calibration improves with model size and few-shot prompting on multiple-choice tasks.",
        "evidence_text": [
            "As can be seen in figure 5, task formatting is important for achieving excellent calibration, and calibration improves as we pass from 0-shot to 5-shot evaluation."
        ],
        "justification_conclusion": ""
    },
    {
        "claim_id": 7,
        "claim_text": "Showing models multiple diverse samples of their own outputs improves self-evaluation (P(True)).",
        "evidence_text": [
            "With this format [showing 5 samples], performance improves significantly on all of the short-form answer tasks, as compared to the version where we only show models a single proposed answer, as summarized in Figure 11."
        ],
        "justification_conclusion": ""
    }

]