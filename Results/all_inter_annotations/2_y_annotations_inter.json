[
    {
      "claim_id": 1,
      "claim_text": "DynMM strikes a good balance between computational efficiency and learning performance. For instance, for RGB-D semantic segmentation tasks, DynMM achieves a +0.7% mIoU improvement with over 21% reductions in multiply-add operations (MAdds) for the depth encoder when compared against [35].",
      "evidence_text": [
        "Furthermore, DynMM-b achieves a mIoU improvement of 0.7% and 21.1% reduction in MAdds at the same time, thus demonstrating the superiority of DynMM over static fusion."
      ],
      "justification_conclusion": "True"
    },
    {
      "claim_id": 2,
      "claim_text": "we find that DynMM yields better predictions than static fusion networks when the input modality is perturbed by noise; this suggests possible use of DynMM to improve the multimodal robustness.",
      "evidence_text": [
        "From the figure, we observe that the performance gap between DynMM and ESANet becomes larger when the noise level of depth images increases; This demonstrates another advantage of DynMM in reducing data noise and improving robustness.",
        "On the contrary, our DynMM is robust to noise and provides a good prediction for both scenarios. These results suggest the potential of a dynamic neural network architecture for improving robustness of multimodal fusion."
      ],
      "justification_conclusion": "True"
    }
  ]
  