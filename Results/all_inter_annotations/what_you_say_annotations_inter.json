[
    {
        "claim_id": 1,
        "claim_text": "we empirically show that multimodal learning with audio and text can indeed reduce prediction error, compared to previous work that relies on text only.",
        "evidence_text": [
            "As shown in Table 1, MDRM (text+audio) significantly outperforms MDRM (text only) and MDRM (audio-only) model for 3-days, 7-days and 15 days stock volatility prediction."
        ],
        "justification_conclusion": ""
    },
    {
        "claim_id": 2,
        "claim_text": "We construct a unique dataset containing conference call audio and text data of S&P 500 companies in recent years.",
        "evidence_text": [
             "We build our dataset by acquir- ing all S&P 500 companies' quarterly earnings conference calls in 2017... The final dataset consists of 576 con- ference calls, with a total number of 88,829 sen- tences."
        ],
        "justification_conclusion": ""
    },
    {
        "claim_id": 3,
        "claim_text": "The proposed MDRM model outperforms baselines like past volatility, tf-idf, word embeddings, simple fusion, and bc-LSTM in volatility prediction.",
        "evidence_text": [
            "Compar- ing with using past volatility only, the improve- ment gain is as substantial as 54.1% for 3-days prediction. The improvement over other baseline methods are 19.1% (tf-idf bag-of-words), 17.8% (word embeddings), 20.4%(simple fusion) respec- tively for 3-days prediction."
        ],
        "justification_conclusion": ""
    },

     {
        "claim_id": 4,
        "claim_text": "Vocal features like pitch and intensity are important for prediction accuracy.",
        "evidence_text": [
            "Our experiment results show that without mean pitch feature, the MSE of our model increases 0.7%. The left-out of standard deviation of pitch also raises MSE by 0.65%. For mean intensity and number of pulses, MSE increases by 0.63% and 0.56% respectively."
        ],
        "justification_conclusion": ""
    }

]