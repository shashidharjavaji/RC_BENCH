[
    {
        "claim_id": 1,
        "claim_text": "First, we define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard. To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015). We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.",
        "evidence_text": "3.1 Task Definition  We assume a set C, which is a fixed set of units to which answers can be attributed. For example, C might be the set of all paragraphs in some corpus. More specifically, each c ∈ C is the ID for some unit; we use text(c) to refer to the actual paragraph text for natural language datasets. The input to an attributed QA system g is a question x. The output from the system is a pair g(x) = (a, c), where a is a text string, and c is a member of C. 3.2 Evaluation  We consider two evaluation metrics for the Attributed QA task: first, human ratings that are the gold-standard, and second, automatic evaluation methods, which we show can be suitable in development settings. Section 5 gives analysis of the correlation between the two.  Human Evaluation. Given a triple (x, a, c), we use the AIS evaluation definitions and guidelines (Rashkin et al., 2021) to judge whether the answer to question x is attributable to c. Raters are asked to answer the following two questions, in the context of the question x (where the system response is the answer a, and the source document is c):  1. Is all of the information relayed by the system response (a, c) interpretable to you?  2. Is all of the information provided by the system response a fully supported by the source document c?  We define the rating of (x, a, c) as “attributable” if the answer to both of these questions is “yes”. Assume a set of test questions x1 . . . xn, and a system g to be evaluated. Define ri to be the (randomly chosen) pool of raters on the ith test example, and h(xi, g(xi), ri) to be 1 if the majority of the annotators mark the system output g(xi) to be attributable, or 0. The test accuracy is then  E[g] = 1  n  n  ∑  i=1  h(xi, g(xi), ri)  That is, the test accuracy is simply the proportion of test examples where the majority of the raters judge the system’s output to be attributable. Automatic Evaluation (AutoAIS). In addition, we will make extensive use of an automatic measure, based on the NLI classifier of Honovich et al. (2022), AutoAIS (Gao et al., 2022). See Section 5 for full details of the classifier. Taking AutoAIS(xi, g(xi)) to be the output of the NLI classifier (1 for attributable vs 0 for non-attributable), we define  EA[g] = 1  n  n  ∑  i=1  AutoAIS(xi, g(xi)); We now focus on the question of how to best measure attribution given our observations so far. To do this, we estimate the correlation between system scores on (human) AIS, and EM and AutoAIS in turn, by calculating the Pearson coefficient between the two sets of scores (i.e. between AIS and EM scores, and between AIS and AutoAIS scores).  EM We saw above that best AIS performance did not necessarily go hand-in-hand with best EM accuracy. Consistent with this, the Pearson correlation coefficient between the system EM and AIS scores is modest, at 0.45 (see Figure 2). Manual analysis of the disagreements revealed multiple factors to be involved, including answers with inexact string matches to the NQ reference answer, stale reference answers, and questions with more than one valid answer able to be retrieved (see Table 4). Overall, we suggest that our results point to the limitation of reference answer corpora and string matching evaluation for future research.  AutoAIS On the other hand, correlation between system AIS and AutoAIS scores is remarkably strong, with a Pearson coefficient of 0.96 (Figure 3). This suggests that AutoAIS is fit-for-purpose as a development metric at the aggregate level (provided it is not used as a system component). To get a deeper understanding of the correlation.",
        "justification_conclusion": "In the first part, the authors define the evaluation framework, the second part demonstrated the AutoAIS has strong correlation with human ratings."
    },
    {
        "claim_id": 2,
        "claim_text": "Further, we perform a systematic analysis of a broad set of systems based on state-ofthe-art components, exploring different architectures and levels of supervision.",
        "evidence_text": "Table 1 shows results for the systems in each architecture class with the best AIS score, with AutoAIS Reranked variants. The most striking result is that the systems which perform best on AIS do not necessarily achieve the strongest EM accuracy (cf. Tables 2 and 3). This is discussed below in Section 5.5, where we find EM correlates only modestly with human judgment of AIS and has important limitations for Attributed QA evaluation. At the same time, we note that we did no special modeling to maximize EM score, such as instruction tuning (Wei et al., 2021) or chain of thought prompting (Wei et al., 2022), and that models tuned for greater EM may also achieve higher AIS scores. Best RTR achieves the highest performance (p 10−5, t = 4.55, in comparison with the best non-RTR system), despite using LLMs with relatively small numbers of parameters (using T5 XL with 3B parameters, compared to PaLM with 540B). However, RTR approaches have the shortcoming that they require relatively large amounts of explicit supervision, for example in the form of NQ examples (an open question is whether RTR systems with much less supervision can be developed). They are also likely to be highly dependent on the accuracy of the retrieval step. It is encouraging that Best Post-hoc achieves relatively high EM because it requires minimal amounts of supervision for answer generation (using prompting). However, these models generally require LLMs with large numbers of parameters10 (presumably needed for memorization). Also, attribution poses a challenge in this setting; as noted above, on AIS the best RTR system is significantly better than the best post-hoc system, and this difference carries over to AutoAIS as well. However, since reranking is able to find good attribution passages, this result suggests that attribution is more difficult in a post-hoc setting than in RTR, and is a key area for future development. Best Low Resource performs competitively with Best Post-hoc on AIS and AutoAIS despite using a sparse retrieval. This is promising for more complex information-seeking tasks where it is challenging to provide explicit supervision, and where LLMs have been shows to provide fluent output. End-to-end models have the potential benefit of not requiring retrieval at all. That the performance of Best LLM-as-retriever is competitive with lowresource post-hoc attribution is promising, given that it is BM25 which is used to select a paragraph from the returned URL. However, they again require LLMs with large numbers of parameters.",
        "justification_conclusion": "The authors reported the performance results of different systems with different architectures and levels of supervision."
    },
    {
        "claim_id": 3,
        "claim_text": "We give some hints as to how to address question 3 (How to build LLMs with attribution?).",
        "evidence_text": "While retrieve-then-read systems achieve strong performance, this class typically requires a large amount of data to train and can be resource intensive. We are excited by the possibility of post-hoc attribution of LLM-generated answers and end-to-end modeling for Attributed QA. Future directions to improve performance in these settings includes studying the challenge of retrieval for posthoc attribution, and devising training signals for end-to-end modeling.",
        "justification_conclusion": "In summary, the authors recommend the post-hoc attribution of LLM and end-to-end modeling for Attributed QA."
    }
]