[
    {
        "claim_id": 1,
        "claim_text": "We propose PromptBERT, a novel contrastive learning method for learning better sentence representation.",
        "evidence_text": "We firstly analyze the drawback of current sentence embedding from original BERT and find that it is mainly due to the static token embedding bias and ineffective BERT layers. Then we propose the first prompt-based sentence embeddings method and discuss two prompt representing methods and three prompt searching methods to make BERT achieve better sentence embeddings.",
        "justification_conclusion": "True. This claim introduces the central innovation of PromptBERT and highlights its novelty in addressing biases and inefficiencies in BERT sentence embeddings."
    },
    {
        "claim_id": 2,
        "claim_text": "PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.",
        "evidence_text": "Compared to SimCSE, PromptBERT achieves 2.29 and 2.58 points of improvement based on BERT and RoBERTa in the unsupervised setting.",
        "justification_conclusion": "True. This claim is supported by performance comparisons showing significant improvements over a strong baseline, SimCSE, on semantic textual similarity (STS) tasks."
    },
    {
        "claim_id": 3,
        "claim_text": "By reformulating the sentence embedding task as the mask language task, we can effectively use original BERT layers by leveraging the large-scale knowledge.",
        "evidence_text": "By reformulating the sentence embedding task as the mask language task, we can effectively use original BERT layers by leveraging the large-scale knowledge. We also avoid the embedding biases by representing sentences from [MASK] tokens.",
        "justification_conclusion": "True. The proposed method repurposes BERT's architecture and effectively reduces biases while enhancing sentence representation quality."
    },
    {
        "claim_id": 4,
        "claim_text": "Manually removing embedding biases is a simple method to improve the performance of sentence embeddings.",
        "evidence_text": "Simply removing a set of tokens, the result can be improved by 9.22, 7.08, and 11.76 respectively. The final result of RoBERTa-base can outperform post-processing methods such as BERT-flow and BERT-whitening with only using static token embeddings.",
        "justification_conclusion": "True. This claim highlights the effectiveness of removing biases, although it acknowledges the limitations of this approach for shorter sentences."
    },
    {
        "claim_id": 5,
        "claim_text": "Prompt-based contrastive learning objective significantly shortens the gap between the unsupervised and supervised methods.",
        "evidence_text": "Prompt-based contrastive learning objective significantly shortens the gap between the unsupervised and supervised methods. It also proves our method can leverage the knowledge of unlabeled data with different templates as positive pairs.",
        "justification_conclusion": "True. The claim is supported by experimental results demonstrating the improved performance of the proposed method across STS benchmarks in both unsupervised and supervised settings."
    }
]

