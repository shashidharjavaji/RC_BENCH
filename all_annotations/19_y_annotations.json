[
    {
        "claim_id": 1,
        "claim_text": "To comprehensively enhance the training of Audio-Visual LLM, we meticulously curate general captions including long descriptions, for the pretraining stage and fine-grained instructions including multiturn conversations and complex reasoning, for the instruction fine-tuning stage.",
        "evidence_text": "Compared to the prior LLM-based works that support video-only input and audio-visual input, our method consistently brings a +4.4% accuracy on MSRVTT-QA, a +1.9% accuracy on MSVD-QA, and a +2.4% accuracy on ActivityNet-QA. From the significant improvements, we can find that the proposed modality-augmented training mechanism, which jointly optimizes diverse modality samples in the same video can significantly enhance video alignment with LLMs compared to works (e.g., Valley and Video-ChatGPT) that focus on visual-only samples. Moreover, we find that the high-quality video instruction dataset plays a crucial role.",
        "justification_conclusion": "True. The method enhances the training of Audio-Visual LLM by curating general captions and fine-grained instructions."
    },
    {
        "claim_id": 2,
        "claim_text": "Extensive experiments demonstrate that our method achieves advantageous performance on various video understanding tasks.",
        "evidence_text": "For instance, we achieve an accuracy of 53.7% on MSRVTT-QA and 47.2% on ActivityNet-QA, outperforming both LLM-based (e.g., Video-LLaMA [86] and Valley [53]) and non-LLM-based (e.g., VideoCoCa [81] and InterVideo [77]) comparison works. We also show comparable performance on audio tasks (e.g., AudioCaps [35]), demonstrating the substantial potential of our method in audio.",
        "justification_conclusion": "True. The method achieves advantageous performance on various video understanding tasks."
    }
]

