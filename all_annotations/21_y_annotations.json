[
    {
        "claim_id": 1,
        "claim_text": "To investigate the existence of localized knowledge regions, we construct two multi-choice QA datasets encompassing various domains and languages.",
        "evidence_text": "To deal with long-form answers, we advocate for the transformation of questions and their corresponding answers into a multiple-choice framework, as illustrated in Figure 1. This approach involves the generation of incorrect options by randomly sampling answers within the same domain. Following this, the LLM is prompted to produce only the option letter. Subsequently, we investigate the neurons correlated with the input query. To mitigate the impact of randomness, we devise multiple prompt templates and systematically shuffle the order of options to prevent the model from learning spurious correlations based on option letters. These prompt templates are detailed in Table A3 in the Supplementary Material (SM)",
        "justification_conclusion": "True. The authors propose a method to investigate the existence of localized knowledge regions by constructing two multi-choice QA datasets encompassing various domains and languages. They advocate for the transformation of questions and their corresponding answers into a multiple-choice framework, generation of incorrect options by randomly sampling answers within the same domain, and prompting the LLM to produce only the option letter. The authors also investigate the neurons correlated with the input query and devise multiple prompt templates to prevent the model from learning spurious correlations based on option letters."
    },
    {
        "claim_id": 2,
        "claim_text": "We visualize the geographical locations of the detected neurons in Llama. Our findings indicate that distinct localized regions emerge in the middle layers, particularly for domain-specific neurons.",
        "evidence_text": "We visualize domainor language-specific neurons on a 2D geographical heatmap. The width of the heatmap corresponds to the dimension of FFNs in Llama-2-7B (11008), and the length represents the layer depth (32). We accumulate the value of naica(nl  i) to populate the heatmap. Figure 4 displays the geographical locations of QR neurons in Llama-2-7B across various academic domains and languages. The distribution of QR neurons appears sparse but with distinct regions, particularly for different domains. Notably, certain regions are visible in the middle layers (10-15), suggesting specific neuron patterns.",
        "justification_conclusion": "True. The authors visualize the geographical locations of the detected neurons in Llama and find that distinct localized regions emerge in the middle layers."
    },
    {
        "claim_id": 3,
        "claim_text": "we observed that common neurons are concentrated in the top layer, predominantly expressing frequently used tokens.",
        "evidence_text": "To gain insights into the function of common neurons, we project the matrix WD in Equation 1 to the vocabulary space and select the top-k tokens with the highest probability. Table 4 lists the predicted tokens, which include common words, punctuation marks, and option letters. These findings reinforce the notion that common neurons are not critical for specific queries. We also visualize their locations within Llama-2-7B and we observe that they tend to appear at the top layer (as shown in Figure A2 in the SM).",
        "justification_conclusion": "Natural. The content does not provide enough information to determine the truth value of the claim."
    }
]

