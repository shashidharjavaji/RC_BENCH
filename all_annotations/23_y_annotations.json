[
    {
        "claim_id": 1,
        "claim_text": "presenting the first comprehensive MLLM Evaluation benchmark MME1. It measures both perception and cognition abilities on a total of 14 subtasks.",
        "evidence_text": "MME covers the examination of perception and cognition abilities. Apart from OCR, the perception includes the recognition of coarse-grained and fine-grained objects. The former identifies the existence, count, position, and color of objects. The latter recognizes movie posters, celebrities, scenes, landmarks, and artworks. The cognition includes commonsense reasoning, numerical calculation, text translation, and code reasoning. The total number of subtasks is up to 14, as shown in Fig",
        "justification_conclusion": "True. The content of the evidence text supports the claim that MME evaluates both perception and cognition abilities on a total of 14 subtasks."
    },
    {
        "claim_id": 2,
        "claim_text": "Benefitting from our instruction design “please answer yes or no”, we can easily perform quantitative statistics based on the “yes” or “no” output of MLLMs, which is accurate and objective.",
        "evidence_text": "In order to facilitate quantitative performance statistics, the orientation of our instruction design is to let the model to answer “yes” or “no”. As a result, the instruction consists of two parts, including a concise question and a description “Please answer yes or no.” For each test image, we manually design two instructions, where the discrepancy lies in the questions. The ground truth answer of the first question is “yes” and that of the second question is “no”,",
        "justification_conclusion": "True. The evidence text supports the claim that the instruction design “please answer yes or no” allows for easy quantitative statistics based on the “yes” or “no” output of MLLMs."
    },
    {
        "claim_id": 3,
        "claim_text": "MME evaluates 30 advanced MLLMs, revealing significant discrepancies in performance and highlighting areas for optimization.",
        "evidence_text": "In this section, a total of 30 MLLMs are evaluated on our MME benchmark, including BLIP-2 [23], InstructBLIP [12], MiniGPT-4 [59], PandaGPT [39], MultimodalGPT [15], VisualGLM-6B [5], ImageBind-LLM [17], VPGTrans [53], LaVIN [33], mPLUG-Owl [48], Octopus [3], Muffin [51], Otter [22], LRV-Instruction [28], Cheetor [24], LLaMA-Adapter-v2 [14], GIT2 [41], BLIVA [18], Lynx [52], MMICL [54], GPT-4V [37], Skywork-MM [4], mPLUG-Owl2 [48], Qwen-VL-Chat [9], XComposer-VL [7], LLaVA [29], Lion [2], SPHINX [27], InfMLLM [1], and WeMM [6].",
        "justification_conclusion": "True. The evidence text supports the claim that MME evaluates 30 advanced MLLMs, revealing significant discrepancies in performance and highlighting areas for optimization."
    }
]
