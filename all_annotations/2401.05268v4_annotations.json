[
    {
        "Claim_id": 1,
        "Claim_text": "AUTOACT, an automatic agent learning framework for question answering, effectively overcomes the limitations of existing language agent systems by eliminating the need for large-scale annotated data and closed-source models.",
        "Evidence_text": "To acquire a sufficient amount of task data and provide an ample training resource, it is necessary to augment the data based on the examples at hand. We accomplish this process through selfinstruct.",
        "Justification_Conclusion": "True. The claim is supported by detailed mathematical notation and explanation of workflow presented in the same paragraph. This is one part of the novelty coming from."
    },
    {
        "Claim_id": 2,
        "Claim_text": "AUTOACT achieves this through its ability to automatically synthesize planning trajectories and implement a division-of-labor strategy, resulting in the creation of specialized sub-agents.",
        "Evidence_text": "In order to establish a clear division-of-labor, we leverage synthesized planning trajectories to differentiate the META-AGENT into three sub-agents with distinct functionalities: • PLAN-AGENTπplan undertakes question decomposition and determines which tool to invoke in each planning loop (Eq. 2). • TOOL-AGENTπtool isresponsible for how to invoke the tool (Eq. 3) by deciding the parameters for the tool invocation. • REFLECT-AGENTπreflect engages in reflection by considering all the historical trajectories and providing a reflection result (Eq. 4).",
        "Justification_Conclusion": "True. This claim is supported by sufficient explanation and visualization of agent system architecture。 This is the other perspective of novelty."
    },
    {
        "Claim_id": 3,
        "Claim_text": "Given limited user-provided data examples, AUTOACT utilizes a META-AGENT to obtain an augmented database through a self-instruct approach.",
        "Evidence_text": "The META-AGENT is responsible for all the preparatory work before self-differentiation and serves as the backbone model for all sub-agents./ To acquire a sufficient amount of task data and provide an ample training resource, it is necessary to augment the data based on the examples at hand. We accomplish this process through self-instruct. Initially, the database D is set to be equal to the task data examples C, with C as the seed for data generation. In each round, the META-AGENT generates new question-answer pairs by few-shot prompting, and the few-shot prompt examples are randomly sampled from D.",
        "Justification_Conclusion": "True. This claim is supported by the detailed tutorials on how to conduct self-instruction in the main text body. This provides sufficient traceability."
    },
    {
        "Claim_id": 4,
        "Claim_text": "The differentiation process in AUTOACT is a parameter-efficient training method that operates on self-synthesized trajectories with low resource consumption.",
        "Evidence_text": "Then, armed with a prepared tool library, the META-AGENT can automatically synthesize planning trajectories without any assistance from humans or strong closed-source models. Finally, we propose the division-of-labor strategy which resembles cell differentiation based on the self-synthesized trajectories (genes), where the META-AGENT acts as a stem cell (Colman, 2008) and differentiates into three sub-agents with distinct functions: task decomposition, tool invocation, and self-reflection, respectively.",
        "Justification_Conclusion": "False. The authors do not provide enough technical details in the paper, but they provide related citations for it. So the support and traceability is limited."
    },
    {
        "Claim_id": 5,
        "Claim_text": "Comprehensive experiments with various LLMs demonstrate that AUTOACT achieves better or comparable performance to strong baseline methods.",
        "Evidence_text": "Main results of AUTOACT compared to various baselines on HotpotQA and ScienceQA. The  icon indicates prompt-based agent learning without fine-tuning, while means fine-tuning-based agent learning.  denotes single-agent learning and  symbolizes multi-agent learning. The best results of each model are marked in bold, and the second-best results are marked with underline. We compare the zero-shot plan performance of GPT-3.5-Turbo to ensure fairness in our evaluations since our setup does not include annotated trajectory examples.",
        "Justification_Conclusion": "True. The experiments developed by the author provides a concrete supports for this claim."
    },
    {
        "Claim_id": 6,
        "Claim_text": "Further analysis shows that the trajectory quality generated by AUTOACT generally outperforms that of other approaches, validating the effectiveness of its division-of-labor strategy.",
        "Evidence_text": "Moderate division-of-labor benefits group planning performance. To explore the impact of different granularity of self-differentiation, we further subdivide the tool agent, assigning dedicated agents to manipulate each specific tool. We compare the performance of One agent, Three agents (AUTOACT), and the Tool-Specified setting on Hot6 Related Work potQA in Fig. 4. It can be observed that excessive differentiation (Tool-Specified) not only fails to achieve better results but can sometimes even be less effective than not differentiating (One) at all.",
        "Justification_Conclusion": "True. The authors include detailed analysis after experiments to discuss the moderate effectiveness of division-of-labor strategy."
    }
    ]