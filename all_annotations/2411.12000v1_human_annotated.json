{
    "annotations": [
        {
            "claim": ". To address this, we introduce ByteScience, a non-profit cloud-based auto fine- tuned Large Language Model (LLM) platform, which is designed to extract structured scientific data and synthesize new scientific knowledge from vast scientific corpora. ",
            "evidences": [
                "ByteScience transforms this process by enabling users to create a customized data extraction tool in hours, achieving 80%-90% human accuracy. It can process a 10-page scientific document in one second,"
            ]
        },
        {
            "claim": "The platform achieves remarkable accuracy with only a small amount of well- annotated articles",
            "evidences": [
                "Using 300 training samples reduced annotation time by 57% compared to a single sample [10]. In the GPT-3/Doping- English model, 10-20 samples were enough to learn the correct structure, with precision, recall, and F1 scores reaching 0.8-0.9 with around 300 samples."
            ]
        },
        {
            "claim": "Zero-code user-friendly semi-automated annotation and processing for uploaded science documents;",
            "evidences": [
                "Thomas configures ByteScience to meet his research needs by designing a custom annotation schema for alloy synthesis, annotating key details like compositions, casting parameters, solution treatment, and aging variables. ByteScience then initiates semi-automatic annotation, where the DARWIN LLM auto-labels papers from his corpus based on this schema. Thomas reviews and corrects the annotations to refine the modelâ€™s understanding. Afterward, ByteScience fine-tunes the LLM using AWS SageMaker, optimizing it for alloy synthe- sis data extraction. The fine-tuned model is deployed to a SageMaker Endpoint for efficient, large-scale processing of complex scientific papers."
            ]
        }
    ]
}