{
    "annotations": [
        {
            "claim": "In this paper, we demonstrate that three sources of bad annotations, both mali- cious and otherwise, can corrupt the reliability of open leaderboard rankings. ",
            "evidences": [
                "3.1 Apathetic Voting The main attraction of open community platforms for end users is that they expose a free and easy-to- use API endpoint for LLMs. This incentivizes di- verse users to interact with the platform and submit queries to explore their use cases. However, these platforms do not explicitly incentivize high-quality preference annotation. We hypothesize that at least r% of users on the arena are apathetic and provide random or low-quality votes on the platform.",
                "3.2 Adversarial Voting We assume there exists a malicious developer who seeks to inflate the rankings of their own target model mT on the arena leaderboard A. We argue that due to the lack of quality controls (e.g. user verification, attention checks, etc.), it is straight- forward to inject preference votes for mT using a simple attack methodology. Our main component is a target model attribu- tion algorithm which, given a query-output pair (q, y), predicts whether y is sampled from the target model mT (q). Given such an algorithm, we can inflate the ranking of the target model mT using the following strategy: (1) Enter a prompt q on the arena, (2) Detect if any of the two shown outputs y1, y2 are sampled from mT , (3) If yes, vote for the target model mT , (4) Repeat.",
                "3.3 Arbitrary Voting We assume an idealized scenario where all users genuinely make their best effort to rank model out- puts. However, we argue that holistically rating a response to an open-ended and inherently sub- jective query is ill-defined and liable to always be arbitrary. To demonstrate this, we conduct a small-scale annotation study for outputs of subjec- tive Researchy questions’ prompts (Rosset et al., 2024).11"
            ]
        },
        {
            "claim": "In particular, we show that only 10% of poor quality votes by apathetic (site visitors not appropriately incen- tivized to give correct votes) or adversarial (bad actors seeking to inflate the ranking of a target model) annotators can change the rankings of models by up to 5 places on the leaderboard.",
            "evidences": [
                "Model Leaderboard Ranking Orig. r=1 r=5 r=10 Llama-2-7b-chat 21 21 20↑1 21 Llama-2-13b-chat 39 39 41↓2 34↑5 Mistral-7b-instruct-v0.2 36 38↓2 38↓2 41↓5 Table 1: Change in leaderboard rankings for 3 test mod- els based on different percentages (r) of arbitrary votes. The subscripts denote gain (↑) or loss (↓) in rankings.We find that only 10% poor quality annotations can change the rank of 2/3 systems by 5 places.",
                "Model Leaderboard Ranking Orig. r=1 r=5 r=10 r=100 Llama-2-7b-chat 21 23↓2 21 17↑4 1↑21 Llama-2-13b-chat 39 36↑3 32↑5 28↑9 1↑39 Mistral-7b-instruct-v0.2 36 34↑2 34↑2 29↑7 2↑34 Table 2: Change in leaderboard rankings for 3 test mod- els based on different percentages (r) of adversarial votes (upvoting the target model). We find that only 10% adversarial annotations can change the rank of all systems by more than 4 places."
            ]
        },
        {
            "claim": "Finally, we discuss open challenges in ensur- ing the reliability and human annotation quality in open-source community-driven benchmarks (Sec- tion 4).",
            "evidences": [
                "Discussion: Can we detect and remove apathetic votes? A major challenge in detecting apathetic votes is that they are often indistinguishable from arbitrary votes. Multiple past studies have found that output-level comparisons using a single la- bel is ill-defined as an annotation task (Krishna et al., 2023; Goyal et al., 2022a) as users often rely on different criteria and disagree with each other. This ambiguity makes it hard to ascertain whether observed disagreements are due to per- sonal variations in quality assessment (arbitrary voting, discussed further in Section 3.3) or due to apathetic or low-quality annotations by certain an- notators. Despite challenges with detecting individ- ual apathetic votes, detecting apathetic users may be viable by computing agreements between model rankings by individual users. This strategy is based on the intuition that while annotators might dis-i agree on specific examples, their aggregate system- level judgments tend to be more aligned (Goyal et al., 2022a). Finally, requesting additional justifi- cations for votes, such as free-text rationales, can also help discourage apathetic votes. We discuss this more in Section 4.",
                "Discussion: Can we detect and remove adver- sarial votes? Open platforms can employ two types of mitigation strategies to address this issue: recognizing bot-like behavior to prevent votes from being cast, or detecting abnormal users post-hoc to filter out their votes. Platforms like Chatbot Arena already implement measures from both categories. For example, Chatbot Arena uses Cloudflare and Google reCAPTCHA to detect bots on their plat- form; however, we were able to bypass both pro- grammatically. We did not find public information indicating that similar measures have been incorpo- rated into the Wildvision Arena platform.",
                "Discussion We argue that arbitrary votes are not “noise” and provide useful signals about models’ relative performance. If most frontier models per- form similarly well on a substantial fraction of real-world queries, this information should not be discarded but inform leaderboard Elo scores. Ar- bitrary votes become problematic when the ma- jority of the leaderboard is dominated by open- ended queries that fail to meaningfully distinguish models, despite the existence of legitimate top- ics or skills along where models exhibit distinct behaviors. Identifying which test examples (or type of test examples) are most informative and up-weighting them when deriving aggregate scores are potential ways of addressing this (Rodriguez et al., 2021).",
                "Richer feedback We encourage the community to explore ideas from past research, such as solicit- ing fine-grained annotations (Krishna et al., 2023; Goyal et al., 2022b) or rationales (McDonnell et al., 2016) in addition to the binary preference feedback.",
                "Stronger Guardrails Other guardrails could in- clude reputation-based systems (Adler and de Al- faro, 2007), CAPTCHA (Von Ahn et al., 2003, 2008), machine learning based anomaly detection (Kumar et al., 2014; Wu et al., 2016) and tech- niques that use annotator behavior traces on the platform to estimate quality (Goyal et al., 2018).",
                "Open access to collected dataset Public release of the collected data on open platforms will spur research to address the annotation issues we discuss in this work. It would provide a more detailed overview into which types of queries are most well- equipped to distinguish between models, and what are the limitations of different families of models."
            ]
        }
    ]
}