{
    "annotations": [
        {
            "claim": "We implement this framework in both a text-based environment, which offers a tightly controlled setting and enables high-throughput parameter sweeps, and in an embodied 3D environment, which requires addressing complexities of multi-modal interaction more relevant to real-world applications. ",
            "evidences": [
                "We begin with simplified, text-based environments where the world is abstractly represented. This allows us to assess the model’s ability to explore and update its beliefs based solely on textual information, without the complexities of visual perception and motor control. We then progress to a more naturalistic setting, evaluating the model on video inputs from agents acting in an embodied 3D environment. This transition allows us to assess the generalizability of exploratory behaviors to a setting more reflective of real-world applications."
            ]
        },
        {
            "claim": " We further investigate whether approaches such as self-correction and increased inference time improve information gathering efficiency. ",
            "evidences": [
                "Self-correction Building upon previous work exploring the self-correction capabilities of LLMs in mathematical problem-solving (e.g., Huang et al. (2023)), we investigate whether LLMs can self-correct within our task framework. We adapt self-correction prompts similar to those used in Zheng et al. (2024), prompting the model at each step to revise its output, including its reasoning traces. This process allows the model to verify its chosen action and make corrections if necessary. Figure 4 presents the results of this process for both single-feature task and conjunction task scenarios using Gemini 1.5 Pro, and the results for the Flash model is found in Figure 7 in the appendix. In single-feature tasks, it improves performance with up to 6 unique colors, but its benefits diminish with a larger number of colors. Notably, self-correction appears more effective in more complex conjunction tasks, either performing comparably, or slightly outperforming the base model.",
                "Longer Inference Time We also explored whether encouraging the model to engage in more deliberate reasoning, by providing it with additional context, could improve performance. Instead of simply providing the model with its previous observations (actions and outcomes), we also included its reasoning traces, explaining why it selected previous actions. This approach, while increasing inference time, allows the model to reflect on its own chain-of-thought, potentially leading to improved reasoning and better decision-making. Figure 4 presents the results of incorporating the model’s reasoning traces. In both single-feature and conjunction tasks, this approach, which encourages more deliberate reasoning, yields comparable performance to the baseline approach without reasoning traces."
            ]
        },
        {
            "claim": "Performance is comparable in both text and 3D embodied environments, although imperfect visual object recognition reduces its accuracy in drawing conclusions from gathered informationi in the 3D embodied case.",
            "evidences": [
                "In the exploration efficiency metric, we see the same trends in the results for the 3D embodied environment as for the text environment, with Gemini’s exploration efficiency significantly out- performing the random baseline and approaching the optimal baseline (Figure 5a). The absolute performance matches that seen in the text experiments, within the margin of error: a mean of 2 steps for Gemini and the optimal baseline, and 4 steps for the random baseline. These results suggest that the additional complexity of an imperfect vision system and partially observed environment state are not significant limitations in generalizing directed exploration capabilities to embodied 3D environments. In the accuracy metric (Figure 5c), the picture is more nuanced. For relevant property accuracy, the difference between performance with the Gemini agent and the random agent was not statistically significant (p > 0.05, paired sample t-test). This result is interesting because VLM vision is also necessary for the exploration phase, where there was no discrepancy in performance. A likely reason for this is that the iterative nature of the exploration task makes it robust to occasional errors. Because the model must re-list all objects placed at each step, chance errors made during one step do not propagate to later steps.",
                "To probe the reason for the gap in accuracy performance, we also computed results where we filtered out trajectories in which the vision step made an error (Figure 5b,d). In these results, accuracies for the Gemini and optimal agents are nearly identical and their differences with the random agent are statistically significant (p < 0.05, two sample t-test). These results suggest that errors in the vision step, rather than reasoning or exploration, are responsible for the relatively reduced accuracy in the Gemini agent condition. Investigating relative numbers of errors, there appear to be more vision errors in the Gemini agent condition than in the optimal or random agent conditions (Figure 8), although these differences aren’t statistically significant. Taken together, results in the Construction Lab show that the directed exploration capabilities of foundation models robustly generalize from text-based environments to embodied 3D environments, though overall accuracy of the system is somewhat reduced by imperfect performance of the VLM’s object and action recognition in videos. This indicates that the challenges of multi-modal reasoning from realistic simulated video could be addressed by focusing on the vision and action recognition capabilities of foundation models separately from their reasoning capabilities."
            ]
        },
        {
            "claim": "For single-feature-based rewards, we find that smaller models curiously perform better; for conjunction-based rewards, incorporating self correction into the model improves performance.",
            "evidences": [
                "p p Self-correction Building upon previous work exploring the self-correction capabilities of LLMs in mathematical problem-solving (e.g., Huang et al. (2023)), we investigate whether LLMs can self-correct within our task framework. We adapt self-correction prompts similar to those used in Zheng et al. (2024), prompting the model at each step to revise its output, including its reasoning traces. This process allows the model to verify its chosen action and make corrections if necessary. Figure 4 presents the results of this process for both single-feature task and conjunction task scenarios using Gemini 1.5 Pro, and the results for the Flash model is found in Figure 7 in the appendix. In single-feature tasks, it improves performance with up to 6 unique colors, but its benefits diminish with a larger number of colors. Notably, self-correction appears more effective in more complex conjunction tasks, either performing comparably, or slightly outperforming the base model."
            ]
        },
        {
            "claim": "• Empirical analysis: We conduct extensive experiments across various environments and tasks, and across several model variants and prompting strategies, to analyze the exploration performance and behaviors of LLMs and VLMs in interactive settings.",
            "evidences": [
                "4.2 EFFECTS OF PROMPTING AND CONTEXT LENGTH We also investigate whether existing methods for enhancing reasoning abilities in LLMs can improve performance on our tasks. We evaluate the impact of two techniques: 1) Self-correction: allowing the LLM to critique and revise its own reasoning, and 2) Increased inference time: providing the model with its previous response and additional time to reason. Self-correction Building upon previous work exploring the self-correction capabilities of LLMs in mathematical problem-solving (e.g., Huang et al. (2023)), we investigate whether LLMs can self-correct within our task framework. We adapt self-correction prompts similar to those used in Zheng et al. (2024), prompting the model at each step to revise its output, including its reasoning traces. This process allows the model to verify its chosen action and make corrections if necessary. Figure 4 presents the results of this process for both single-feature task and conjunction task scenarios using Gemini 1.5 Pro, and the results for the Flash model is found in Figure 7 in the appendix. In single-feature tasks, it improves performance with up to 6 unique colors, but its benefits diminish with a larger number of colors. Notably, self-correction appears more effective in more complex conjunction tasks, either performing comparably, or slightly outperforming the base model.",
                "Longer Inference Time We also explored whether encouraging the model to engage in more deliberate reasoning, by providing it with additional context, could improve performance. Instead of simply providing the model with its previous observations (actions and outcomes), we also included its reasoning traces, explaining why it selected previous actions. This approach, while increasing inference time, allows the model to reflect on its own chain-of-thought, potentially leading to improved reasoning and better decision-making. Figure 4 presents the results of incorporating the model’s reasoning traces. In both single-feature and conjunction tasks, this approach, which encourages more deliberate reasoning, yields comparable performance to the baseline approach without reasoning traces. Statistical comparisons across models and approaches In order to quantitatively assess whether model size or prompting method affected exploration efficiency, controlling for the number of colors, we performed analyses of covariance for the single-feature and conjunction tasks separately. First we compared Gemini 1.5 Pro to Gemini Flash: in the single-feature tasks Gemini Flash was significantly better (F(1, 7649) = 6.1, p < 0.05), in the conjunction tasks there is no significant difference. Second we compared the variants of each model to the base model: in the single-feature task there were no significant differences, however for Gemini 1.5 Pro in the conjunction task we found that the guided reasoning and self-correcting variants were significantly better than the base",
                "4.3 IMPACT OF REASONING AND IN-CONTEXT MEMORY Efficient exploration requires agents to reason effectively about exploration strategies and maintain a working memory of untried options. However, we observed suboptimal exploration performance by our agent on the conjunction task (Figure 3). To investigate this performance gap, we sought to disentangle the effects of reasoning and memory limitations. We provided Gemini with a guided reasoning strategy, eliminating the need for the agent to independently derive the optimal approach, refer to Table 4 in the appendix for detailed prompts). Figure 4 (b) demonstrates a clear and consistent performance improvement with guided reasoning, indicating that reasoning challenges contribute significantly to the performance gap. While imperfect adherence to the guided strategy could be a factor, the gap between the guided reasoning model and the optimal policy widens as the number of unique colors increases. This strongly suggests that memory constraints also play a crucial role in limiting the performance of the standard Gemini policy."
            ]
        },
        {
            "claim": "• Insights and implications: We provide a detailed discussion on the implications of our findings for future research in foundation models and the development of autonomous intelligent agents.",
            "evidences": [
                "In a text-based implementation, we evaluate leading foundation models across varying environment and reward complexities. We find that exploration efficiency remains relatively constant compared to an optimal baseline, even as complexity increases. However, performance declines with reward functions based on multiple features, partly due to limitations in policy translation and in-context memory use. (See Section 4.3 for details). Statistical analysis reveals that Gemini Flash excels with simpler reward functions, while Gemini Pro with self-correction performs better on those with multiple factors. This suggests a potential trade-off between model size/reasoning complexity and reward function complexity. Further research is needed to understand how iterative reasoning influences effective exploration strategies. In 3D environments, Gemini 1.5 Pro achieves near-optimal exploration efficiency, mirroring its performance in text-based settings. However, accurate interpretation of visual observations remains a challenge due to limitations in the vision system. Improving visual accuracy, potentially through fine-tuning, is important for achieving comparable performance in 3D embodied environments. The strong performance of foundation models in our exploration tasks motivates further research with more complex environments and methods for improving visual perception. Future directions include replacing the human actor in our 3D setup with language-conditioned agents (Abi Raad et al., 2024; Wang et al., 2023a;b; Feng et al., 2023; Tan et al., 2024) or utilizing real-world footage from head-mounted cameras to further enhance exploration capabilities. We are excited by the potential of foundation models to autonomously explore and test hypotheses in interactive environments, a crucial aspect of human learning and scientific progress. We anticipate further research in this promising area."
            ]
        },
        {
            "claim": "• Framework development: We propose a novel framework for evaluating the directed explo- ration capabilities of LLMs and VLMs in interactive environments, outlining methodologies for assessment in the zero-shot setting, without the need for fine-tuning or other post-training modifications.",
            "evidences": [
                "Existing RL environments (e.g., Todorov et al., 2012; Brockman et al., 2016; Tassa et al., 2018) often conflate exploration with other aspects of agent performance, making it difficult to isolate and assess a model’s inherent exploratory capabilities. Such aspects include sparse or deceptive rewards and noisy, non-stationary, or multi-agent environments. We therefore designed a suite of environments that allows us to systematically disentangle and control the factors influencing exploration. We begin with simplified, text-based environments where the world is abstractly represented. This allows us to assess the model’s ability to explore and update its beliefs based solely on textual information, without the complexities of visual perception and motor control. We then progress to a more naturalistic setting, evaluating the model on video inputs from agents acting in an embodied 3D environment. This transition allows us to assess the generalizability of exploratory behaviors to a setting more reflective of real-world applications.",
                "3.2 CONSTRUCTION LAB ENVIRONMENTS To further evaluate the foundation models in a 3D embodied environment, we implement an analogous task to the text-based environment in a factory-style simulation called Construction Lab. Construction Lab was introduced in [reference-anonymized] as a simulation environment that includes both game-like mechanics and simplified but non-trivial object manipulation and physical reasoning. In this work, we focus on a task that requires the player to operate a simple machine called the Exchanger. The Exchanger requires objects with specific properties to be placed on an input conveyor belt (Figure 2). If an object matches the requirement, the input is consumed, a green light shows for a few seconds, and an output object is produced on an output belt. If the object is invalid, the machine rejects it by reversing the input belt and a red error light is activated. No cues are provided regarding the correct input object required, and thus the task entails determining what the correct object properties are through trial and error, observing how the machine responds to input objects, and drawing appropriate inferences. Through the use of this 3D, visually rich environment that mirrors the challenges of the text-based environment, we are able to disentangle the effects of visual complexity and imperfect image understanding from the patterns of performance related to reasoning from language alone."
            ]
        }
    ]
}