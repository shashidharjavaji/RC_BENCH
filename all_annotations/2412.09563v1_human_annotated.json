{
    "annotations": [
        {
            "claim": "We find that intermediate layers often yield more informative representations for downstream tasks than the final layers.",
            "evidences": [
                "( , ) Our findings indicate that intermediate layers consistently outperform the final layer across all three architectures (Table 1). Selecting the best-performing intermediate layer yields at least a 2% improvement in average accuracy compared to using the last layer. While prior work (Fan et al., 2024) noted similar trends for generation tasks, our results extend this observation to embedding-based evaluations."
            ]
        },
        {
            "claim": " To measure the representation quality, we adapt and apply a suite of metrics—such as prompt entropy, curvature, and augmentation-invariance—originally proposed in other contexts. ",
            "evidences": [
                "InfoNCE InfoNCE (Oord et al., 2018) provides a mutual information lower bound between paired augmentations. Lower InfoNCE loss suggests that augmentations of the same prompt map to similar representations, indicating invariance to perturbations. This loss is widely used to train augmentation- invariant networks in self-supervised learning for vision and is well-suited to capturing the semantic similarity underlying the augmented prompts (Chen et al., 2020a,b; Shwartz Ziv & LeCun, 2024; Ben-Shaul et al., 2023). DiME DiME (Skean et al., 2023) compares the alignment of paired samples to that of randomly paired samples. Similar to InfoNCE, it is used to estimate the mutual information between two augmented sets of prompts. DiME is grounded in the matrix-based entropy defined in Eq. 1. In essence, it quantifies how closely the pairings in (Z1, Z2) resemble each other, compared to pairings of (Z1, ΠZ2) for a permutation matrix Π. Higher DiME values imply that correct augmentation pairs yield representations that are significantly more similar than random pairings, indicating stronger augmentation invariance. LiDAR LiDAR (Thilak et al., 2024) employs a linear discriminant analysis (LDA) framework to assess how well augmentations of a single prompt cluster together. Each prompt is considered a separate class, with its augmentations serving as class samples. By examining the variances of the linear discriminant components, LiDAR quantifies the tightness of these clusters. Higher LiDAR scores indicate that augmentations belonging to the same prompt form more coherent groups, reflecting stronger invariance. To compute the LDA matrix, LiDAR uses augmentations to construct the class scatter matrix. In our setup, we use N classes (one for each prompt) and J = 16 samples per class. This is a larger sample size than the J = 2 used in DiME or InfoNCE, reflecting the more complex requirements of computing the LDA matrix.",
                "3.3.1 Token Embedding Diversity Metrics Token embedding diversity metrics evaluate the variability and richness of the representations at the token level within a single sequence. These metrics are designed to capture how distinctively each token is represented within the context of the entire prompt, providing insight into how effectively the model encodes information and differentiates between different parts of the input. Prompt Entropy: Following Wei et al. (2024), we use the α-order matrix-based entropy (Giraldo et al., 2014) as a surrogate for Rényi entropy. For a sequence of token representations Z ∈RL×d, theGram matrix is KZ = ZZ⊤. The entropy is: L 1 λi(KZ) Sα(Z) = log . (1) α! 1 −α i=1 tr(KZ) X In this context, prompt entropy quantifies the degree of diversity and dispersion in token embeddings within a single sequence. Higher entropy values indicate that the model preserves more nuanced and varied token-level information. Conversely, lower entropy suggests that the model compresses the input representations into fewer dimensions or patterns. As such, prompt entropy provides a useful measure of how well the model maintains complexity and richness in its intermediate representations. Unless otherwise specified, we use the limit case α = 1 in our calculations. At this limit, the metric is equivalent to the RankMe measure defined in Garrido et al. (2023). We explore the effects of different α values in Appendix D. For a more in-depth examination of prompt entropy, refer to Appendix C. Curvature As introduced by Hosseini & Fedorenko (2023), curvature measures how rapidly the direction between two adjacent token embedding vectors changes. Define their difference as vk = zk+1 −zk. The average curvature of a prompt is: 1 L−2 v⊤ k+1vk ¯C = arccos . (2) ! L −2 k=1 ∥vk+1∥∥vk∥ X"
            ]
        },
        {
            "claim": "Our empirical study reveals significant architectural differences, how representations evolve throughout training, and how factors like input randomness and prompt length affect each layer.",
            "evidences": [
                "4.2 Downstream Performance and Entropy Are Negatively Correlated We next examine how prompt entropy relates to downstream performance on the Massive Multitask Language Understanding (MMLU) benchmark (Hendrycks et al., 2021), which tests comprehensive knowledge across 57 diverse subjects, covering topics from elementary mathematics to professional law. We compare two similarly sized models, Llama3-8B and Mamba2-8B. Despite having the same parameter count, Llama3 achieves 63.85± 0.38% accuracy, far surpassing Mamba2’s 26.76± 0.37%.We hypothesize that Llama3’s intermediate layers compress information more effectively, helping it discard irrelevant details and focus on task-relevant features. As shown in Figure 1, the correlation between intermediate-layer entropy and MMLU performance in Llama3 is strongly negative (-0.43 between the second and later layers) (Figure 5). In contrast, Mamba2 shows no such relationship, nor evidence of similar compression (Figure 6).",
                "4.3.1 Architectural Differences Our analysis reveals notable differences in representation quality between Transformer-based archi- tectures (e.g., Pythia) and SSMs (e.g., Mamba). Figure 1 compares entropy, InfoNCE, LiDAR, and DiME metrics as a function of model depth, normalized to allow fair comparisons across models with different numbers of layers. For entropy and LiDAR, Pythia shows a pronounced decrease at intermediate layers, suggesting greater information compression and consolidation. In contrast, Mamba maintains more stable values, indicating less compression in its intermediate representations. Meanwhile, Mamba exhibits lower DiME and InfoNCE values than Pythia, implying reduced variability in its intermediate-layer representations. Overall, these metric shifts are more pronounced in Pythia than in Mamba, suggesting that Pythia undergoes stronger representational transformations at intermediate depths. By comparison, Mamba’s representations remain more uniform across layers. These differences may influence how each model encodes and leverages information for downstream tasks.",
                "The results show that the most significant changes occur in the intermediate layers. As training progresses, prompt entropy in these layers decreases, indicating that the model is learning to compress and abstract input information more efficiently. In contrast, the InfoNCE metric peaks in the intermediate layers, suggesting that the representations become more distinct. Meanwhile, LiDAR and DiME values both decline, reflecting a reduction in variability along certain representational dimensions. Interestingly, the metrics for the earliest layers remain relatively stable throughout training. This observation aligns with the detokenization hypothesis proposed by (Lad et al., 2024), which posits that initial layers primarily handle the mapping of raw input tokens into an initial embedding space. Their roles appear to solidify early on, exhibiting less ongoing change than the intermediate layers.",
                "4.3.3 Prompt Entropy under Extreme Input Conditions To gain deeper insights into how prompt entropy behaves under various input perturbations, we investigate the impact of extreme prompt modifications on the model’s internal representations. Specifically, we analyze how prompt entropy evolves across different layers of the Pythia 410M model when subjected to high levels of token repetition, randomness, or increased prompt length. We design three types of extreme prompts: 1. Prompts with Increasing Token Repetition: We select 1,000 standard prompts from the WikiText dataset and randomly replace tokens with a fixed token from the prompt at varying probabilities p. As p increases, the amount of repetition in the prompt increases. 2. Prompts with Increasing Token Randomness: We introduce randomness by randomly substituting tokens in the prompts with arbitrary tokens from the vocabulary at varying probabilities p. Higher values of p correspond to greater randomness in the prompts. 3. Random Prompts of Increasing Length: We generate random prompts by sampling tokens uniformly from the vocabulary, creating prompts of varying lengths T.",
                "1. Increasing token repetition reduces entropy in intermediate layers. As the probability p of token repetition rises, the model compresses redundant information, leading to lower entropy values in the middle layers. This compression indicates that the model effectively recognizes and encodes repetitive patterns within the input. 2. Increasing token randomness elevates entropy, particularly in initial layers. Introducing random tokens enhances the diversity of token representations, resulting in higher entropy values. The initial layers exhibit the most significant increases, suggesting that these layers are more sensitive to input noise and variability. 3. Prompt length influences entropy in Both normalized and unnormalized Forms. Unnormalized entropy naturally grows with prompt length due to the increased number of tokens. Although not displayed, normalized entropy demonstrates sublinear growth, implying that each additional token contributes progressively less to the overall diversity as the prompt lengthens. These findings illustrate that extreme input conditions distinctly affect the model’s internal representa- tions, especially within intermediate layers. The varying compression and encoding behaviors based on the nature of input perturbations provide valuable insights into the model’s processing mechanisms and its capacity to maintain or reduce information complexity under different scenarios.",
                "4.4 Bimodal Behavior in Prompt Entropy During our analysis of average prompt entropy across different layers, we identified an intriguing phenomenon: a distinct bimodal distribution of entropy values in certain layers of Transformer models, which was absent in SSMs. Figure 4 presents the entropy distributions for both the WikiText and AI-Medical-Chatbot datasets (Vsevolodovna, 2024). Notably, the AI-Medical-Chatbot dataset exhibits a pronounced bimodal distribution in the middle layers of Transformer models. This suggests that the model processes some prompts in fundamentally different ways at these intermediate stages. To investigate the underlying causes of this bimodality, we conducted several experiments detailed in Appendix A. Our findings indicate that factors such as prompt length, semantic complexity, and overlap with training data do not account for this behavior. Consequently, the root cause of the bimodal entropy distribution remains an open question.",
                "5 Discussion and Conclusion In this work, we explored the representation quality of intermediate layers in LLMs, providing insights into their critical role in downstream task performance. By applying a diverse set of evaluation metrics, including prompt entropy, curvature, InfoNCE, LiDAR, and DiME, we highlighted distinct behaviors in Transformer-based architectures and SSMs. Our findings demonstrate that intermediate layers"
            ]
        },
        {
            "claim": "Overall, our results illuminate the internal mechanics of LLMs and guide strategies for architectural optimization and training.",
            "evidences": [
                "In this work, we explored the representation quality of intermediate layers in LLMs, providing insights into their critical role in downstream task performance. By applying a diverse set of evaluation metrics, including prompt entropy, curvature, InfoNCE, LiDAR, and DiME, we highlighted distinct behaviors in Transformer-based architectures and SSMs. Our findings demonstrate that intermediate layers often outperform final layers in representation quality, underscoring their significance for feature extraction and transfer learning.",
                "Transformers exhibited greater representational variability and information compression within intermediate layers, whereas SSMs displayed more stable and consistent representations. This suggests differing strategies in encoding information, with Transformers excelling in adaptability and SSMs prioritizing robustness. Furthermore, the training analysis revealed that the most substantial improvements in representation quality occur in intermediate layers, reinforcing their importance in learning dynamics. Our investigation into extreme input conditions revealed that intermediate layers play a pivotal role in adapting to diverse input scenarios, with distinct responses to token repetition, randomness, and prompt length. Additionally, the observation of bimodal entropy distributions in intermediate layers of Transformer models remains an open question, offering avenues for further research. In conclusion, our research advances the understanding of internal representation dynamics in LLMs, highlighting the pivotal role of intermediate layers and the distinct behaviors of different architectures. These findings not only enhance the theoretical knowledge of model representations but also provide practical guidance for optimizing model design, training, and application. Future work should delve deeper into the causes of phenomena such as bimodal entropy distributions and explore the development of new metrics specifically tailored to LLMs to further enhance representation evaluation."
            ]
        }
    ]
}