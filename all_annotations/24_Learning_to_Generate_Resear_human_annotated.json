{
    "annotations": [
        {
            "claim": "Our framework provides a balanced approach to research ideation, achieving high-quality outcomes by dynamically navigating the trade-offs among novelty, feasibility, and effectiveness.",
            "evidences": [
                "The baseline models establish foundational performance levels, with T5-SFT and T5-RLHF showing modest im- provements in Feasibility and Effectiveness due to rein- forcement learning, though their Novelty scores remain limited by the lack of mechanisms to encourage innova- tion. In contrast, LLaMA2-SFT achieves higher overall scores, benefiting from larger model capacity and supe- rior pretraining, yet its reliance on supervised fine-tuning leaves room for enhancement through reinforcement learn- ing and control strategies. Adding targeted controls to LLaMA2-RLHF demonstrates the potential for metric-specific optimizations. For in- stance, introducing Novelty Control significantly boosts creativity while maintaining balanced practicality and performance, highlighting the feasibility of improving originality without major trade-offs. Similarly, Feasibility Control achieves the highest observed feasibility, albeit with minor reductions in novelty and effectiveness, show- casing its focus on practicality. The Effectiveness Control, on the other hand, enhances impact without compromising the balance across dimensions. When all controls are combined, Static Decoding provides reliable, balanced performance, but its fixed nature lim- its adaptability. In contrast, Dynamic Decoding emerges as the most effective approach, leveraging contextual dy- namic strategy to balance creativity, practicality, and im- pact, ultimately producing higher-quality ideas. These results show the importance of rl and dynamic con- trol in tailoring model behavior to complex requirements, while also illustrating trade-offs inherent in single-metric optimizations. To validate the observed improvements, we conducted paired t-tests to evaluate statistical significance. Results show that LLaMA2-RLHF + Novelty Ctrl achieved a sta- tistically significant improvement in Novelty (p-value < 0.01) compared to LLaMA2-RLHF without controls. Sim- ilarly, Feasibility Ctrl significantly enhanced Feasibility (p-value < 0.05), while Effectiveness Ctrl showed a no- table gain in Effectiveness (p-value < 0.05). Furthermore, Dynamic Decoding demonstrated statistically significant improvements across all metrics (p-value < 0.01) com-"
            ]
        },
        {
            "claim": "• We propose a research ideation framework to dynamically control the optimization of the generated idea towards novelty, feasibility, and effectiveness.",
            "evidences": [
                "Overview Suppose we have a training set D = {Xi, Yi}N i=1, whereXi and Yi are research paper and idea, respectively. Then we fine-tune the language model M with the trainingset. Thereafter, we collect a reward training set Dr = e n f {(Xr i , Y i , Y i , Y i )N i=1}, where Xi include the n textual f e con-tent of research paper and research idea, and Y i , Y i , Y i are the labels which show the scores of novelty, feasibility, and effectiveness of research idea. We could utilize this training set to train three reward models as follows, Fn = Rn(Xr i , Y i n |Θn), f (1)  Ff = i , Y i Rf(Xr |Θf),   Fe = i , Y i e Re(Xr |Θe)where Θn/f/e is   the parameters of the reward model Rn/f/e. Rn/f/e denotes reward models that aim to score the novelty,feasibility, and effectiveness of the research idea. Fn/f/e is reward values from reward models. Then, we use a set of i=1 as input to the language modelNf research papers {Pi}Nfto generate research ideas, which are assessed with reward models based on three criteria: novelty, feasibility, and effec- tiveness. Finally, we conduct reinforcement learning on the language model as, H = M(P|Θm, Θn, Θf, Θe), (2) where Θm is final optimized parameters of the language model M. During which the dimensional controllers arejointly trained to improve its ability to generate high-quality research ideas with fine-grained control at inference time. During this process, three dimensional controllers are trained jointly with the language model to enable fine-grained control at inference time.",
                "Multi-dimension Feedback Collection. To train reward models, we need to collect three kinds of feedback. Similar to the supervised fine-tuning stage, we also use the papers from ICLR2. Specifically, we collect the review data from OpenReview platform3, and we also get the research idea by prompting the language model M. For the Novelty score ofthe research idea in ICLR 2023, we could use the novelty score from the review directly. As for ICLR 2024, we prompt the LLM to get novelty scores since they don’t provide direct ratings (see Appendix for prompt). Similarly, since there is no feasibility score or effectiveness score in the review, we prompt the LLM to get scores for every research idea. Fea- sibility score is mainly based on the experiment setup and method sections, taking into account factors such as dataset size, model complexity, and relevant review comments, while Effectiveness score is derived primarily from the experimen- tal results and corresponding review comments. The detailed Scoring Criteria for Novelty, Feasibility, and Effectiveness are outlined in Appendix .",
                "Multi-dimension Reward Augmented Controllable Reinforcement Learning In this stage, we fine-tune the research idea proposer with controllable steering through reinforcement learning ??, re- fining the model based on feedback across three dimensions: novelty, feasibility, and effectiveness. Dimensional Controllers Inspired by the existing work (Han et al. 2024), we introduce the dimensional controllers of the novelty, feasibility, and effectiveness of the generated idea, as these dimensions often exhibit interdependency and",
                "g y y Goal-driven Dynamic Decoding. The goal of achieving a good research idea is not only to blindly improve the result of a certain dimension but also to consider the overall quality. For example, too high a degree of novelty may result in a low effectiveness (Si, Yang, and Hashimoto 2024a), while different parts of a research idea, such as the method and experiment planning, may require varying levels of focus on novelty and feasibility. Therefore, how to balance novelty, feasibility, and effectiveness in the inference stage is impor- tant for generating a good idea. To achieve this, we utilize an RNN (Sherstinsky 2020) to predict the steer value ϵn, ϵf, and ϵe, because RNN is good at sequence-level prediction (Figure 3). To optimize the RNN for steer values prediction, we first collect 1,000 high-quality research ideas generated with Idea Proposer (above 8 in overall score). hereafter, we get the cor- responding controller weights using our three reward models for each sentence of the high-quality research idea. Specifi- cally, we feed each sentence in the research idea into our re- ward models to get the rewards as ˆrn, ˆrf, ˆre. Furthermore, we normalize the reward and get the corresponding steer values of each sentence as ˆϵn/f/e = (ˆrn/f/e −sn/f/e)/(an/f/e − sn/f/e) × w′, where sn/f/e and an/f/e are the minimumvalue and maximum value for all rewards and w′ is the max- imal controller weight, which is 5 in our case. This reflects the controller-weight ratios between 3 controllers, as well as the absolute scale of each controller weight from 0-5. After the data collection, we can use the pair (St, st+1 n/f/e) to train the model as follows, Lrnn = CE(RNN(S<t), st n/f/e), (8) where S<t is the previous t −1 sentences in the researchidea and st n/f/e is steer values ϵn, ϵf, and ϵe of t-th sentence. Therefore, we can use the well-trained RNN to predict the controller weights of the next sentence based on the current generated sentence in the inference phrase."
            ]
        },
        {
            "claim": "• We train our reward models using collected real-world datasets, enabling research idea scoring in a fine-grained manner.",
            "evidences": [
                "We collect a dataset of 6,765 usable research papers in to- tal submitted to ICLR4and NeurIPS5 in the years 2023 and 2024, including both accepted and rejected submissions and filtered 5,687 usable data. 4,271 of them are used for training, and 500 are sampled for evaluation. Each paper contains its abstract, methodology, and experiment sections. Addition- ally, review data from OpenReview6 provides human ratings for overall quality as well as the review contents and key sub-dimensions - novelty, feasibility, and effectiveness. Paper content is scraped with title from the Semantic Scholar7 and arXiv APIs8 and then cleaned up with regular expression to extract corresponding sections. These papers and ratings are used to: 1. Derive ground-truth ideas for supervised fine- tuning. 2. Train reward models for the key dimensions. 3. Optimize idea generation using reinforcement learning with multi-dimensional steering. The dataset is split into the following subsets: 1. Supervised Fine-Tuning split.: We use 1,000 papers from only ICLR to derive the golden generated idea, paired with the most supporting related work idea as input to fine-tune the model.",
                "2. Reinforcement Learning split.: 3,271 research papers from both ICLR and NeurIPS with detailed reviews are used to train three distinct reward models for novelty, feasibility, and effectiveness, each capturing expert evalu- ations for further reinforcement learning. 3. Evaluation split.: 500 research papers from both ICLR and NeurIPS are sampled for evaluation, of which 30 are randomly selected for manual expert evaluation."
            ]
        },
        {
            "claim": "• We conduct a comprehensive evaluation with a human study, demonstrating the effectiveness of our proposed method for optimized, controllable research ideation.",
            "evidences": [
                "ations for further reinforcement learning. 3. Evaluation split.: 500 research papers from both ICLR and NeurIPS are sampled for evaluation, of which 30 are randomly selected for manual expert evaluation.",
                "Evaluation The evaluation is performed on two datasets: 500 papers of the evaluation split for automatic evaluation, and a subset of 30 papers are selected for manual expert eval- uation. We measure performance across three core metrics (details in Appendix): • Novelty: Evaluates how original and creative the gener- ated ideas are, compared to existing works. • Feasibility: Assesses the practical implementation and the likelihood that the idea can be executed within typical resource constraints. • Effectiveness: Measures the potential improvement or impact of the generated idea when compared to baseline models.",
                "2. Manual Evaluation: For manual evaluation, we select 30 papers and have domain experts assess the quality of the generated ideas of the selected model (SFT, RLHF and RLHF with Dynamic Controls), providing human scores for novelty, feasibility, and effectiveness. These scores are then compared with the scores generated by our automatic reviewing agent to measure the alignment between human judgment and the agent’s reviews."
            ]
        }
    ]
}