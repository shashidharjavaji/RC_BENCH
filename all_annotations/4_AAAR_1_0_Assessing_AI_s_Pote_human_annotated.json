{
    "source_pdf": "/Users/student-lab-1/Documents/PhD/R-Claims-Data/Shashi_papers/4_AAAR_1_0_Assessing_AI_s_Pote.pdf",
    "annotations": [
        {
            "claim": "In this study, we intro- duce AAAR-1.0, a benchmark dataset designed to evaluate LLM performance in three fundamental, expertise-intensive research tasks: (i) EQUATIONINFERENCE, assessing the cor- rectness of equations based on the contextual information in paper submissions; (ii) EXPERIMENTDESIGN, designing ex- periments to validate research ideas and solutions; and (iii) PAPERWEAKNESS, identifying weaknesses in paper submis- sions.i",
            "evidences": [
                "EQUATIONINFERENCE Writing a correct scientific equation is challenging because it involves an in-depth understanding towards an algorithm or the relations between the massive notations. However, di- rectly asking LLMs to generate equations is over-challenging. For this reason, in this task, we adopt the conventional multi- choice classification paradigm for building EQINFER , as shown in Figure 1.",
                "Final data. We finally shuffle the four equations for each classification instance and randomly assign letters (A, B, C, and D) to the equations. We show the data statistics of the final EQINFER in Table 4 and the sample data cases in Ap- pendix .",
                "EXPERIMENTDESIGN Given a research topic, such as a novel ML algorithm, a qualified researcher can design a solid experiment plan for it, and clarify underlying motivation to ensure the reliability of the designed experiment. Unlike the concurrent works that focus on the experiment implementation (Lu et al. 2024; Huang et al. 2024), we emphasize the importance of assess- ing the high-level experiment design of LLMs before the subsequent implementation to avoid any expensive execu- tion iteration. Therefore, as shown in Figure 1, we formu- late EXPDESIGN as a text-generation task that takes pre- experiment paper context as input, and then generates the experiment and explanation list.",
                "PAPERWEAKNESS Another critical research task is paper review. Previous works have demonstrated the usefulness of the LLM-based review feedback (Gao, Brantley, and Joachims 2024; Jin et al. 2024; Lu et al. 2024). However, as indicated by Du et al. (2024); Liang et al. (2024), LLMs only excel at summarizing the research strengths while falling significantly short on weak- ness criticism. Hence, we build WEAKNESS for particularly investigating the LLM-generated weaknesses."
            ]
        },
        {
            "claim": " AAAR-1.0 differs from prior benchmarks in two key ways: first, it is explicitly research-oriented, with tasks requir- ing deep domain expertise; second, it is researcher-oriented, mirroring the primary activities that researchers engage in on a daily basis.",
            "evidences": [
                "PAPERWEAKNESS Another critical research task is paper review. Previous works have demonstrated the usefulness of the LLM-based review feedback (Gao, Brantley, and Joachims 2024; Jin et al. 2024; Lu et al. 2024). However, as indicated by Du et al. (2024); Liang et al. (2024), LLMs only excel at summarizing the research strengths while falling significantly short on weak- ness criticism. Hence, we build WEAKNESS for particularly investigating the LLM-generated weaknesses.",
                "EXPERIMENTDESIGN Given a research topic, such as a novel ML algorithm, a qualified researcher can design a solid experiment plan for it, and clarify underlying motivation to ensure the reliability of the designed experiment. Unlike the concurrent works that focus on the experiment implementation (Lu et al. 2024; Huang et al. 2024), we emphasize the importance of assess- ing the high-level experiment design of LLMs before the subsequent implementation to avoid any expensive execu- tion iteration. Therefore, as shown in Figure 1, we formu- late EXPDESIGN as a text-generation task that takes pre- experiment paper context as input, and then generates the experiment and explanation list.",
                "EQUATIONINFERENCE Writing a correct scientific equation is challenging because it involves an in-depth understanding towards an algorithm or the relations between the massive notations. However, di- rectly asking LLMs to generate equations is over-challenging. For this reason, in this task, we adopt the conventional multi- choice classification paradigm for building EQINFER , as shown in Figure 1. ①D t li d l i F th d t"
            ]
        },
        {
            "claim": "Closed-source LLMs generally outperform open-source LLMs on AAAR-1.0, likely due to their richer scientific knowledge stemming from a larger model size.",
            "evidences": [
                "Q: do more contexts boost performance? Table 1 unifiesthe input context lengths to 1,000 words for various LLMs. In this paragraph, we experiment with long-context LLMs to in- vestigate the impact of the input context lengths. Particularly, we scale the input length (per side) from 100 to 1,500 words. As shown in Figure 4, for the open-source LLMs (Llama and Qwen), after 300 words length, increasing the input context doesn’t help the performance and even significantly drops Qwen’s scores. While for the closed-source GPT-4-Turbo and GPT-4o, scaling up input length gradually boosts the per- formances at the first 1,000 words, but stabilizes afterwards.",
                "while the score difference is not significant. Furthermore, we find the negative correlation between S-Match and the ROUGE, where the ROUGE scores of closed-source LLMs are broadly inferior. We find that the open-source LLMs often try to copy the terms or phrases from the given experiment, or even simply paraphrase the experiment instead of explaining, which results in a high superficial overlap with the ground- truth explanation.",
                "Main results. Table 3 shows the main results, where the closed-source LLMs’ overall performances are generally su- perior to the results of open-source LLMs. ",
                "Review Diversity Methods SN-F1 (%) SN-Precision (%) SN-Recall (%) ITF-IDF (↑) Peer Review — — — 7.69 Open-source LLMs OLMo-7B (Groeneveld et al. 2024) 43.25 40.38 47.04 2.45 Falcon-40B (Almazrouei et al. 2023) 27.34 25.13 30.88 1.06 Gemma 2-27B (Gemma Team, 2024) 35.85 34.68 37.91 1.43 Mistral-7B (Jiang et al. 2023) 42.03 43.80 40.77 1.17 Mixtral-8x22B-MoE (Jiang et al. 2024) 43.23 44.59 42.23 0.98 Llama 3.1-70B (MetaAI 2024) 42.78 43.19 42.70 2.60 Qwen 2.5-72B (Qwen Team, 2024) 42.74 43.80 42.05 1.21 Closed-source LLMs Gemini 1.5 Pro (Anil et al. 2023) 48.75 43.97 55.08 5.88 Claude 3.5 sonnet (Anthropic 2024) 47.85 41.97 56.00 3.91 GPT-4 (OpenAI et al. 2023) 47.66 42.15 55.19 5.31 GPT-4o (OpenAI 2024a) 47.73 42.09 55.48 5.95 o1-preview (OpenAI 2024b) 48.62 42.54 57.08 5.63 LLM Agent Framework AI-SCI (GPT-4o) (Lu et al. 2024) 45.05 40.02 51.91 2.23 Table 3: Various LLMs’ performances on the 993 instances of WEAKNESS ."
            ]
        },
        {
            "claim": "• Contrary to human behaviour, neither extending the input modality (i.e., leveraging text and figures) nor enlarging the input context guarantees enhanced perfor- mance. This underlines most current LLMs’ limitations in processing diverse, extensive information coming from scientific documents.",
            "evidences": [
                "Q2: does multi-modal input boost performance? Ourdataset covers both tables and figure illustrations extracted from the paper PDF as inputs. Intuitively, when reviewing a paper, both figures and tables are critical, not only for a better understanding, but also because some weaknesses are related to tables/figures.6 Therefore, in Table 11, we adopt two MLLMs to investigate the effectiveness of image inputs. Overall, image information, including both figures and ta- bles, doesn’t bring significant performance improvement, i.e., only InternVL2 gains a performance boost after incorporat- ing figures; while tables slightly drop both models’ results. This is probably because the MLLMs cannot reason well over the information-intensive images, especially the table images (Deng et al. 2024).",
                "Q3: do more contexts boost performance? We also in-vestigate the impact of input context length for EXPDESIGN. As shown in Figure 5, we scale up the input pre-experiment context length from 0.1k to 10k words (10k words is the max- imum paper context length in the dataset). For the experiment planning, more input context does improve the performance of different LLMs, while this benefit stops after exceeding 5k words, which is similar to EQINFER’s scaling results — after the necessary information has been covered, scaling more up doesn’t boost the performance. Meanwhile, the results of the motivation explanation demonstrate that explaining motivations almost doesn’t require any paper context, i.e., the LLMs solely rely on the given experiments. However, we do not expect this because we hope LLMs can explain the motivation based on a thorough understanding of the pa- per, just like how human experts do. Hence, there is still a considerable gap between the LLMs and humans in terms of grasping research motivations."
            ]
        },
        {
            "claim": "• LLM-designed experiments are innovative and more diverse than those by humans; however, many are trivial, lack feasibility, and stray from the original research objectives.",
            "evidences": [
                "We find that closed-source LLMs are morecreative in experiment design and tend to generate more ex- periment ideas than open-source LLMs (though most of the experiment ideas are trivial), leading to excellent S-Recall. As for the motivation explanation, the S-Match scores of closed-source LLMs still surpass the open-source LLMs, while the score difference is not significant. Furthermore, we find the negative correlation between S-Match and the ROUGE, where the ROUGE scores of closed-source LLMs are broadly inferior. We find that the open-source LLMs often try to copy the terms or phrases from the given experiment, or even simply paraphrase the experiment instead of explaining, which results in a high superficial overlap with the ground- truth explanation. This observation highlights the importance of adopting the proposed S-Match to avoid evaluation bias of traditional generation metrics."
            ]
        },
        {
            "claim": "• LLM-generated weaknesses often lack ample domain knowledge, especially on cutting-edge research topics, leading to the vague weaknesses applicable to various papers.",
            "evidences": [
                "However, there is still a considerable gap in the weakness diversity between the LLMs and human experts.5 Compared with human review, most LLM-generated weaknesses are vague and lack the necessary knowledge about some frontier research works. Surprisingly, AI-SCI performs worse than backbone GPT-4o, especially on ITF-IDF, which suggests the challenge of WEAKNESS, i.e., simply adopting popular prompting techniques cannot well address this task. "
            ]
        }
    ]
}