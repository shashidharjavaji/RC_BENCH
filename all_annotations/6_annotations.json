[
    {
        "claim_id": 1,
        "claim_text": "We introduce kNN-LM, an approach that extends a pre-trained LM by linearly interpolating its next word distribution with a k-nearest neighbors (kNN) model. The nearest neighbors are computed according to distance in the pre-trained embedding space and can be drawn from any text collection, including the original LM training data. This approach allows rare patterns to be memorized explicitly, rather than implicitly in model parameters.",
        "evidence_text": "Language models (LMs) assign probabilities to sequences. Given a context sequence of tokens ct = (w1, . . . wt−1), autoregressive LMs estimate p(wt|ct), the distribution over the target token wt.  The kNN-LM involves augmenting such a pre-trained LM with a nearest neighbors retrieval mechanism, without any additional training (the representations learned by the LM remain unchanged). This can be done with a single forward pass over a text collection (potentially including the original LM training set), where the resulting context-target pairs are stored in a key-value datastore that is queried during inference, as illustrated in Figure 1. Datastore Let f (·) be the function that maps a context c to a fixed-length vector representation computed by the pre-trained LM. For instance, in a Transformer LM, f (c) could map c to an intermediate representation that is output by an arbitrary self-attention layer. Then, given the i-th training example (ci, wi) ∈ D, we define the key-value pair (ki, vi), where the key ki is the vector representation of the context f (ci) and the value vi is the target word wi. The datastore (K, V) is thus the set of all key-value pairs constructed from all the training examples in D: Inference At test time, given the input context x the model generates the output distribution over next words pLM(y|x) and the context representation f (x). The model queries the datastore with f (x)  to retrieve its k-nearest neighbors N according to a distance function d(·, ·) (squared L2 distance in our experiments, making the similarity function an RBF kernel).Then, it computes a distribution over neighbors based on a softmax of their negative distances, while aggregating probability mass for each vocabulary item across all its occurrences in the retrieved targets (items that do not appear in the retrieved targets have zero probability): pkNN(y|x) ∝  ∑  (ki ,vi )∈N  1y=vi exp(−d(ki, f (x))). Finally, we follow Grave et al. (2017a) and interpolate the nearest neighbor distribution pkNN with the model distribution pLM using a tuned parameter λ to produce the final kNN-LM distribution:  p(y|x) = λ pkNN(y|x) + (1 − λ) pLM(y|x).",
        "justification_conclusion": "The evidence explains the kNN-LM approach."
    },
    {
        "claim_id": 2,
        "claim_text": "To better measure these effects, we conduct an extensive empirical evaluation. Applying our kNN augmentation to a strong WIKITEXT-103 LM using only the original dataset achieves a new stateof-the-art perplexity of 15.79 – a 2.86 point improvement over the base model (Baevski & Auli, 2019) – with no additional training.",
        "evidence_text": "We first experiment with creating a datastore from the same data used to train the LM. Table 1 shows that kNN-LM improves perplexity on WIKITEXT-103 from 18.65 (Baevski & Auli, 2019) to a new state-of-the-art of 16.12.",
        "justification_conclusion": "The evidence text supports the claim that kNN-LM improves perplexity."
    },
    {
        "claim_id": 3,
        "claim_text": "We also show that the approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore. Training a model on 100-million tokens and using kNN search over a 3-billion token dataset can outperform training the same model on all 3-billion tokens, opening a new path for efficiently using large datasets in language models.",
        "evidence_text": "Section 4.1 has shown that retrieving neighbors from the training data can significantly improve language modeling performance. This raises the question: can retrieving nearest neighbors from data be a substitute for training on it? To test this, we train a LM on WIKI-100M and use it to build a datastore from WIKI-3B, a corpus 30 times larger than the training set. We then compare this kNN-LM to a vanilla LM trained on the entire WIKI-3B corpus. Table 3 shows that, as expected, the model trained on 3B tokens dramatically outperforms the model trained on 100M tokens, improving perplexity from 19.59 to 15.17. However, adding nearest neighbors retrieval over those 3B examples to the model trained on 100M tokens improves perplexity from 19.59 to 13.73; i.e. retrieving nearest neighbors from the corpus outperforms training on it.",
        "justification_conclusion": "The evidence suggests that kNN-LM can outperform training on large datasets."
    }
]