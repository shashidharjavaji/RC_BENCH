[
    {
        "claim_id": 1,
        "claim_text": "We propose a self-supervised multimodal opinion summarization framework called MultimodalSum.",
        "evidence_text": "To use the abundant information contained in non-text data, we propose a self-supervised multimodal opinion summarization framework called MultimodalSum. Our framework obtains a representation of each modality using a separate encoder for each modality, and the text decoder generates a summary.",
        "justification_conclusion": "True. The claim is supported by the evidence. The evidence describes the proposed self-supervised multimodal opinion summarization framework called MultimodalSum."
    },
    {
        "claim_id": 2,
        "claim_text": "We propose a multimodal training pipeline to resolve the heterogeneity of multimodal data.",
        "evidence_text": "To address this challenge, we propose a multimodal training pipeline. The pipeline regards the text modality as a pivot modality. Therefore, we pretrain the text modality encoder and decoder for a specific business or product via the self-supervised opinion summarization framework.",
        "justification_conclusion": "True. The pipeline effectively aligns representations from heterogeneous modalities, which is a critical innovation described and evaluated experimentally."
    },
    {
        "claim_id": 3,
        "claim_text": "MultimodalSum achieves superior results on Yelp and Amazon datasets compared to existing frameworks.",
        "evidence_text": "MultimodalSum showed superior results compared with extractive and abstractive baselines for both token-level and document-level measures. From the results, we conclude that the multimodal framework outperformed the unimodal framework for unsupervised opinion summarization.",
        "justification_conclusion": "True. Experimental results, including metrics such as ROUGE and BERT-score, validate this claim."
    },
    {
        "claim_id": 4,
        "claim_text": "The table modality contributes more to the summarization task compared to the image modality.",
        "evidence_text": "Another characteristic of the result is that aggregated values of the table were higher than those of the image: mean values for the table and image in the entire test data were 0.103 and 0.045, respectively.",
        "justification_conclusion": "True. The higher contribution of table metadata is attributed to its structured and detailed information, which is validated by analysis of the multimodal gates."
    },
    {
        "claim_id": 5,
        "claim_text": "Our study is the first work on self-supervised multimodal opinion summarization.",
        "evidence_text": "This study is the first work on self-supervised multimodal opinion summarization; we propose a multimodal training pipeline to resolve the heterogeneity between input modalities.",
        "justification_conclusion": "True. The claim is supported by the evidence. The evidence states that this study is the first work on self-supervised multimodal opinion summarization."
    }
]