[
    {
        "claim_id": "1",
        "claim_text": "Implementation Invariance: The attributions are always identical for two functionally equivalent networks.",
        "evidence_text": "First, notice that gradients are invariant to implementation. In fact, the chain-rule for gradients $\\partial f / \\partial y = \\partial f / \\partial h \\times \\partial h / \\partial g$ is essentially about implementation invariance. To see this, think of g and f as the input and output of a system, and h being some implementation detail of the system. The gradient of output f to input g can be computed either directly by $\\partial f \\ \\partial g$ ignoring the intermediate function h (implementation detail), or by invoking the chain rule via h. This is exactly how backpropagation works.",
        "justification_conclusion": "Yes, the claim is supported by a mathematical proof."
    },
    {
        "claim_id": "2",
        "claim_text": "An attribution method satisfies sensitivity iff (1) For every input and baseline that differ in one feature but have different prediction then the differing feature should be given a non-zero attribution. (2) If the function implemented by the deep network does not depend (mathematically) on some variables, then the attribution to that variable is always zero.",
        "evidence_text": "The first point of the claim is addressed by the remark 2. In detail: 'Remark 2. Integrated gradients satisfies Sensivity(a) because Completeness implies Sensivity(a) and is thus a strengthening of the Sensitivity(a) axiom. This is because Sensitivity(a) refers to a case where the baseline and the input differ only in one variable, for which Completeness asserts that the difference in the two output values is equal to the attribution to this variable. Attributions generated by integrated gradients satisfy Implementation Invariance since they are based only on the gradients of the function represented by the network.' For the second part of the claim, the authors defer the proof to the established work Friedman 2004.",
        "justification_conclusion": "Yes, despite the evidence span from multiple sections. The claim is well supported by the evidence."
    },
    {
        "claim_id": "3",
        "claim_text": "Suppose that we linearly composed two deep networks modeled by the function $f_1$ and function $f_2$ to form a third network that models the function $a times f_1 + b times f_2$. Then, the attribution for the third network is expected to be the weighted sum of the attribution for $f_1$ and $f_2$ with weights $a$ and $b$, respectively.",
        "evidence_text": "N/A. The paper does not provide proof or any other evidence. The property is implied by the properties of integral and derivative.",
        "justification_conclusion": "Yes, the claim is correct but is not directly supported by the text in the paper."
    },
    {
        "claim_id": "4",
        "claim_text": "Integrated gradients satisfy completeness that the attributions add up to the difference between the output of $bold(F)$ at the input $bold(x)$ and the baseline $bold(x')$.",
        "evidence_text": "N/A. The property is implied by the properties of path integral.",
        "justification_conclusion": "Correct but lack direct proof from the text."
    },
    {
        "claim_id": "5",
        "claim_text": "An attribution method is symmetry-preserving if, for all inputs that have identical values for symmetric variables and baselines that have identical values for symmetric variables, the symmetric variables receive identical attributions.",
        "evidence_text": "The paper provides a detailed mathematical proof in Appendix A.",
        "justification_conclusion": "Yes, the claim is well supported."
    },
    {
        "claim_id": "6",
        "claim_text": "In Section 3, we use the axioms to identify a new method called integrated gradients.",
        "evidence_text": "In Section 6, we demonstrate the ease of applicability over several deep networks, including two images networks, two text processing networks, and a chemistry network. These applications demonstrate the use of our technique in either improving our understanding of the network, performing debugging, performing rule extraction, or aiding an end user in understanding the networkâ€™s prediction.",
        "justification_conclusion": "The authors provides a concrete example about how to applying the proposed method in computer vision and NLP."
    }
]
