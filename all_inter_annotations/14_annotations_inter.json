[
    {
        "claim_id": 1,
        "claim_text": "We introduce RETRO, a retrieval-enhanced autoregressive language model. We show that retrieving based on a pre-trained frozen BERT model works at scale, removing the need for training and updating a retriever network.",
        "evidence_text": " Retrieval neighbours... The corresponding key is the BERT embedding of N, averaged over time, that we denote BERT(N)... Performing retrieval on-the-fly is too slow to keep up with the training calculations—we leverage the frozen aspect of the embedding operator BERT to precompute all approximate nearest neighbours and save the results as part of the data&#8203;:contentReference[oaicite:1]{index=1}.",
        "justification_conclusion": "True"
    },
    {
        "claim_id": 2,
        "claim_text": "We show that our method scales well with model size and database size.",
        "evidence_text": "Figure 1. Scaling of RETRO. The performance gain of our retrieval models remains constant with model scale (left), and is comparable to multiplying the parametric model size by ∼10×. The gain increases with the size of the retrieval database (middle) and the number of retrieved neighbours (right)...",
        "justification_conclusion": "True"
    },
    {
        "claim_id": 2,
        "claim_text": "We propose an evaluation aware of proximity of test documents with the training set (x2.6), addressing the problem of test set leakage (Lee et al., 2021). We show that the performance of RETRO comes from both explicit neighbour copying and general knowledge extraction",
        "evidence_text": " RETRO outperforms baseline models at all leakage levels, down to α = 12.5%... Retrieval thus improves predictions on both chunks that are syntactically similar to chunks in the training set, and on chunks that are syntactically different from all training chunks. This points toward a non trivial RETRO capacity of generalizing based on both model parameters and retrieval database.",
        "justification_conclusion": "True"
    }
]