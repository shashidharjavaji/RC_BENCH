[
    {
      "claim_id": 1,
      "claim_text": "we are among the first to study the impact of both verbal and vocal features on financial markets, specifically, stock volatility.",
      "evidence_text": [
        "To the best of our knowledge, none of existing NLP research on stock volatility prediction considers the usage of vocal features from audio data, especially the interplay between vocal and verbal features."
      ],
      "justification_conclusion": "True"
    },
    {
      "claim_id": 2,
      "claim_text": "We empirically demonstrates that MDRM models outperform other benchmark methods significantly and substantially.",
      "evidence_text": [
        "The results show that our multimodal deep regression model (MDRM) outperforms all baselines. Using both text and audio data, the model has prediction error of 1.371, 0.420, 0.300 and 0.217 for 3-days, 7-days, 15-days and 30-days following the conference call respectively.",
        "Comparing with using past volatility only, the improvement gain is as substantial as 54.1% for 3-days prediction. The improvement over other baseline methods are 19.1% (tf-idf bag-of-words), 17.8% (word embeddings), 20.4% (simple fusion) respectively for 3-days prediction."
      ],
      "justification_conclusion": "True"
    },
    {
      "claim_id": 3,
      "claim_text": "we empirically show that multimodal learning with audio and text can indeed reduce prediction error, compared to previous work that relies on text only.",
      "evidence_text": [
        "Secondly, we empirically show that multimodal learning with audio and text can indeed reduce prediction error, compared to previous work that relies on text only.",
        "Multimodal (1.371) outperform unimodal (1.431) by 4.2%.",
        "MDRM (text+audio) significantly outperforms MDRM (text only) and MDRM (audio-only) model for 3-days, 7-days and 15 days stock volatility prediction."
      ],
      "justification_conclusion": "True"
    },
    {
      "claim_id": 4,
      "claim_text": "We construct a unique dataset containing conference call audio and text data of S&P 500 companies in recent years.",
      "evidence_text": [
        "We build our dataset by acquiring all S&P 500 companiesâ€™ quarterly earnings conference calls in 2017.",
        "The final dataset consists of 576 conference calls, with a total number of 88,829 sentences.",
        "We release our processed earnings conference calls dataset (text and audio) for readers who are interested in reproducing the results."
      ],
      "justification_conclusion": "True"
    }
  ]
  