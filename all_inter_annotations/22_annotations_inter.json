[
    {
        "claim_id": 1,
        "claim_text": "TruthfulQA is a benchmark made up of questions designed to cause imitative falsehoods. One reason to focus on imitative falsehoods is that they are less likely to be covered by existing question-answering benchmarks.",
        "evidence_text": [
            "The questions in TruthfulQA were designed to be \"adversarial\" in the sense of testing for a weakness in the truthfulness of language models (rather than testing models on a useful task). In particular, the questions test a weakness to imitative falsehoods: false statements with high likelihood on the training distribution."
        ],
        "justification_conclusion": "True"
    },
    {
        "claim_id": 2,
        "claim_text": "Larger models are less truthful. Across differ ent model families, the largest models were generally less truthful. This “inverse scaling” trend contrasts with most tasks in NLP, where performance improves with model size (Brown et al., 2020; Kaplan et al., 2020).",
        "evidence_text": [
            "Figure 2 shows that larger models generally do worse than smaller models in the same family (inverse scaling)."
        ],
        "justification_conclusion": "True"
    },

    {
        "claim_id": 3,
        "claim_text": "Models perform poorly on truthfulness compared to humans, even on questions designed to elicit misconceptions.",
        "evidence_text": [
             "The human participant produced 94% true answers (Fig. 4). Across all model sizes and prompts, the best model (GPT-3-175B with helpful prompt) produced 58% true answers..."
        ],
        "justification_conclusion": "True"
    },
    {
        "claim_id": 4,
        "claim_text": "GPT-judge, a finetuned GPT-3 model, achieves high accuracy in automatically evaluating truthfulness.",
        "evidence_text": [
            "The finetuned GPT-judge model is able to predict human evaluations of truthfulness with 90-96% validation accuracy."
        ],
        "justification_conclusion": "True"
    }
]