{
    "annotations": [
        {
            "claim": "we develop two modes of evaluation for natural language explanations that claim individual neurons represent a con- cept in a text input.",
            "evidences": [
                "In the observational mode, we evaluate the claim that a neuron a activates on all and only input strings that refer to a concept picked out by the proposed explanation E. Relative to a set of inputs, we can then use the error rates to assess the quality of E for a." ,
                "Thus, we propose an interven- tion mode to evaluate the claim that a is a causally active representation of the concept denoted by E. We construct next token prediction tasks that hinge on the concept and intervene on the neuron a to study whether the neuron is a causal mediator of concepts picked out by E." 
            ],
            "justification_conclusion": "True"
        },
        {
            "claim": "We apply our frame- work to the GPT-4-generated explanations of GPT-2 XL neurons of Bills et al. (2023) and show that even the most confident explanations have high error rates and little to no causal effi- cacy.",
            "evidences": [
                "Results over 300 neuron explanations are shown in Table 1. For single neuron without probing, the GPT-4 explanations have a mean F1 score of 0.56 (with a precision of 0.64 and a recall of 0.50), whereas the random baseline has a Fl score of zero." ,
                "GPT-4 generated explanations have similar causal effects as the random baseline on most tasks. The only exception is the explanation for neurons re- lated to numerical expressions, which has higher IIA than the random baseline, but still far below the token-activation correlation baseline." ,
                "In other words, if we were using GPT-4 gen- erated explanation to inform us which weights to modify in a model editing task, we would have sim- ilar performance as randomly selecting neurons to edit." 
            ],
            "justification_conclusion": "True"
        }
    ]
}