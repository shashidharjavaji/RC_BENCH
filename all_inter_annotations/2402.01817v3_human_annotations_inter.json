{
    "annotations": [
        {
            "claim": "Auto-regressive LLMs cannot perform planning or self-verification autonomously.",
            "evidences": [
                "We argue that auto-regressive LLMs cannot, by themselves, do planning or self-verification (which is after all a form of reasoning), and shed some light on the reasons for misunderstandings in the literature." ,
                "...establish[es] that LLMs cannot be used as planners or plan verifiers themselves (Section 2)." ,
                "Even from a pure engineering perspective, a system that takes constant time to produce the next token cannot possibly be doing principled reasoning on its own." 
            ],
            "justification_conclusion": "True"
        },
        {
            "claim": "LLMs struggle to generate executable plans, with low success rates even for state-of-the-art models, and performance degrades further with obfuscated inputs.",
            "evidences": [
                "On average, only about 12% of the plans that the best LLM (GPT-4) generates are actually executable without errors and goal-reaching." ,
                "Table 1 shows that all the state of the art LLMs show dismal performance on PlanBench (Valmeekam et al., 2023b)." ,
                "We demonstrate that the performance deteriorates further if the names of the actions and objects in the domain are obfuscated-a change that doesn't in any way affect the performance of the standard Al planners." 
            ],
            "justification_conclusion": ""
        },


         {
            "claim": "The LLM-Modulo framework leverages LLMs as knowledge sources/generators alongside external verifiers for robust planning.",
            "evidences": [
                "We present a vision of LLM-Modulo Frameworks that combines the strengths of LLMs with external model-based verifiers in a tighter bi-directional interaction regime." ,
                "As can be seen readily, the underlying architecture is a Generate-Test-Critique loop, with the LLM generating candidate plans and a bank of critics critiquing the candidate." ,
                "The LLM ideas are vetted by external critics, thus ensuring that the plans generated in this architecture can have formal correctness guarantees where possible." 
            ],
            "justification_conclusion": "True"
        },
        {
            "claim": "LLM-Modulo improves planning performance significantly over baseline LLMs, especially with iterative feedback from critics.",
            "evidences": [
                "(Classical Planning Case Study) ...show that with back prompting from VAL (Howey et al., 2004) acting as the external verifier and critic, LLM performance in Blocks World improves to 82% within 15 back prompting rounds, while in Logistics, it improves to 70%." ,
                "(Travel Planning Case Study) ...LLM-Modulo based agentification with automated critics in the loop significantly improves the performance (6x of baselines) even with a limit of 10 back prompting cycles, and weaker models such as GPT-3.5- turbo." 
             ],
            "justification_conclusion": "True"
        }
    ]
}