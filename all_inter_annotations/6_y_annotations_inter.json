[
    {
        "claim_id": 1,
        "claim_text": "We propose a novel and effective visual grounding framework by combining early joint representation and deep cross-modal interaction.",
        "evidence_text": [
            "Without early alignment and deep fusion, the grounding performance decreases dramatically. Using either our proposed early alignment or deep fusion alone will result in substantial performance gains, but it is clear that the highest accuracy is achieved by combining the two modules."
        ],
        "justification_conclusion": "True"
    },
    {
        "claim_id": 2,
        "claim_text": "We propose to use CLIP to extract and align visual and linguistic features, ensuring that the resulting features are in the same semantic space, which is beneficial for the subsequent cross-modal fusion.",
        "evidence_text": [
             "Using either our proposed early alignment [CLIP] or deep fusion alone will result in substantial performance gains, but it is clear that the highest accuracy is achieved by combining the two modules."
        ],
        "justification_conclusion": "True"
    },
    {
        "claim_id": 3,
        "claim_text": "The results show that the proposed model can perform zero-shot grounding on certain new visual concepts in the open world.",
        "evidence_text": [
            "The results show that the proposed model can perform zero-shot grounding on certain new visual concepts in the open world, such as Sun Wukong, white dragon, mountain wall, and even abstract words."
        ],
        "justification_conclusion": "True"
    },
    {
        "claim_id": 4,
        "claim_text": "JMRI achieves state-of-the-art or comparable performance on multiple visual grounding benchmarks.",
        "evidence_text": [
            "On the RefCOCOg dataset, our method obtains the best accuracy on both RefCOCOg-google and RefCOCOg-umd splits. JMRI II surpasses the previous state-of-the-art VLTVG/SeqTR by an improvement of 4.24/5.72 points on val-g, 2.08/3.26 points on val-u, and 3.32/3.29 points on test-u."
        ],
        "justification_conclusion": "True"
    }

]