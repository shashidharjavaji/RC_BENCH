[
    {
        "claim_id": 1,
        "claim_text": "We introduce REPLUG , the first retrieval augmented language modeling framework for enhancing black-box LMs with retrieval. Unlike previous methods that require updating the LMâ€™s parameters, REPLUG could be easily plugged into any existing LM without additional finetuning.",
        "evidence_text": 
            "We observe that both REPLUG and REPLUG LSR significantly outperform the baselines [Original LMs]. This demonstrates that simply adding a retrieval mod- ule to a frozen language model (i.e., the black-box setting) is effective at improving the performance of different sized language models on language modeling tasks."
        ,
        "justification_conclusion": "True"
    },
    {
        "claim_id": 2,
        "claim_text": "We propose a training scheme to further adapt an off-the-shelf retrieval model to the LM, using the language modeling scores as supervision signals, resulting in improved retrieval quality.",
        "evidence_text": 
            "Figure 6 shows Wikitext- 103 perplexity of GPT-2 XL (1.5B) and GPT-2 Large (774M) augmented with different retrievers... it still lags behind our LM supervised retriever (Contriever LSR), demonstrating the effectiveness of our train- ing scheme that adapts the retriever to LMs."
        ,
        "justification_conclusion": "True"
    },
    {
        "claim_id": 3,
        "claim_text": "REPLUG LSR significantly improves language modeling performance over baseline LMs and standard REPLUG.",
        "evidence_text": 
            "Specifically, REPLUG LSR results in 7.7% improvement over baselines compared to 4.7% im- provement of REPLUG averaged over the 8 models [including GPT-3 Davinci 175B]."
        ,
        "justification_conclusion": "True"
    },
    {
        "claim_id": 4,
        "claim_text": "REPLUG improves performance on few-shot in-context learning tasks like MMLU, achieving results comparable to much larger models.",
        "evidence_text": 
             "Although our models [Codex + REPLUG LSR] slightly underperform Flan-PaLM, this is still a strong re- sult because Flan-PaLM has three times more pa- rameters."
        ,
        "justification_conclusion": "True"
    }


]