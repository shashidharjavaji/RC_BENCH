=== Paper Analysis Summary ===

Claim 1:
Statement: The authors developed an experimental framework to identify qualitative categories of erroneous behavior in large language models by drawing inspiration from human cognitive biases
Location: Abstract
Type: Methodology contribution
Quote: To hypothesize and test for such qualitative errors, we draw inspiration from human cognitive biasesâ€”systematic patterns of deviation from rational judgement.

Evidence:
Conclusion:
Justified: True
Robustness: high
Limitations: Framework is novel but builds on existing cognitive bias research
Confidence: high

==================================================

Claim 2:
Statement: OpenAI's Codex produces predictable errors based on input framing, adjusts outputs towards anchors, and is biased towards frequent training examples
Location: Abstract
Type: Result finding
Quote: Using code generation as a case study, we find that OpenAI's Codex errs predictably based on how the input prompt is framed, adjusts outputs towards anchors, and is biased towards outputs that mimic frequent training examples.

Evidence:
Conclusion:
Justified: True
Robustness: high
Limitations: Results are model-specific and may not generalize to other systems
Confidence: high

==================================================

Claim 3:
Statement: Adding irrelevant preceding functions consistently lowers functional accuracy by 22.3-30.5 points for Codex across different framing lines
Location: Section 3.3.1
Type: Result finding
Quote: We find that adding irrelevant preceding functions consistently lowers functional accuracy, by between 22.3 and 30.5 points for Codex, across the different framing lines we tested.

Evidence:
- Adding irrelevant preceding functions decreases functional accuracy by 22.3-30.5 points for Codex across framing lines
  Strength: strong
  Location: Section 3.3.1
  Limitations: Only tested on specific framing lines
  Quote: We find that adding irrelevant preceding functions consistently lowers functional accuracy, by between 22.3 and 30.5 points for Codex, across the different framing lines we tested.

Conclusion:
Justified: True
Robustness: high
Limitations: Limited to specific framing lines tested
Confidence: high

==================================================

Claim 4:
Statement: Both Codex and CodeGen frequently generate the framing line from irrelevant preceding functions
Location: Section 3.3.1
Type: Result finding
Quote: Moreover, both models frequently generate the framing line: 81% of the time for Codex and 70.7% of time for CodeGen, compared to only 4.5% and 0.0% over untransformed prompts respectively.

Evidence:
- Codex and CodeGen generate framing lines at high rates
  Strength: strong
  Location: Section 3.3.1
  Limitations: Limited to specific framing lines tested
  Quote: both models frequently generate the framing line: 81% of the time for Codex and 70.7% of time for CodeGen, compared to only 4.5% and 0.0% over untransformed prompts respectively

Conclusion:
Justified: True
Robustness: high
Limitations: Specific to tested framing lines and models
Confidence: high

==================================================

Claim 5:
Statement: Codex's accuracy drops from 50% to 17% when flipping operation order from unary-first to binary-first
Location: Section 3.3.3
Type: Result finding
Quote: Focusing on Codex, we find that accuracy drops from 50% to 17% when flipping the order from unary-first to binary-first.

Evidence:
- Codex accuracy drops when order flipped
  Strength: strong
  Location: Section 3.3.3
  Limitations: Focused only on specific operation types
  Quote: Focusing on Codex, we find that accuracy drops from 50% to 17% when flipping the order from unary-first to binary-first.

Conclusion:
Justified: True
Robustness: medium
Limitations: Limited to specific operation types tested
Confidence: medium

==================================================

Claim 6:
Statement: When prompted with conflicting function names, Codex's accuracy drops from 100% to only 4.4%-4.6%
Location: Section 3.3.4
Type: Result finding
Quote: When we request a conflicting function name, Codex's accuracy drops from 100% to only 4.4%-4.6%.

Evidence:
- Accuracy drops with conflicting function names
  Strength: strong
  Location: Section 3.3.4
  Limitations: Limited to specific name formats tested
  Quote: When we request a conflicting function name, Codex's accuracy drops from 100% to only 4.4%-4.6%.

Conclusion:
Justified: True
Robustness: high
Limitations: Limited to tested function name conflicts
Confidence: high

==================================================

Claim 7:
Statement: GPT-3 routinely updates its estimates when an anchor is prepended and tends to shift estimates towards the anchor
Location: Section 4
Type: Result finding
Quote: We find that GPT-3 routinely updates its estimate when an anchor is prepended, and tends to shift the estimate towards the anchor.

Evidence:
Conclusion:
Justified: True
Robustness: medium
Limitations: Small sample size and many gibberish outputs (41%)
Confidence: medium

==================================================

Claim 8:
Statement: Codex erroneously deletes files on at least 80% of prompts when the number of package imports is at least three
Location: Section 5
Type: Result finding
Quote: We find that Codex erroneously deletes files on at least 80% of prompts when the number of package imports is at least three, despite producing a correct output on 90% of prompts when the number of packages is at most two.

Evidence:
- High error rate with 3+ package imports
  Strength: strong
  Location: Section 5
  Limitations: Limited to specific package import scenarios
  Quote: We find that Codex erroneously deletes files on at least 80% of prompts when the number of package imports is at least three

Conclusion:
Justified: True
Robustness: medium
Limitations: Limited to specific package import scenarios tested
Confidence: medium

==================================================

