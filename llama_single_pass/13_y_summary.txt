Claim 1:
Type: contribution
Statement: Original BERT is underestimated in sentence embeddings due to inappropriate sentence representation methods.
Location: Introduction
Exact Quote: Original BERT is underestimated in sentence embeddings due to inappropriate sentence representation methods.

Evidence:
- Evidence Text: Table 1: Spearman correlation and sentence level anisotropy of different pre-trained models with different sentence embedding methods.
  Strength: strong
  Location: Section 3
  Limitations: Limited to specific pre-trained models and sentence embedding methods.
  Exact Quote: Table 1: Spearman correlation and sentence level anisotropy of different pre-trained models with different sentence embedding methods.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence supports the claim by showing that original BERT layers harm sentence embeddings performance, and the anisotropy is not the primary cause of poor semantic similarity.
Key Limitations: Limited to specific pre-trained models and sentence embedding methods.

--------------------------------------------------

Claim 2:
Type: methodology
Statement: Prompt-based sentence embeddings can avoid embedding bias and utilize the original BERT layers.
Location: Section 4
Exact Quote: Prompt-based method can avoid embedding bias and utilize the original BERT layers.

Evidence:
- Evidence Text: Table 3: The influence of static embedding biases in spearman correlation.
  Strength: strong
  Location: Section 4
  Limitations: Limited to static token embeddings and specific pre-trained models.
  Exact Quote: Table 3: The influence of static embedding biases in spearman correlation.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence supports the claim by showing that removing embedding biases improves sentence embeddings performance.
Key Limitations: Limited to static token embeddings and specific pre-trained models.

--------------------------------------------------

Claim 3:
Type: contribution
Statement: Prompt-based contrastive learning with template denoising significantly shortens the gap between the unsupervised and supervised methods.
Location: Section 5
Exact Quote: Prompt-based contrastive learning with template denoising significantly shortens the gap between the unsupervised and supervised methods.

Evidence:
- Evidence Text: Table 6: The performance comparison of our fine-tuned BERT methods on STS tasks.
  Strength: strong
  Location: Section 5
  Limitations: Limited to specific tasks and pre-trained models.
  Exact Quote: Table 6: The performance comparison of our fine-tuned BERT methods on STS tasks.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence supports the claim by showing that the proposed method outperforms other methods in both unsupervised and supervised settings.
Key Limitations: Limited to specific tasks and pre-trained models.

--------------------------------------------------

