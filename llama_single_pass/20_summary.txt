Claim 1:
Type: contribution
Statement: Large language models can generate complex, open-ended outputs, but assessing their reliability is crucial.
Location: Abstract
Exact Quote: Large language models generate complex, open-ended outputs: instead of outputting a class label they write summaries, generate dialogue, or produce working code.

Evidence:
- Evidence Text: The paper presents a framework for identifying qualitative categories of erroneous behavior in large language models.
  Strength: strong
  Location: Introduction
  Limitations: None mentioned
  Exact Quote: To hypothesize and test for such qualitative errors, we draw inspiration from human cognitive biases—systematic patterns of deviation from rational judgement.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The framework's effectiveness in identifying errors is demonstrated through its application to various language models.
Key Limitations: None mentioned

--------------------------------------------------

Claim 2:
Type: methodology
Statement: The paper uses cognitive biases as inspiration to identify potential failures of large language models.
Location: Introduction
Exact Quote: Given a potential failure mode (e.g. relying on irrelevant information in the input), we construct a transformation over inputs that largely preserves semantics, but that we suspect will elicit the failure.

Evidence:
- Evidence Text: The paper applies this methodology to various language models, including Codex, CodeGen, and GPT-3.
  Strength: strong
  Location: Introduction, Sections 3 and 4
  Limitations: Limited to specific models and tasks
  Exact Quote: We draw on four different cognitive biases to hypothesize potential failures of OpenAI’s Codex and Salesforce’s CodeGen.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The application of cognitive biases to identify failures demonstrates the methodology's versatility.
Key Limitations: Limited model and task scope

--------------------------------------------------

Claim 3:
Type: result
Statement: Codex and CodeGen exhibit anchoring behavior, adjusting their output towards related solutions when included in the prompt.
Location: Section 3.3.2
Exact Quote: We find that prepending anchor functions consistently lowers functional accuracy, and elements of the anchor function often appear in the generated solution.

Evidence:
- Evidence Text: Experimental results showing decreased functional accuracy and increased presence of anchor function elements in outputs.
  Strength: strong
  Location: Section 3.3.2, Figure 4
  Limitations: Specific to Codex and CodeGen
  Exact Quote: We measure the influence of the anchor function on the generated solution by plotting the fraction of generated solutions that contain elements of the anchor function.

- Evidence Text: Control experiments rule out alternative explanations, such as models simply copying the anchor function verbatim.
  Strength: moderate
  Location: Section 3.3.2
  Limitations: Limited to specific control experiments
  Exact Quote: Changing the anchor function name leads to negligible changes in results.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The anchoring behavior is consistently demonstrated across experiments, indicating a robust finding.
Key Limitations: Specific to Codex and CodeGen

--------------------------------------------------

Claim 4:
Type: result
Statement: GPT-3 exhibits framing effect, selecting different options based on the framing of the problem.
Location: Section 4
Exact Quote: GPT-3 chooses the probabilistic option more frequently under the 'not save' framing than under the'save' framing, mirroring human behavior.

Evidence:
- Evidence Text: Experimental results comparing GPT-3's responses to different framings.
  Strength: strong
  Location: Section 4, Table 3
  Limitations: Specific to GPT-3 and the framing effect experiment
  Exact Quote: GPT-3's behavior is qualitatively similar to humans in the original study.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: GPT-3's framing effect behavior aligns with human studies, indicating a shared cognitive bias.
Key Limitations: Specific to GPT-3 and the framing effect experiment

--------------------------------------------------

Claim 5:
Type: result
Statement: The framework can elicit high-impact errors, such as Codex incorrectly deleting files, by leveraging attribute substitution.
Location: Section 5
Exact Quote: Codex often incorrectly deletes files when prompted with specific sets of package imports, relying more on the first package as the problem complexity increases.

Evidence:
- Evidence Text: Experimental results demonstrating Codex's high-impact errors in file deletion tasks.
  Strength: strong
  Location: Section 5, Figure 6
  Limitations: Specific to Codex and the file deletion task
  Exact Quote: Codex's error rate increases with the number of package imports, indicating a reliance on simpler heuristics.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The framework effectively identifies high-impact errors in Codex, highlighting its utility in ensuring model reliability.
Key Limitations: Specific to Codex and the file deletion task

--------------------------------------------------

