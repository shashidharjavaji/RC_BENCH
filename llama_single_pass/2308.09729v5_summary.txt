Claim 1:
Type: methodology
Statement: MindMap enables LLMs to comprehend KG inputs and infer with a combined implicit knowledge and the retrieved external knowledge.
Location: Abstract
Exact Quote: MindMap: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models

Evidence:
- Evidence Text: Experimental results on three question & answering datasets (GenMedGPT-5k, CMCQA, and ExplainCPE)
  Strength: strong
  Location: Section 4: Experiments
  Limitations: Limited to specific medical domains
  Exact Quote: We demonstrate that our approach, MindMap, achieves remarkable empirical gains over vanilla LLMs and retrieval-augmented generation methods, and is robust to mismatched retrieval knowledge.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence supports the claim by showcasing MindMap's superior performance across various datasets and its robustness to mismatched knowledge.
Key Limitations: Domain specificity

--------------------------------------------------

Claim 2:
Type: performance
Statement: MindMap outperforms baselines in pairwise winning rates as judged by GPT-4.
Location: Section 4.3: Long Dialogue Question Answering
Exact Quote: MindMap consistently outperforms baselines in pairwise winning rates as judged by GPT-4.

Evidence:
- Evidence Text: Pairwise comparison by GPT-4 on the winning rate of MindMap vs. baselines on disease diagnosis and drug recommendation on CMCQA
  Strength: strong
  Location: Table 5
  Limitations: Specific to CMCQA dataset
  Exact Quote: MindMap vs Baseline... Metrics Win Tie Lose... Disease diagnosis 35.68 39.96 24.36

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence supports the claim by showing MindMap's consistent outperformance across baselines in pairwise comparisons.
Key Limitations: Dataset specificity

--------------------------------------------------

Claim 3:
Type: robustness
Statement: MindMap is robust to unmatched fact queries.
Location: Section 4.6.2: How robust is MindMap to unmatched fact queries?
Exact Quote: The question in Figure 6 contains misleading symptom facts, such as ‘jaundice in my eyes’ leading baseline models to retrieve irrelevant knowledge linked to ‘eye’.

Evidence:
- Evidence Text: Example in Figure 6 where MindMap accurately identifies cirrhosis despite misleading symptom facts
  Strength: moderate
  Location: Figure 6
  Limitations: Single example, not comprehensive testing
  Exact Quote: This results in failure to identify the correct disease, with recommended drugs and tests unrelated to liver disease. In contrast, our model MindMap accurately identifies cirrhosis...

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence supports the claim by showcasing MindMap's ability to accurately identify the correct disease despite misleading symptoms.
Key Limitations: Limited to a single example

--------------------------------------------------

