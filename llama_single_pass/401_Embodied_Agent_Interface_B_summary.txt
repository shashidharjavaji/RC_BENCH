Claim 1:
Type: result
Statement: Most LLMs struggle to faithfully translate natural language instructions into grounded states (objects, object states, and relations) in the environment.
Location: Section A, Summary of Empirical Findings
Exact Quote: Most LLMs still struggle to faithfully translate natural language instructions into grounded states (objects, object states, and relations) in the environment.

Evidence:
- Evidence Text: A common error is generating intermediate goals instead of final goals, e.g., predicting the state open(freezer) for the task “drinking water”.
  Strength: strong
  Location: Section A, Summary of Empirical Findings
  Limitations: Limited to specific tasks and environments
  Exact Quote: A common error is generating intermediate goals instead of final goals, e.g., predicting the state open(freezer) for the task “drinking water”.

- Evidence Text: Another common error is omitting conversationally uncommon spatial relationship goals. For example, in the task “serving a meal”, with ground truth goal condition _ontop(chicken.0, plate.2) and _ontop(plate.2, table.1), GPT-4o mistakenly predicts _ontop(chicken.0, table.1), ignoring the crucial spatial relationship between the chicken, plate, and table.
  Strength: strong
  Location: Section A, Summary of Empirical Findings
  Limitations: Limited to specific tasks and environments
  Exact Quote: Another common error is omitting conversationally uncommon spatial relationship goals. For example, in the task “serving a meal”, with ground truth goal condition _ontop(chicken.0, plate.2) and _ontop(plate.2, table.1), GPT-4o mistakenly predicts _ontop(chicken.0, table.1), ignoring the crucial spatial relationship between the chicken, plate, and table.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided demonstrates the struggle of LLMs in translating natural language instructions into grounded states, highlighting common errors in goal interpretation.
Key Limitations: Limited to specific tasks and environments

--------------------------------------------------

Claim 2:
Type: result
Statement: Reasoning ability is a crucial aspect that LLMs should improve. Trajectory runtime errors are common (41.2%), with a large portion of missing step (15.5%) and additional step (16.2%) errors, often due to overlooking preconditions.
Location: Section A, Summary of Empirical Findings
Exact Quote: Reasoning ability is a crucial aspect that LLMs should improve. Trajectory runtime errors are common (41.2%), with a large portion of missing step (15.5%) and additional step (16.2%) errors, often due to overlooking preconditions.

Evidence:
- Evidence Text: For instance, LLMs may ignore the agent’s sitting or lying state and fail to include a standup action before executing other actions.
  Strength: strong
  Location: Section A, Summary of Empirical Findings
  Limitations: Limited to specific tasks and environments
  Exact Quote: For instance, LLMs may ignore the agent’s sitting or lying state and fail to include a standup action before executing other actions.

- Evidence Text: They sometimes also fail to understand the need to open a closed object before fetching items from inside. Additional step errors frequently occur when LLMs output actions for previously achieved goals.
  Strength: strong
  Location: Section A, Summary of Empirical Findings
  Limitations: Limited to specific tasks and environments
  Exact Quote: They sometimes also fail to understand the need to open a closed object before fetching items from inside. Additional step errors frequently occur when LLMs output actions for previously achieved goals.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence provided demonstrates the importance of improving reasoning ability in LLMs, highlighting common errors in action sequencing.
Key Limitations: Limited to specific tasks and environments

--------------------------------------------------

