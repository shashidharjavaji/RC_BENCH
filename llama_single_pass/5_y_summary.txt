Claim 1:
Type: contribution
Statement: A simple ResNet-like architecture can serve as an effective baseline for tabular DL.
Location: Section 1: Introduction
Exact Quote: First, we have demonstrated that a simple ResNet-like architecture can serve as an effective baseline.

Evidence:
- Evidence Text: Experimental results on 11 public datasets (Table 2)
  Strength: strong
  Location: Section 4.4: Comparing DL models
  Limitations: Limited to the specific datasets and tasks used in the study
  Exact Quote: Table 2 reports the results for deep architectures.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim, showing ResNet's competitive performance across various tasks.
Key Limitations: Dataset and task specificity

--------------------------------------------------

Claim 2:
Type: performance
Statement: FT-Transformer outperforms other DL solutions on most tasks.
Location: Section 1: Introduction
Exact Quote: Second, we have proposed FT-Transformer â€” a simple adaptation of the Transformer architecture that outperforms other DL solutions on most of the tasks.

Evidence:
- Evidence Text: Experimental results on 11 public datasets (Table 2), comparing FT-Transformer with other DL models
  Strength: strong
  Location: Section 4.4: Comparing DL models
  Limitations: Limited to the specific datasets and tasks used in the study
  Exact Quote: Table 2 reports the results for deep architectures.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence strongly supports the claim, showing FT-Transformer's superior performance across most tasks.
Key Limitations: Dataset and task specificity

--------------------------------------------------

Claim 3:
Type: contribution
Statement: There is still no universal solution among DL models and GBDT.
Location: Section 4.5: Comparing DL models and GBDT
Exact Quote: there is still no universal solution among DL models and GBDT.

Evidence:
- Evidence Text: Comparative analysis of ensembles of GBDT and DL models (Table 4)
  Strength: moderate
  Location: Section 4.5: Comparing DL models and GBDT
  Limitations: Focused on ensemble performance, might not reflect single model capabilities
  Exact Quote: Table 4: Results for ensembles of GBDT and the main DL models.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence supports the claim, highlighting the lack of a universal solution, but with some limitations in the comparison approach.
Key Limitations: Ensemble vs. single model comparison

--------------------------------------------------

Claim 4:
Type: contribution
Statement: FT-Transformer is a more universal model for tabular data problems.
Location: Section 5.1: When FT-Transformer is better than ResNet?
Exact Quote: FT-Transformer provides competitive performance on all tasks, while GBDT and ResNet perform well only on some subsets of the tasks.

Evidence:
- Evidence Text: Synthetic tasks experiment (Figure 3) and comparative analysis with ResNet and GBDT
  Strength: moderate
  Location: Section 5.1: When FT-Transformer is better than ResNet?
  Limitations: Synthetic setup might not fully represent real-world scenarios
  Exact Quote: Figure 3: Test RMSE averaged over five seeds.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence supports the claim, indicating FT-Transformer's broader applicability, though with some limitations in the experimental setup.
Key Limitations: Synthetic task specificity

--------------------------------------------------

