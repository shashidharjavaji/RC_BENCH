**Analysis of the Research Paper**

**Claim 1:** LLMs prefer external knowledge that forms a Chain of Evidence (CoE), which maintains both relevance to the question and mutual support among knowledge pieces.

*   **Claim Text:** "We characterize that knowledge preferred by LLMs should maintain both relevance to the question and mutual support among knowledge pieces."
*   **Claim Type:** Methodology
*   **Claim Location:** Section 3.1 CoE Characterization
*   **Exact Quote:** "We characterize that knowledge preferred by LLMs should maintain both relevance to the question and mutual support among knowledge pieces."
*   **Evidence:**
    *   **Evidence Text:** Experimental results showing CoE outperforming Non-CoE in effectiveness, faithfulness, and robustness assessments.
    *   **Strength:** Strong
    *   **Limitations:** Limited to multi-hop QA scenarios and specific LLMs.
    *   **Location:** Sections 5, 6, and 7
    *   **Exact Quote:** Tables 2, 3, and 4
*   **Evaluation:**
    *   **Conclusion Justified:** True
    *   **Robustness:** High
    *   **Justification:** CoE's structured knowledge leads to better performance in LLMs.
    *   **Key Limitations:** Limited generalizability to other QA scenarios and LLMs.
    *   **Confidence Level:** High

**Claim 2:** The proposed CoE discrimination approach effectively identifies CoE in external knowledge.

*   **Claim Text:** "We propose an automated CoE discrimination approach and explore LLMs’ preferences from their effectiveness, faithfulness, and robustness, as well as CoE’s usability in a naive RAG case."
*   **Claim Type:** Methodology
*   **Claim Location:** Section 3.2 CoE Discrimination Approach
*   **Exact Quote:** "We propose an automated CoE discrimination approach and explore LLMs’ preferences from their effectiveness, faithfulness, and robustness, as well as CoE’s usability in a naive RAG case."
*   **Evidence:**
    *   **Evidence Text:** Description of the CoE discrimination approach and its application in the paper.
    *   **Strength:** Moderate
    *   **Limitations:** Limited to the specific approach proposed and its evaluation in the paper.
    *   **Location:** Section 3.2
    *   **Exact Quote:** Figure 3 and surrounding text
*   **Evaluation:**
    *   **Conclusion Justified:** True
    *   **Robustness:** Medium
    *   **Justification:** The approach effectively identifies CoE, enabling further analysis.
    *   **Key Limitations:** Limited to the specific context of the paper.
    *   **Confidence Level:** Medium

**Claim 3:** CoE-guided retrieval strategy (ScopeCoE) improves LLMs' accuracy in a naive RAG framework.

*   **Claim Text:** "For the selected case, the CoE-guided retrieval strategy can effectively improve LLM’s accuracy after substituting the reranking component in the naive RAG framework."
*   **Claim Type:** Contribution
*   **Claim Location:** Section 8 Usability Assessment
*   **Exact Quote:** "For the selected case, the CoE-guided retrieval strategy can effectively improve LLM’s accuracy after substituting the reranking component in the naive RAG framework."
*   **Evidence:**
    *   **Evidence Text:** Experimental results comparing RAG and RAG+ScopeCoE in terms of accuracy.
    *   **Strength:** Strong
    *   **Limitations:** Limited to the specific RAG scenario and LLMs evaluated.
    *   **Location:** Section 8
    *   **Exact Quote:** Table 5
*   **Evaluation:**
    *   **Conclusion Justified:** True
    *   **Robustness:** High
    *   **Justification:** ScopeCoE enhances LLM performance in the naive RAG framework.
    *   **Key Limitations:** Limited generalizability to other RAG scenarios and LLMs.
    *   **Confidence Level:** High