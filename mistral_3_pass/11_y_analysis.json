{
    "paper_analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "The audio-visual video parsing task aims to parse a video into modality- and category-aware temporal segments.",
                "location": "Abstract",
                "type": "Nature of the claim",
                "exact_quote": "The audio-visual video parsing task aims to parse a video into modality- and category-aware temporal segments."
            },
            "evidence": [
                {
                    "evidence_id": 1,
                    "evidence_text": "The audio-visual video parsing task aims to parse a video into modality- and category-aware temporal segments.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Abstract",
                    "exact_quote": "The audio-visual video parsing task aims to parse a video into modality- and category-aware temporal segments."
                }
            ],
            "conclusion": {
                "claim_id": 1,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "Previous work mainly focuses on weakly-supervised approaches, which learn from video-level event labels.",
                "location": "Abstract",
                "type": "Nature of the claim",
                "exact_quote": "Previous work mainly focuses on weakly-supervised approaches, which learn from video-level event labels."
            },
            "evidence": [
                {
                    "evidence_id": 2,
                    "evidence_text": "Previous work mainly focuses on weakly-supervised approaches, which learn from video-level event labels.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Abstract",
                    "exact_quote": "Previous work mainly focuses on weakly-supervised approaches, which learn from video-level event labels."
                }
            ],
            "conclusion": {
                "claim_id": 2,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "During training, they do not know which modality perceives and meanwhile which temporal segment contains the video event.",
                "location": "Abstract",
                "type": "Nature of the claim",
                "exact_quote": "During training, they do not know which modality perceives and meanwhile which temporal segment contains the video event."
            },
            "evidence": [
                {
                    "evidence_id": 3,
                    "evidence_text": "During training, they do not know which modality perceives and meanwhile which temporal segment contains the video event.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Abstract",
                    "exact_quote": "During training, they do not know which modality perceives and meanwhile which temporal segment contains the video event."
                }
            ],
            "conclusion": {
                "claim_id": 3,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "Since there is no explicit grouping in the existing frameworks, the modality and temporal uncertainties make these methods suffer from false predictions.",
                "location": "Abstract",
                "type": "Nature of the claim",
                "exact_quote": "Since there is no explicit grouping in the existing frameworks, the modality and temporal uncertainties make these methods suffer from false predictions."
            },
            "evidence": [
                {
                    "evidence_id": 4,
                    "evidence_text": "Since there is no explicit grouping in the existing frameworks, the modality and temporal uncertainties make these methods suffer from false predictions.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Abstract",
                    "exact_quote": "Since there is no explicit grouping in the existing frameworks, the modality and temporal uncertainties make these methods suffer from false predictions."
                }
            ],
            "conclusion": {
                "claim_id": 4,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "Learning compact and discriminative multi-modal subspaces is essential for mitigating the issue.",
                "location": "Abstract",
                "type": "Nature of the claim",
                "exact_quote": "Learning compact and discriminative multi-modal subspaces is essential for mitigating the issue."
            },
            "evidence": [
                {
                    "evidence_id": 5,
                    "evidence_text": "Learning compact and discriminative multi-modal subspaces is essential for mitigating the issue.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Abstract",
                    "exact_quote": "Learning compact and discriminative multi-modal subspaces is essential for mitigating the issue."
                }
            ],
            "conclusion": {
                "claim_id": 5,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "To this end, in this paper, we propose a novel Multi-modal Grouping Network, namely MGN, for explicitly semantic-aware grouping.",
                "location": "Abstract",
                "type": "Nature of the claim",
                "exact_quote": "To this end, in this paper, we propose a novel Multi-modal Grouping Network, namely MGN, for explicitly semantic-aware grouping."
            },
            "evidence": [
                {
                    "evidence_id": 6,
                    "evidence_text": "To this end, in this paper, we propose a novel Multi-modal Grouping Network, namely MGN, for explicitly semantic-aware grouping.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Abstract",
                    "exact_quote": "To this end, in this paper, we propose a novel Multi-modal Grouping Network, namely MGN, for explicitly semantic-aware grouping."
                }
            ],
            "conclusion": {
                "claim_id": 6,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 7,
            "claim": {
                "text": "Specifically, MGN aggregates event-aware unimodal features through unimodal grouping in terms of learnable categorical embedding tokens.",
                "location": "Abstract",
                "type": "Nature of the claim",
                "exact_quote": "Specifically, MGN aggregates event-aware unimodal features through unimodal grouping in terms of learnable categorical embedding tokens."
            },
            "evidence": [
                {
                    "evidence_id": 7,
                    "evidence_text": "Specifically, MGN aggregates event-aware unimodal features through unimodal grouping in terms of learnable categorical embedding tokens.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Abstract",
                    "exact_quote": "Specifically, MGN aggregates event-aware unimodal features through unimodal grouping in terms of learnable categorical embedding tokens."
                }
            ],
            "conclusion": {
                "claim_id": 7,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 8,
            "claim": {
                "text": "Furthermore, it leverages the cross-modal grouping for modality-aware prediction to match the video-level target.",
                "location": "Abstract",
                "type": "Nature of the claim",
                "exact_quote": "Furthermore, it leverages the cross-modal grouping for modality-aware prediction to match the video-level target."
            },
            "evidence": [
                {
                    "evidence_id": 8,
                    "evidence_text": "Furthermore, it leverages the cross-modal grouping for modality-aware prediction to match the video-level target.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Abstract",
                    "exact_quote": "Furthermore, it leverages the cross-modal grouping for modality-aware prediction to match the video-level target."
                }
            ],
            "conclusion": {
                "claim_id": 8,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 9,
            "claim": {
                "text": "Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing.",
                "location": "Abstract",
                "type": "Nature of the claim",
                "exact_quote": "Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing."
            },
            "evidence": [
                {
                    "evidence_id": 9,
                    "evidence_text": "Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Abstract",
                    "exact_quote": "Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing."
                }
            ],
            "conclusion": {
                "claim_id": 9,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 10,
            "claim": {
                "text": "In addition, our MGN is much more lightweight, using only 47.2% of the parameters of baselines (17 MB vs. 36 MB).",
                "location": "Abstract",
                "type": "Nature of the claim",
                "exact_quote": "In addition, our MGN is much more lightweight, using only 47.2% of the parameters of baselines (17 MB vs. 36 MB)."
            },
            "evidence": [
                {
                    "evidence_id": 10,
                    "evidence_text": "In addition, our MGN is much more lightweight, using only 47.2% of the parameters of baselines (17 MB vs. 36 MB).",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Abstract",
                    "exact_quote": "In addition, our MGN is much more lightweight, using only 47.2% of the parameters of baselines (17 MB vs. 36 MB)."
                }
            ],
            "conclusion": {
                "claim_id": 10,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 11,
            "claim": {
                "text": "Code is available at https://github.com/stoneMo/MGN.",
                "location": "Abstract",
                "type": "Nature of the claim",
                "exact_quote": "Code is available at https://github.com/stoneMo/MGN."
            },
            "evidence": [
                {
                    "evidence_id": 11,
                    "evidence_text": "Code is available at https://github.com/stoneMo/MGN.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Abstract",
                    "exact_quote": "Code is available at https://github.com/stoneMo/MGN."
                }
            ],
            "conclusion": {
                "claim_id": 11,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 12,
            "claim": {
                "text": "Our main contributions can be summarized as follows:",
                "location": "Abstract",
                "type": "Nature of the claim",
                "exact_quote": "Our main contributions can be summarized as follows:"
            },
            "evidence": [
                {
                    "evidence_id": 12,
                    "evidence_text": "Our main contributions can be summarized as follows:",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Abstract",
                    "exact_quote": "Our main contributions can be summarized as follows:"
                }
            ],
            "conclusion": {
                "claim_id": 12,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 13,
            "claim": {
                "text": "We propose a new audio-visual video parsing baseline: Multi-modal Grouping Network (MGN) that enables explicit grouping in a multi-modal network to learn compact and discriminative audio and visual embeddings.",
                "location": "Abstract",
                "type": "Nature of the claim",
                "exact_quote": "We propose a new audio-visual video parsing baseline: Multi-modal Grouping Network (MGN) that enables explicit grouping in a multi-modal network to learn compact and discriminative audio and visual embeddings."
            },
            "evidence": [
                {
                    "evidence_id": 13,
                    "evidence_text": "We propose a new audio-visual video parsing baseline: Multi-modal Grouping Network (MGN) that enables explicit grouping in a multi-modal network to learn compact and discriminative audio and visual embeddings.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Abstract",
                    "exact_quote": "We propose a new audio-visual video parsing baseline: Multi-modal Grouping Network (MGN) that enables explicit grouping in a multi-modal network to learn compact and discriminative audio and visual embeddings."
                }
            ],
            "conclusion": {
                "claim_id": 13,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 14,
            "claim": {
                "text": "We introduce class-aware unimodal grouping and modality-aware cross-modal grouping modules to aggregate multi-modal temporal contexts.",
                "location": "Abstract",
                "type": "Nature of the claim",
                "exact_quote": "We introduce class-aware unimodal grouping and modality-aware cross-modal grouping modules to aggregate multi-modal temporal contexts."
            },
            "evidence": [
                {
                    "evidence_id": 14,
                    "evidence_text": "We introduce class-aware unimodal grouping and modality-aware cross-modal grouping modules to aggregate multi-modal temporal contexts.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Abstract",
                    "exact_quote": "We introduce class-aware unimodal grouping and modality-aware cross-modal grouping modules to aggregate multi-modal temporal contexts."
                }
            ],
            "conclusion": {
                "claim_id": 14,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 15,
            "claim": {
                "text": "The experiments can demonstrate the superiority of our MGN over state-of-the-art AVVP approaches and its generalizability to contrastive learning and label refinement.",
                "location": "Abstract",
                "type": "Nature of the claim",
                "exact_quote": "The experiments can demonstrate the superiority of our MGN over state-of-the-art AVVP approaches and its generalizability to contrastive learning and label refinement."
            },
            "evidence": [
                {
                    "evidence_id": 15,
                    "evidence_text": "The experiments can demonstrate the superiority of our MGN over state-of-the-art AVVP approaches and its generalizability to contrastive learning and label refinement.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Abstract",
                    "exact_quote": "The experiments can demonstrate the superiority of our MGN over state-of-the-art AVVP approaches and its generalizability to contrastive learning and label refinement."
                }
            ],
            "conclusion": {
                "claim_id": 15,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 16,
            "claim": {
                "text": "Humans understand the surrounding environment by integrating signals from different senses.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "Humans understand the surrounding environment by integrating signals from different senses."
            },
            "evidence": [
                {
                    "evidence_id": 16,
                    "evidence_text": "Humans understand the surrounding environment by integrating signals from different senses.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "Humans understand the surrounding environment by integrating signals from different senses."
                }
            ],
            "conclusion": {
                "claim_id": 16,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 17,
            "claim": {
                "text": "In our daily life, sound and sight are two of the most commonly used modalities, which have drawn much attention from researchers to explore computational audio-visual scene understanding.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "In our daily life, sound and sight are two of the most commonly used modalities, which have drawn much attention from researchers to explore computational audio-visual scene understanding."
            },
            "evidence": [
                {
                    "evidence_id": 17,
                    "evidence_text": "In our daily life, sound and sight are two of the most commonly used modalities, which have drawn much attention from researchers to explore computational audio-visual scene understanding.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "In our daily life, sound and sight are two of the most commonly used modalities, which have drawn much attention from researchers to explore computational audio-visual scene understanding."
                }
            ],
            "conclusion": {
                "claim_id": 17,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 18,
            "claim": {
                "text": "Previous audio-visual work [1, 2] usually assumes audio and visual data are temporally aligned.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "Previous audio-visual work [1, 2] usually assumes audio and visual data are temporally aligned."
            },
            "evidence": [
                {
                    "evidence_id": 18,
                    "evidence_text": "Previous audio-visual work [1, 2] usually assumes audio and visual data are temporally aligned.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "Previous audio-visual work [1, 2] usually assumes audio and visual data are temporally aligned."
                }
            ],
            "conclusion": {
                "claim_id": 18,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 19,
            "claim": {
                "text": "However, the alignment does not always exist in real-world videos.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "However, the alignment does not always exist in real-world videos."
            },
            "evidence": [
                {
                    "evidence_id": 19,
                    "evidence_text": "However, the alignment does not always exist in real-world videos.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "However, the alignment does not always exist in real-world videos."
                }
            ],
            "conclusion": {
                "claim_id": 19,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 20,
            "claim": {
                "text": "For these non-aligned cases, audio signals become more reliable in understanding the events of interest.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "For these non-aligned cases, audio signals become more reliable in understanding the events of interest."
            },
            "evidence": [
                {
                    "evidence_id": 20,
                    "evidence_text": "For these non-aligned cases, audio signals become more reliable in understanding the events of interest.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "For these non-aligned cases, audio signals become more reliable in understanding the events of interest."
                }
            ],
            "conclusion": {
                "claim_id": 20,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 21,
            "claim": {
                "text": "In this work, we address the audio-visual video parsing (AVVP) task [3] that aims to parse a video into temporal event segments and predict the audible, visible, or audio-visible event categories.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "In this work, we address the audio-visual video parsing (AVVP) task [3] that aims to parse a video into temporal event segments and predict the audible, visible, or audio-visible event categories."
            },
            "evidence": [
                {
                    "evidence_id": 21,
                    "evidence_text": "In this work, we address the audio-visual video parsing (AVVP) task [3] that aims to parse a video into temporal event segments and predict the audible, visible, or audio-visible event categories.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "In this work, we address the audio-visual video parsing (AVVP) task [3] that aims to parse a video into temporal event segments and predict the audible, visible, or audio-visible event categories."
                }
            ],
            "conclusion": {
                "claim_id": 21,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 22,
            "claim": {
                "text": "Existing approaches [3, 4, 5] usually focus on learning to leverage the unimodal and cross-modal temporal contexts from weak supervisions.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "Existing approaches [3, 4, 5] usually focus on learning to leverage the unimodal and cross-modal temporal contexts from weak supervisions."
            },
            "evidence": [
                {
                    "evidence_id": 22,
                    "evidence_text": "Existing approaches [3, 4, 5] usually focus on learning to leverage the unimodal and cross-modal temporal contexts from weak supervisions.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "Existing approaches [3, 4, 5] usually focus on learning to leverage the unimodal and cross-modal temporal contexts from weak supervisions."
                }
            ],
            "conclusion": {
                "claim_id": 22,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 23,
            "claim": {
                "text": "HAN [3] introduced a simple Multimodal Multiple Instance Learning framework with cross-modal and self-modal attention layers to utilize the videolevel labels.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "HAN [3] introduced a simple Multimodal Multiple Instance Learning framework with cross-modal and self-modal attention layers to utilize the videolevel labels."
            },
            "evidence": [
                {
                    "evidence_id": 23,
                    "evidence_text": "HAN [3] introduced a simple Multimodal Multiple Instance Learning framework with cross-modal and self-modal attention layers to utilize the videolevel labels.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "HAN [3] introduced a simple Multimodal Multiple Instance Learning framework with cross-modal and self-modal attention layers to utilize the videolevel labels."
                }
            ],
            "conclusion": {
                "claim_id": 23,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 24,
            "claim": {
                "text": "Recent state-of-the-art methods usually use the HAN as the baseline and modify it to further improve parsing performance.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "Recent state-of-the-art methods usually use the HAN as the baseline and modify it to further improve parsing performance."
            },
            "evidence": [
                {
                    "evidence_id": 24,
                    "evidence_text": "Recent state-of-the-art methods usually use the HAN as the baseline and modify it to further improve parsing performance.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "Recent state-of-the-art methods usually use the HAN as the baseline and modify it to further improve parsing performance."
                }
            ],
            "conclusion": {
                "claim_id": 24,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 25,
            "claim": {
                "text": "Particularly, contrastive learning and label refinement are proposed in Wu and Yang [4], where they adopted a contrastive loss to enforce the temporal alignment between the audio and visual features at the same timestamp and augmented training data with modality-aware event labels generation.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "Particularly, contrastive learning and label refinement are proposed in Wu and Yang [4], where they adopted a contrastive loss to enforce the temporal alignment between the audio and visual features at the same timestamp and augmented training data with modality-aware event labels generation."
            },
            "evidence": [
                {
                    "evidence_id": 25,
                    "evidence_text": "Particularly, contrastive learning and label refinement are proposed in Wu and Yang [4], where they adopted a contrastive loss to enforce the temporal alignment between the audio and visual features at the same timestamp and augmented training data with modality-aware event labels generation.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "Particularly, contrastive learning and label refinement are proposed in Wu and Yang [4], where they adopted a contrastive loss to enforce the temporal alignment between the audio and visual features at the same timestamp and augmented training data with modality-aware event labels generation."
                }
            ],
            "conclusion": {
                "claim_id": 25,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 26,
            "claim": {
                "text": "Furthermore, Lin et al. [5] proposed to leverage audio-visual class co-occurrence to jointly explore the relationship of different categories among all modality streams.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "Furthermore, Lin et al. [5] proposed to leverage audio-visual class co-occurrence to jointly explore the relationship of different categories among all modality streams."
            },
            "evidence": [
                {
                    "evidence_id": 26,
                    "evidence_text": "Furthermore, Lin et al. [5] proposed to leverage audio-visual class co-occurrence to jointly explore the relationship of different categories among all modality streams.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "Furthermore, Lin et al. [5] proposed to leverage audio-visual class co-occurrence to jointly explore the relationship of different categories among all modality streams."
                }
            ],
            "conclusion": {
                "claim_id": 26,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 27,
            "claim": {
                "text": "Our key motivation is to learn compact and discriminative audio and visual representations by explicit multi-modal grouping for mitigating the modality and temporal uncertainties in the weakly-supervised audio-visual video parsing problem.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "Our key motivation is to learn compact and discriminative audio and visual representations by explicit multi-modal grouping for mitigating the modality and temporal uncertainties in the weakly-supervised audio-visual video parsing problem."
            },
            "evidence": [
                {
                    "evidence_id": 27,
                    "evidence_text": "Our key motivation is to learn compact and discriminative audio and visual representations by explicit multi-modal grouping for mitigating the modality and temporal uncertainties in the weakly-supervised audio-visual video parsing problem.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "Our key motivation is to learn compact and discriminative audio and visual representations by explicit multi-modal grouping for mitigating the modality and temporal uncertainties in the weakly-supervised audio-visual video parsing problem."
                }
            ],
            "conclusion": {
                "claim_id": 27,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 28,
            "claim": {
                "text": "During training, segment-wise event labels are unavailable for audio and visual temporal segments in videos.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "During training, segment-wise event labels are unavailable for audio and visual temporal segments in videos."
            },
            "evidence": [
                {
                    "evidence_id": 28,
                    "evidence_text": "During training, segment-wise event labels are unavailable for audio and visual temporal segments in videos.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "During training, segment-wise event labels are unavailable for audio and visual temporal segments in videos."
                }
            ],
            "conclusion": {
                "claim_id": 28,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 29,
            "claim": {
                "text": "Thus, the multi-modal temporal modeling in the above existing methods might aggregate and implicitly group irrelevant semantic information due to lacking of fine-grained supervisions, which causes false positives for predicting categories of events.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "Thus, the multi-modal temporal modeling in the above existing methods might aggregate and implicitly group irrelevant semantic information due to lacking of fine-grained supervisions, which causes false positives for predicting categories of events."
            },
            "evidence": [
                {
                    "evidence_id": 29,
                    "evidence_text": "Thus, the multi-modal temporal modeling in the above existing methods might aggregate and implicitly group irrelevant semantic information due to lacking of fine-grained supervisions, which causes false positives for predicting categories of events.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "Thus, the multi-modal temporal modeling in the above existing methods might aggregate and implicitly group irrelevant semantic information due to lacking of fine-grained supervisions, which causes false positives for predicting categories of events."
                }
            ],
            "conclusion": {
                "claim_id": 29,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 30,
            "claim": {
                "text": "That is, audio or visual temporal segments in the same event category might be far away from the class center in the embedding space since there are no segment-level and modality-wise constraints during training.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "That is, audio or visual temporal segments in the same event category might be far away from the class center in the embedding space since there are no segment-level and modality-wise constraints during training."
            },
            "evidence": [
                {
                    "evidence_id": 30,
                    "evidence_text": "That is, audio or visual temporal segments in the same event category might be far away from the class center in the embedding space since there are no segment-level and modality-wise constraints during training.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "That is, audio or visual temporal segments in the same event category might be far away from the class center in the embedding space since there are no segment-level and modality-wise constraints during training."
                }
            ],
            "conclusion": {
                "claim_id": 30,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 31,
            "claim": {
                "text": "In the meanwhile, there is no constraint for modality category prediction to match the video-level target at the end.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "In the meanwhile, there is no constraint for modality category prediction to match the video-level target at the end."
            },
            "evidence": [
                {
                    "evidence_id": 31,
                    "evidence_text": "In the meanwhile, there is no constraint for modality category prediction to match the video-level target at the end.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "In the meanwhile, there is no constraint for modality category prediction to match the video-level target at the end."
                }
            ],
            "conclusion": {
                "claim_id": 31,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 32,
            "claim": {
                "text": "Different from past approaches, we propose a new Multi-modal Grouping Network, namely MGN, to explicitly group semantic-aware multi-modal contexts, which enables learning more compact and discriminative audio and visual representations.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "Different from past approaches, we propose a new Multi-modal Grouping Network, namely MGN, to explicitly group semantic-aware multi-modal contexts, which enables learning more compact and discriminative audio and visual representations."
            },
            "evidence": [
                {
                    "evidence_id": 32,
                    "evidence_text": "Different from past approaches, we propose a new Multi-modal Grouping Network, namely MGN, to explicitly group semantic-aware multi-modal contexts, which enables learning more compact and discriminative audio and visual representations.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "Different from past approaches, we propose a new Multi-modal Grouping Network, namely MGN, to explicitly group semantic-aware multi-modal contexts, which enables learning more compact and discriminative audio and visual representations."
                }
            ],
            "conclusion": {
                "claim_id": 32,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 33,
            "claim": {
                "text": "Specifically, we first extract event-aware unimodal features through unimodal grouping in terms of learnable categorical embedding tokens for each individual modality.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "Specifically, we first extract event-aware unimodal features through unimodal grouping in terms of learnable categorical embedding tokens for each individual modality."
            },
            "evidence": [
                {
                    "evidence_id": 33,
                    "evidence_text": "Specifically, we first extract event-aware unimodal features through unimodal grouping in terms of learnable categorical embedding tokens for each individual modality.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "Specifically, we first extract event-aware unimodal features through unimodal grouping in terms of learnable categorical embedding tokens for each individual modality."
                }
            ],
            "conclusion": {
                "claim_id": 33,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 34,
            "claim": {
                "text": "Then, we introduce a cross-attention layer with a hard attention mechanism to aggregate cross-modal temporal contexts.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "Then, we introduce a cross-attention layer with a hard attention mechanism to aggregate cross-modal temporal contexts."
            },
            "evidence": [
                {
                    "evidence_id": 34,
                    "evidence_text": "Then, we introduce a cross-attention layer with a hard attention mechanism to aggregate cross-modal temporal contexts.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "Then, we introduce a cross-attention layer with a hard attention mechanism to aggregate cross-modal temporal contexts."
                }
            ],
            "conclusion": {
                "claim_id": 34,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 35,
            "claim": {
                "text": "Finally, we utilize a cross-modal grouping module to predict the modality category from updated class-aware unimodal embeddings.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "Finally, we utilize a cross-modal grouping module to predict the modality category from updated class-aware unimodal embeddings."
            },
            "evidence": [
                {
                    "evidence_id": 35,
                    "evidence_text": "Finally, we utilize a cross-modal grouping module to predict the modality category from updated class-aware unimodal embeddings.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "Finally, we utilize a cross-modal grouping module to predict the modality category from updated class-aware unimodal embeddings."
                }
            ],
            "conclusion": {
                "claim_id": 35,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 36,
            "claim": {
                "text": "Experimental results on the LLP [3] dataset validate that our new audio-visual video parsing framework achieves superior results over previous state-of-the-art methods [1, 2, 3, 4].",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "Experimental results on the LLP [3] dataset validate that our new audio-visual video parsing framework achieves superior results over previous state-of-the-art methods [1, 2, 3, 4]."
            },
            "evidence": [
                {
                    "evidence_id": 36,
                    "evidence_text": "Experimental results on the LLP [3] dataset validate that our new audio-visual video parsing framework achieves superior results over previous state-of-the-art methods [1, 2, 3, 4].",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "Experimental results on the LLP [3] dataset validate that our new audio-visual video parsing framework achieves superior results over previous state-of-the-art methods [1, 2, 3, 4]."
                }
            ],
            "conclusion": {
                "claim_id": 36,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 37,
            "claim": {
                "text": "Empirical results also demonstrate the generalizability of our approach to contrastive learning and label refinement proposed in MA [4].",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "Empirical results also demonstrate the generalizability of our approach to contrastive learning and label refinement proposed in MA [4]."
            },
            "evidence": [
                {
                    "evidence_id": 37,
                    "evidence_text": "Empirical results also demonstrate the generalizability of our approach to contrastive learning and label refinement proposed in MA [4].",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "Empirical results also demonstrate the generalizability of our approach to contrastive learning and label refinement proposed in MA [4]."
                }
            ],
            "conclusion": {
                "claim_id": 37,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 38,
            "claim": {
                "text": "In addition, we substantially reduce the parameters of previous work by using only 47.2% parameters of baselines (17 MB vs. 36 MB).",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "In addition, we substantially reduce the parameters of previous work by using only 47.2% parameters of baselines (17 MB vs. 36 MB)."
            },
            "evidence": [
                {
                    "evidence_id": 38,
                    "evidence_text": "In addition, we substantially reduce the parameters of previous work by using only 47.2% parameters of baselines (17 MB vs. 36 MB).",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "In addition, we substantially reduce the parameters of previous work by using only 47.2% parameters of baselines (17 MB vs. 36 MB)."
                }
            ],
            "conclusion": {
                "claim_id": 38,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 39,
            "claim": {
                "text": "Our main contributions can be summarized as follows:",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "Our main contributions can be summarized as follows:"
            },
            "evidence": [
                {
                    "evidence_id": 39,
                    "evidence_text": "Our main contributions can be summarized as follows:",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "Our main contributions can be summarized as follows:"
                }
            ],
            "conclusion": {
                "claim_id": 39,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 40,
            "claim": {
                "text": "We propose a new audio-visual video parsing baseline: Multi-modal Grouping Network (MGN) that enables explicit grouping in a multi-modal network to learn compact and discriminative audio and visual embeddings.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "We propose a new audio-visual video parsing baseline: Multi-modal Grouping Network (MGN) that enables explicit grouping in a multi-modal network to learn compact and discriminative audio and visual embeddings."
            },
            "evidence": [
                {
                    "evidence_id": 40,
                    "evidence_text": "We propose a new audio-visual video parsing baseline: Multi-modal Grouping Network (MGN) that enables explicit grouping in a multi-modal network to learn compact and discriminative audio and visual embeddings.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "We propose a new audio-visual video parsing baseline: Multi-modal Grouping Network (MGN) that enables explicit grouping in a multi-modal network to learn compact and discriminative audio and visual embeddings."
                }
            ],
            "conclusion": {
                "claim_id": 40,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 41,
            "claim": {
                "text": "We introduce class-aware unimodal grouping and modality-aware cross-modal grouping modules to aggregate multi-modal temporal contexts.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "We introduce class-aware unimodal grouping and modality-aware cross-modal grouping modules to aggregate multi-modal temporal contexts."
            },
            "evidence": [
                {
                    "evidence_id": 41,
                    "evidence_text": "We introduce class-aware unimodal grouping and modality-aware cross-modal grouping modules to aggregate multi-modal temporal contexts.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "We introduce class-aware unimodal grouping and modality-aware cross-modal grouping modules to aggregate multi-modal temporal contexts."
                }
            ],
            "conclusion": {
                "claim_id": 41,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 42,
            "claim": {
                "text": "The experiments can demonstrate the superiority of our MGN over state-of-the-art AVVP approaches and its generalizability to contrastive learning and label refinement.",
                "location": "1 Introduction",
                "type": "Nature of the claim",
                "exact_quote": "The experiments can demonstrate the superiority of our MGN over state-of-the-art AVVP approaches and its generalizability to contrastive learning and label refinement."
            },
            "evidence": [
                {
                    "evidence_id": 42,
                    "evidence_text": "The experiments can demonstrate the superiority of our MGN over state-of-the-art AVVP approaches and its generalizability to contrastive learning and label refinement.",
                    "strength": "strong",
                    "limitations": "N/A",
                    "location": "Introduction",
                    "exact_quote": "The experiments can demonstrate the superiority of our MGN over state-of-the-art AVVP approaches and its generalizability to contrastive learning and label refinement."
                }
            ],
            "conclusion": {
                "claim_id": 42,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        }
    ],
    "execution_times": {
        "claims_analysis_time": "157.65 seconds",
        "evidence_analysis_time": "202.55 seconds",
        "conclusions_analysis_time": "79.28 seconds",
        "total_execution_time": "449.41 seconds"
    }
}