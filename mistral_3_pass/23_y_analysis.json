{
    "paper_analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "We propose a new benchmark MME to meet the urgent need of MLLM evaluation.",
                "location": "Introduction",
                "type": "Contribution",
                "exact_quote": "We propose a new benchmark MME to meet the urgent need of MLLM evaluation."
            },
            "evidence": [
                {
                    "evidence_id": 1,
                    "evidence_text": "We propose a new benchmark MME to meet the urgent need of MLLM evaluation.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We propose a new benchmark MME to meet the urgent need of MLLM evaluation."
                }
            ],
            "conclusion": {
                "claim_id": 1,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "A total of 30 up-todate MLLMs are evaluated on our MME.",
                "location": "Introduction",
                "type": "Contribution",
                "exact_quote": "A total of 30 up-todate MLLMs are evaluated on our MME."
            },
            "evidence": [
                {
                    "evidence_id": 2,
                    "evidence_text": "A total of 30 up-todate MLLMs are evaluated on our MME.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "A total of 30 up-todate MLLMs are evaluated on our MME."
                }
            ],
            "conclusion": {
                "claim_id": 2,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "We summarize the exposed problems in experiments, proving guidance for the evolution of MLLMs.",
                "location": "Introduction",
                "type": "Contribution",
                "exact_quote": "We summarize the exposed problems in experiments, proving guidance for the evolution of MLLMs."
            },
            "evidence": [
                {
                    "evidence_id": 3,
                    "evidence_text": "We summarize the exposed problems in experiments, proving guidance for the evolution of MLLMs.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We summarize the exposed problems in experiments, proving guidance for the evolution of MLLMs."
                }
            ],
            "conclusion": {
                "claim_id": 3,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "We argue that a universal comprehensive evaluation benchmark should have the following four characteristics: (1) It should cover as much as possible, including both perception and cognition abilities. The former refers to recognizing the specific object, such as its existence, count, position, and color. The latter refers to compositing the perception information and the knowledge in LLM to deduce more complex answers. It is obvious that the former is the premise of the latter. (2) Its data or annotations should not come from existing publicly available datasets as much as possible, avoiding the risk of data leakage. (3) Its instructions should be as concise as possible and in line with human cognition. Although instruction design may have a large impact on the output, all models should be tested under the same unified instructions for fair comparison. A good MLLM should be able to generalize to such concise instructions, which are also very common in everyday life. (4) The responses of MLLMs to the instructions should be intuitive and convenient for quantitative analysis. The open-ended answer of MLLMs poses significant challenges to the quantization. Existing methods tend to use GPT or manual scoring, but there may be problems of inaccuracy and subjectivity.",
                "location": "Introduction",
                "type": "Contribution",
                "exact_quote": "We argue that a universal comprehensive evaluation benchmark should have the following four characteristics: (1) It should cover as much as possible, including both perception and cognition abilities. The former refers to recognizing the specific object, such as its existence, count, position, and color. The latter refers to compositing the perception information and the knowledge in LLM to deduce more complex answers. It is obvious that the former is the premise of the latter. (2) Its data or annotations should not come from existing publicly available datasets as much as possible, avoiding the risk of data leakage. (3) Its instructions should be as concise as possible and in line with human cognition. Although instruction design may have a large impact on the output, all models should be tested under the same unified instructions for fair comparison. A good MLLM should be able to generalize to such concise instructions, which are also very common in everyday life. (4) The responses of MLLMs to the instructions should be intuitive and convenient for quantitative analysis. The open-ended answer of MLLMs poses significant challenges to the quantization. Existing methods tend to use GPT or manual scoring, but there may be problems of inaccuracy and subjectivity."
            },
            "evidence": [
                {
                    "evidence_id": 4,
                    "evidence_text": "It should cover as much as possible, including both perception and cognition abilities.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "It should cover as much as possible, including both perception and cognition abilities."
                },
                {
                    "evidence_id": 5,
                    "evidence_text": "The former refers to recognizing the specific object, such as its existence, count, position, and color.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "The former refers to recognizing the specific object, such as its existence, count, position, and color."
                },
                {
                    "evidence_id": 6,
                    "evidence_text": "The latter refers to compositing the perception information and the knowledge in LLM to deduce more complex answers.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "The latter refers to compositing the perception information and the knowledge in LLM to deduce more complex answers."
                },
                {
                    "evidence_id": 7,
                    "evidence_text": "It is obvious that the former is the premise of the latter.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "It is obvious that the former is the premise of the latter."
                },
                {
                    "evidence_id": 8,
                    "evidence_text": "Its data or annotations should not come from existing publicly available datasets as much as possible, avoiding the risk of data leakage.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "Its data or annotations should not come from existing publicly available datasets as much as possible, avoiding the risk of data leakage."
                },
                {
                    "evidence_id": 9,
                    "evidence_text": "Its instructions should be as concise as possible and in line with human cognition.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "Its instructions should be as concise as possible and in line with human cognition."
                },
                {
                    "evidence_id": 10,
                    "evidence_text": "Although instruction design may have a large impact on the output, all models should be tested under the same unified instructions for fair comparison.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "Although instruction design may have a large impact on the output, all models should be tested under the same unified instructions for fair comparison."
                },
                {
                    "evidence_id": 11,
                    "evidence_text": "A good MLLM should be able to generalize to such concise instructions, which are also very common in everyday life.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "A good MLLM should be able to generalize to such concise instructions, which are also very common in everyday life."
                },
                {
                    "evidence_id": 12,
                    "evidence_text": "The responses of MLLMs to the instructions should be intuitive and convenient for quantitative analysis.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "The responses of MLLMs to the instructions should be intuitive and convenient for quantitative analysis."
                },
                {
                    "evidence_id": 13,
                    "evidence_text": "The open-ended answer of MLLMs poses significant challenges to the quantization.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "The open-ended answer of MLLMs poses significant challenges to the quantization."
                },
                {
                    "evidence_id": 14,
                    "evidence_text": "Existing methods tend to use GPT or manual scoring, but there may be problems of inaccuracy and subjectivity.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "Existing methods tend to use GPT or manual scoring, but there may be problems of inaccuracy and subjectivity."
                }
            ],
            "conclusion": {
                "claim_id": 4,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "We collect a comprehensive MLLM Evaluation benchmark, named as MME, which meets the above four characteristics at the same time:",
                "location": "Introduction",
                "type": "Contribution",
                "exact_quote": "We collect a comprehensive MLLM Evaluation benchmark, named as MME, which meets the above four characteristics at the same time:"
            },
            "evidence": [
                {
                    "evidence_id": 15,
                    "evidence_text": "We collect a comprehensive MLLM Evaluation benchmark, named as MME, which meets the above four characteristics at the same time.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We collect a comprehensive MLLM Evaluation benchmark, named as MME, which meets the above four characteristics at the same time."
                }
            ],
            "conclusion": {
                "claim_id": 5,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "MME covers the examination of perception and cognition abilities. Apart from OCR, the perception includes the recognition of coarse-grained and fine-grained objects. The former identifies the existence, count, position, and color of objects. The latter recognizes movie posters, celebrities, scenes, landmarks, and artworks. The cognition includes commonsense reasoning, numerical calculation, text translation, and code reasoning. The total number of subtasks is up to 14, as shown in Fig. 1.",
                "location": "Introduction",
                "type": "Contribution",
                "exact_quote": "MME covers the examination of perception and cognition abilities. Apart from OCR, the perception includes the recognition of coarse-grained and fine-grained objects. The former identifies the existence, count, position, and color of objects. The latter recognizes movie posters, celebrities, scenes, landmarks, and artworks. The cognition includes commonsense reasoning, numerical calculation, text translation, and code reasoning. The total number of subtasks is up to 14, as shown in Fig. 1."
            },
            "evidence": [
                {
                    "evidence_id": 16,
                    "evidence_text": "MME covers the examination of perception and cognition abilities.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "MME covers the examination of perception and cognition abilities."
                },
                {
                    "evidence_id": 17,
                    "evidence_text": "Apart from OCR, the perception includes the recognition of coarse-grained and fine-grained objects.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "Apart from OCR, the perception includes the recognition of coarse-grained and fine-grained objects."
                },
                {
                    "evidence_id": 18,
                    "evidence_text": "The former identifies the existence, count, position, and color of objects.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "The former identifies the existence, count, position, and color of objects."
                },
                {
                    "evidence_id": 19,
                    "evidence_text": "The latter recognizes movie posters, celebrities, scenes, landmarks, and artworks.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "The latter recognizes movie posters, celebrities, scenes, landmarks, and artworks."
                },
                {
                    "evidence_id": 20,
                    "evidence_text": "The cognition includes commonsense reasoning, numerical calculation, text translation, and code reasoning.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "The cognition includes commonsense reasoning, numerical calculation, text translation, and code reasoning."
                },
                {
                    "evidence_id": 21,
                    "evidence_text": "The total number of subtasks is up to 14, as shown in Fig. 1.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "The total number of subtasks is up to 14, as shown in Fig. 1."
                }
            ],
            "conclusion": {
                "claim_id": 6,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 7,
            "claim": {
                "text": "All instruction-answer pairs are manually constructed.",
                "location": "Introduction",
                "type": "Contribution",
                "exact_quote": "All instruction-answer pairs are manually constructed."
            },
            "evidence": [
                {
                    "evidence_id": 22,
                    "evidence_text": "All instruction-answer pairs are manually constructed.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "All instruction-answer pairs are manually constructed."
                }
            ],
            "conclusion": {
                "claim_id": 7,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 8,
            "claim": {
                "text": "The instructions of MME are designed concisely to avoid the impact of prompt engineering on the model output.",
                "location": "Introduction",
                "type": "Contribution",
                "exact_quote": "The instructions of MME are designed concisely to avoid the impact of prompt engineering on the model output."
            },
            "evidence": [
                {
                    "evidence_id": 23,
                    "evidence_text": "The instructions of MME are designed concisely to avoid the impact of prompt engineering on the model output.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "The instructions of MME are designed concisely to avoid the impact of prompt engineering on the model output."
                }
            ],
            "conclusion": {
                "claim_id": 8,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 9,
            "claim": {
                "text": "We argue that a good MLLM should be able to generalize to such simple and frequently used instructions, which are also very common in everyday life.",
                "location": "Introduction",
                "type": "Contribution",
                "exact_quote": "We argue that a good MLLM should be able to generalize to such simple and frequently used instructions, which are also very common in everyday life."
            },
            "evidence": [
                {
                    "evidence_id": 24,
                    "evidence_text": "We argue that a good MLLM should be able to generalize to such simple and frequently used instructions, which are also very common in everyday life.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We argue that a good MLLM should be able to generalize to such simple and frequently used instructions, which are also very common in everyday life."
                }
            ],
            "conclusion": {
                "claim_id": 9,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 10,
            "claim": {
                "text": "We conduct massive experiments to evaluate the zeroshot performance of 30 advanced MLLMs on the 14 subtasks.",
                "location": "Introduction",
                "type": "Contribution",
                "exact_quote": "We conduct massive experiments to evaluate the zeroshot performance of 30 advanced MLLMs on the 14 subtasks."
            },
            "evidence": [
                {
                    "evidence_id": 25,
                    "evidence_text": "We conduct massive experiments to evaluate the zeroshot performance of 30 advanced MLLMs on the 14 subtasks.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We conduct massive experiments to evaluate the zeroshot performance of 30 advanced MLLMs on the 14 subtasks."
                }
            ],
            "conclusion": {
                "claim_id": 10,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 11,
            "claim": {
                "text": "The evaluated MLLMs include BLIP-2, InstructBLIP, MiniGPT-4, PandaGPT, Multimodal-GPT, VisualGLM-6B, ImageBindLLM, VPGTrans, LaVIN, mPLUGOwl, Octopus, Muffin, Otter, LRVInstruction, Cheetor, LLaMA-Adapter-v2, GIT2, BLIVA, Lynx, MMICL, GPT4V, Skywork-MM, mPLUG-Owl2, QwenVL-Chat, XComposer-VL, LLaVA, Lion, SPHINX, InfMLLM, and WeMM.",
                "location": "Introduction",
                "type": "Contribution",
                "exact_quote": "The evaluated MLLMs include BLIP-2, InstructBLIP, MiniGPT-4, PandaGPT, Multimodal-GPT, VisualGLM-6B, ImageBindLLM, VPGTrans, LaVIN, mPLUGOwl, Octopus, Muffin, Otter, LRVInstruction, Cheetor, LLaMA-Adapter-v2, GIT2, BLIVA, Lynx, MMICL, GPT4V, Skywork-MM, mPLUG-Owl2, QwenVL-Chat, XComposer-VL, LLaVA, Lion, SPHINX, InfMLLM, and WeMM."
            },
            "evidence": [
                {
                    "evidence_id": 26,
                    "evidence_text": "The evaluated MLLMs include BLIP-2, InstructBLIP, MiniGPT-4, PandaGPT, Multimodal-GPT, VisualGLM-6B, ImageBindLLM, VPGTrans, LaVIN, mPLUGOwl, Octopus, Muffin, Otter, LRVInstruction, Cheetor, LLaMA-Adapter-v2, GIT2, BLIVA, Lynx, MMICL, GPT4V, Skywork-MM, mPLUG-Owl2, QwenVL-Chat, XComposer-VL, LLaVA, Lion, SPHINX, InfMLLM, and WeMM.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "The evaluated MLLMs include BLIP-2, InstructBLIP, MiniGPT-4, PandaGPT, Multimodal-GPT, VisualGLM-6B, ImageBindLLM, VPGTrans, LaVIN, mPLUGOwl, Octopus, Muffin, Otter, LRVInstruction, Cheetor, LLaMA-Adapter-v2, GIT2, BLIVA, Lynx, MMICL, GPT4V, Skywork-MM, mPLUG-Owl2, QwenVL-Chat, XComposer-VL, LLaVA, Lion, SPHINX, InfMLLM, and WeMM."
                }
            ],
            "conclusion": {
                "claim_id": 11,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 12,
            "claim": {
                "text": "As displayed in Fig. 2 that consists of 2 overall leaderboards (perception and cognition) and 14 individual leaderboards, these MLLMs show clear discrepancies in our MME evaluation benchmark.",
                "location": "Introduction",
                "type": "Contribution",
                "exact_quote": "As displayed in Fig. 2 that consists of 2 overall leaderboards (perception and cognition) and 14 individual leaderboards, these MLLMs show clear discrepancies in our MME evaluation benchmark."
            },
            "evidence": [
                {
                    "evidence_id": 27,
                    "evidence_text": "As displayed in Fig. 2 that consists of 2 overall leaderboards (perception and cognition) and 14 individual leaderboards, these MLLMs show clear discrepancies in our MME evaluation benchmark.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "As displayed in Fig. 2 that consists of 2 overall leaderboards (perception and cognition) and 14 individual leaderboards, these MLLMs show clear discrepancies in our MME evaluation benchmark."
                }
            ],
            "conclusion": {
                "claim_id": 12,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 13,
            "claim": {
                "text": "Fig. 3 also provides a comparison from the other perspective. We can see the range that current MLLMs can reach in each capability dimension.",
                "location": "Introduction",
                "type": "Contribution",
                "exact_quote": "Fig. 3 also provides a comparison from the other perspective. We can see the range that current MLLMs can reach in each capability dimension."
            },
            "evidence": [
                {
                    "evidence_id": 28,
                    "evidence_text": "Fig. 3 also provides a comparison from the other perspective. We can see the range that current MLLMs can reach in each capability dimension.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "Fig. 3 also provides a comparison from the other perspective. We can see the range that current MLLMs can reach in each capability dimension."
                }
            ],
            "conclusion": {
                "claim_id": 13,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 14,
            "claim": {
                "text": "More importantly, we have summarized four prominent problems exposed in experiments, including inability to follow basic instructions, a lack of basic perception and reasoning, as well as object hallucination.",
                "location": "Introduction",
                "type": "Contribution",
                "exact_quote": "More importantly, we have summarized four prominent problems exposed in experiments, including inability to follow basic instructions, a lack of basic perception and reasoning, as well as object hallucination."
            },
            "evidence": [
                {
                    "evidence_id": 29,
                    "evidence_text": "More importantly, we have summarized four prominent problems exposed in experiments, including inability to follow basic instructions, a lack of basic perception and reasoning, as well as object hallucination.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "More importantly, we have summarized four prominent problems exposed in experiments, including inability to follow basic instructions, a lack of basic perception and reasoning, as well as object hallucination."
                }
            ],
            "conclusion": {
                "claim_id": 14,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 15,
            "claim": {
                "text": "It is expected that these findings are instructive for the subsequent model optimization.",
                "location": "Introduction",
                "type": "Contribution",
                "exact_quote": "It is expected that these findings are instructive for the subsequent model optimization."
            },
            "evidence": [
                {
                    "evidence_id": 30,
                    "evidence_text": "It is expected that these findings are instructive for the subsequent model optimization.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "It is expected that these findings are instructive for the subsequent model optimization."
                }
            ],
            "conclusion": {
                "claim_id": 15,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 16,
            "claim": {
                "text": "In summary, the contributions of this work are as follows: (1) We propose a new benchmark MME to meet the urgent need of MLLM evaluation. (2) A total of 30 up-todate MLLMs are evaluated on our MME. (3) We summarize the exposed problems in experiments, proving guidance for the evolution of MLLMs.",
                "location": "Introduction",
                "type": "Contribution",
                "exact_quote": "In summary, the contributions of this work are as follows: (1) We propose a new benchmark MME to meet the urgent need of MLLM evaluation. (2) A total of 30 up-todate MLLMs are evaluated on our MME. (3) We summarize the exposed problems in experiments, proving guidance for the evolution of MLLMs."
            },
            "evidence": [
                {
                    "evidence_id": 31,
                    "evidence_text": "In summary, the contributions of this work are as follows: (1) We propose a new benchmark MME to meet the urgent need of MLLM evaluation. (2) A total of 30 up-todate MLLMs are evaluated on our MME. (3) We summarize the exposed problems in experiments, proving guidance for the evolution of MLLMs.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "In summary, the contributions of this work are as follows: (1) We propose a new benchmark MME to meet the urgent need of MLLM evaluation. (2) A total of 30 up-todate MLLMs are evaluated on our MME. (3) We summarize the exposed problems in experiments, proving guidance for the evolution of MLLMs."
                }
            ],
            "conclusion": {
                "claim_id": 16,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        }
    ],
    "execution_times": {
        "claims_analysis_time": "93.69 seconds",
        "evidence_analysis_time": "146.47 seconds",
        "conclusions_analysis_time": "33.33 seconds",
        "total_execution_time": "278.42 seconds"
    }
}