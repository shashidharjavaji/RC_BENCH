{
    "paper_analysis": [],
    "raw_claims": " For example:\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"Large language models (LLMs) have shown impressive results across a variety of natural language understanding and generation tasks while requiring little or no direct supervision.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"Large language models (LLMs) have shown impressive results across a variety of natural language understanding and generation tasks while requiring little or no direct supervision.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"We formulate and study Attributed QA as a key first step in the development of attributed LLMs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We formulate and study Attributed QA as a key first step in the development of attributed LLMs.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"We propose a reproducible evaluation framework for the task and benchmark a broad set of architectures.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We propose a reproducible evaluation framework for the task and benchmark a broad set of architectures.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"We take human annotations as a gold standard and show that a correlated automatic metric is suitable for development.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We take human annotations as a gold standard and show that a correlated automatic metric is suitable for development.\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"Our experimental work gives concrete answers to two key questions (How to measure attribution?, and How well do current state-of-the-art methods perform on attribution?), and give some hints as to how to address a third (How to build LLMs with attribution?).\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Our experimental work gives concrete answers to two key questions (How to measure attribution?, and How well do current state-of-the-art methods perform on attribution?), and give some hints as to how to address a third (How to build LLMs with attribution?).\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\"\n        },\n        {\n            \"claim_id\": 8,\n            \"claim_text\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\"\n        },\n        {\n            \"claim_id\": 9,\n            \"claim_text\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\"\n        },\n        {\n            \"claim_id\": 10,\n            \"claim_text\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\"\n        },\n        {\n            \"claim_id\": 11,\n            \"claim_text\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\"\n        },\n        {\n            \"claim_id\": 12,\n            \"claim_text\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\"\n        },\n        {\n            \"claim_id\": 13,\n            \"claim_text\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\"\n        },\n        {\n            \"claim_id\": 14,\n            \"claim_text\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\"\n        },\n        {\n            \"claim_id\": 15,\n            \"claim_text\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\"\n        },\n        {\n            \"claim_id\": 16,\n            \"claim_text\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\"\n        },\n        {\n            \"claim_id\": 17,\n            \"claim_text\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\"\n        },\n        {\n            \"claim_id\": 18,\n            \"claim_text\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\"\n        },\n        {\n            \"claim_id\": 19,\n            \"claim_text\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\"\n        },\n        {\n            \"claim_id\": 20,\n            \"claim_text\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\"\n        },\n        {\n            \"claim_id\": 21,\n            \"claim_text\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\"\n        },\n        {\n            \"claim_id\": 22,\n            \"claim_text\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\"\n        },\n        {\n            \"claim_id\": 23,\n            \"claim_text\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\"\n        },\n        {\n            \"claim_id\": 24,\n            \"claim_text\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\"\n        },\n        {\n            \"claim_id\": 25,\n            \"claim_text\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\"\n        },\n        {\n            \"claim_id\": 26,\n            \"claim_text\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\"\n        },\n        {\n            \"claim_id\": 27,\n            \"claim_text\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\"\n        },\n        {\n            \"claim_id\": 28,\n            \"claim_text\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\"\n        },\n        {\n            \"claim_id\": 29,\n            \"claim_text\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\"\n        },\n        {\n            \"claim_id\": 30,\n            \"claim_text\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\"\n        },\n        {\n            \"claim_id\": 31,\n            \"claim_text\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\"\n        },\n        {\n            \"claim_id\": 32,\n            \"claim_text\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\"\n        },\n        {\n            \"claim_id\": 33,\n            \"claim_text\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\"\n        },\n        {\n            \"claim_id\": 34,\n            \"claim_text\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\"\n        },\n        {\n            \"claim_id\": 35,\n            \"claim_text\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\"\n        },\n        {\n            \"claim_id\": 36,\n            \"claim_text\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\"\n        },\n        {\n            \"claim_id\": 37,\n            \"claim_text\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\"\n        },\n        {\n            \"claim_id\": 38,\n            \"claim_text\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\"\n        },\n        {\n            \"claim_id\": 39,\n            \"claim_text\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\"\n        },\n        {\n            \"claim_id\": 40,\n            \"claim_text\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\"\n        },\n        {\n            \"claim_id\": 41,\n            \"claim_text\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\"\n        },\n        {\n            \"claim_id\": 42,\n            \"claim_text\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\"\n        },\n        {\n            \"claim_id\": 43,\n            \"claim_text\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\"\n        },\n        {\n            \"claim_id\": 44,\n            \"claim_text\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\"\n        },\n        {\n            \"claim_id\": 45,\n            \"claim_text\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\"\n        },\n        {\n            \"claim_id\": 46,\n            \"claim_text\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\"\n        },\n        {\n            \"claim_id\": 47,\n            \"claim_text\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\"\n        },\n        {\n            \"claim_id\": 48,\n            \"claim_text\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\"\n        },\n        {\n            \"claim_id\": 49,\n            \"claim_text\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\"\n        },\n        {\n            \"claim_id\": 50,\n            \"claim_text\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\"\n        },\n        {\n            \"claim_id\": 51,\n            \"claim_text\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\"\n        },\n        {\n            \"claim_id\": 52,\n            \"claim_text\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\"\n        },\n        {\n            \"claim_id\": 53,\n            \"claim_text\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\"\n        },\n        {\n            \"claim_id\": 54,\n            \"claim_text\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\"\n        },\n        {\n            \"claim_id\": 55,\n            \"claim_text\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\"\n        },\n        {\n            \"claim_id\": 56,\n            \"claim_text\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\"\n        },\n        {\n            \"claim_id\": 57,\n            \"claim_text\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\"\n        },\n        {\n            \"claim_id\": 58,\n            \"claim_text\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\"\n        },\n        {\n            \"claim_id\": 59,\n            \"claim_text\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\"\n        },\n        {\n            \"claim_id\": 60,\n            \"claim_text\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\"\n        },\n        {\n            \"claim_id\": 61,\n            \"claim_text\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\"\n        },\n        {\n            \"claim_id\": 62,\n            \"claim_text\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\"\n        },\n        {\n            \"claim_id\": 63,\n            \"claim_text\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\"\n        },\n        {\n            \"claim_id\": 64,\n            \"claim_text\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\"\n        },\n        {\n            \"claim_id\": 65,\n            \"claim_text\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\"\n        },\n        {\n            \"claim_id\": 66,\n            \"claim_text\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\"\n        },\n        {\n            \"claim_id\": 67,\n            \"claim_text\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\"\n        },\n        {\n            \"claim_id\": 68,\n            \"claim_text\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\"\n        },\n        {\n            \"claim_id\": 69,\n            \"claim_text\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\"\n        },\n        {\n            \"claim_id\": 70,\n            \"claim_text\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\"\n        },\n        {\n            \"claim_id\": 71,\n            \"claim_text\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.\"\n        },\n        {\n            \"claim_id\": 72,\n            \"claim_text\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.\"\n        },\n        {\n            \"claim_id\": 73,\n            \"claim_text\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Result\",\n            \"exact_quote\": \"While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.\"\n        },\n        {\n            \"claim_id\": 74,\n            \"claim_text\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.\"\n        },\n        {\n            \"claim_id\": 75,\n            \"claim_text\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.\"\n        },\n        {\n            \"claim_id\": 76,\n            \"claim_text\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.\"\n        },\n        {\n            \"claim_id\": 77,\n            \"claim_text\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\",\n            \"location\": \"1 Introduction\",\n            \"claim_type\": \"Contribution\",\n            \"exact_quote\": \"To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).\"\n        },\n        {\n            \"claim_id\": 78,\n           ",
    "raw_evidence": "",
    "raw_conclusions": "",
    "execution_times": {
        "claims_analysis_time": "351.36 seconds",
        "evidence_analysis_time": "2.04 seconds",
        "conclusions_analysis_time": "2.05 seconds",
        "total_execution_time": "357.86 seconds"
    }
}