=== Paper Analysis Summary ===

Raw Claims:
 For example:
```json
{
    "claims": [
        {
            "claim_id": 1,
            "claim_text": "Large language models (LLMs) have shown impressive results across a variety of natural language understanding and generation tasks while requiring little or no direct supervision.",
            "location": "Abstract",
            "claim_type": "Result",
            "exact_quote": "Large language models (LLMs) have shown impressive results across a variety of natural language understanding and generation tasks while requiring little or no direct supervision."
        },
        {
            "claim_id": 2,
            "claim_text": "We formulate and study Attributed QA as a key first step in the development of attributed LLMs.",
            "location": "Abstract",
            "claim_type": "Contribution",
            "exact_quote": "We formulate and study Attributed QA as a key first step in the development of attributed LLMs."
        },
        {
            "claim_id": 3,
            "claim_text": "We propose a reproducible evaluation framework for the task and benchmark a broad set of architectures.",
            "location": "Abstract",
            "claim_type": "Contribution",
            "exact_quote": "We propose a reproducible evaluation framework for the task and benchmark a broad set of architectures."
        },
        {
            "claim_id": 4,
            "claim_text": "We take human annotations as a gold standard and show that a correlated automatic metric is suitable for development.",
            "location": "Abstract",
            "claim_type": "Contribution",
            "exact_quote": "We take human annotations as a gold standard and show that a correlated automatic metric is suitable for development."
        },
        {
            "claim_id": 5,
            "claim_text": "Our experimental work gives concrete answers to two key questions (How to measure attribution?, and How well do current state-of-the-art methods perform on attribution?), and give some hints as to how to address a third (How to build LLMs with attribution?).",
            "location": "Abstract",
            "claim_type": "Contribution",
            "exact_quote": "Our experimental work gives concrete answers to two key questions (How to measure attribution?, and How well do current state-of-the-art methods perform on attribution?), and give some hints as to how to address a third (How to build LLMs with attribution?)."
        },
        {
            "claim_id": 6,
            "claim_text": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard."
        },
        {
            "claim_id": 7,
            "claim_text": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015)."
        },
        {
            "claim_id": 8,
            "claim_text": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings."
        },
        {
            "claim_id": 9,
            "claim_text": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision."
        },
        {
            "claim_id": 10,
            "claim_text": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive."
        },
        {
            "claim_id": 11,
            "claim_text": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples."
        },
        {
            "claim_id": 12,
            "claim_text": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA."
        },
        {
            "claim_id": 13,
            "claim_text": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard."
        },
        {
            "claim_id": 14,
            "claim_text": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015)."
        },
        {
            "claim_id": 15,
            "claim_text": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings."
        },
        {
            "claim_id": 16,
            "claim_text": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision."
        },
        {
            "claim_id": 17,
            "claim_text": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive."
        },
        {
            "claim_id": 18,
            "claim_text": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples."
        },
        {
            "claim_id": 19,
            "claim_text": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA."
        },
        {
            "claim_id": 20,
            "claim_text": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard."
        },
        {
            "claim_id": 21,
            "claim_text": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015)."
        },
        {
            "claim_id": 22,
            "claim_text": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings."
        },
        {
            "claim_id": 23,
            "claim_text": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision."
        },
        {
            "claim_id": 24,
            "claim_text": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive."
        },
        {
            "claim_id": 25,
            "claim_text": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples."
        },
        {
            "claim_id": 26,
            "claim_text": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA."
        },
        {
            "claim_id": 27,
            "claim_text": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard."
        },
        {
            "claim_id": 28,
            "claim_text": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015)."
        },
        {
            "claim_id": 29,
            "claim_text": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings."
        },
        {
            "claim_id": 30,
            "claim_text": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision."
        },
        {
            "claim_id": 31,
            "claim_text": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive."
        },
        {
            "claim_id": 32,
            "claim_text": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples."
        },
        {
            "claim_id": 33,
            "claim_text": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA."
        },
        {
            "claim_id": 34,
            "claim_text": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard."
        },
        {
            "claim_id": 35,
            "claim_text": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015)."
        },
        {
            "claim_id": 36,
            "claim_text": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings."
        },
        {
            "claim_id": 37,
            "claim_text": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision."
        },
        {
            "claim_id": 38,
            "claim_text": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive."
        },
        {
            "claim_id": 39,
            "claim_text": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples."
        },
        {
            "claim_id": 40,
            "claim_text": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA."
        },
        {
            "claim_id": 41,
            "claim_text": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard."
        },
        {
            "claim_id": 42,
            "claim_text": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015)."
        },
        {
            "claim_id": 43,
            "claim_text": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings."
        },
        {
            "claim_id": 44,
            "claim_text": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision."
        },
        {
            "claim_id": 45,
            "claim_text": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive."
        },
        {
            "claim_id": 46,
            "claim_text": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples."
        },
        {
            "claim_id": 47,
            "claim_text": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA."
        },
        {
            "claim_id": 48,
            "claim_text": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard."
        },
        {
            "claim_id": 49,
            "claim_text": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015)."
        },
        {
            "claim_id": 50,
            "claim_text": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings."
        },
        {
            "claim_id": 51,
            "claim_text": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision."
        },
        {
            "claim_id": 52,
            "claim_text": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive."
        },
        {
            "claim_id": 53,
            "claim_text": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples."
        },
        {
            "claim_id": 54,
            "claim_text": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA."
        },
        {
            "claim_id": 55,
            "claim_text": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard."
        },
        {
            "claim_id": 56,
            "claim_text": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015)."
        },
        {
            "claim_id": 57,
            "claim_text": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings."
        },
        {
            "claim_id": 58,
            "claim_text": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision."
        },
        {
            "claim_id": 59,
            "claim_text": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive."
        },
        {
            "claim_id": 60,
            "claim_text": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples."
        },
        {
            "claim_id": 61,
            "claim_text": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA."
        },
        {
            "claim_id": 62,
            "claim_text": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard."
        },
        {
            "claim_id": 63,
            "claim_text": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015)."
        },
        {
            "claim_id": 64,
            "claim_text": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings."
        },
        {
            "claim_id": 65,
            "claim_text": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision."
        },
        {
            "claim_id": 66,
            "claim_text": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive."
        },
        {
            "claim_id": 67,
            "claim_text": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples."
        },
        {
            "claim_id": 68,
            "claim_text": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA."
        },
        {
            "claim_id": 69,
            "claim_text": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard."
        },
        {
            "claim_id": 70,
            "claim_text": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015)."
        },
        {
            "claim_id": 71,
            "claim_text": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings."
        },
        {
            "claim_id": 72,
            "claim_text": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "Further, we perform a systematic analysis of a broad set of systems based on state-of-the-art components, exploring different architectures and levels of supervision."
        },
        {
            "claim_id": 73,
            "claim_text": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive.",
            "location": "1 Introduction",
            "claim_type": "Result",
            "exact_quote": "While retrievethen-read architectures are attractive for their strong performance, they typically require a large amount of data to train and can be resource intensive."
        },
        {
            "claim_id": 74,
            "claim_text": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We are excited by the possibility of post-hoc attribution of LLM-generated answers (though this remains challenging), and end-to-end modeling that makes limited use of QA examples."
        },
        {
            "claim_id": 75,
            "claim_text": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We release scored system outputs to foster further exploration, at https://github.com/google-research-datasets/Attributed-QA."
        },
        {
            "claim_id": 76,
            "claim_text": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard.",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "We define a reproducible evaluation framework for Attributed QA, using human annotations as a gold standard."
        },
        {
            "claim_id": 77,
            "claim_text": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015).",
            "location": "1 Introduction",
            "claim_type": "Contribution",
            "exact_quote": "To facilitate progress, we additionally study AutoAIS (Gao et al., 2022), an automatic metric that formulates evaluation as a Natural Language Inference task (Dagan et al., 2005; Bowman et al., 2015)."
        },
        {
            "claim_id": 78,
           

Raw Evidence:


Raw Conclusions:


Execution Times:
claims_analysis_time: 351.36 seconds
evidence_analysis_time: 2.04 seconds
conclusions_analysis_time: 2.05 seconds
total_execution_time: 357.86 seconds
