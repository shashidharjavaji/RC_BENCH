{
    "paper_analysis": [],
    "raw_claims": " For example:\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Large Language Models (LLMs) have excelled as high-level semantic planners for sequential decision-making tasks.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"However, harnessing them to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"We bridge this fundamental gap and present EUREKA, a human-level reward design algorithm powered by LLMs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"We bridge this fundamental gap and present EUREKA, a human-level reward design algorithm powered by LLMs.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"EUREKA exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"EUREKA exploits the remarkable zero-shot generation, code-writing, and in-context improvement capabilities of state-of-the-art LLMs, such as GPT-4, to perform evolutionary optimization over reward code.\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"The resulting rewards can then be used to acquire complex skills via reinforcement learning.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"The resulting rewards can then be used to acquire complex skills via reinforcement learning.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"Without any task-specific prompting or pre-defined reward templates, EUREKA generates reward functions that outperform expert human-engineered rewards.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Without any task-specific prompting or pre-defined reward templates, EUREKA generates reward functions that outperform expert human-engineered rewards.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, EUREKA outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"In a diverse suite of 29 open-source RL environments that include 10 distinct robot morphologies, EUREKA outperforms human experts on 83% of the tasks, leading to an average normalized improvement of 52%.\"\n        },\n        {\n            \"claim_id\": 8,\n            \"claim_text\": \"The generality of EUREKA also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"The generality of EUREKA also enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF), readily incorporating human inputs to improve the quality and the safety of the generated rewards without model updating.\"\n        },\n        {\n            \"claim_id\": 9,\n            \"claim_text\": \"Finally, using EUREKA rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Finally, using EUREKA rewards in a curriculum learning setting, we demonstrate for the first time, a simulated Shadow Hand capable of performing pen spinning tricks, adeptly manipulating a pen in circles at rapid speed.\"\n        },\n        {\n            \"claim_id\": 10,\n            \"claim_text\": \"Large Language Models (LLMs) have excelled as high-level semantic planners for robotics tasks (Ahn et al., 2022; Singh et al., 2023), but whether they can be used to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem.\",\n            \"location\": \"Introduction\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Large Language Models (LLMs) have excelled as high-level semantic planners for robotics tasks (Ahn et al., 2022; Singh et al., 2023), but whether they can be used to learn complex low-level manipulation tasks, such as dexterous pen spinning, remains an open problem.\"\n        },\n        {\n            \"claim_id\": 11,\n            \"claim_text\": \"Existing attempts require substantial domain expertise to construct task prompts or learn only simple skills, leaving a substantial gap in achieving human-level dexterity (Yu et al., 2023; Brohan et al., 2023).\",\n            \"location\": \"Introduction\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Existing attempts require substantial domain expertise to construct task prompts or learn only simple skills, leaving a substantial gap in achieving human-level dexterity (Yu et al., 2023; Brohan et al., 2023).\"\n        },\n        {\n            \"claim_id\": 12,\n            \"claim_text\": \"On the other hand, reinforcement learning (RL) has achieved impressive results in dexterity (Andrychowicz et al., 2020; Handa et al., 2023) as well as many other domains-if the human designers can carefully construct reward functions that accurately codify and provide learning signals for the desired behavior; likewise, many real-world RL tasks admit sparse rewards that are difficult for learning, necessitating reward shaping that provides incremental learning signals.\",\n            \"location\": \"Introduction\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"On the other hand, reinforcement learning (RL) has achieved impressive results in dexterity (Andrychowicz et al., 2020; Handa et al., 2023) as well as many other domains-if the human designers can carefully construct reward functions that accurately codify and provide learning signals for the desired behavior; likewise, many real-world RL tasks admit sparse rewards that are difficult for learning, necessitating reward shaping that provides incremental learning signals.\"\n        },\n        {\n            \"claim_id\": 13,\n            \"claim_text\": \"Despite their fundamental importance, reward functions are known to be notoriously difficult to design in practice (Russell & Norvig, 1995; Sutton & Barto, 2018); a recent survey conducted finds 92% of polled reinforcement learning researchers and practitioners report manual trial-and-error reward design and 89% indicate that their designed rewards are sub-optimal (Booth et al., 2023) and lead to unintended behavior (Hadfield-Menell et al., 2017).\",\n            \"location\": \"Introduction\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Despite their fundamental importance, reward functions are known to be notoriously difficult to design in practice (Russell & Norvig, 1995; Sutton & Barto, 2018); a recent survey conducted finds 92% of polled reinforcement learning researchers and practitioners report manual trial-and-error reward design and 89% indicate that their designed rewards are sub-optimal (Booth et al., 2023) and lead to unintended behavior (Hadfield-Menell et al., 2017).\"\n        },\n        {\n            \"claim_id\": 14,\n            \"claim_text\": \"Given the paramount importance of reward design, we ask whether it is possible to develop a universal reward programming algorithm using state-of-the-art coding LLMs, such as GPT-4. Their remarkable abilities in code writing, zero-shot generation, and in-context learning have previously enabled effective programmatic agents (Shinn et al., 2023; Wang et al., 2023a). Ideally, this reward design algorithm should achieve human-level reward generation capabilities that scale to a broad spectrum of tasks, automate the tedious trial-and-error procedure without human supervision, and yet be compatible with human oversight to assure safety and alignment.\",\n            \"location\": \"Introduction\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Given the paramount importance of reward design, we ask whether it is possible to develop a universal reward programming algorithm using state-of-the-art coding LLMs, such as GPT-4. Their remarkable abilities in code writing, zero-shot generation, and in-context learning have previously enabled effective programmatic agents (Shinn et al., 2023; Wang et al., 2023a). Ideally, this reward design algorithm should achieve human-level reward generation capabilities that scale to a broad spectrum of tasks, automate the tedious trial-and-error procedure without human supervision, and yet be compatible with human oversight to assure safety and alignment.\"\n        },\n        {\n            \"claim_id\": 15,\n            \"claim_text\": \"We introduce Evolution-driven Universal REward Kit for Agent (EUREKA), a novel reward design algorithm powered by coding LLMs with the following contributions:\",\n            \"location\": \"Introduction\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"We introduce Evolution-driven Universal REward Kit for Agent (EUREKA), a novel reward design algorithm powered by coding LLMs with the following contributions:\"\n        },\n        {\n            \"claim_id\": 16,\n            \"claim_text\": \"1. Achieves human-level performance on reward design across a diverse suite of 29 open-sourced RL environments that include 10 distinct robot morphologies, including quadruped, quadcopter, biped, manipulator, as well as several dexterous hands; see Fig. 1. Without any task-specific prompting or reward templates, EUREKA autonomously generates rewards that outperform expert human rewards on 83% of the tasks and realizes an average normalized improvement of 52%.\",\n            \"location\": \"Introduction\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"1. Achieves human-level performance on reward design across a diverse suite of 29 open-sourced RL environments that include 10 distinct robot morphologies, including quadruped, quadcopter, biped, manipulator, as well as several dexterous hands; see Fig. 1. Without any task-specific prompting or reward templates, EUREKA autonomously generates rewards that outperform expert human rewards on 83% of the tasks and realizes an average normalized improvement of 52%.\"\n        },\n        {\n            \"claim_id\": 17,\n            \"claim_text\": \"2. Solves dexterous manipulation tasks that were previously not feasible by manual reward engineering. We consider pen spinning, in which a five-finger hand needs to rapidly rotate a pen in pre-defined spinning configurations for as many cycles as possible. Combining EUREKA with curriculum learning, we demonstrate for the first time rapid pen spinning maneuvers on a simulated anthropomorphic Shadow Hand (see Figure 1 bottom).\",\n            \"location\": \"Introduction\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"2. Solves dexterous manipulation tasks that were previously not feasible by manual reward engineering. We consider pen spinning, in which a five-finger hand needs to rapidly rotate a pen in pre-defined spinning configurations for as many cycles as possible. Combining EUREKA with curriculum learning, we demonstrate for the first time rapid pen spinning maneuvers on a simulated anthropomorphic Shadow Hand (see Figure 1 bottom).\"\n        },\n        {\n            \"claim_id\": 18,\n            \"claim_text\": \"3. Enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF) that can generate more performant and human-aligned reward functions based on various forms of human inputs without model updating.\",\n            \"location\": \"Introduction\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"3. Enables a new gradient-free in-context learning approach to reinforcement learning from human feedback (RLHF) that can generate more performant and human-aligned reward functions based on various forms of human inputs without model updating.\"\n        },\n        {\n            \"claim_id\": 19,\n            \"claim_text\": \"Unlike prior work L2R on using LLMs to aid reward design (Yu et al., 2023), EUREKA is completely free of task-specific prompts, reward templates, as well as few-shot examples.\",\n            \"location\": \"Introduction\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Unlike prior work L2R on using LLMs to aid reward design (Yu et al., 2023), EUREKA is completely free of task-specific prompts, reward templates, as well as few-shot examples.\"\n        },\n        {\n            \"claim_id\": 20,\n            \"claim_text\": \"In our experiments, EUREKA significantly outperforms L2R due to its ability to generate free-form, expressive reward programs.\",\n            \"location\": \"Introduction\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"In our experiments, EUREKA significantly outperforms L2R due to its ability to generate free-form, expressive reward programs.\"\n        },\n        {\n            \"claim_id\": 21,\n            \"claim_text\": \"EUREKA\u2019s generality is made possible through three key algorithmic design choices: environment as context, evolutionary search, and reward reflection.\",\n            \"location\": \"Introduction\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"EUREKA\u2019s generality is made possible through three key algorithmic design choices: environment as context, evolutionary search, and reward reflection.\"\n        },\n        {\n            \"claim_id\": 22,\n            \"claim_text\": \"First, by taking the environment source code as context, EUREKA can zero-shot generate executable reward functions from the backbone coding LLM (GPT-4).\",\n            \"location\": \"Introduction\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"First, by taking the environment source code as context, EUREKA can zero-shot generate executable reward functions from the backbone coding LLM (GPT-4).\"\n        },\n        {\n            \"claim_id\": 23,\n            \"claim_text\": \"Then, EUREKA substantially improves the quality of its rewards by performing evolutionary search, iteratively proposing batches of reward candidates and refining the most promising ones within the LLM context window.\",\n            \"location\": \"Introduction\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Then, EUREKA substantially improves the quality of its rewards by performing evolutionary search, iteratively proposing batches of reward candidates and refining the most promising ones within the LLM context window.\"\n        },\n        {\n            \"claim_id\": 24,\n            \"claim_text\": \"This in-context improvement is made effective via reward reflection, a textual summary of the reward quality based on policy training statistics that enables automated and targeted reward editing; see Fig. 3 for an example of EUREKA zero-shot reward as well as various improvements accumulated during its optimization.\",\n            \"location\": \"Introduction\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"This in-context improvement is made effective via reward reflection, a textual summary of the reward quality based on policy training statistics that enables automated and targeted reward editing; see Fig. 3 for an example of EUREKA zero-shot reward as well as various improvements accumulated during its optimization.\"\n        },\n        {\n            \"claim_id\": 25,\n            \"claim_text\": \"To ensure that EUREKA can scale up its reward search to maximum potential, EUREKA evaluates intermediate rewards using GPU-accelerated distributed reinforcement learning on IsaacGym (Makoviychuk et al., 2021), which offers up to three orders of magnitude in policy learning speed, making EUREKA an extensive algorithm that scales naturally with more compute.\",\n            \"location\": \"Introduction\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"To ensure that EUREKA can scale up its reward search to maximum potential, EUREKA evaluates intermediate rewards using GPU-accelerated distributed reinforcement learning on IsaacGym (Makoviychuk et al., 2021), which offers up to three orders of magnitude in policy learning speed, making EUREKA an extensive algorithm that scales naturally with more compute.\"\n        },\n        {\n            \"claim_id\": 26,\n            \"claim_text\": \"See Fig. 2 for an overview.\",\n            \"location\": \"Introduction\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"See Fig. 2 for an overview.\"\n        },\n        {\n            \"claim_id\": 27,\n            \"claim_text\": \"We are committed to open-sourcing all prompts, environments, and generated reward functions to promote further research on LLM-based reward design.\",\n            \"location\": \"Introduction\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"We are committed to open-sourcing all prompts, environments, and generated reward functions to promote further research on LLM-based reward design.\"\n        },\n        {\n            \"claim_id\": 28,\n            \"claim_text\": \"The goal of reward design is to return a shaped reward function for a ground-truth reward function that may be difficult to optimize directly (e.g., sparse rewards); this ground-truth reward function may only be accessed via queries by the designer.\",\n            \"location\": \"Problem Setting and Definitions\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"The goal of reward design is to return a shaped reward function for a ground-truth reward function that may be difficult to optimize directly (e.g., sparse rewards); this ground-truth reward function may only be accessed via queries by the designer.\"\n        },\n        {\n            \"claim_id\": 29,\n            \"claim_text\": \"We first introduce the formal definition from Singh et al. (2010), which we then adapt to the program synthesis setting, which we call reward generation.\",\n            \"location\": \"Problem Setting and Definitions\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"We first introduce the formal definition from Singh et al. (2010), which we then adapt to the program synthesis setting, which we call reward generation.\"\n        },\n        {\n            \"claim_id\": 30,\n            \"claim_text\": \"Definition 2.1. (Reward Design Problem (Singh et al., 2010)) A reward design problem (RDP) is a tuple P = \u27e8M, R, \u03c0M _, F_ _\u27e9, where M = (S, A, T_ ) is the world model with state space S, action space A, and transition function T. R is the space of reward functions; AM (\u00b7) : R \u2192 \u03a0 is a learning algorithm that outputs a policy \u03c0 : S \u2206(A) that optimizes reward R in the resulting Markov _\u2192_ _\u2208R_\n_Decision Process (MDP), (M, R); F : \u03a0 \u2192_ R is the fitness function that produces a scalar evaluation of any policy, which may only be accessed via policy queries (i.e., evaluate the policy using the ground truth reward function). In an RDP, the goal is to output a reward function R such that\n_\u2208R_\nthe policy \u03c0 := AM (R) that optimizes R achieves the highest fitness score F (\u03c0).\",\n            \"location\": \"Problem Setting and Definitions\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Definition 2.1. (Reward Design Problem (Singh et al., 2010)) A reward design problem (RDP) is a tuple P = \u27e8M, R, \u03c0M _, F_ _\u27e9, where M = (S, A, T_ ) is the world model with state space S, action space A, and transition function T. R is the space of reward functions; AM (\u00b7) : R \u2192 \u03a0 is a learning algorithm that outputs a policy \u03c0 : S \u2206(A) that optimizes reward R in the resulting Markov _\u2192_ _\u2208R_\n_Decision Process (MDP), (M, R); F : \u03a0 \u2192_ R is the fitness function that produces a scalar evaluation of any policy, which may only be accessed via policy queries (i.e., evaluate the policy using the ground truth reward function). In an RDP, the goal is to output a reward function R such that\n_\u2208R_\nthe policy \u03c0 := AM (R) that optimizes R achieves the highest fitness score F (\u03c0).\"\n        },\n        {\n            \"claim_id\": 31,\n            \"claim_text\": \"Reward Generation Problem. In our problem setting, every component within a RDP is specified via code. Then, given a string l that specifies the task, the objective of the reward generation problem is to output a reward function code R such that F (AM (R)) is maximized.\",\n            \"location\": \"Problem Setting and Definitions\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Reward Generation Problem. In our problem setting, every component within a RDP is specified via code. Then, given a string l that specifies the task, the objective of the reward generation problem is to output a reward function code R such that F (AM (R)) is maximized.\"\n        },\n        {\n            \"claim_id\": 32,\n            \"claim_text\": \"EUREKA consists of three algorithmic components: 1) environment as context that enables zero-shot generation of executable rewards, 2) evolutionary search that iteratively proposes and refines reward candidates, and 3) reward reflection that enables fine-grained reward improvement.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"EUREKA consists of three algorithmic components: 1) environment as context that enables zero-shot generation of executable rewards, 2) evolutionary search that iteratively proposes and refines reward candidates, and 3) reward reflection that enables fine-grained reward improvement.\"\n        },\n        {\n            \"claim_id\": 33,\n            \"claim_text\": \"See Alg. 1 for pseudocode; all prompts are included in App. A.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"See Alg. 1 for pseudocode; all prompts are included in App. A.\"\n        },\n        {\n            \"claim_id\": 34,\n            \"claim_text\": \"3.1 ENVIRONMENT AS CONTEXT\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"3.1 ENVIRONMENT AS CONTEXT\"\n        },\n        {\n            \"claim_id\": 35,\n            \"claim_text\": \"Reward design requires the environment specification to be provided to the LLM. We propose directly feeding the raw environment source code (without the reward code, if exists) as context.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Reward design requires the environment specification to be provided to the LLM. We propose directly feeding the raw environment source code (without the reward code, if exists) as context.\"\n        },\n        {\n            \"claim_id\": 36,\n            \"claim_text\": \"Given that any reward function is a function over the environment\u2019s state and action variables, the only requirement in the source code is that it exposes these environment variables, which is easy to satisfy.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Given that any reward function is a function over the environment\u2019s state and action variables, the only requirement in the source code is that it exposes these environment variables, which is easy to satisfy.\"\n        },\n        {\n            \"claim_id\": 37,\n            \"claim_text\": \"In cases where the source code is not available, relevant state information can also be supplied via an API, for example.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"In cases where the source code is not available, relevant state information can also be supplied via an API, for example.\"\n        },\n        {\n            \"claim_id\": 38,\n            \"claim_text\": \"In practice, to ensure that the environment code fits within the LLM\u2019s context window and does not leak simulation internals (so that we can expect the same prompt to generalize to new simulators), we have an automatic script to extract just the environment code snippets that expose and fully specify the environment state and action variables.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"In practice, to ensure that the environment code fits within the LLM\u2019s context window and does not leak simulation internals (so that we can expect the same prompt to generalize to new simulators), we have an automatic script to extract just the environment code snippets that expose and fully specify the environment state and action variables.\"\n        },\n        {\n            \"claim_id\": 39,\n            \"claim_text\": \"Given environment as context, EUREKA instructs the coding LLM to directly return executable Python code with only generic reward design and formatting tips, such as exposing 1: Require: Task description l, environment code M, individual components in the reward coding LLM LLM, fitness function F, initial prompt prompt as a dictionary output (for reasons 2: Hyperparameters: search iteration N, iteration batch size K\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Given environment as context, EUREKA instructs the coding LLM to directly return executable Python code with only generic reward design and formatting tips, such as exposing 1: Require: Task description l, environment code M, individual components in the reward coding LLM LLM, fitness function F, initial prompt prompt as a dictionary output (for reasons 2: Hyperparameters: search iteration N, iteration batch size K\"\n        },\n        {\n            \"claim_id\": 40,\n            \"claim_text\": \"3: for N iterations do\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"3: for N iterations do\"\n        },\n        {\n            \"claim_id\": 41,\n            \"claim_text\": \"// Sample K reward code from LLM\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"// Sample K reward code from LLM\"\n        },\n        {\n            \"claim_id\": 42,\n            \"claim_text\": \"R1,..., Rk \u223c LLM(l, M, prompt)\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"R1,..., Rk \u223c LLM(l, M, prompt)\"\n        },\n        {\n            \"claim_id\": 43,\n            \"claim_text\": \"Remarkably, with only these minimal instructions, EUREKA can already zero-shot generate plausibly-looking rewards in diverse environments in its first attempts.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Remarkably, with only these minimal instructions, EUREKA can already zero-shot generate plausibly-looking rewards in diverse environments in its first attempts.\"\n        },\n        {\n            \"claim_id\": 44,\n            \"claim_text\": \"An example EUREKA where best = arg maxk s1,..., sK output is shown in Fig. 3.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"An example EUREKA where best = arg maxk s1,..., sK output is shown in Fig. 3.\"\n        },\n        {\n            \"claim_id\": 45,\n            \"claim_text\": \"As seen, EUREKA adeptly composes over existing observation variables (e.g., fingertip pos) in the provided environment code and produces a competent reward code \u2013 all without any environment-specific prompt engineering or reward templating.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"As seen, EUREKA adeptly composes over existing observation variables (e.g., fingertip pos) in the provided environment code and produces a competent reward code \u2013 all without any environment-specific prompt engineering or reward templating.\"\n        },\n        {\n            \"claim_id\": 46,\n            \"claim_text\": \"On the first try, however, the generated reward may not always be executable, and even if it is, it can be quite sub-optimal with respect to the task fitness metric F. While we can improve the prompt with task-specific formatting and reward design hints, doing so does not scale to new tasks and hinders the overall generality of our system.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"On the first try, however, the generated reward may not always be executable, and even if it is, it can be quite sub-optimal with respect to the task fitness metric F. While we can improve the prompt with task-specific formatting and reward design hints, doing so does not scale to new tasks and hinders the overall generality of our system.\"\n        },\n        {\n            \"claim_id\": 47,\n            \"claim_text\": \"How can we effectively overcome the sub-optimality of single-sample reward generation?\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"How can we effectively overcome the sub-optimality of single-sample reward generation?\"\n        },\n        {\n            \"claim_id\": 48,\n            \"claim_text\": \"In this section, we will demonstrate how evolutionary search presents a natural solution that addresses the aforementioned execution error and sub-optimality challenges.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"In this section, we will demonstrate how evolutionary search presents a natural solution that addresses the aforementioned execution error and sub-optimality challenges.\"\n        },\n        {\n            \"claim_id\": 49,\n            \"claim_text\": \"In each iteration, EUREKA samples several independent outputs from the LLM (Line 5 in Alg. 1). Since the generations are i.i.d, the probability that all reward functions from an iteration are buggy exponentially decreases as the number of samples increases.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"In each iteration, EUREKA samples several independent outputs from the LLM (Line 5 in Alg. 1). Since the generations are i.i.d, the probability that all reward functions from an iteration are buggy exponentially decreases as the number of samples increases.\"\n        },\n        {\n            \"claim_id\": 50,\n            \"claim_text\": \"We find that for all environments we consider, sampling just a modest number of samples (16) contains at least one executable reward code in the first iteration.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"We find that for all environments we consider, sampling just a modest number of samples (16) contains at least one executable reward code in the first iteration.\"\n        },\n        {\n            \"claim_id\": 51,\n            \"claim_text\": \"Given executable reward functions from an earlier iteration, EUREKA performs in-context reward mutation, proposing new improved reward functions from the best one in the previous iteration.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Given executable reward functions from an earlier iteration, EUREKA performs in-context reward mutation, proposing new improved reward functions from the best one in the previous iteration.\"\n        },\n        {\n            \"claim_id\": 52,\n            \"claim_text\": \"Concretely, a new EUREKA iteration will take the best-performing reward from the previous iteration, its reward reflection (Sec. 3.3), and the mutation prompt (Prompt 2 in App. A) as context and generate K more i.i.d reward outputs from the LLM; several illustrative reward modifications are visualized in Fig. 3.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Concretely, a new EUREKA iteration will take the best-performing reward from the previous iteration, its reward reflection (Sec. 3.3), and the mutation prompt (Prompt 2 in App. A) as context and generate K more i.i.d reward outputs from the LLM; several illustrative reward modifications are visualized in Fig. 3.\"\n        },\n        {\n            \"claim_id\": 53,\n            \"claim_text\": \"This iterative optimization continues until a specified number of iterations has been reached.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"This iterative optimization continues until a specified number of iterations has been reached.\"\n        },\n        {\n            \"claim_id\": 54,\n            \"claim_text\": \"Finally, we perform multiple random restarts to find better maxima; this is a standard strategy in global optimization.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Finally, we perform multiple random restarts to find better maxima; this is a standard strategy in global optimization.\"\n        },\n        {\n            \"claim_id\": 55,\n            \"claim_text\": \"In all our experiments, EUREKA conducts 5 independent runs per environment, and for each run, searches for 5 iterations with K = 16 samples per iteration.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"In all our experiments, EUREKA conducts 5 independent runs per environment, and for each run, searches for 5 iterations with K = 16 samples per iteration.\"\n        },\n        {\n            \"claim_id\": 56,\n            \"claim_text\": \"3.3 REWARD REFLECTION\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"3.3 REWARD REFLECTION\"\n        },\n        {\n            \"claim_id\": 57,\n            \"claim_text\": \"In order to ground the in-context reward mutation, we must be able to put into words the quality of the generated rewards.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"In order to ground the in-context reward mutation, we must be able to put into words the quality of the generated rewards.\"\n        },\n        {\n            \"claim_id\": 58,\n            \"claim_text\": \"We propose reward reflection, an automated feedback that summarizes the policy training dynamics in texts.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"We propose reward reflection, an automated feedback that summarizes the policy training dynamics in texts.\"\n        },\n        {\n            \"claim_id\": 59,\n            \"claim_text\": \"Specifically, given that EUREKA reward functions are asked to expose their individual components in the reward program (e.g., reward components in Fig. 3), reward reflection tracks the scalar values of all reward components and the task fitness function at intermediate policy checkpoints throughout training.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"Specifically, given that EUREKA reward functions are asked to expose their individual components in the reward program (e.g., reward components in Fig. 3), reward reflection tracks the scalar values of all reward components and the task fitness function at intermediate policy checkpoints throughout training.\"\n        },\n        {\n            \"claim_id\": 60,\n            \"claim_text\": \"For instance, consider the illustrative example in Fig. 2, where the snapshot values of av penalty are provided as a list in the reward feedback.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"For instance, consider the illustrative example in Fig. 2, where the snapshot values of av penalty are provided as a list in the reward feedback.\"\n        },\n        {\n            \"claim_id\": 61,\n            \"claim_text\": \"See App. G.1 for full examples.\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"See App. G.1 for full examples.\"\n        },\n        {\n            \"claim_id\": 62,\n            \"claim_text\": \"This reward reflection procedure, though simple to construct, is important due to two reasons: (1) the lack of fine-grained reward improvement signal in the task fitness function, and (2) the algorithm-dependent nature of reward optimization (Booth et al., 2023).\",\n            \"location\": \"Method\",\n            \"claim_type\": \"Observation\",\n            \"exact_quote\": \"This reward reflection procedure, though simple to construct, is important due to two reasons: (1) the lack of fine-grained reward improvement signal in the task fitness function, and (2) the algorithm-dependent nature of reward optimization (Booth",
    "raw_evidence": "",
    "raw_conclusions": "",
    "execution_times": {
        "claims_analysis_time": "317.75 seconds",
        "evidence_analysis_time": "1.63 seconds",
        "conclusions_analysis_time": "1.62 seconds",
        "total_execution_time": "323.81 seconds"
    }
}