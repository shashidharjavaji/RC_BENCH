{
    "paper_analysis": [],
    "raw_claims": " For example:\n        ```json\n        {\n            \"claims\": [\n                {\n                    \"claim_id\": 1,\n                    \"claim_text\": \"Example claim text\",\n                    \"location\": \"Section X\",\n                    \"claim_type\": \"Example type\",\n                    \"exact_quote\": \"Exact quote from the paper\"\n                }\n            ]\n        }\n        ```\n\n        Paper text: # Learning to Generate Research Idea with Dynamic Control\n\n## Abstract\n\nThe rapid advancements in large language models (LLMs)\nhave demonstrated their potential to accelerate scientific discovery, particularly in automating the process of research\nideation. LLM-based systems have shown promise in generating hypotheses and research ideas. However, current approaches predominantly rely on prompting-based pre-trained\nmodels, limiting their ability to optimize generated content\neffectively. Moreover, they also lack the capability to deal with\nthe complex interdependence and inherent restrictions among\nnovelty, feasibility, and effectiveness, which remains challenging due to the inherent trade-offs among these dimensions,\nsuch as the innovation-feasibility conflict. To address these\nlimitations, we propose a novel framework that employs a\ntwo-stage approach combining Supervised Fine-Tuning (SFT)\nand controllable Reinforcement Learning (RL). In the SFT\nstage, the model learns foundational patterns from pairs of\nresearch papers and follow-up ideas. In the RL stage, multidimensional reward modeling, guided by fine-grained feedback, evaluates and optimizes the generated ideas across key\nmetrics. Dimensional controllers enable dynamic adjustment\nof generation, while a sentence-level decoder ensures contextaware emphasis during inference. Our framework provides a\nbalanced approach to research ideation, achieving high-quality\noutcomes by dynamically navigating the trade-offs among\nnovelty, feasibility, and effectiveness.\n\n## Introduction\n\nIn recent years, advances in LLM have been made in their\ncapacity to accelerate scientific discovery. Specifically, LLMs\nlike GPT-4 and LLama have demonstrated their capability to\nproduce coherent and contextually relevant text across diverse\napplications, including sentiment analysis (Zhang et al. 2023;\nYang et al. 2024a; Zhu et al. 2024), question answering (spr\n2024), and summarization (Authors 2023). Moreover, their\nremarkable performance in multi-step reasoning and complex\ndecision-making (Zhou et al. 2023; Park et al. 2023) has\nunderscored their potential in the field of research ideation.\nTypically, a well-developed research idea (or hypothesis)[1] is\nestablished with a methodology and an experiment plan, as\nshown in Figure 1.\nBy automating the research ideation process, these research idea generation systems can swiftly synthesize vast\n\n1In this paper, research idea and hypothesis are used interchangeably.\n\n\nFigure 1: Research ideas generation from research papers.\nEach idea is measured across dimensions of novelty, feasibility, and effectiveness.\n\ndata and insights, uncovering novel connections that might\nelude human researchers. This capability is evidenced by the\ngrowing number of studies employing research agents to autonomously generate and validate new ideas (Wang and Zhou\n2023; Du, Kim, and Park 2023; Bornstein and Singh 2024).\nHowever, despite notable progress in LLM-based research\nideation as demonstrated in prior work, these efforts primarily rely on pre-trained models with no task-specific learning.\nSuch reliance restricts the full exploitation of optimizing the\ngenerated content, underscoring the urgent need for further\nrefinement and development in this area.\nThe quality assessment of a research idea involves multiple aspects, centered around key metrics in three dimensions: (1) Novelty, which assesses how unique or original\nthe ideas generated by the system are, distinguishing them\nfrom existing ideas; (2) Feasibility evaluates how practical\nor implementable the ideas are given current resources and\nconstraints and (3) Effectiveness, which measures how likely\nthe generated ideas will achieve their intended outcomes or\nsolve identified problems. These metrics can serve as optimization objectives to guide the research ideation process, by\nleveraging techniques such as reinforcement learning (RL)().\nRecent studies have explored Reinforcement Learning from\nHuman Feedback (RLHF) to benefit LLM training (). However, existing techniques lack of capability to deal with the\ncomplex interdependence and inherent restrictions among\nthe metrics used for assessing research idea quality. For instance, a recent study highlights these challenges by revealing\nthe inevitable innovation-feasibility trade-off: highly novel\nhypotheses may lack feasibility, while overly feasible ideas\noften limit the scope for groundbreaking discoveries. (Chen\net al. 2024b). How to optimize research ideation towards\neach of the key metrics while balancing them with satisfying\ntrade-offs remains a critical, unresolved question.\nTo address this issue, we propose a novel research ideation\nframework designed to dynamically control the emphasis\non key assessment metrics through a two-stage approach:\nSFT and controllable RL. In the SFT stage, the idea generator learns foundational patterns by training on pairs of\nresearch papers and corresponding follow-up ideas. In the\nRL stage, we employ multi-dimensional reward modeling as\na real-world assessment approximation (Wu et al. 2023). Reward models, trained on fine-grained feedback from review\ndata, score each metric\u2014novelty, feasibility, and effectiveness\u2014providing detailed guidance for model refinement. To\nenable precise and adaptive control, we introduce dimensional controllers, trained alongside the RL process, which\nadjusts the generation style to prioritize specific metric dimensions when necessary. This is complemented at inference\ntime by a sentence-level decoder that dynamically adjusts\nthe weights of controllers, ensuring context-aware emphasis such as prioritizing novelty in the method section and feasibility in the experiment planning. Together, these mechanisms,\nguided by feedback signals from the reward models, result in\nmore balanced and high-quality idea generation.\nOur contributions are summarized as follows:\n\n - We propose a research ideation framework to dynamically\ncontrol the optimization of the generated idea towards\nnovelty, feasibility, and effectiveness.\n\n - We first introduce dynamic decoding into the RL-based\nsupervised fine-tuning framework, achieving satisfying\nperformance with a balanced trade-off among different\nassessment metrics of research ideation.\n\n - We train our reward models using collected real-world\ndatasets, enabling research idea scoring in a fine-grained\nmanner.\n\n - We conduct a comprehensive evaluation with a human\nstudy, demonstrating the effectiveness of our proposed\nmethod for optimized, controllable research ideation.\n\n\n## Related Work\n\n**NLP for scientific discovery NLP techniques have signifi-**\ncantly advanced scientific discovery by enabling researchers\nto manage extensive literature, identify knowledge gaps, and\nanalyze trends effectively (Raghu and Schmidt 2020; Hope\net al. 2021). Models such as SciBERT (Beltagy, Lo, and Cohan 2019) and BioBERT (Lee et al. 2020) pre-trained on\nscientific materials have enhanced these abilities by improving performance on fundamental tasks. Recent developments\nin LLMs have extended their utility to creative and generative tasks in scientific research. For example, LLMs have\nbeen employed to formulate research questions, generate\nhypotheses, draft research proposals, and even outline experimental designs (Brown et al. 2020; Zhong et al. 2023; Qi\net al. 2023; Yang et al. 2023; Wang et al. 2024a). Several\nprior works have specifically explored methods to enhance\nidea generation. Approaches such as iterative novelty boosting (Wang et al. 2024b), multi-agent collaboration (Baek\net al. 2024), and multi-module retrieval and revision (Yang\net al. 2024b) have been proposed to advance ideation capabilities beyond baseline prompting methods. Beyond ideation,\nanother branch of research leverages LLMs for automating\nexperimental workflows. Works like MLAgent (Huang et al.\n2024) and SciCode (Tian et al. 2024) have used LLMs to generate code for executing research experiments, while systems\nsuch as AI Scientist (Lu et al. 2024) and MLR-Copilot (Li\net al. 2024) combine idea generation with code implementation to directly test AI-generated concepts. However, these\napproaches are often limited to constrained problem spaces\nor rely on proxy metrics for evaluation, such as LLM-based\nscoring, which can be inconsistent and unreliable.\n\n**Fine-tuning LLM with RL** RLHF has shown success in diverse NLP tasks (Christiano et al. 2017; Stiennon et al. 2020;\nOuyang et al. 2022), including text summarization (Ziegler\net al. 2019), instruction following (Ouyang et al. 2022), and\nquestion answering (Nakano et al. 2021). While most works\nfocus on optimizing a single holistic reward combining multiple objectives, recent efforts have explored fine-grained\nrewards for specific attributes, such as reasoning or ethical\nconsiderations (Glaese et al. 2022; Uesato et al. 2022).\nAdditionally, non-RL methods have leveraged feedback to\nimprove model outputs. For example, supervised fine-tuning\nhas been used with high-scoring samples selected by reward\nmodels (Rafailov et al. 2023). Conversational models have\nincorporated binary user satisfaction signals to enhance response generation (Askell et al. 2021), while natural language feedback has been stored in memory banks and retrieved during task execution (Madaan et al. 2022). Some\napproaches refine outputs conditioned on human feedback\nand subsequently use reward models to select the best refinements (Scheurer et al. 2022; Menick et al. 2022).\n\n## Method\n\nWe introduce a scientific idea proposer with multi-dimension\nfeedback, which consists of two stages: supervised finetuning stage, and reinforcement learning stage that has three\ncomponents: reward modeling, multi-dimension reward augmented controllable reinforcement learning, and decoding.\n\n\n-----\n\n### Overview\n\nSuppose we have a training set D = {Xi, Yi}i[N]=1[, where]\n_Xi and Yi are research paper and idea, respectively. Then_\nwe fine-tune the language model M with the training\nset. Thereafter, we collect a reward training set Dr =\n_{(Xi[r][, Y][ n]i_ _[, Y]i[ f]_ _[, Y][ e]i_ [)]i[N]=1[}][, where][ X][i][ include the textual con-]\ntent of research paper and research idea, and Yi[n][, Y][ f]i _[, Y][ e]i_ [are]\nthe labels which show the scores of novelty, feasibility, and\neffectiveness of research idea. We could utilize this training\nset to train three reward models as follows,\n\uf8f1\uf8f4\uf8f2Fn = Rn(Xi[r][, Y]i[ n][|][\u0398][n][)][,]\n\n_Ff = Rf_ (Xi[r][, Y][ f]i _[|][\u0398][f]_ [)][,] (1)\n\n\uf8f4\uf8f3Fe = Re(Xi[r][, Y][ e]i _[|][\u0398][e][)]_\n\nwhere \u0398n/f/e is the parameters of the reward model Rn/f/e.\ndenotes reward models that aim to score the novelty,\n_R[n/f/e]_\nfeasibility, and effectiveness of the research idea. Fn/f/e is\nreward values from reward models. Then, we use a set of\n_Nf research papers {Pi}i[N]=1[f]_ [as input to the language model]\nto generate research ideas, which are assessed with reward\nmodels based on three criteria: novelty, feasibility, and effectiveness. Finally, we conduct reinforcement learning on the\nlanguage model as,\n_H = M(P_ _|\u0398m, \u0398n, \u0398f_ _, \u0398e),_ (2)\nwhere \u0398m is final optimized parameters of the language\nmodel. During which the dimensional controllers are\n_M_\njointly trained to improve its ability to generate high-quality\nresearch ideas with fine-grained control at inference time.\nDuring this process, three dimensional controllers are trained\njointly with the language model to enable fine-grained control\nat inference time.\n\n### Supervised Fine-Tuning\n\nTo make the model training more stable in reinforcement\nlearning (Chen et al. 2024a), we also introduce the supervised\nfin-tuning stage.\n**Data Collection. To conduct a Supervised Fine-Tuning**\nstage, we first collect papers from the ICLR 2023 and 2024.\nWe selected papers from ICLR as training data due to its\nprestigious standing as a top-tier conference in the field of\nmachine learning, offering cutting-edge research and highquality technical discussions. We sample 1,000 instances\n_P =_ _p_ for training. We utilize the LLaMA with a prompt\n_{_ _}_\n(detailed in appendix ) to extract the research idea y from\nthe sampled paper p as the golden output. To extract the one corresponding input paper x for each output, we select\nthe one most significant supporting paper from all related\nworks \u02c6x1, \u02c6x2..., \u02c6xn by prompting LLaMA of the abstract and\nintroduction section of p, together with the citation counts of\n_x\u02c61, \u02c6x2..., \u02c6xn within the sampled paper p._\n**Fine-Tuning. Based on the collected training set, we fine-**\ntune the language model as follows,\n_M_\n\n_Lsup = CE(Y,_ _Y[\u02c6] )_ (3)\n\nwhere CE( ) denotes the cross-entropy loss and _Y[\u02c6] is the_\n\n_\u00b7_\npredicted research idea from, formulated as _Y[\u02c6] =_ (X).\n_M_ _M_\n_X is the research paper and Y is the research idea._\n\n\n### Reward Modeling\n\nResearchers mainly consider three aspects when they devise a research idea: novelty, feasibility, and effectiveness.\nTherefore, we train three distinct reward models to score the\ngenerated idea in reinforcement learning, each corresponding\nto one of the quality dimensions.\n**Multi-dimension Feedback Collection. To train reward**\nmodels, we need to collect three kinds of feedback. Similar\nto the supervised fine-tuning stage, we also use the papers\nfrom ICLR[2]. Specifically, we collect the review data from\nOpenReview platform[3], and we also get the research idea by\nprompting the language model. For the Novelty score of\n_M_\nthe research idea in ICLR 2023, we could use the novelty\nscore from the review directly. As for ICLR 2024, we prompt\nthe LLM to get novelty scores since they don\u2019t provide direct\nratings (see Appendix for prompt). Similarly, since there is\nno feasibility score or effectiveness score in the review, we\nprompt the LLM to get scores for every research idea. Feasibility score is mainly based on the experiment setup and\nmethod sections, taking into account factors such as dataset\nsize, model complexity, and relevant review comments, while\nEffectiveness score is derived primarily from the experimental results and corresponding review comments. The detailed\nScoring Criteria for Novelty, Feasibility, and Effectiveness\nare outlined in Appendix.\nNotably, all the collected novelty, feasibility, and effectiveness are subsequently normalized to a 0-1 scale for training.\n**Reward Model Training. We select an LLM as the back-**\nbone of reward models. To make the model predict the score\nfor each dimension, we add a Multi-Layer Perceptron as\nfollows,\n\ufffdFn/f/e = An/f/e (X _r),_\n\n(4)\n_F\u02c6n/f/e = Cn/f/e(Fn/f/e),_\n\nwhere Cn/f/e are MLPs which can output score for each dimension. An/f/e is the LLM backbone. Each reward model\ntakes the generated idea as input and outputs a score Fn/f/e\nbetween 0 and 1, representing its evaluation of novelty, feasibility, or effectiveness. To optimize the reward models, we\nutilize cross-entropy loss as follows,\n\n_Ln/f/e = CE( F[\u02c6]n/f/e, Fn/f/e),_ (5)\n\nwhere Fn/f/e is the ground-truth label.\n\n### Multi-dimension Reward Augmented Controllable Reinforcement Learning\n\nIn this stage, we fine-tune the research idea proposer with\ncontrollable steering through reinforcement learning??, refining the model based on feedback across three dimensions:\nnovelty, feasibility, and effectiveness.\n**Dimensional Controllers Inspired by the existing work**\n(Han et al. 2024), we introduce the dimensional controllers\nof the novelty, feasibility, and effectiveness of the generated\nidea, as these dimensions often exhibit interdependency and\n\n2https://iclr.cc/\n3https://docs.openreview.net/reference/api-v2.\n\n\n-----\n\n**Research** **Proposer** **Controller** novelty Method: We apply semantic Novelty\n\n**Papers** score = 0.5 divergence minimized RM\n\nprompt [...] and prioritize\n\nFeasibility candidate concepts with Feasibility\n\nscore = 0.6 semantic similarity to reduce RM\n\nhallucinations [...]\n\nEffectiveness Effective\nExperiment Plan: [....]\n\nscore = 0.4 ness RM\n\nFigure 2: The learning framework with dynamic control across 3 dimensions. Generated research ideas are assessed by\ncorresponding reward models, which provide scores for each dimension. These scores guide the fine-tuning process during\nreinforcement learning, optimizing both the idea proposer and the corresponding dimensional control parameters to enhance the\nquality of idea generation.\n\n\ntrade-offs. We achieve this by adding additional control parameters (i.e. the steers) as follows,\n\uf8f1M[l]n [=][ M][l] [+][ \u03f5][n][W][n][M][l][,]\n\uf8f4\uf8f2\n\n**M[l]f** [=][ M][l] [+][ \u03f5][f] **[W][f]** **[M][l][,]** (6)\n\n\uf8f4\uf8f3M[l]e [=][ M][l] [+][ \u03f5][e][W][e][M][l][,]\n\nwhere Ml represents the output of l-th layer in the LLM.\n_\u03f5n, \u03f5f_, and \u03f5e are the hyper-parameters for controlling novelty, feasibility, and effectiveness. Wn, Wf, and We are\nlearnable parameters. In the training stage, we set all \u03f5n,\n_\u03f5f_, and \u03f5e as 1. By this, we use M[l]n/f/e [to replace the]\noriginal output of the l-th layer. We denote the parameters for each resulting model as \u0398n = {\u0398LLM _, \u0398\u03f5nWnMl_ _},_\n\u0398f = {\u0398LLM _, \u0398\u03f5f Wf Ml_ _} and \u0398e = {\u0398LLM_ _, \u0398\u03f5eWeMl_ _}._\n**Reward. Specifically, we get all three kinds of rewards for**\neach research idea based on the well-trained reward model.\nWe define rn, rf, and re as the novelty, feasibility, and effectiveness rewards for the research idea. Then we have a\nreward function for each dimension of the research idea at\ntimestep t as follows,\n\n\uf8f1 _t_\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2rt[n] [=][ \u2212]\ufffd\ufffdi=1t I(i = K)wlrn,\n\n_rt[f]_ [=][ \u2212] I(i = K)wlrf _,_ (7)\n\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3rt[e] [=][ \u2212]\ufffdi=1t I(i = K)wlre,\n\n_i=1_\n\nwhere K is the token length of the research idea. t is the\ntimestep. I(\u00b7) is the indicator function. wl is a weight assigned to rewards. Thereafter, we utilize the PPO algorithm\n(Schulman et al. 2017) to train the model following the existing work (Jing and Du 2024). More details of PPO algorithm\ncan be found in Appendix.\n\n### Decoding\n\nIn this part, we devise two decoding methods for the inference\nstage.\n\n\nFigure 3: Decoding RNN dynamically steers the dimensions\nfor a balanced and context-aware generation. It starts with\n(\u03f5[0]n[, \u03f5][0]f _[, \u03f5]e[0][)][, and predicts the control parameter weights for]_\nthe next sentence, based on the context generated by the\ncombined proposer and controller.\n\n**Naive Static Decoding. In this decoding method, we set**\n_\u03f5n, \u03f5f_, and \u03f5e as fixed values for the steers. To achieve a high\nscore over novelty, feasibility, and effectiveness, we set all\n_\u03f5n, \u03f5f_, and \u03f5e as 1, because we set them as 1 in the training\nstage for maximum novelty, feasibility, and effectiveness.\n**Goal-driven Dynamic Decoding. The goal of achieving a**\ngood research idea is not only to blindly improve the result\nof a certain dimension but also to consider the overall quality.\nFor example, too high a degree of novelty may result in a\nlow effectiveness (Si, Yang, and Hashimoto 2024a), while\ndifferent parts of a research idea, such as the method and\nexperiment planning, may require varying levels of focus on\nnovelty and feasibility. Therefore, how to balance novelty,\nfeasibility, and effectiveness in the inference stage is important for generating a good idea. To achieve this, we utilize an\n\n\n-----\n\nRNN (Sherstinsky 2020) to predict the steer value \u03f5n, \u03f5f, and\n_\u03f5e, because RNN is good at sequence-level prediction (Figure_\n3).\nTo optimize the RNN for steer values prediction, we first\ncollect 1,000 high-quality research ideas generated with Idea\nProposer (above 8 in overall score). hereafter, we get the corresponding controller weights using our three reward models\nfor each sentence of the high-quality research idea. Specifically, we feed each sentence in the research idea into our reward models to get the rewards as \u02c6rn, \u02c6rf, \u02c6re. Furthermore, we\nnormalize the reward and get the corresponding steer values\nof each sentence as \u02c6\u03f5n/f/e = (\u02c6rn/f/e _sn/f/e)/(an/f/e_\n_\u2212_ _\u2212_\n_sn/f/e) \u00d7 w[\u2032], where sn/f/e and an/f/e are the minimum_\nvalue and maximum value for all rewards and w[\u2032] is the maximal controller weight, which is 5 in our case. This reflects\nthe controller-weight ratios between 3 controllers, as well as\nthe absolute scale of each controller weight from 0-5. After\nthe data collection, we can use the pair (S[t], s[t]n/f/e[+1] [)][ to train]\nthe model as follows,\n\n_Lrnn = CE(RNN_ (S[<t]), s[t]n/f/e[)][,] (8)\n\nwhere S[<t] is the previous t 1 sentences in the research\n_\u2212_\nidea and s[t]n/f/e [is steer values][ \u03f5][n][,][ \u03f5][f] [, and][ \u03f5][e][ of][ t][-th sentence.\nTherefore, we can use the well-trained RNN to predict the\ncontroller weights of the next sentence based on the current\ngenerated sentence in the inference phrase.\n\n## Experiment\n\n### Dataset and Analysis\n\nWe collect a dataset of 6,765 usable research papers in total submitted to ICLR[4]and NeurIPS[5] in the years 2023 and\n2024, including both accepted and rejected submissions and\nfiltered 5,687 usable data. 4,271 of them are used for training,\nand 500 are sampled for evaluation. Each paper contains its\nabstract, methodology, and experiment sections. Additionally, review data from OpenReview[6] provides human ratings\nfor overall quality as well as the review contents and key\nsub-dimensions - novelty, feasibility, and effectiveness. Paper\ncontent is scraped with title from the Semantic Scholar[7] and\narXiv APIs[8] and then cleaned up with regular expression\nto extract corresponding sections. These papers and ratings\nare used to: 1. Derive ground-truth ideas for supervised finetuning. 2. Train reward models for the key dimensions. 3.\nOptimize idea generation using reinforcement learning with\nmulti-dimensional steering.\nThe dataset is split into the following subsets:\n\n1. Supervised Fine-Tuning split.: We use 1,000 papers from\nonly ICLR to derive the golden generated idea, paired with\nthe most supporting related work idea as input to fine-tune\nthe model.\n\n4https://iclr.cc/\n5https://neurips.cc/\n6https://docs.openreview.net/reference/api-v2.\n7https://www.semanticscholar.org/product/api.\n8https://arxiv.org/help/api.\n\n\n2. Reinforcement Learning split.: 3,271 research papers\nfrom both ICLR and NeurIPS with detailed reviews are\nused to train three distinct reward models for novelty,\nfeasibility, and effectiveness, each capturing expert evaluations for further reinforcement learning.\n\n3. Evaluation split.: 500 research papers from both ICLR\nand NeurIPS are sampled for evaluation, of which 30 are\nrandomly selected for manual expert evaluation.\nFigures 4 and 5 provide an overview of the dataset distribution and top keywords.\n\nFigure 4: Rating distribution statistics of our dataset.\n\nFigure 5: Top 10 topic distribution of our dataset.\n\n**Evaluation** The evaluation is performed on two datasets:\n500 papers of the evaluation split for automatic evaluation,\nand a subset of 30 papers are selected for manual expert evaluation. We measure performance across three core metrics\n(details in Appendix):\n\n - Novelty: Evaluates how original and creative the generated ideas are, compared to existing works.\n\n - Feasibility: Assesses the practical implementation and\nthe likelihood that the idea can be executed within typical\nresource constraints.\n\n - Effectiveness: Measures the potential improvement or\nimpact of the generated idea when compared to baseline\nmodels.\nWe split our evaluation into two types:\n\n1. Automatic Evaluation: For automatic evaluation, we\nevaluate novelty, feasibility, and effectiveness of the generated ideas with prompt-based method. We adopt GPT-4\nas our reviewing agent.\n\n\n-----\n\n|Model|Novelty Feasibility Effectiveness|Overall|\n|---|---|---|\n\n\n_T5-SFT_ 3.3 5.1 4.2 4.2\n_T5-RLHF_ 3.9 5.3 4.9 4.7\n_LLaMA2-SFT_ 4.8 5.9 5.2 5.3\n_LLaMA2-RLHF_ 5.5 6.1 5.6 5.8\n\n_LLaMA2-RLHF + Novelty Ctrl_ 6.4 5.9 5.5 6.0\n_LLaMA2-RLHF + Feasibility Ctrl_ 5.3 7.2 5.2 5.6\n_LLaMA2-RLHF + Effectiveness Ctrl_ 5.6 6.0 6.4 5.9\n\n_LLaMA2-RLHF + All Ctrls (Static)_ 5.8 6.0 5.5 5.9\n_LLaMA2-RLHF + All Ctrls (Dynamic)_ 6.0 6.1 5.8 6.2\n\nTable 1: Experiment Results with Novelty (N), Feasibility (F), Effectiveness (E), and Overall Scores. N/F/E Ctrl (abbrev. for\n_Control) represents that only 1 corresponding controller is enabled, whereas All Ctrl activate all the 3 controllers. Static and_\ndynamic denote different decoding strategies.\n\n|T5-SFT T5-RLHF LLaMA2-SFT LLaMA2-RLHF|3.3 5.1 4.2 3.9 5.3 4.9 4.8 5.9 5.2 5.5 6.1 5.6|4.2 4.7 5.3 5.8|\n|---|---|---|\n\n|LLaMA2-RLHF + Novelty Ctrl LLaMA2-RLHF + Feasibility Ctrl LLaMA2-RLHF + Effectiveness Ctrl|6.4 5.9 5.5 5.3 7.2 5.2 5.6 6.0 6.4|6.0 5.6 5.9|\n|---|---|---|\n\n|LLaMA2-RLHF + All Ctrls (Static) LLaMA2-RLHF + All Ctrls (Dynamic)|5.8 6.0 5.5 6.0 6.1 5.8|5.9 6.2|\n|---|---|---|\n\n\n2. Manual Evaluation: For manual evaluation, we select 30\npapers and have domain experts assess the quality of the\ngenerated ideas of the selected model (SFT, RLHF and\nRLHF with Dynamic Controls), providing human scores\nfor novelty, feasibility, and effectiveness. These scores are\nthen compared with the scores generated by our automatic\nreviewing agent to measure the alignment between human\njudgment and the agent\u2019s reviews.\n\n### Main Experiments\n\n**Baselines and Setups** We establish several baselines to\nevaluate the effectiveness of different control strategies applied to the LLaMA2-RLHF model. The baselines include\nT5-SFT, T5-RLHF, and LLaMA2-SFT, representing varying\nlevels of model capacity and reinforcement learning application. These baselines are chosen to compare the impact\nof applying reinforcement learning fine-tuning (RLHF) and\nenabling targeted controls for Novelty, Feasibility, and Effective.\n\n - T5-SFT: A version of the T5 model trained using SFT\non 1,000 examples, without reinforcement learning or\ncontrol strategies, in which ideas are generated based on\nthe prompt structure, serving as the simplest baseline.\n\n - These baselines allow for a comprehensive comparison,\nhighlighting the incremental improvements brought by\nRLHF, control strategies, and advanced decoding techniques.\nFor the RL for idea proper and dimensional controllers\ntraining, we use The RL split to optimize our model using PPO and multi-dimensional reward augmentation. We\nincorporate the three distinct reward models for novelty,\nfeasibility, and effectiveness, allowing for controllable\ngeneration combined with 3 control parameters, and experiment with different decoding stategies.\n\n## Main Results\n\n Main Results and Statistical Analysis\n\nTable 1 presents the experimental results for Novelty (N),\n_Feasibility (F), Effectiveness (E), and Overall metrics._\n\nThe baseline models establish foundational performance\nlevels, with T5-SFT and T5-RLHF showing modest improvements in Feasibility and Effectiveness due to reinforcement learning, though their Novelty scores remain\nlimited by the lack of mechanisms to encourage innovation. In contrast, LLaMA2-SFT achieves higher overall\nscores, benefiting from larger model capacity and superior pretraining, yet its reliance on supervised fine-tuning\nleaves room for enhancement through reinforcement learning and control strategies.\nAdding targeted controls to LLaMA2-RLHF demonstrates\nthe potential for metric-specific optimizations. For instance, introducing Novelty Control significantly boosts\ncreativity while maintaining balanced practicality and\nperformance, highlighting the feasibility of improving\noriginality without major trade-offs. Similarly, Feasibility\n_Control achieves the highest observed feasibility, albeit_\nwith minor reductions in novelty and effectiveness, showcasing its focus on practicality. The Effectiveness Control,\non the other hand, enhances impact without compromising\nthe balance across dimensions.\nWhen all controls are combined, Static Decoding provides\nreliable, balanced performance, but its fixed nature limits adaptability. In contrast, Dynamic Decoding emerges\nas the most effective approach, leveraging contextual dynamic strategy to balance creativity, practicality, and impact, ultimately producing higher-quality ideas.\nThese results show the importance of rl and dynamic control in tailoring model behavior to complex requirements,\nwhile also illustrating trade-offs inherent in single-metric\noptimizations.\nTo validate the observed improvements, we conducted\npaired t-tests to evaluate statistical significance. Results\nshow that LLaMA2-RLHF + Novelty Ctrl achieved a statistically significant improvement in Novelty (p-value <\n0.01) compared to LLaMA2-RLHF without controls. Similarly, Feasibility Ctrl significantly enhanced Feasibility\n(p-value < 0.05), while Effectiveness Ctrl showed a notable gain in Effectiveness (p-value < 0.05). Furthermore,\n_Dynamic Decoding demonstrated statistically significant_\nimprovements across all metrics (p-value < 0.01) com\n\n-----\n\n|Model|Novelty Feasibility Effectiveness|Overall|\n|---|---|---|\n\n\n_LLaMA2-SFT_ 4.3 5.6 4.8 4.6\n_LLaMA2-RLHF_ 4.9 6.2 5.2 5.3\n_LLaMA2-RLHF + Dynamic_ 5.5 6.4 5.1 5.5\n\nTable 2: Human evaluation results, LLaMA2-RLHF + Dynamic denotes the Dynamic decoding with all the 3 controllers enabled.\n\npared to the static approach, validating its superior adapt- **Novelty Weight** **Novelty Score** **Feasibility Score**\nability and performance.\n\n1.0 6.4 6.1\n2.0 6.7 5.8\n\n### Human Evaluation\n\n3.0 7.0 5.3\n4.0 7.3 4.9\n\nTable 4: Novelty and Feasibility trade-off by increasing the\nnovelty controller weight.\n\nFigure 6: Human Evaluation Results\n\n**Metrics** **Novelty** **Feasibility** **Effectiveness** **Overall**\n\n_Pearson (r)_ 0.995 0.972 0.839 0.970\n_Spearman (p)_ 1.000 0.866 0.500 1.000\n\nTable 3: Correlation Coefficients (Pearson and Spearman)\nbetween human and reviewing agent scores.\n\n|LLaMA2-SFT LLaMA2-RLHF LLaMA2-RLHF + Dynamic|4.3 5.6 4.8 4.9 6.2 5.2 5.5 6.4 5.1|4.6 5.3 5.5|\n|---|---|---|\n\n|Novelty Weight|Novelty Score Feasibility Score|\n|---|---|\n\n|1.0 2.0 3.0 4.0|6.4 6.1 6.7 5.8 7.0 5.3 7.3 4.9|\n|---|---|\n\n|Metrics|Novelty Feasibility Effectiveness|Overall|\n|---|---|---|\n|Pearson (r) Spearman (p)|0.995 0.972 0.839 1.000 0.866 0.500|0.970 1.000|\n\n\nDomain experts validated the effectiveness of our framework of the generated idea as shown in Table 2 and bar\nplot in Figure 6, with human scores showing a strong correlation with the automatic scores produced by our reward\nmodels. The Correlation Coefficients computed with both\nPearson and Spearman between human and reviewing\nagent scores are shown in table 3.\nExperts also highlighted the trade-off between novelty and\nfeasibility, noting that the fine-tuned model with novelty\nsteering produced more creative, though sometimes less\npractical, ideas compared to the equal-weighted model.\n\n## Analysis\n\n### Novelty and Feasibility Trade-off\n\nWe learn from (Si, Yang, and Hashimoto 2024b) that\nincreasing novelty will likely reduce the feasibility of an\nidea. To test this idea, we controlled the weight of the\nnovelty steer in the RLHF + Steer1 setup and observed its\nimpact on both novelty and feasibility scores. The results\nare shown in Table 4.\n\n\nFigure 7: Novelty and Feasibility control analysis\n\nAs expected, increasing the novelty steer weight led to\nhigher novelty scores but lower feasibility scores. This\ndemonstrates the trade-off between generating highly creative ideas and ensuring their practical feasibility.\n\n### Decoding Strategy Motivation",
    "raw_evidence": "",
    "raw_conclusions": "",
    "execution_times": {
        "claims_analysis_time": "318.79 seconds",
        "evidence_analysis_time": "1.58 seconds",
        "conclusions_analysis_time": "1.57 seconds",
        "total_execution_time": "324.84 seconds"
    }
}