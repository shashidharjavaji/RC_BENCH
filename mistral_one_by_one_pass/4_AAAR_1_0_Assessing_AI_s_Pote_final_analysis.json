{
    "paper_analysis": [],
    "raw_claims": " For example:\n        ```json\n        {\n            \"claims\": [\n                {\n                    \"claim_id\": 1,\n                    \"claim_text\": \"Example claim text\",\n                    \"location\": \"Section X\",\n                    \"claim_type\": \"Example type\",\n                    \"exact_quote\": \"Exact quote from the paper\"\n                }\n            ]\n        }\n        ```\n\n        Analyze the paper and extract all possible claims made by the authors.\n\n        Paper text: # AAAR-1.0: Assessing AI\u2019s Potential to Assist Research\n\n## Anonymous submission\n\n**Abstract** **Task InstructionGiven the context of a paper, identify the missed** **Task Instruction**\n\n\nNumerous studies have assessed the proficiency of AI systems,\nparticularly large language models (LLMs), in facilitating everyday tasks such as email writing, question answering, and\ncreative content generation. However, researchers face unique\nchallenges and opportunities in leveraging LLMs for their own\nwork, such as brainstorming research ideas, designing experiments, and writing or reviewing papers. In this study, we introduce AAAR-1.0, a benchmark dataset designed to evaluate\nLLM performance in three fundamental, expertise-intensive\nresearch tasks: (i) EQUATIONINFERENCE, assessing the correctness of equations based on the contextual information in\npaper submissions; (ii) EXPERIMENTDESIGN, designing experiments to validate research ideas and solutions; and (iii)\nPAPERWEAKNESS, identifying weaknesses in paper submissions. AAAR-1.0 differs from prior benchmarks in two key\nways: first, it is explicitly research-oriented, with tasks requiring deep domain expertise; second, it is researcher-oriented,\nmirroring the primary activities that researchers engage in on\na daily basis. An evaluation of both open-source and proprietary LLMs reveals their potential as well as limitations in\nconducting sophisticated research tasks. We will release the\nAAAR-1.0 and keep iterating it to new versions.\n\n## Introduction\n\nAlthough AI has brought transformative changes to various aspects of life, its impact on researchers unfolds in a\nnuanced manner. On the one hand, AI assists in various research disciplines, such as Social Science, Finance, Medicine,\nGeoScience, Math, etc.(Yue et al. 2023; Li et al. 2023b), significantly expediting academic processes. However, many\nof these applications are superficial, often limited to datadriven clustering or classification. On the flip side, the AI\nera poses challenges for researchers. Despite its ability to\nstreamline some activities, researchers still face demanding,\ncognitively intensive tasks such as staying current through extensive paper reading, rapidly generating ideas in response to\nfast-paced advancements, conducting rigorous experiments\nto substantiate claims, and managing an increasing volume\nof peer reviews. Then a question looms: How effectively\ncan AI assist researchers in tasks that are domain-specific,\nexpertise-demanding, and knowledge-intensive?\nExisting works proved the promising potential for using\nLLMs in assisting AI research. Si, Yang, and Hashimoto\n\n\n_This paper proposes an algorithm_\n\n_for the robustness of [\u2026]_\n_In the below sections, we conduct_\n_the experiments_\n\n\n_Given the context of a paper, identify the missed_\n\n_equation from the provided options (A, B, C, D)._\n\n\n_Given a paper, critique the weaknesses_\n_within this research work._\n\n\n_1. To prove the effectiveness[\u2026]_\n_2. To study the impact of [\u2026]_\n_3. To avoid randomness in [\u2026]_\n\n\n_Given a partial paper, create a brief_\n_experiment plan and explanations._\n\n\n_(A). z = W*a+b_\n\n\nFigure 1: The input-output illustration of three tasks in the\nproposed AAAR-1.0 benchmark.\n\n(2024) conducted a large-scale human study and found that\nLLMs can generate creative research ideas. Lu et al. (2024)\nproposed an autonomous agent to handle complicated research workflow and write a whole research paper. However,\nmost of these works focus on addressing highly subjective\nproblems that require a high degree of expertise, making evaluation laborious and hard to reproduce. This underscores the\nneed for a comprehensive benchmark that rigorously assesses\nLLMs\u2019 capabilities in expertise-intensive research activities\nTo this end, in this work, we introduce AAAR-1.0, a\nnovel benchmark that aims to comprehensively assess the\nLLMs\u2019 capacity on expert-level research tasks. As illustrated\nin Figure 1, AAAR-1.0 decomposes three distinct expert-level AI research tasks from the researcher\u2019s daily activities,\nincluding i) EQUATIONINFERENCE, investigating whether\nthe LLMs can infer the equation correctness based on the\npaper context; ii) EXPERIMENTDESIGN, validating LLMs\u2019\nability on designing reliable experiments for a research idea;\niii) PAPERWEAKNESS, testing the quality of the weaknesses\ncriticism written by the LLMs. To ensure data quality, senior\n\n\n**Paper Context**\n\n_This paper proposes an algorithm_\n\n_[\u2026], the result z is defined as below:_\n_z =_\n\n_where W is the parameter, a and b_\n_are the [\u2026]_\n\n\n_1. Compare performance on [\u2026]_\n_2. Ablation study with [\u2026]_\n_3. Significance test [\u2026]_\n\n\n_(A). z = W*a+b_\n_(B). z = W*b+a_\n_(C). z = W*a*b_\n_(D). z = W*a/b_\n\n\n-----\n\n|GPT-4|Col2|\n|---|---|\n\n\n.\n\n**This paper is about a new**\n**reinforcement learning [\u2026].]**\n\n**In this section, we conduct**\n**ablation to show the [\u2026]**\n\n**We found that [\u2026]**\n\n\ncontext before\n**z = W*a + b**\ncontext after\n\n\n**Task #3: WEAKNESS**\n\n\n**Reviewer#1**\nThis paper\nproposes a\nnovel metric\nabout the [\u2026]\n\n\n**+**\n\n.\n\nPaper PDF\n\n\nFigure 2: Data construction workflows of the three tasks in AAAR-1.0.\n\n\ncontext before\n**z = W * a + b**\ncontext after\n\n\n**Reviewer#1**\nThis paper\nproposes a\n**+** novel metric\nabout the [\u2026]\n\n\nPaper PDF\n\n\nAI researchers with extensive domain expertise perform data\nannotation for AAAR-1.0, followed by rigorous multi-round\ndata examination and filtering. All three tasks require models\nto possess strong domain knowledge covering various cuttingedge research findings, as well as expert-level research experience, to the extent that even humans need substantial\nresearch accumulation to tackle the tasks we designed. Crucially, tasks here are singular, stand-alone challenges (with\nclear input and output expectations) rather than a complicated\ntask chain (Li et al. 2024; Lu et al. 2024), providing a more\ntransparent assessment of the model\u2019s intermediate output.\n\nBenefiting from the proposed automatic metrics, we conduct extensive experiments across numerous mainstream\nLLMs, where we find that:\n\n  - Closed-source LLMs generally outperform open-source\nLLMs on AAAR-1.0, likely due to their richer scientific\nknowledge stemming from a larger model size.\n\n  - Contrary to human behaviour, neither extending the\ninput modality (i.e., leveraging text and figures) nor\nenlarging the input context guarantees enhanced performance. This underlines most current LLMs\u2019 limitations\nin processing diverse, extensive information coming\nfrom scientific documents.\n\n  - LLM-designed experiments are innovative and more\ndiverse than those by humans; however, many are trivial,\nlack feasibility, and stray from the original research\nobjectives.\n\n  - LLM-generated weaknesses often lack ample domain\nknowledge, especially on cutting-edge research topics,\nleading to the vague weaknesses applicable to various\npapers.\n\n\n**Task #2: EXPDESIGN**\n\n\n## AAAR-1.0\n\nFigure 2 provides an overview of constructing AAAR1.0. In the following sections, we elaborate on the\ndata collection details of the aforementioned three\ntasks, including EQUATIONINFERENCE ( EQINFER ), EX\nPERIMENTDESIGN ( EXPDESIGN ), and PAPERWEAKNESS ( WEAKNESS ).\n\n### EQUATIONINFERENCE\n\nWriting a correct scientific equation is challenging because\nit involves an in-depth understanding towards an algorithm\nor the relations between the massive notations. However, directly asking LLMs to generate equations is over-challenging.\nFor this reason, in this task, we adopt the conventional multichoice classification paradigm for building EQINFER, as\nshown in Figure 1.\n\n\u2460 **Data crawling and cleaning.** For the data source, we\nadopt the pre-compilation LaTeX code for two reasons: i)\nexisting PDF parsing tools, such as PyMuPDF and PaperMage (Lo et al. 2023), can introduce considerable noise to the\nparsed equation text; ii) considering most of exiting LLMs are\ncapable with processing LaTeX code, using LaTeX source\ninstead of parsed text can be more accurate and provide\nLLMs with richer information. Meanwhile, to avoid using\nany low-quality human-written equations, we only crawl\nthose peer-reviewed papers accepted by top-tier conferences.\nAccordingly, we first obtain the accepted paper list from ACL\nAnthology, from year 2019 to 2023. Next, we search each\npaper on arXiv to crawl its LaTeX source (if it exists). Finally,\nwe get a total of 1,762 papers\u2019 source LaTeX packages.\nWe then clean the LaTeX sources by deleting all the comments and combining multiple cross-referred.tex files into\na main file. Afterwards, we use regex to randomly extract (at\nmost) 3 equations\u2019 code snippets per paper, finally resulting\n\n\n**Task #1: EQINFER**\n\n\n-----\n\nin 3,877 human-written equations are extracted.\n\n\u2461 **LLM-based equation synthesis.** As we formulate this\ntask as classification, for each human-written positive equation, we have to craft at least three counterpart negative equations. To this end, we prompt GPT-4 to synthesize more equations based on the paper context. For each positive equation,\nwe repeat this prompting (with a high decoding temperature)\nuntil three different negative equations are synthesized.\n\n\u2462 **LLM-based filtering.** However, the LLM-synthetic\nequations can sometimes be context-unaligned, i.e., some\nsynthesized equations contain notations that are never defined\nin the paper context, which is a superficial shortcut for the\nclassification tasks (Geirhos et al. 2020). To improve the data\nquality, we prompt GPT-4 to identify those context-unaligned\nnegative equations. We then discard those instances where\nall three negative equations are identified as contextually unaligned. This filtering leads to a final of 1,449 classification\ninstances (62.3% instances are filtered).\n\n\u2463 **Expert-based examination.** Furthermore, it\u2019s also possible that synthesized negative equations are actually correct\n(i.e., false negative options) \u2014 even if the negative and positive equations are written differently, the final compiled\nresults might be the same. To filter out the false negative\nequations and to have a final check on the classification instances, we then employ human experts to conduct a further\ndata review.\nWe asked 5 senior PhD students who are experienced in AI\nresearch to manually check all the instances. For each classification instance, we ask human experts to consider the following criteria: i) are all four equations (both positive and\n**negative) grammatically correct? ii) after compilation, is**\n**there only one correct answer? We ask every human ex-**\npert to use external LaTeX compilation tools (e.g., TeXlive),\nand identify the instances that cannot meet the criteria. Each\ninstance is examined by at least two experts, and we only\nkeep instances that all experts decide to keep. After this strict\nexamination, a total of 1,049 instances are eventually kept\n(27.6% instances are filtered)\n\n**Final data.** We finally shuffle the four equations for each\nclassification instance and randomly assign letters (A, B, C,\nand D) to the equations. We show the data statistics of the\nfinal EQINFER in Table 4 and the sample data cases in Appendix.\n\n### EXPERIMENTDESIGN\nGiven a research topic, such as a novel ML algorithm, a\nqualified researcher can design a solid experiment plan for\nit, and clarify underlying motivation to ensure the reliability\nof the designed experiment. Unlike the concurrent works\nthat focus on the experiment implementation (Lu et al. 2024;\nHuang et al. 2024), we emphasize the importance of assessing the high-level experiment design of LLMs before the\nsubsequent implementation to avoid any expensive execution iteration. Therefore, as shown in Figure 1, we formulate EXPDESIGN as a text-generation task that takes preexperiment paper context as input, and then generates the\nexperiment and explanation list.\n\n\n\u2460 **Data crawling.** As for the data source, we first collect\n10k papers\u2019 data from arXiv, including LaTeX sources and\n_\u2265_\nPDFs, which cover broad AI categories, including cs.AI,\ncs.CL, and cs.CV, from year 2018 to 2023. Similarly, to\nensure the source data quality, we only use papers that have\nappeared at well-known conferences.\n\n\u2461 **Domain-expert annotation.** Making a reliable and executable experiment plan requires solid foundation knowledge\nof a specific research area. Consequently, we set a high standard for choosing annotators: i) be a senior PhD student with\nat least one peer-reviewed publication; ii) have more than 4\nyears of AI research experience; iii) frequently serve as conference reviewers. Finally, we invite a total of 10 qualified\nexperts to participate in our data collection procedure. Given\nthe 10k crawled papers, we first ask every annotator to bid on\nthe papers that they are interested in. After bidding, each of\nthem is assigned 10 papers by us, i.e., a total of 100 papers to\nbe annotated. During annotation, we post each paper PDF on\nonline Google Drive and ask the annotator to first carefully\nread the whole paper. Then, we ask them to identify and locate the key experiments in each paper (i.e., highlighting the\nrelevant paragraphs of each experiment). We don\u2019t consider\nsome trivial experiments, such as those supplemental analyses in the appendix section. For each identified experiment,\nthe annotator has to concisely answer two questions: i) what\n**did this experiment do? ii) why did the paper authors**\n**conduct this experiment? In other words, we ask the an-**\nnotator to summarize all the key experiments in this paper\nand explain the underlying motivations based on their rich\ndomain experience.\n\n\u2462 **Multi-round peer discussion.** Intuitively, different experts might have different opinions on the same research\ntopic. Particularly, when explaining the underlying motivation of an experiment, adopting only a single expert\u2019s opinion\nmight introduce bias to our annotation. Hence, we conduct\na further multi-round peer discussion. For each online paper\nPDF, where all the key experiments are identified, summarized, and explained, we ask a different expert (reviewer)\nto review the annotation by considering the following three\ncriteria: i) are the identified experiments all the key exper**iments? ii) does each experiment summarization covers**\n**all key information? iii) does each explanation sound rea-**\n**sonable and reliable? Each reviewer has to leave comments**\nto the online PDF regarding the above criteria, and then the\nannotator has to respond to each comment \u2014 either accept\nthe suggestion and revise the previous annotation, or provide\na \u201crebuttal\u201d to the reviewer to uphold the annotation. This\ndiscussion iterates until both opinions align with each other.\nEventually, for each paper, we collect two lists: i) the experiment list, summarizing each experiment step of the paper;\nii) the explanation list, the underlying motivations that are\none-one corresponding to the experiment.\n\n**Final data.** After annotation, we use the pre-experiment\ncontext of each paper (according to the first-experiment location identified by the annotator) as the input. Furthermore,\nwe use GPT-4 to delete any sentence that potentially leaks\n\n\n-----\n\nthe experiment from the input.[1] Similar to the EQINFER, we\nutilize the source LaTeX as the input text to avoid PDF paring\nnoise. As for the image input, we collect those figures within\neach paper\u2019s source LaTeX package and only keep figures\nthat are used in the pre-experiment context. Overall, a total of\n100 instances are collected. As shown in Figure 1, the input\nof each instance is the pre-experiment context (including the\nfigures), and the ground-truth output is the expert-annotated\nexperiment plan and the explanations. Table 5 shows data\nstatistics.\n\n### PAPERWEAKNESS\n\nAnother critical research task is paper review. Previous works\nhave demonstrated the usefulness of the LLM-based review\nfeedback (Gao, Brantley, and Joachims 2024; Jin et al. 2024;\nLu et al. 2024). However, as indicated by Du et al. (2024);\nLiang et al. (2024), LLMs only excel at summarizing the\nresearch strengths while falling significantly short on weakness criticism. Hence, we build WEAKNESS for particularly\ninvestigating the LLM-generated weaknesses.\n\n\u2460 **Data crawling.** We first crawl a total of 3,779 anonymous submissions of ICLR 2023 from OpenReview,[2] including PDF and other meta information (e.g., scores, decisions,\nand tracks). As the ICLR 2023 has 13 distinct tracks while\nthe paper distribution across different tracks is highly biased,\nwe then uniformly sample papers from different research\ntracks to improve the domain diversity. Meanwhile, during\nsampling, we also keep the accept/reject papers distributed\nequally to avoid data bias. In a word, we finally collect a\ntotal of 1,000 papers (500 accepted; 500 rejected), uniformly\ncovering all 13 tracks. Please refer to Figure 3 for the track\nand score distribution of the 1,000 papers.\n\n\u2461 **LLM-based weakness extraction.** Since the raw comments crawled from ICLR 2023 are mixed with both strengths\nand weaknesses, we further employ GPT-4 to extract all the\nweaknesses from each reviewer\u2019s comments and compose\nmultiple weaknesses into a list. Notably, we force GPT-4 to\nkeep the original text of the reviewer, i.e., all weaknesses\nin our dataset are those original sentences written by the reviewer without any modifications.[3] What\u2019s more, sometimes\none reviewer might repeatedly mention the same weakness\nthroughout the comment. In this case, we simply keep all\nthe repeated weaknesses because, if one weakness is repeatedly mentioned by the reviewer, it\u2019s intuitively an important\nweakness that the reviewer wants to emphasise; accordingly,\nkeeping the repeat items can penalize LLMs more on missing\nthis weakness.\nAll in all, for each paper, we can finally get multiple weakness lists (one weakness list per reviewer, one paper can have\nmultiple reviewers). We further delete a few papers without\n\n1About 9.8% sentences are deleted.\n2We adopt ICLR because it releases full submissions, while\nsome other conferences only release accepted papers.\n3We manually checked GPT-4\u2019s extraction results of 200 cases\n\u2014 GPT-4 only missed \u22641% of reviewer-written weaknesses and\nmaintained almost all the original text.\n\n\nany weaknesses found in the raw comments, resulting in a\ntotal of 993 instances, i.e., 993 {paper, weakness lists} pairs.\n\n\u2462 **Input data processing.** As we mentioned before, we\ncrawl papers from OpenReview instead of arXiv because the\nunder-review paper draft is required for this task. However,\nnot every paper from OpenReview can be found on arXiv,\ni.e., the source LaTeX code and figures of most under-review\npapers are unavailable. Therefore, we utilize VILA (Lin et al.\n2023) to parse text data out from the PDF; we also employ\nPDFFigures-2.0 (Clark and Divvala 2016) to extract all the\nfigures and tables (in image) from the paper, as Vila is not\ngood at processing the table data.\n\n**Final data.** Our final data is composed of 993 instances,\neach input is paper text along with figure/table images, and\neach output is peer reviewers\u2019 weakness lists. Table 6 shows\ndata statistics.\n\n## Evaluation Criteria\n\nFor EQINFER, we adopt accuracy as the classification criterion. For EXPDESIGN and WEAKNESS, since both tasks have\nnatural language outputs, semantic-based metrics are necessary. Hence, in addition to the conventional ROUGE (Lin\n2004), we also develop several novel similarity-based metrics\nfor each specific task, including:\n\n  - S-F1 (equation 1 and 2): similarity-based F1 for assessing the experiment design quality. It measures how well\neach model-generated experiment aligns with the human\nexperiments.\n\n  - S-Match (equation 3): \u201csoft\u201d match score for evaluating the explanation. It calculates the similarity between\nhuman and model-generated explanations.\n\n  - SN-F1 (equation 4 and 5): updated version of S-F1 to\ndeal with the \u201cnested\u201d review weaknesses.\n\n  - ITF-IDF (equation 6): inspired by the classic TF-IDF;\nmeasures the inter- and intra-paper diversity of model-generated weaknesses.\n\nWe sincerely recommend referring to Appendix for the\nformal equation definitions of the above metrics.\n\n## Experiments and Analyses\n\nIn this section, we conduct extensive experiments\non AAAR-1.0, across various popular LLMs, to quantify the current LLMs\u2019 capacity to tackle high-level\nresearch tasks. Specifically, the following sections include **EQINFER,** **EXPDESIGN, and** **WEAKNESS.**\nPlease refer to the Appendix for details on how to reproduce\nour experiment results.\n\n### EQUATIONINFERENCE\n\n**Settings.** As different LLMs have distinct context windows,\nto ensure a fair comparison, we fix the maximum input length\nfor all models. According to the data statistics of Table 4,\nwe empirically use 1,000 words for both contexts before and\nafter equations, i.e., 2,000 surrounded words.\n\n\n-----\n\n**Main results.** Table 1 shows the main results. Firstly, the\nopen-source LLMs, especially the Falcon and Gemma, perform unexpectedly disappointing (even worse than random\nguesses). These screwed scores are mainly due to the poor\nlong-context instruction following ability, where we find\nsome open-source LLMs are confused with the massive\ninput and often copy the LaTeX code from the input. In\ncontrast, closed-source LLMs generally achieve superior accuracy, probably owing to the richer scientific knowledge\nfrom the larger model parameters. However, considering the\nconventional multi-choice QA formulation of EQINFER, the\nrecently-released GPT-4o solely gets 43.18, implying the\nunique challenge of EQINFER compared with other scientific QA benchmarks (Song et al. 2023). Notably, with the\nhelp of internal CoT, o1 gains stronger performances than\nGPT-4/GPT-4o, indicating the potential benefits of adopting\nreasoning for this task.\n\n_Q: do more contexts boost performance?_ Table 1 unifies\nthe input context lengths to 1,000 words for various LLMs. In\nthis paragraph, we experiment with long-context LLMs to investigate the impact of the input context lengths. Particularly,\nwe scale the input length (per side) from 100 to 1,500 words.\nAs shown in Figure 4, for the open-source LLMs (Llama and\nQwen), after 300 words length, increasing the input context\ndoesn\u2019t help the performance and even significantly drops\nQwen\u2019s scores. While for the closed-source GPT-4-Turbo\nand GPT-4o, scaling up input length gradually boosts the performances at the first 1,000 words, but stabilizes afterwards.\nThis is in line with human intuition, i.e., surrounding context\nis required for the equation inference, as the adjacent context\nusually provides important information, such as the target algorithm description or the notation definition. However, after\nexceeding a specific threshold, more context information is\nnot beneficial anymore and even confuses those LLMs with\npoor long-context handling capacity (Wang et al. 2024; Liu\net al. 2024).\n\n### EXPERIMENTDESIGN\n**Settings.** Similarly, we unify the input context length of\ndifferent LLMs to ensure a fair comparison. According to\nTable 5, we set 2,000 and 3,000 input words for open- and\nclosed-source LLMs, respectively. Meanwhile, as motivation explanation is the subsequent task of experiment design,\nusing model-generated experiments can propagate errors in\nexplanation, leading to inferior results for most LLMs. To\nthis end, we provide LLMs with the oracle experiments when\ngenerating explanations.\n\n**Main results.** Table 2 shows the main results. For the experiment design, the closed-source LLMs generally outperform\nopen-source LLMs, and both closed-/open-source LLMs are\nsuperior to the \u201cCopy Input\u201d baseline (except the Falcon).\nDespite the higher S-Precision, the open-source LLMs are\nseriously deficient in S-Recall compared with closed-source\nLLMs ( 10% ). We find that closed-source LLMs are more\n_\u223c_ _\u2193_\ncreative in experiment design and tend to generate more experiment ideas than open-source LLMs (though most of the\nexperiment ideas are trivial), leading to excellent S-Recall.\nAs for the motivation explanation, the S-Match scores of\n\n\n**Methods** **Accuracy (%)**\n\nRandom Guess 25.00\n\n**_Open-source LLMs_**\n\nOLMo-7B (Groeneveld et al. 2024) 19.00\nFalcon-40B (Almazrouei et al. 2023) 4.39\nGemma 2-27B (Gemma Team, 2024) 3.24\nMistral-7B (Jiang et al. 2023) 22.21\nMixtral-8x22B-MoE (Jiang et al. 2024) 37.08\nLlama 3.1-70B (MetaAI 2024) 38.13\nQwen 2.5-72B (Qwen Team, 2024) 35.93\n\n**_Closed-source LLMs_**\n\nGemini 1.5 Pro (Anil et al. 2023) 34.31\nClaude 3.5 sonnet (Anthropic 2024) **61.10**\nGPT-4 (OpenAI et al. 2023) 49.85\nGPT-4o (OpenAI 2024a) 43.18\no1-preview (OpenAI 2024b) 59.49\n\nTable 1: Various LLMs\u2019 performances on the 1,049 instances\nof EQINFER task.\n\nclosed-source LLMs still surpass the open-source LLMs,\nwhile the score difference is not significant. Furthermore,\nwe find the negative correlation between S-Match and the\nROUGE, where the ROUGE scores of closed-source LLMs\nare broadly inferior. We find that the open-source LLMs often\ntry to copy the terms or phrases from the given experiment, or\neven simply paraphrase the experiment instead of explaining,\nwhich results in a high superficial overlap with the groundtruth explanation. This observation highlights the importance\nof adopting the proposed S-Match to avoid evaluation bias of\ntraditional generation metrics.\n\n_Q1: can self-contained experiments enhance the explana-_\n**tion of motivation?** When generating the explanation in\nTable 2, we provide LLMs with each individual experiment\nand let them explain one by one, because we find that, when\nproviding the whole experiment list, those open-source models only explain partial experiments because of their poor\ninstruction-following capacity. However, there are intuitively\nsome semantic or logical relations between different experiments, e.g., some experiments are prerequisites to others.\nTherefore, this one-by-one prompting might break the selfcontainment of an experiment plan. Consequently, we test\nwith the \u201cwhole-list\u201d prompting, where the LLMs are given\nthe complete experiment list and are asked to explain all\nexperiment steps together.\nAs shown in Table 8, unlike the open-source LLMs, the\nexplanation performances of those closed-source LLMs are\ngenerally improved after adopting whole-list prompting. According to further manual checking, after maintaining the\nself-containment of the experiments, the LLMs can refer to\nother experiments and better grasp the underlying motivation\nof the current experiment.\n\n_Q2: do human evaluation results align with automatic_\n**metrics for explanation?** As the explanation can be openended, in this paragraph, we provide the human evaluation\n\n\n-----\n\n**Experiment Design** **Motivation Explanation**\n\n**Methods**\n**S-F1** **S-Precision** **S-Recall** **S-Match** **ROUGE-L** **ROUGE-1**\n\nCopy Input 21.13 17.94 26.76 40.32 22.06 25.28\n\n**_Open-source LLMs_**\n\nOLMo-7B (Groeneveld et al. 2024) 33.94 37.25 31.79 45.78 26.30 30.38\n\nFalcon-40B (Almazrouei et al. 2023) 17.87 21.78 15.35 17.03 12.10 12.72\n\nGemma 2-27B (Gemma Team, 2024) 34.33 39.71 30.51 42.77 26.20 29.63\n\nMistral-7B (Jiang et al. 2023) 37.62 43.09 34.19 50.18 **30.20** 34.69\n\nMixtral-8x22B-MoE (Jiang et al. 2024) 42.21 50.13 36.82 49.07 29.96 34.53\n\nLlama 3.1-70B (MetaAI 2024) 40.57 48.43 35.43 50.05 29.33 34.11\n\nQwen 2.5-72B (Qwen Team, 2024) 43.24 **51.73** 37.55 51.12 29.46 34.68\n\n**_Closed-source LLMs_**\n\nGemini 1.5 Pro (Anil et al. 2023) 51.87 50.77 53.37 52.87 28.52 33.80\n\nClaude 3.5 sonnet (Anthropic 2024) 48.74 46.49 51.53 53.03 18.75 26.15\n\nGPT-4 (OpenAI et al. 2023) 43.89 42.34 45.82 55.03 22.82 30.01\n\nGPT-4o (OpenAI 2024a) **53.00** 51.24 **55.12** 54.79 27.54 34.31\n\no1-preview (OpenAI 2024b) 46.67 45.04 48.70 **58.55** 29.11 **36.70**\n\nTable 2: Various LLMs\u2019 performances on the 100 instances of EXPDESIGN. The motivation explanation is based on the oracle\nexperiments to prevent error propagation. \u201cCopy Input\u201d is a random baseline: for experiment design, randomly select 5 sentences\nfrom the input paper; for motivation explanation, directly copy each experiment idea.\n\n\nresults on different LLMs\u2019 motivation explanation outputs.\nIn detail, we randomly select 20 out of 100 papers and ask 5\nannotators to read the experiments along with each model\u2019s\nexplanations; we then let the annotator decide whether each\nmodel\u2019s explanation is acceptable (see Appendix for more details). Table 9 illustrates the results, where the score variance\nis higher than Table 2. However, the performance ranking\nof both tables is perfectly correlated with each other (Spearman\u2019s rank correlation coefficient = 1), demonstrating the\neffectiveness of S-Match.\n\n_Q3: do more contexts boost performance?_ We also investigate the impact of input context length for EXPDESIGN.\nAs shown in Figure 5, we scale up the input pre-experiment\ncontext length from 0.1k to 10k words (10k words is the maximum paper context length in the dataset). For the experiment\nplanning, more input context does improve the performance\nof different LLMs, while this benefit stops after exceeding 5k\nwords, which is similar to EQINFER\u2019s scaling results \u2014 after\nthe necessary information has been covered, scaling more\nup doesn\u2019t boost the performance. Meanwhile, the results\nof the motivation explanation demonstrate that explaining\nmotivations almost doesn\u2019t require any paper context, i.e.,\nthe LLMs solely rely on the given experiments. However,\nwe do not expect this because we hope LLMs can explain\nthe motivation based on a thorough understanding of the paper, just like how human experts do. Hence, there is still a\nconsiderable gap between the LLMs and humans in terms of\ngrasping research motivations.\n\n_Q4: does multi-modal input boost performance?_ Intuitively, besides the text, when designing experiments for a\ngiven research topic, the figures can provide rich supplementary information, such as an algorithm illustration that can\nhelp better understand this research topic and underlying mo\n\ntivations. Hence, we test different MLLMs\u2019 performances,\nincluding GPT4-o, GPT-4, and InternVL2 (Chen et al. 2024b).\nTable 10 shows the ablation results on the figure data. To our\nsurprise, the figure data doesn\u2019t improve the MLLMs\u2019 results\nin this task, even harming the performances. This might be\ndue to the low informativeness of the figures, as figures usually consume more input tokens but act only as supplementary\ninformation to the text, indicating future work on developing\nMLLMs that can effectively leverage the scientific figures.\n\n### PAPERWEAKNESS\n**Settings.** Intuitively, the full paper context is necessary for\nconducting a review. Therefore, instead of setting a maximum\ninput length, in WEAKNESS, we try to feed all the paper context into the LLMs. As the input length of WEAKNESS is\nextremely long (see Table 6), we adopt a \u201csplit-combine\u201d\nmethod \u2014 we first split the whole paper into several smaller\npieces and let LLMs predict the weaknesses of each piece\nseparately; after that, we combine all pieces\u2019 weaknesses as\na final complete prediction. In practice, for the length of each\nsmall piece, we set 2,000 and 3,000 words for open- and\nclosed-source LLMs, respectively. Additionally, in this task,\nwe also examine the performance of a recent agent framework, namely AI-SCI (Lu et al. 2024), which enhances GPT4o\u2019s paper review ability by leveraging advanced prompting\ntechniques, e.g., self-reflection (Shinn et al. 2024) and response ensembling (Wang et al. 2023).[4]\n\n**Main results.** Table 3 shows the main results, where the\nclosed-source LLMs\u2019 overall performances are generally superior to the results of open-source LLMs. Similarly, closed\n4We don\u2019t run AI-SCI on EXPDESIGN, because AI-SCI takes\nmodel-generated ideas as the inputs, which are incompatible with\nour task setting.\n\n\n-----\n\n**Review Diversity**\n\n\n**Methods** **SN-F1 (%)** **SN-Precision (%)",
    "raw_evidence": "",
    "raw_conclusions": "",
    "execution_times": {
        "claims_analysis_time": "334.48 seconds",
        "evidence_analysis_time": "1.83 seconds",
        "conclusions_analysis_time": "1.83 seconds",
        "total_execution_time": "343.79 seconds"
    }
}