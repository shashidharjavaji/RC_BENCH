{
    "paper_analysis": [],
    "raw_claims": " For example:\n        ```json\n        {\n            \"claims\": [\n                {\n                    \"claim_id\": 1,\n                    \"claim_text\": \"Deep multimodal learning has achieved great progress in recent years.\",\n                    \"location\": \"Abstract\",\n                    \"claim_type\": \"Result\",\n                    \"exact_quote\": \"Deep multimodal learning has achieved great progress in recent years.\"\n                },\n                {\n                    \"claim_id\": 2,\n                    \"claim_text\": \"Current fusion approaches are static in nature, i.e., they process and fuse multimodal inputs with identical computation, without accounting for diverse computational demands of different multimodal data.\",\n                    \"location\": \"Abstract\",\n                    \"claim_type\": \"Result\",\n                    \"exact_quote\": \"Current fusion approaches are static in nature, i.e., they process and fuse multimodal inputs with identical computation, without accounting for diverse computational demands of different multimodal data.\"\n                },\n                {\n                    \"claim_id\": 3,\n                    \"claim_text\": \"In this work, we propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses input data from multiple modalities.\",\n                    \"location\": \"Abstract\",\n                    \"claim_type\": \"Contribution\",\n                    \"exact_quote\": \"In this work, we propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses input data from multiple modalities.\"\n                },\n                {\n                    \"claim_id\": 4,\n                    \"claim_text\": \"DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.\",\n                    \"location\": \"Abstract\",\n                    \"claim_type\": \"Result\",\n                    \"exact_quote\": \"DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.\"\n                },\n                {\n                    \"claim_id\": 5,\n                    \"claim_text\": \"We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.\",\n                    \"location\": \"Abstract\",\n                    \"claim_type\": \"Conclusion\",\n                    \"exact_quote\": \"We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.\"\n                }\n            ]\n        }\n        ```\n\n        ```json\n        {\n            \"claims\": [\n                {\n                    \"claim_id\": 1,\n                    \"claim_text\": \"Deep multimodal learning has achieved great progress in recent years.\",\n                    \"location\": \"Abstract\",\n                    \"claim_type\": \"Result\",\n                    \"exact_quote\": \"Deep multimodal learning has achieved great progress in recent years.\"\n                },\n                {\n                    \"claim_id\": 2,\n                    \"claim_text\": \"Current fusion approaches are static in nature, i.e., they process and fuse multimodal inputs with identical computation, without accounting for diverse computational demands of different multimodal data.\",\n                    \"location\": \"Abstract\",\n                    \"claim_type\": \"Result\",\n                    \"exact_quote\": \"Current fusion approaches are static in nature, i.e., they process and fuse multimodal inputs with identical computation, without accounting for diverse computational demands of different multimodal data.\"\n                },\n                {\n                    \"claim_id\": 3,\n                    \"claim_text\": \"In this work, we propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses input data from multiple modalities.\",\n                    \"location\": \"Abstract\",\n                    \"claim_type\": \"Contribution\",\n                    \"exact_quote\": \"In this work, we propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses input data from multiple modalities.\"\n                },\n                {\n                    \"claim_id\": 4,\n                    \"claim_text\": \"DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.\",\n                    \"location\": \"Abstract\",\n                    \"claim_type\": \"Result\",\n                    \"exact_quote\": \"DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.\"\n                },\n                {\n                    \"claim_id\": 5,\n                    \"claim_text\": \"We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.\",\n                    \"location\": \"Abstract\",\n                    \"claim_type\": \"Conclusion\",\n                    \"exact_quote\": \"We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.\"\n                }\n            ]\n        }\n        ```",
    "raw_evidence": "",
    "raw_conclusions": "",
    "execution_times": {
        "claims_analysis_time": "41.89 seconds",
        "evidence_analysis_time": "0.89 seconds",
        "conclusions_analysis_time": "0.89 seconds",
        "total_execution_time": "50.64 seconds"
    }
}