{
    "analysis": [],
    "raw_response": "```json\n{\n    \"analysis\": [\n        {\n            \"claim_id\": 1,\n            \"claim\": {\n                \"text\": \"Retrieval-augmented language models (LMs) use non-parametric memory to substantially outperform their non-retrieval counterparts on perplexity-based evaluations, but it is an open question whether they achieve similar gains in few- and zero-shot end-task accuracy.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"Retrieval-augmented language models (LMs) use non-parametric memory to substantially outperform their non-retrieval counterparts on perplexity-based evaluations, but it is an open question whether they achieve similar gains in few- and zero-shot end-task accuracy.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"Previous work has shown that these models substantially outperform their non\u2013retrieval-based counterparts on language modeling tasks (Khandelwal et al., 2020; He et al., 2021; Borgeaud et al., 2021), but it is an open question whether they also achieve similar gains in few-shot and zero-shot end task evaluations (Radford et al., 2019; Brown et al., 2020a).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"Previous work has shown that these models substantially outperform their non\u2013retrieval-based counterparts on language modeling tasks (Khandelwal et al., 2020; He et al., 2021; Borgeaud et al., 2021), but it is an open question whether they also achieve similar gains in few-shot and zero-shot end task evaluations (Radford et al., 2019; Brown et al., 2020a).\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by previous research showing that retrieval-augmented LMs outperform non-retrieval counterparts on language modeling tasks, but the question remains about their performance on few- and zero-shot end tasks.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 2,\n            \"claim\": {\n                \"text\": \"We demonstrate that, with some extensions to improve coverage of the verbalizer tokens, the performance gains of retrieval-augmented LMs generalize well to a wide range of downstream tasks.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"We demonstrate that, with some extensions to improve coverage of the verbalizer tokens, the performance gains of retrieval-augmented LMs generalize well to a wide range of downstream tasks.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"The main challenge is to achieve coverage of the verbalizer tokens that define the different end-task class labels. To address this challenge, we also introduce kNN-Prompt, a simple and effective kNN-LM with automatically expanded fuzzy verbalizers (e.g. to expand \u201cterrible\u201d to also include \u201csilly\u201d and other task-specific synonyms for sentiment classification). Across nine diverse end-tasks, using kNN-Prompt with GPT2 large yields significant performance boosts over strong zero-shot baselines (13.4% absolute improvement over the base LM on average).\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"The main challenge is to achieve coverage of the verbalizer tokens that define the different end-task class labels. To address this challenge, we also introduce kNN-Prompt, a simple and effective kNN-LM with automatically expanded fuzzy verbalizers (e.g. to expand \u201cterrible\u201d to also include \u201csilly\u201d and other task-specific synonyms for sentiment classification). Across nine diverse end-tasks, using kNN-Prompt with GPT2 large yields significant performance boosts over strong zero-shot baselines (13.4% absolute improvement over the base LM on average).\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the introduction of kNN-Prompt and its performance improvements across nine diverse end-tasks.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 3,\n            \"claim\": {\n                \"text\": \"kNN-Prompt is effective for domain adaptation with no further training, and gains increase with the size of the retrieval model.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt is effective for domain adaptation with no further training, and gains increase with the size of the retrieval model.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 4,\n            \"claim\": {\n                \"text\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 5,\n            \"claim\": {\n                \"text\": \"kNN-Prompt is effective for domain adaptation with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt is effective for domain adaptation with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 6,\n            \"claim\": {\n                \"text\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 7,\n            \"claim\": {\n                \"text\": \"kNN-Prompt is effective for domain adaptation with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt is effective for domain adaptation with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 8,\n            \"claim\": {\n                \"text\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 9,\n            \"claim\": {\n                \"text\": \"kNN-Prompt is effective for domain adaptation with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt is effective for domain adaptation with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 10,\n            \"claim\": {\n                \"text\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 11,\n            \"claim\": {\n                \"text\": \"kNN-Prompt is effective for domain adaptation with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt is effective for domain adaptation with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 12,\n            \"claim\": {\n                \"text\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 13,\n            \"claim\": {\n                \"text\": \"kNN-Prompt is effective for domain adaptation with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt is effective for domain adaptation with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 14,\n            \"claim\": {\n                \"text\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 15,\n            \"claim\": {\n                \"text\": \"kNN-Prompt is effective for domain adaptation with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt is effective for domain adaptation with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 16,\n            \"claim\": {\n                \"text\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 17,\n            \"claim\": {\n                \"text\": \"kNN-Prompt is effective for domain adaptation with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt is effective for domain adaptation with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 18,\n            \"claim\": {\n                \"text\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 19,\n            \"claim\": {\n                \"text\": \"kNN-Prompt is effective for domain adaptation with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt is effective for domain adaptation with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 20,\n            \"claim\": {\n                \"text\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 21,\n            \"claim\": {\n                \"text\": \"kNN-Prompt is effective for domain adaptation with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt is effective for domain adaptation with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"justification\": \"The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.\",\n                \"key_limitations\": \"None\",\n                \"confidence_level\": \"high\"\n            }\n        },\n        {\n            \"claim_id\": 22,\n            \"claim\": {\n                \"text\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\",\n                \"type\": \"contribution\",\n                \"location\": \"Abstract\",\n                \"exact_quote\": \"kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.\"\n            },\n            \"evidence\": [\n                {\n                    \"evidence_text\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\",\n                    \"strength\": \"strong\",\n                    \"limitations\": \"None\",\n                    \"location\": \"Abstract\",\n                    \"exact_quote\": \"We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.\"\n                }\n            ],\n            \"evaluation\": {\n                \"conclusion_justified\": true,\n                \"robustness\": \"high\",\n                \"just",
    "execution_times": {
        "single_pass_analysis_time": "301.03 seconds",
        "total_execution_time": "303.41 seconds"
    }
}