Claim 1:
Type: contribution
Statement: The audio-visual video parsing task aims to parse a video into modality- and category-aware temporal segments.
Location: Abstract
Exact Quote: The audio-visual video parsing task aims to parse a video into modality- and category-aware temporal segments.

Evidence:
- Evidence Text: Previous work mainly focuses on weakly-supervised approaches, which learn from video-level event labels.
  Strength: strong
  Location: Abstract
  Limitations: Limited to video-level labels
  Exact Quote: Previous work mainly focuses on weakly-supervised approaches, which learn from video-level event labels.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is well-supported by the description of the task and the focus on weakly-supervised approaches.
Key Limitations: Limited to video-level labels

--------------------------------------------------

Claim 2:
Type: methodology
Statement: The Multi-modal Grouping Network (MGN) is proposed to explicitly group semantic-aware multi-modal contexts.
Location: Abstract
Exact Quote: The Multi-modal Grouping Network (MGN) is proposed to explicitly group semantic-aware multi-modal contexts.

Evidence:
- Evidence Text: The MGN aggregates event-aware unimodal features through unimodal grouping in terms of learnable categorical embedding tokens.
  Strength: strong
  Location: Abstract
  Limitations: Limited to unimodal features
  Exact Quote: The MGN aggregates event-aware unimodal features through unimodal grouping in terms of learnable categorical embedding tokens.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is well-supported by the description of the MGN's methodology.
Key Limitations: Limited to unimodal features

--------------------------------------------------

Claim 3:
Type: performance
Statement: The MGN achieves improving results against previous baselines on weakly-supervised audiovisual video parsing.
Location: Abstract
Exact Quote: The MGN achieves improving results against previous baselines on weakly-supervised audiovisual video parsing.

Evidence:
- Evidence Text: Experimental results on the LLP dataset validate that our new audio-visual video parsing framework achieves superior results over previous state-of-the-art methods.
  Strength: strong
  Location: Abstract
  Limitations: Limited to the LLP dataset
  Exact Quote: Experimental results on the LLP dataset validate that our new audio-visual video parsing framework achieves superior results over previous state-of-the-art methods.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is well-supported by the experimental results on the LLP dataset.
Key Limitations: Limited to the LLP dataset

--------------------------------------------------

Claim 4:
Type: performance
Statement: The MGN is much more lightweight, using only 47.2% of the parameters of baselines (17 MB vs. 36 MB).
Location: Abstract
Exact Quote: The MGN is much more lightweight, using only 47.2% of the parameters of baselines (17 MB vs. 36 MB).

Evidence:
- Evidence Text: Code is available at https://github.com/stoneMo/MGN.
  Strength: strong
  Location: Abstract
  Limitations: Limited to the provided code repository
  Exact Quote: Code is available at https://github.com/stoneMo/MGN.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is well-supported by the availability of the code repository.
Key Limitations: Limited to the provided code repository

--------------------------------------------------

Claim 5:
Type: performance
Statement: The MGN achieves superior results over previous state-of-the-art methods [1, 2, 3, 4] on the LLP dataset.
Location: Abstract
Exact Quote: The MGN achieves superior results over previous state-of-the-art methods [1, 2, 3, 4] on the LLP dataset.

Evidence:
- Evidence Text: Experimental results on the LLP dataset validate that our new audio-visual video parsing framework achieves superior results over previous state-of-the-art methods.
  Strength: strong
  Location: Abstract
  Limitations: Limited to the LLP dataset
  Exact Quote: Experimental results on the LLP dataset validate that our new audio-visual video parsing framework achieves superior results over previous state-of-the-art methods.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is well-supported by the experimental results on the LLP dataset.
Key Limitations: Limited to the LLP dataset

--------------------------------------------------

Claim 6:
Type: methodology
Statement: The MGN is a fully novel network architecture to alleviate implicit audio-visual matching and modality category uncertainty in the hybrid attention network.
Location: Abstract
Exact Quote: The MGN is a fully novel network architecture to alleviate implicit audio-visual matching and modality category uncertainty in the hybrid attention network.

Evidence:
- Evidence Text: We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.
  Strength: strong
  Location: Abstract
  Limitations: Limited to the methodology described
  Exact Quote: We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is well-supported by the description of the methodology.
Key Limitations: Limited to the methodology described

--------------------------------------------------

Claim 7:
Type: methodology
Statement: The MGN introduces class-aware unimodal grouping and modality-aware cross-modal grouping modules to aggregate multi-modal temporal contexts.
Location: Abstract
Exact Quote: The MGN introduces class-aware unimodal grouping and modality-aware cross-modal grouping modules to aggregate multi-modal temporal contexts.

Evidence:
- Evidence Text: The MGN aggregates event-aware unimodal features through unimodal grouping in terms of learnable categorical embedding tokens.
  Strength: strong
  Location: Abstract
  Limitations: Limited to the methodology described
  Exact Quote: The MGN aggregates event-aware unimodal features through unimodal grouping in terms of learnable categorical embedding tokens.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is well-supported by the description of the methodology.
Key Limitations: Limited to the methodology described

--------------------------------------------------

Claim 8:
Type: performance
Statement: The MGN achieves significant performance gains of 1.6 Type@AV and 1.8 Event@AV.
Location: Section 4.2
Exact Quote: The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics. For the overall evaluation of segment-level predictions, we achieve significant performance gains of 1.6 Type@AV and 1.8 Event@AV.

Evidence:
- Evidence Text: The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics. For the overall evaluation of segment-level predictions, we achieve significant performance gains of 1.6 Type@AV and 1.8 Event@AV.
  Strength: strong
  Location: Section 4.2
  Limitations: Limited to the LLP dataset
  Exact Quote: The proposed MGN achieves the overall best results against previous network baselines in terms most of metrics. For the overall evaluation of segment-level predictions, we achieve significant performance gains of 1.6 Type@AV and 1.8 Event@AV.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is well-supported by the experimental results on the LLP dataset.
Key Limitations: Limited to the LLP dataset

--------------------------------------------------

Claim 9:
Type: performance
Statement: The MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Type@AV for event-level predictions.
Location: Section 4.2
Exact Quote: When evaluated on segment-level predictions of each sample, our MGN also improves the baseline by large margins, 2.6 Visual and 1.7 Audio-Visual. Meanwhile, our MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Type@AV for event-level predictions.

Evidence:
- Evidence Text: When evaluated on segment-level predictions of each sample, our MGN also improves the baseline by large margins, 2.6 Visual and 1.7 Audio-Visual. Meanwhile, our MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Type@AV for event-level predictions.
  Strength: strong
  Location: Section 4.2
  Limitations: Limited to the LLP dataset
  Exact Quote: When evaluated on segment-level predictions of each sample, our MGN also improves the baseline by large margins, 2.6 Visual and 1.7 Audio-Visual. Meanwhile, our MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Type@AV for event-level predictions.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is well-supported by the experimental results on the LLP dataset.
Key Limitations: Limited to the LLP dataset

--------------------------------------------------

Claim 10:
Type: performance
Statement: The MGN with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV.
Location: Section 4.2
Exact Quote: These improvements imply the strong generalizability of the proposed MGN to the audio-visual contrastive learning and the label refinement.

Evidence:
- Evidence Text: Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual. With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.
  Strength: strong
  Location: Section 4.2
  Limitations: Limited to the LLP dataset
  Exact Quote: Adding the contrastive learning to our MGN achieves the segment-level performance gain of 3.6 Visual and 2.8 Audio-Visual, and the event-level gain of 3.8 Visual and 2.6 Audio-Visual. With the label refinement in MA, we significantly improve MA (w R) by 3.1 segment-level and 4.0 event-level for visual predictions.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is well-supported by the experimental results on the LLP dataset.
Key Limitations: Limited to the LLP dataset

--------------------------------------------------

Claim 11:
Type: performance
Statement: The MGN with explicit grouping mechanisms significantly eliminates false predictions caused by the modality and temporal uncertainties existing in the baseline.
Location: Section 4.3
Exact Quote: The proposed MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494.

Evidence:
- Evidence Text: The proposed MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494.
  Strength: strong
  Location: Section 4.3
  Limitations: Limited to the LLP dataset
  Exact Quote: The proposed MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is well-supported by the experimental results on the LLP dataset.
Key Limitations: Limited to the LLP dataset

--------------------------------------------------

Claim 12:
Type: methodology
Statement: The learned class tokens are essential to grouping class-aware semantics from audio and visual features.
Location: Section 4.3
Exact Quote: The learned class tokens are essential to grouping class-aware semantics from audio and visual features.

Evidence:
- Evidence Text: To better evaluate the quality of those learned class-level features, we visualize the learned audio and visual representations of 25 categories by t-SNE.
  Strength: strong
  Location: Section 4.3
  Limitations: Limited to the methodology described
  Exact Quote: To better evaluate the quality of those learned class-level features, we visualize the learned audio and visual representations of 25 categories by t-SNE.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is well-supported by the description of the methodology.
Key Limitations: Limited to the methodology described

--------------------------------------------------

Claim 13:
Type: performance
Statement: The proposed MGN successfully learns compact and discriminative features for each modality.
Location: Section 4.3
Exact Quote: Features extracted by the proposed MGN are intra-class compact and inter-class separable.

Evidence:
- Evidence Text: Features extracted by the proposed MGN are intra-class compact and inter-class separable.
  Strength: strong
  Location: Section 4.3
  Limitations: Limited to the methodology described
  Exact Quote: Features extracted by the proposed MGN are intra-class compact and inter-class separable.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is well-supported by the description of the methodology.
Key Limitations: Limited to the methodology described

--------------------------------------------------

