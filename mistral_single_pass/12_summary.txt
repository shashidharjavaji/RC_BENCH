Claim 1:
Type: performance
Statement: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match.
Location: 5.1 SHELL SCRIPTING RESULTS
Exact Quote: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match.

Evidence:
- Evidence Text: Table 1 shows that DocPrompting consistently improves the base models. For example, T5+DocPrompting achieves more than twice higher accuracy in predicting the command name, more than 16 charBLEU points on the entire prediction, and almost 9% of absolute exact match gain, compared to the vanilla T5.
  Strength: strong
  Location: 5.1 SHELL SCRIPTING RESULTS
  Limitations: None
  Exact Quote: Table 1 shows that DocPrompting consistently improves the base models. For example, T5+DocPrompting achieves more than twice higher accuracy in predicting the command name, more than 16 charBLEU points on the entire prediction, and almost 9% of absolute exact match gain, compared to the vanilla T5.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The results are supported by concrete experimental data showing significant improvements in various metrics.
Key Limitations: None

--------------------------------------------------

Claim 2:
Type: performance
Statement: Retrieving documentation also benefits strong models such as Codex, and with only few examples in the context.
Location: 5.1 SHELL SCRIPTING RESULTS
Exact Quote: Retrieving documentation also benefits strong models such as Codex, and with only few examples in the context.

Evidence:
- Evidence Text: Table 1 shows that DocPrompting brings gains of 6.7 charBLEU points, and consistent improvement across all metrics over the baseline that observes only NL-code pairs in its prompt.
  Strength: strong
  Location: 5.1 SHELL SCRIPTING RESULTS
  Limitations: None
  Exact Quote: Table 1 shows that DocPrompting brings gains of 6.7 charBLEU points, and consistent improvement across all metrics over the baseline that observes only NL-code pairs in its prompt.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The results are supported by concrete experimental data showing significant improvements in various metrics.
Key Limitations: None

--------------------------------------------------

Claim 3:
Type: performance
Statement: Using DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match.
Location: 5.1 SHELL SCRIPTING RESULTS
Exact Quote: Using DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match.

Evidence:
- Evidence Text: Table 1 shows that DocPrompting consistently improves the base models. For example, T5+DocPrompting achieves more than twice higher accuracy in predicting the command name, more than 16 charBLEU points on the entire prediction, and almost 9% of absolute exact match gain, compared to the vanilla T5.
  Strength: strong
  Location: 5.1 SHELL SCRIPTING RESULTS
  Limitations: None
  Exact Quote: Table 1 shows that DocPrompting consistently improves the base models. For example, T5+DocPrompting achieves more than twice higher accuracy in predicting the command name, more than 16 charBLEU points on the entire prediction, and almost 9% of absolute exact match gain, compared to the vanilla T5.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The results are supported by concrete experimental data showing significant improvements in various metrics.
Key Limitations: None

--------------------------------------------------

Claim 4:
Type: performance
Statement: Retrieving documentation also benefits strong models such as Codex, and with only few examples in the context.
Location: 5.1 SHELL SCRIPTING RESULTS
Exact Quote: Retrieving documentation also benefits strong models such as Codex, and with only few examples in the context.

Evidence:
- Evidence Text: Table 1 shows that DocPrompting brings gains of 6.7 charBLEU points, and consistent improvement across all metrics over the baseline that observes only NL-code pairs in its prompt.
  Strength: strong
  Location: 5.1 SHELL SCRIPTING RESULTS
  Limitations: None
  Exact Quote: Table 1 shows that DocPrompting brings gains of 6.7 charBLEU points, and consistent improvement across all metrics over the baseline that observes only NL-code pairs in its prompt.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The results are supported by concrete experimental data showing significant improvements in various metrics.
Key Limitations: None

--------------------------------------------------

Claim 5:
Type: performance
Statement: Using DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match.
Location: 5.1 SHELL SCRIPTING RESULTS
Exact Quote: Using DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match.

Evidence:
- Evidence Text: Table 1 shows that DocPrompting consistently improves the base models. For example, T5+DocPrompting achieves more than twice higher accuracy in predicting the command name, more than 16 charBLEU points on the entire prediction, and almost 9% of absolute exact match gain, compared to the vanilla T5.
  Strength: strong
  Location: 5.1 SHELL SCRIPTING RESULTS
  Limitations: None
  Exact Quote: Table 1 shows that DocPrompting consistently improves the base models. For example, T5+DocPrompting achieves more than twice higher accuracy in predicting the command name, more than 16 charBLEU points on the entire prediction, and almost 9% of absolute exact match gain, compared to the vanilla T5.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The results are supported by concrete experimental data showing significant improvements in various metrics.
Key Limitations: None

--------------------------------------------------

Claim 6:
Type: performance
Statement: Retrieving documentation also benefits strong models such as Codex, and with only few examples in the context.
Location: 5.1 SHELL SCRIPTING RESULTS
Exact Quote: Retrieving documentation also benefits strong models such as Codex, and with only few examples in the context.

Evidence:
- Evidence Text: Table 1 shows that DocPrompting brings gains of 6.7 charBLEU points, and consistent improvement across all metrics over the baseline that observes only NL-code pairs in its prompt.
  Strength: strong
  Location: 5.1 SHELL SCRIPTING RESULTS
  Limitations: None
  Exact Quote: Table 1 shows that DocPrompting brings gains of 6.7 charBLEU points, and consistent improvement across all metrics over the baseline that observes only NL-code pairs in its prompt.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The results are supported by concrete experimental data showing significant improvements in various metrics.
Key Limitations: None

--------------------------------------------------

Claim 7:
Type: performance
Statement: Using DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match.
Location: 5.1 SHELL SCRIPTING RESULTS
Exact Quote: Using DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match.

Evidence:
- Evidence Text: Table 1 shows that DocPrompting consistently improves the base models. For example, T5+DocPrompting achieves more than twice higher accuracy in predicting the command name, more than 16 charBLEU points on the entire prediction, and almost 9% of absolute exact match gain, compared to the vanilla T5.
  Strength: strong
  Location: 5.1 SHELL SCRIPTING RESULTS
  Limitations: None
  Exact Quote: Table 1 shows that DocPrompting consistently improves the base models. For example, T5+DocPrompting achieves more than twice higher accuracy in predicting the command name, more than 16 charBLEU points on the entire prediction, and almost 9% of absolute exact match gain, compared to the vanilla T5.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The results are supported by concrete experimental data showing significant improvements in various metrics.
Key Limitations: None

--------------------------------------------------

Claim 8:
Type: performance
Statement: Retrieving documentation also benefits strong models such as Codex, and with only few examples in the context.
Location: 5.1 SHELL SCRIPTING RESULTS
Exact Quote: Retrieving documentation also benefits strong models such as Codex, and with only few examples in the context.

Evidence:
- Evidence Text: Table 1 shows that DocPrompting brings gains of 6.7 charBLEU points, and consistent improvement across all metrics over the baseline that observes only NL-code pairs in its prompt.
  Strength: strong
  Location: 5.1 SHELL SCRIPTING RESULTS
  Limitations: None
  Exact Quote: Table 1 shows that DocPrompting brings gains of 6.7 charBLEU points, and consistent improvement across all metrics over the baseline that observes only NL-code pairs in its prompt.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The results are supported by concrete experimental data showing significant improvements in various metrics.
Key Limitations: None

--------------------------------------------------

Claim 9:
Type: performance
Statement: Using DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match.
Location: 5.1 SHELL SCRIPTING RESULTS
Exact Quote: Using DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match.

Evidence:
- Evidence Text: Table 1 shows that DocPrompting consistently improves the base models. For example, T5+DocPrompting achieves more than twice higher accuracy in predicting the command name, more than 16 charBLEU points on the entire prediction, and almost 9% of absolute exact match gain, compared to the vanilla T5.
  Strength: strong
  Location: 5.1 SHELL SCRIPTING RESULTS
  Limitations: None
  Exact Quote: Table 1 shows that DocPrompting consistently improves the base models. For example, T5+DocPrompting achieves more than twice higher accuracy in predicting the command name, more than 16 charBLEU points on the entire prediction, and almost 9% of absolute exact match gain, compared to the vanilla T5.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The results are supported by concrete experimental data showing significant improvements in various metrics.
Key Limitations: None

--------------------------------------------------

Claim 10:
Type: performance
Statement: Retrieving documentation also benefits strong models such as Codex, and with only few examples in the context.
Location: 5.1 SHELL SCRIPTING RESULTS
Exact Quote: Retrieving documentation also benefits strong models such as Codex, and with only few examples in the context.

Evidence:
- Evidence Text: Table 1 shows that DocPrompting brings gains of 6.7 charBLEU points, and consistent improvement across all metrics over the baseline that observes only NL-code pairs in its prompt.
  Strength: strong
  Location: 5.1 SHELL SCRIPTING RESULTS
  Limitations: None
  Exact Quote: Table 1 shows that DocPrompting brings gains of 6.7 charBLEU points, and consistent improvement across all metrics over the baseline that observes only NL-code pairs in its prompt.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The results are supported by concrete experimental data showing significant improvements in various metrics.
Key Limitations: None

--------------------------------------------------

