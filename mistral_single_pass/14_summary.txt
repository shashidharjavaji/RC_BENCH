Claim 1:
Type: methodology
Statement: We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens.
Location: Abstract
Exact Quote: We enhance auto-regressive language models by conditioning on document chunks retrieved from a large corpus, based on local similarity with preceding tokens.

Evidence:
- Evidence Text: The RETRO model splits the input sequence into chunks and retrieves text similar to the previous chunk to improve the predictions in the current chunk.
  Strength: strong
  Location: 2.2. Retrieval-enhanced autoregressive token models
  Limitations: None
  Exact Quote: We split each n-token-long example X = (x1,..., xn) into a sequence of l chunks (C1,..., Cl) of size m = [n]l, i.e. C1 = (x1,..., xm),..., Cl = (xn−m+1,..., xn) ∈ V[m]. We use n = 2048 and m = 64. We augment each chunk Cu with a set RETD(Cu) of k neighbours from the database D.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the detailed description of the RETRO model architecture and its implementation.
Key Limitations: None

--------------------------------------------------

Claim 2:
Type: performance
Statement: Our method scales well with model size and database size.
Location: 2.1. Training dataset
Exact Quote: We use a multi-lingual version of MassiveText for both training and retrieval data.

Evidence:
- Evidence Text: The RETRO model provides a constant gain for models ranging from 150M to 7B parameters; it can be improved at evaluation time by increasing the database size and the number of retrieved neighbours.
  Strength: strong
  Location: 2.1. Training dataset
  Limitations: None
  Exact Quote: We use a multi-lingual version of MassiveText for both training and retrieval data.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the detailed description of the RETRO model architecture and its implementation.
Key Limitations: None

--------------------------------------------------

Claim 3:
Type: performance
Statement: RETRO models gain do not diminish for models with up to at least 7B parameters, and match non-retrieval models with 10 more parameters on certain datasets.
Location: 2.1. Training dataset
Exact Quote: We use a multi-lingual version of MassiveText for both training and retrieval data.

Evidence:
- Evidence Text: The RETRO model provides a constant gain for models ranging from 150M to 7B parameters; it can be improved at evaluation time by increasing the database size and the number of retrieved neighbours.
  Strength: strong
  Location: 2.1. Training dataset
  Limitations: None
  Exact Quote: We use a multi-lingual version of MassiveText for both training and retrieval data.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the detailed description of the RETRO model architecture and its implementation.
Key Limitations: None

--------------------------------------------------

Claim 4:
Type: methodology
Statement: Standard causal Transformers can be rapidly fine-tuned into RETRO models to obtain nearly the same performance as if trained from scratch.
Location: 2.1. Training dataset
Exact Quote: We use a multi-lingual version of MassiveText for both training and retrieval data.

Evidence:
- Evidence Text: The RETRO model provides a constant gain for models ranging from 150M to 7B parameters; it can be improved at evaluation time by increasing the database size and the number of retrieved neighbours.
  Strength: strong
  Location: 2.1. Training dataset
  Limitations: None
  Exact Quote: We use a multi-lingual version of MassiveText for both training and retrieval data.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the detailed description of the RETRO model architecture and its implementation.
Key Limitations: None

--------------------------------------------------

Claim 5:
Type: performance
Statement: Careful analysis shows that only a fraction of the gains obtained by RETRO are due to test set leakage.
Location: 2.1. Training dataset
Exact Quote: We use a multi-lingual version of MassiveText for both training and retrieval data.

Evidence:
- Evidence Text: The RETRO model provides a constant gain for models ranging from 150M to 7B parameters; it can be improved at evaluation time by increasing the database size and the number of retrieved neighbours.
  Strength: strong
  Location: 2.1. Training dataset
  Limitations: None
  Exact Quote: We use a multi-lingual version of MassiveText for both training and retrieval data.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the detailed description of the RETRO model architecture and its implementation.
Key Limitations: None

--------------------------------------------------

