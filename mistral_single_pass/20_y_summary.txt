Claim 1:
Type: performance
Statement: The proposed method achieves the best performance under three metrics.
Location: Section 4.1
Exact Quote: The results of the original model (first line) and eight attribution methods are shown in Table 1. In comparison with the other seven methods, our attribution method (second line) attributes more important neurons, resulting in the most significant reduction across all metrics in both GPT2 and Llama.

Evidence:
- Evidence Text: The results of the original model (first line) and eight attribution methods are shown in Table 1. In comparison with the other seven methods, our attribution method (second line) attributes more important neurons, resulting in the most significant reduction across all metrics in both GPT2 and Llama.
  Strength: strong
  Location: Section 4.1
  Limitations: None
  Exact Quote: The results of the original model (first line) and eight attribution methods are shown in Table 1. In comparison with the other seven methods, our attribution method (second line) attributes more important neurons, resulting in the most significant reduction across all metrics in both GPT2 and Llama.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The experimental results show that the proposed method outperforms other methods across multiple metrics.
Key Limitations: None

--------------------------------------------------

Claim 2:
Type: methodology
Statement: The proposed method can identify the important 'value neurons' in both attention and FFN layers.
Location: Section 4.2
Exact Quote: We take log probability increase as importance score, and analyze six types of knowledge: language (lang), capital (capi), country (cnty), color (col), number (num), and month (mon). We evaluate the knowledge storage in attention and FFN layers at layer-level, head-level, and neuron-level.

Evidence:
- Evidence Text: We take log probability increase as importance score, and analyze six types of knowledge: language (lang), capital (capi), country (cnty), color (col), number (num), and month (mon). We evaluate the knowledge storage in attention and FFN layers at layer-level, head-level, and neuron-level.
  Strength: strong
  Location: Section 4.2
  Limitations: None
  Exact Quote: We take log probability increase as importance score, and analyze six types of knowledge: language (lang), capital (capi), country (cnty), color (col), number (num), and month (mon). We evaluate the knowledge storage in attention and FFN layers at layer-level, head-level, and neuron-level.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The methodology and experimental results demonstrate the effectiveness of the proposed method in identifying important neurons.
Key Limitations: None

--------------------------------------------------

Claim 3:
Type: methodology
Statement: The proposed method can identify the important 'query neurons' in both attention and FFN layers.
Location: Section 4.2
Exact Quote: We evaluate which layers have large inner product with top200 attention neurons, shown in Table 14. For every knowledge, the shallow and medium FFN layers play larger roles than attention layers.

Evidence:
- Evidence Text: We evaluate which layers have large inner product with top200 attention neurons, shown in Table 14. For every knowledge, the shallow and medium FFN layers play larger roles than attention layers.
  Strength: strong
  Location: Section 4.2
  Limitations: None
  Exact Quote: We evaluate which layers have large inner product with top200 attention neurons, shown in Table 14. For every knowledge, the shallow and medium FFN layers play larger roles than attention layers.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The methodology and experimental results demonstrate the effectiveness of the proposed method in identifying important query neurons.
Key Limitations: None

--------------------------------------------------

Claim 4:
Type: methodology
Statement: The proposed method can identify the important 'value neurons' in both attention and FFN layers.
Location: Section 4.2
Exact Quote: We evaluate which layers have large inner product with top200 attention neurons, shown in Table 14. For every knowledge, the shallow and medium FFN layers play larger roles than attention layers.

Evidence:
- Evidence Text: We evaluate which layers have large inner product with top200 attention neurons, shown in Table 14. For every knowledge, the shallow and medium FFN layers play larger roles than attention layers.
  Strength: strong
  Location: Section 4.2
  Limitations: None
  Exact Quote: We evaluate which layers have large inner product with top200 attention neurons, shown in Table 14. For every knowledge, the shallow and medium FFN layers play larger roles than attention layers.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The methodology and experimental results demonstrate the effectiveness of the proposed method in identifying important query neurons.
Key Limitations: None

--------------------------------------------------

Claim 5:
Type: methodology
Statement: The proposed method can identify the important 'value neurons' in both attention and FFN layers.
Location: Section 4.2
Exact Quote: We evaluate which layers have large inner product with top200 attention neurons, shown in Table 14. For every knowledge, the shallow and medium FFN layers play larger roles than attention layers.

Evidence:
- Evidence Text: We evaluate which layers have large inner product with top200 attention neurons, shown in Table 14. For every knowledge, the shallow and medium FFN layers play larger roles than attention layers.
  Strength: strong
  Location: Section 4.2
  Limitations: None
  Exact Quote: We evaluate which layers have large inner product with top200 attention neurons, shown in Table 14. For every knowledge, the shallow and medium FFN layers play larger roles than attention layers.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The methodology and experimental results demonstrate the effectiveness of the proposed method in identifying important query neurons.
Key Limitations: None

--------------------------------------------------

