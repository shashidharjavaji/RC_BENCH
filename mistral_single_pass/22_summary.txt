Claim 1:
Type: contribution
Statement: The TruthfulQA benchmark tests language models on generating truthful answers to questions in the zero-shot setting.
Location: 1.1 Contributions
Exact Quote: TruthfulQA tests language models on generating truthful answers to questions in the zero-shot setting.

Evidence:
- Evidence Text: The benchmark comprises 817 questions that span 38 categories.
  Strength: strong
  Location: 1.1 Contributions
  Limitations: None
  Exact Quote: The benchmark comprises 817 questions that span 38 categories.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the specific number of questions and categories in the benchmark.
Key Limitations: None

--------------------------------------------------

Claim 2:
Type: performance
Statement: The best model (GPT-3-175B with 'helpful' prompt) was truthful on 58% of questions, while human performance was 94%.
Location: 4.1 Truthfulness of models vs humans
Exact Quote: The best model (GPT-3-175B with 'helpful' prompt) was truthful on 58% of questions, while human performance was 94%.

Evidence:
- Evidence Text: The human participant produced 94% true answers.
  Strength: strong
  Location: 4.1 Truthfulness of models vs humans
  Limitations: None
  Exact Quote: The human participant produced 94% true answers.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the specific human performance data.
Key Limitations: None

--------------------------------------------------

Claim 3:
Type: result
Statement: Larger models are less truthful.
Location: 4.2 Larger models are less truthful
Exact Quote: Larger models are less truthful.

Evidence:
- Evidence Text: Figure 2 shows that larger models generally do worse than smaller models in the same family.
  Strength: strong
  Location: 4.2 Larger models are less truthful
  Limitations: None
  Exact Quote: Figure 2 shows that larger models generally do worse than smaller models in the same family.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the specific data in Figure 2.
Key Limitations: None

--------------------------------------------------

Claim 4:
Type: performance
Statement: The GPT-judge model is able to predict human evaluations of truthfulness with 90-96% validation accuracy.
Location: 4.4 Automated metrics vs human evaluation
Exact Quote: The GPT-judge model is able to predict human evaluations of truthfulness with 90-96% validation accuracy.

Evidence:
- Evidence Text: GPT-judge achieves 90% validation accuracy on UnifiedQA when finetuned only on answers from the GPT families.
  Strength: strong
  Location: 4.4 Automated metrics vs human evaluation
  Limitations: None
  Exact Quote: GPT-judge achieves 90% validation accuracy on UnifiedQA when finetuned only on answers from the GPT families.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the specific validation accuracy data.
Key Limitations: None

--------------------------------------------------

