Claim 1:
Type: contribution
Statement: The ability to generate natural language explanations of large language models (LLMs) would be an enormous step forward for explainability research.
Location: Introduction
Exact Quote: The ability to generate natural language explanations of large language models (LLMs) would be an enormous step forward for explainability research.

Evidence:
- Evidence Text: Such explanations could form the basis for safety assessments, bias detection, and model editing, in addition to yielding fundamental insights into how LLMs represent concepts.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: Such explanations could form the basis for safety assessments, bias detection, and model editing, in addition to yielding fundamental insights into how LLMs represent concepts.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the potential applications of natural language explanations.
Key Limitations: None

--------------------------------------------------

Claim 2:
Type: methodology
Statement: We develop two modes of evaluation for natural language explanations that claim individual neurons represent a concept in a text input.
Location: Introduction
Exact Quote: We develop two modes of evaluation for natural language explanations that claim individual neurons represent a concept in a text input.

Evidence:
- Evidence Text: In the observational mode, we evaluate claims that a neuron a activates on all and only input strings that refer to a concept picked out by the proposed explanation E.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: In the observational mode, we evaluate claims that a neuron a activates on all and only input strings that refer to a concept picked out by the proposed explanation E.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the description of the observational mode.
Key Limitations: None

--------------------------------------------------

Claim 3:
Type: methodology
Statement: We consider two modes of evaluation (Figure 1). In the observational mode, we evaluate the claim that a neuron a activates on all and only input strings that refer to a concept picked out by the proposed explanation E.
Location: Introduction
Exact Quote: We consider two modes of evaluation (Figure 1). In the observational mode, we evaluate the claim that a neuron a activates on all and only input strings that refer to a concept picked out by the proposed explanation E.

Evidence:
- Evidence Text: Relative to a set of inputs, we can then use the error rates to assess the quality of E for a.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: Relative to a set of inputs, we can then use the error rates to assess the quality of E for a.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the description of the observational mode.
Key Limitations: None

--------------------------------------------------

Claim 4:
Type: methodology
Statement: We propose an intervention mode to evaluate the claim that a is a causally active representation of the concept denoted by E.
Location: Introduction
Exact Quote: We propose an intervention mode to evaluate the claim that a is a causally active representation of the concept denoted by E.

Evidence:
- Evidence Text: We construct next token prediction tasks that hinge on the concept and intervene on the neuron a to study whether the neuron is a causal mediator of concepts picked out by E.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: We construct next token prediction tasks that hinge on the concept and intervene on the neuron a to study whether the neuron is a causal mediator of concepts picked out by E.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the description of the intervention mode.
Key Limitations: None

--------------------------------------------------

Claim 5:
Type: result
Statement: We apply our framework to the GPT-4-generated explanations of GPT-2 XL neurons of Bills et al. (2023) and show that even the most confident explanations have high error rates and little to no causal efficacy.
Location: Introduction
Exact Quote: We apply our framework to the GPT-4-generated explanations of GPT-2 XL neurons of Bills et al. (2023) and show that even the most confident explanations have high error rates and little to no causal efficacy.

Evidence:
- Evidence Text: In the observational mode, we find that even among the top 0.6% of neurons which are considered well-explained by GPT-4’s own assessment, the explanation is far from faithful; construed as predictions about neuron activations, GPT-4 generated explanations achieve a precision of 0.64 and a recall of 0.50.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: In the observational mode, we find that even among the top 0.6% of neurons which are considered well-explained by GPT-4’s own assessment, the explanation is far from faithful; construed as predictions about neuron activations, GPT-4 generated explanations achieve a precision of 0.64 and a recall of 0.50.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the experimental results.
Key Limitations: None

--------------------------------------------------

Claim 6:
Type: contribution
Statement: We close the paper by critically assessing whether natural language is a good choice for explanations and whether neurons are the best level of analysis.
Location: Introduction
Exact Quote: We close the paper by critically assessing whether natural language is a good choice for explanations and whether neurons are the best level of analysis.

Evidence:
- Evidence Text: Is natural language the best medium for explaining large language models? The benefits of using natural language in this context are that it is intuitive and expressive; one needn’t learn a specialized formal language or data visualization language in order to consume explanations in this format and draw inferences from them to inform subsequent work.
  Strength: strong
  Location: Introduction
  Limitations: None
  Exact Quote: Is natural language the best medium for explaining large language models? The benefits of using natural language in this context are that it is intuitive and expressive; one needn’t learn a specialized formal language or data visualization language in order to consume explanations in this format and draw inferences from them to inform subsequent work.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the discussion on the benefits and limitations of natural language explanations.
Key Limitations: None

--------------------------------------------------

