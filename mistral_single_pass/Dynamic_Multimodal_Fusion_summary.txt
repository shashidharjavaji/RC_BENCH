Claim 1:
Type: contribution
Statement: Deep multimodal learning has achieved great progress in recent years.
Location: Abstract
Exact Quote: Deep multimodal learning has achieved great progress in recent years.

Evidence:
- Evidence Text: The paper discusses the progress of deep learning in multimodal tasks.
  Strength: strong
  Location: Abstract
  Limitations: The claim is general and does not provide specific examples.
  Exact Quote: Deep multimodal learning has achieved great progress in recent years.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the general discussion of progress in deep learning.
Key Limitations: The claim is general and lacks specific examples.

--------------------------------------------------

Claim 2:
Type: methodology
Statement: Current fusion approaches are static in nature, i.e., they process and fuse multimodal inputs with identical computation, without accounting for diverse computational demands of different multimodal data.
Location: Abstract
Exact Quote: Current fusion approaches are static in nature, i.e., they process and fuse multimodal inputs with identical computation, without accounting for diverse computational demands of different multimodal data.

Evidence:
- Evidence Text: The paper discusses the limitations of static fusion approaches.
  Strength: strong
  Location: Abstract
  Limitations: The claim is general and does not provide specific examples.
  Exact Quote: Current fusion approaches are static in nature, i.e., they process and fuse multimodal inputs with identical computation, without accounting for diverse computational demands of different multimodal data.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the discussion of the limitations of static fusion approaches.
Key Limitations: The claim is general and lacks specific examples.

--------------------------------------------------

Claim 3:
Type: contribution
Statement: In this work, we propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses multimodal data and generates data-dependent forward paths during inference.
Location: Abstract
Exact Quote: In this work, we propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses multimodal data and generates data-dependent forward paths during inference.

Evidence:
- Evidence Text: The paper introduces the DynMM approach.
  Strength: strong
  Location: Abstract
  Limitations: The claim is general and does not provide specific examples.
  Exact Quote: In this work, we propose dynamic multimodal fusion (DynMM), a new approach that adaptively fuses multimodal data and generates data-dependent forward paths during inference.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the introduction of the DynMM approach.
Key Limitations: The claim is general and lacks specific examples.

--------------------------------------------------

Claim 4:
Type: methodology
Statement: To this end, we propose a gating function to provide modality-level or fusion-level decisions on-the-fly based on multimodal features and a resource-aware loss function that encourages computational efficiency.
Location: Abstract
Exact Quote: To this end, we propose a gating function to provide modality-level or fusion-level decisions on-the-fly based on multimodal features and a resource-aware loss function that encourages computational efficiency.

Evidence:
- Evidence Text: The paper discusses the gating function and resource-aware loss function.
  Strength: strong
  Location: Abstract
  Limitations: The claim is general and does not provide specific examples.
  Exact Quote: To this end, we propose a gating function to provide modality-level or fusion-level decisions on-the-fly based on multimodal features and a resource-aware loss function that encourages computational efficiency.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the discussion of the gating function and resource-aware loss function.
Key Limitations: The claim is general and lacks specific examples.

--------------------------------------------------

Claim 5:
Type: performance
Statement: Results on various multimodal tasks demonstrate the efficiency and wide applicability of our approach.
Location: Abstract
Exact Quote: Results on various multimodal tasks demonstrate the efficiency and wide applicability of our approach.

Evidence:
- Evidence Text: The paper provides experimental results on various multimodal tasks.
  Strength: strong
  Location: Abstract
  Limitations: The claim is general and does not provide specific examples.
  Exact Quote: Results on various multimodal tasks demonstrate the efficiency and wide applicability of our approach.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the experimental results on various multimodal tasks.
Key Limitations: The claim is general and lacks specific examples.

--------------------------------------------------

Claim 6:
Type: performance
Statement: For instance, DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.
Location: Abstract
Exact Quote: For instance, DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.

Evidence:
- Evidence Text: The paper provides specific experimental results on CMU-MOSEI and NYU Depth V2.
  Strength: strong
  Location: Abstract
  Limitations: The claim is specific and provides concrete examples.
  Exact Quote: For instance, DynMM can reduce the computation costs by 46.5% with only a negligible accuracy loss (CMU-MOSEI sentiment analysis) and improve segmentation performance with over 21% savings in computation (NYU Depth V2 semantic segmentation) when compared with static fusion approaches.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the specific experimental results on CMU-MOSEI and NYU Depth V2.
Key Limitations: The claim is specific and provides concrete examples.

--------------------------------------------------

Claim 7:
Type: contribution
Statement: We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.
Location: Abstract
Exact Quote: We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.

Evidence:
- Evidence Text: The paper discusses the potential applications of the DynMM approach.
  Strength: strong
  Location: Abstract
  Limitations: The claim is general and does not provide specific examples.
  Exact Quote: We believe our approach opens a new direction towards dynamic multimodal network design, with applications to a wide range of multimodal tasks.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is supported by the discussion of the potential applications of the DynMM approach.
Key Limitations: The claim is general and lacks specific examples.

--------------------------------------------------

