```json
{
    "analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "Retrieval-augmented language models (LMs) use non-parametric memory to substantially outperform their non-retrieval counterparts on perplexity-based evaluations, but it is an open question whether they achieve similar gains in few- and zero-shot end-task accuracy.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "Retrieval-augmented language models (LMs) use non-parametric memory to substantially outperform their non-retrieval counterparts on perplexity-based evaluations, but it is an open question whether they achieve similar gains in few- and zero-shot end-task accuracy."
            },
            "evidence": [
                {
                    "evidence_text": "Previous work has shown that these models substantially outperform their non–retrieval-based counterparts on language modeling tasks (Khandelwal et al., 2020; He et al., 2021; Borgeaud et al., 2021), but it is an open question whether they also achieve similar gains in few-shot and zero-shot end task evaluations (Radford et al., 2019; Brown et al., 2020a).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "Previous work has shown that these models substantially outperform their non–retrieval-based counterparts on language modeling tasks (Khandelwal et al., 2020; He et al., 2021; Borgeaud et al., 2021), but it is an open question whether they also achieve similar gains in few-shot and zero-shot end task evaluations (Radford et al., 2019; Brown et al., 2020a)."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by previous research showing that retrieval-augmented LMs outperform non-retrieval counterparts on language modeling tasks, but the question remains about their performance on few- and zero-shot end tasks.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "We demonstrate that, with some extensions to improve coverage of the verbalizer tokens, the performance gains of retrieval-augmented LMs generalize well to a wide range of downstream tasks.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "We demonstrate that, with some extensions to improve coverage of the verbalizer tokens, the performance gains of retrieval-augmented LMs generalize well to a wide range of downstream tasks."
            },
            "evidence": [
                {
                    "evidence_text": "The main challenge is to achieve coverage of the verbalizer tokens that define the different end-task class labels. To address this challenge, we also introduce kNN-Prompt, a simple and effective kNN-LM with automatically expanded fuzzy verbalizers (e.g. to expand “terrible” to also include “silly” and other task-specific synonyms for sentiment classification). Across nine diverse end-tasks, using kNN-Prompt with GPT2 large yields significant performance boosts over strong zero-shot baselines (13.4% absolute improvement over the base LM on average).",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "The main challenge is to achieve coverage of the verbalizer tokens that define the different end-task class labels. To address this challenge, we also introduce kNN-Prompt, a simple and effective kNN-LM with automatically expanded fuzzy verbalizers (e.g. to expand “terrible” to also include “silly” and other task-specific synonyms for sentiment classification). Across nine diverse end-tasks, using kNN-Prompt with GPT2 large yields significant performance boosts over strong zero-shot baselines (13.4% absolute improvement over the base LM on average)."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the introduction of kNN-Prompt and its performance improvements across nine diverse end-tasks.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "kNN-Prompt is effective for domain adaptation with no further training, and gains increase with the size of the retrieval model.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt is effective for domain adaptation with no further training, and gains increase with the size of the retrieval model."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "kNN-Prompt is effective for domain adaptation with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt is effective for domain adaptation with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 7,
            "claim": {
                "text": "kNN-Prompt is effective for domain adaptation with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt is effective for domain adaptation with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 8,
            "claim": {
                "text": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 9,
            "claim": {
                "text": "kNN-Prompt is effective for domain adaptation with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt is effective for domain adaptation with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 10,
            "claim": {
                "text": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 11,
            "claim": {
                "text": "kNN-Prompt is effective for domain adaptation with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt is effective for domain adaptation with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 12,
            "claim": {
                "text": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 13,
            "claim": {
                "text": "kNN-Prompt is effective for domain adaptation with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt is effective for domain adaptation with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 14,
            "claim": {
                "text": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 15,
            "claim": {
                "text": "kNN-Prompt is effective for domain adaptation with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt is effective for domain adaptation with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 16,
            "claim": {
                "text": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 17,
            "claim": {
                "text": "kNN-Prompt is effective for domain adaptation with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt is effective for domain adaptation with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 18,
            "claim": {
                "text": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 19,
            "claim": {
                "text": "kNN-Prompt is effective for domain adaptation with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt is effective for domain adaptation with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 20,
            "claim": {
                "text": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 21,
            "claim": {
                "text": "kNN-Prompt is effective for domain adaptation with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt is effective for domain adaptation with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "justification": "The claim is supported by the demonstration that kNN-Prompt can adapt LMs to new domains and tasks with no further training.",
                "key_limitations": "None",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 22,
            "claim": {
                "text": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training.",
                "type": "contribution",
                "location": "Abstract",
                "exact_quote": "kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training."
            },
            "evidence": [
                {
                    "evidence_text": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus.",
                    "strength": "strong",
                    "limitations": "None",
                    "location": "Abstract",
                    "exact_quote": "We also show that kNN-Prompt can be used to adapt LMs to new domains and tasks with no further training (Section 5). With a domain-specific datastore corpus, we achieve comparable or better performance to prompting the LM after domain-adaptive pretraining (Gururangan et al., 2020) on that corpus."
                }
            ],
            "evaluation": {
                "conclusion_justified": true,
                "robustness": "high",
                "just