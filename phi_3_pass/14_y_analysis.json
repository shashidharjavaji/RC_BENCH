{
    "paper_analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.",
                "location": "Abstract",
                "type": "Problem Statement",
                "exact_quote": "Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image."
            },
            "evidence": [
                {
                    "raw": " ```json\n{\n  \"evidence_sets\": [\n    {\n      \"claim_id\": 1,\n      \"evidence\": [\n        {\n          \"evidence_id\": 1,\n          \"evidence_text\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Abstract\",\n          \"exact_quote\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\"\n        },\n        {\n          \"evidence_id\": 2,\n          \"evidence_text\": \"VLMs, similarly to large language models (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Introduction\",\n          \"exact_quote\": \"However, VLMs, similarly to large language mod\nels (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\"\n        },\n        {\n          \"evidence_id\": 3,\n          \"evidence_text\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 3. Analysis of hallucinations in VLMs\",\n          \"exact_quote\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\"\n        },\n        {\n          \"evidence_id\": 4,\n          \"evidence_text\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 5,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        },\n        {\n          \"evidence_id\": 6,\n          \"evidence_text\": \"M3ID and M3ID+DPO maintain the fluency and linguistic capabilities of pre-trained VLMs.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 7,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 8,\n          \"evidence_text\": \"M3ID and M3ID+DPO can be paired with Direct Preference Optimization (DPO) to improve the model\u2019s reliance on the prompt image without requiring any labels.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\"\n        },\n        {\n          \"evidence_id\": 9,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28% respectively and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        }\n      ]\n    }\n  ]\n}\n```"
                }
            ],
            "conclusion": {
                "claim_id": 1,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None specified in the abstract",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "The reliance on the language prior decreases as more tokens are generated, leading to hallucinations.",
                "location": "Abstract",
                "type": "Causal Claim",
                "exact_quote": "the reliance on the visual prompt decreases, and this behavior strongly correlates with the emergence of hallucinations."
            },
            "evidence": [
                {
                    "raw": " ```json\n{\n  \"evidence_sets\": [\n    {\n      \"claim_id\": 1,\n      \"evidence\": [\n        {\n          \"evidence_id\": 1,\n          \"evidence_text\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Abstract\",\n          \"exact_quote\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\"\n        },\n        {\n          \"evidence_id\": 2,\n          \"evidence_text\": \"VLMs, similarly to large language models (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Introduction\",\n          \"exact_quote\": \"However, VLMs, similarly to large language mod\nels (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\"\n        },\n        {\n          \"evidence_id\": 3,\n          \"evidence_text\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 3. Analysis of hallucinations in VLMs\",\n          \"exact_quote\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\"\n        },\n        {\n          \"evidence_id\": 4,\n          \"evidence_text\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 5,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        },\n        {\n          \"evidence_id\": 6,\n          \"evidence_text\": \"M3ID and M3ID+DPO maintain the fluency and linguistic capabilities of pre-trained VLMs.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 7,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 8,\n          \"evidence_text\": \"M3ID and M3ID+DPO can be paired with Direct Preference Optimization (DPO) to improve the model\u2019s reliance on the prompt image without requiring any labels.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\"\n        },\n        {\n          \"evidence_id\": 9,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28% respectively and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        }\n      ]\n    }\n  ]\n}\n```"
                }
            ],
            "conclusion": {
                "claim_id": 2,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None specified in the abstract",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "Multi-Modal Mutual-Information Decoding (M3ID) is a new sampling method for prompt amplification that reduces hallucinations.",
                "location": "Abstract",
                "type": "Methodological Claim",
                "exact_quote": "To reduce hallucinations, we introduce Multi-Modal Mutual-Information Decoding (M3ID), a new sampling method for prompt amplification."
            },
            "evidence": [
                {
                    "raw": " ```json\n{\n  \"evidence_sets\": [\n    {\n      \"claim_id\": 1,\n      \"evidence\": [\n        {\n          \"evidence_id\": 1,\n          \"evidence_text\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Abstract\",\n          \"exact_quote\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\"\n        },\n        {\n          \"evidence_id\": 2,\n          \"evidence_text\": \"VLMs, similarly to large language models (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Introduction\",\n          \"exact_quote\": \"However, VLMs, similarly to large language mod\nels (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\"\n        },\n        {\n          \"evidence_id\": 3,\n          \"evidence_text\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 3. Analysis of hallucinations in VLMs\",\n          \"exact_quote\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\"\n        },\n        {\n          \"evidence_id\": 4,\n          \"evidence_text\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 5,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        },\n        {\n          \"evidence_id\": 6,\n          \"evidence_text\": \"M3ID and M3ID+DPO maintain the fluency and linguistic capabilities of pre-trained VLMs.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 7,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 8,\n          \"evidence_text\": \"M3ID and M3ID+DPO can be paired with Direct Preference Optimization (DPO) to improve the model\u2019s reliance on the prompt image without requiring any labels.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\"\n        },\n        {\n          \"evidence_id\": 9,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28% respectively and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        }\n      ]\n    }\n  ]\n}\n```"
                }
            ],
            "conclusion": {
                "claim_id": 3,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None specified in the abstract",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "M3ID can be applied to any pre-trained autoregressive VLM at inference time without further training.",
                "location": "Abstract",
                "type": "Methodological Claim",
                "exact_quote": "M3ID can be applied to any pre-trained autoregressive VLM at inference time without necessitating further training."
            },
            "evidence": [
                {
                    "raw": " ```json\n{\n  \"evidence_sets\": [\n    {\n      \"claim_id\": 1,\n      \"evidence\": [\n        {\n          \"evidence_id\": 1,\n          \"evidence_text\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Abstract\",\n          \"exact_quote\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\"\n        },\n        {\n          \"evidence_id\": 2,\n          \"evidence_text\": \"VLMs, similarly to large language models (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Introduction\",\n          \"exact_quote\": \"However, VLMs, similarly to large language mod\nels (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\"\n        },\n        {\n          \"evidence_id\": 3,\n          \"evidence_text\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 3. Analysis of hallucinations in VLMs\",\n          \"exact_quote\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\"\n        },\n        {\n          \"evidence_id\": 4,\n          \"evidence_text\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 5,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        },\n        {\n          \"evidence_id\": 6,\n          \"evidence_text\": \"M3ID and M3ID+DPO maintain the fluency and linguistic capabilities of pre-trained VLMs.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 7,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 8,\n          \"evidence_text\": \"M3ID and M3ID+DPO can be paired with Direct Preference Optimization (DPO) to improve the model\u2019s reliance on the prompt image without requiring any labels.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\"\n        },\n        {\n          \"evidence_id\": 9,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28% respectively and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        }\n      ]\n    }\n  ]\n}\n```"
                }
            ],
            "conclusion": {
                "claim_id": 4,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None specified in the abstract",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively.",
                "location": "Results",
                "type": "Empirical Claim",
                "exact_quote": "M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively."
            },
            "evidence": [
                {
                    "raw": " ```json\n{\n  \"evidence_sets\": [\n    {\n      \"claim_id\": 1,\n      \"evidence\": [\n        {\n          \"evidence_id\": 1,\n          \"evidence_text\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Abstract\",\n          \"exact_quote\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\"\n        },\n        {\n          \"evidence_id\": 2,\n          \"evidence_text\": \"VLMs, similarly to large language models (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Introduction\",\n          \"exact_quote\": \"However, VLMs, similarly to large language mod\nels (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\"\n        },\n        {\n          \"evidence_id\": 3,\n          \"evidence_text\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 3. Analysis of hallucinations in VLMs\",\n          \"exact_quote\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\"\n        },\n        {\n          \"evidence_id\": 4,\n          \"evidence_text\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 5,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        },\n        {\n          \"evidence_id\": 6,\n          \"evidence_text\": \"M3ID and M3ID+DPO maintain the fluency and linguistic capabilities of pre-trained VLMs.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 7,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 8,\n          \"evidence_text\": \"M3ID and M3ID+DPO can be paired with Direct Preference Optimization (DPO) to improve the model\u2019s reliance on the prompt image without requiring any labels.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\"\n        },\n        {\n          \"evidence_id\": 9,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28% respectively and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        }\n      ]\n    }\n  ]\n}\n```"
                }
            ],
            "conclusion": {
                "claim_id": 5,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None specified in the abstract",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "M3ID and M3ID+DPO improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.",
                "location": "Results",
                "type": "Empirical Claim",
                "exact_quote": "M3ID and M3ID+DPO improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively."
            },
            "evidence": [
                {
                    "raw": " ```json\n{\n  \"evidence_sets\": [\n    {\n      \"claim_id\": 1,\n      \"evidence\": [\n        {\n          \"evidence_id\": 1,\n          \"evidence_text\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Abstract\",\n          \"exact_quote\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\"\n        },\n        {\n          \"evidence_id\": 2,\n          \"evidence_text\": \"VLMs, similarly to large language models (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Introduction\",\n          \"exact_quote\": \"However, VLMs, similarly to large language mod\nels (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\"\n        },\n        {\n          \"evidence_id\": 3,\n          \"evidence_text\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 3. Analysis of hallucinations in VLMs\",\n          \"exact_quote\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\"\n        },\n        {\n          \"evidence_id\": 4,\n          \"evidence_text\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 5,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        },\n        {\n          \"evidence_id\": 6,\n          \"evidence_text\": \"M3ID and M3ID+DPO maintain the fluency and linguistic capabilities of pre-trained VLMs.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 7,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 8,\n          \"evidence_text\": \"M3ID and M3ID+DPO can be paired with Direct Preference Optimization (DPO) to improve the model\u2019s reliance on the prompt image without requiring any labels.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\"\n        },\n        {\n          \"evidence_id\": 9,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28% respectively and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        }\n      ]\n    }\n  ]\n}\n```"
                }
            ],
            "conclusion": {
                "claim_id": 6,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None specified in the abstract",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 7,
            "claim": {
                "text": "M3ID and M3ID+DPO maintain the fluency and linguistic capabilities of pre-trained VLMs.",
                "location": "Results",
                "type": "Methodological Claim",
                "exact_quote": "Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs."
            },
            "evidence": [
                {
                    "raw": " ```json\n{\n  \"evidence_sets\": [\n    {\n      \"claim_id\": 1,\n      \"evidence\": [\n        {\n          \"evidence_id\": 1,\n          \"evidence_text\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Abstract\",\n          \"exact_quote\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\"\n        },\n        {\n          \"evidence_id\": 2,\n          \"evidence_text\": \"VLMs, similarly to large language models (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Introduction\",\n          \"exact_quote\": \"However, VLMs, similarly to large language mod\nels (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\"\n        },\n        {\n          \"evidence_id\": 3,\n          \"evidence_text\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 3. Analysis of hallucinations in VLMs\",\n          \"exact_quote\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\"\n        },\n        {\n          \"evidence_id\": 4,\n          \"evidence_text\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 5,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        },\n        {\n          \"evidence_id\": 6,\n          \"evidence_text\": \"M3ID and M3ID+DPO maintain the fluency and linguistic capabilities of pre-trained VLMs.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 7,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 8,\n          \"evidence_text\": \"M3ID and M3ID+DPO can be paired with Direct Preference Optimization (DPO) to improve the model\u2019s reliance on the prompt image without requiring any labels.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\"\n        },\n        {\n          \"evidence_id\": 9,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28% respectively and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        }\n      ]\n    }\n  ]\n}\n```"
                }
            ],
            "conclusion": {
                "claim_id": 7,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None specified in the abstract",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 8,
            "claim": {
                "text": "M3ID and M3ID+DPO reduce hallucinations by mitigating visually ungrounded answers.",
                "location": "Conclusion",
                "type": "Conclusion",
                "exact_quote": "Our algorithms reduce hallucinations by mitigating visually ungrounded answers."
            },
            "evidence": [
                {
                    "raw": " ```json\n{\n  \"evidence_sets\": [\n    {\n      \"claim_id\": 1,\n      \"evidence\": [\n        {\n          \"evidence_id\": 1,\n          \"evidence_text\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Abstract\",\n          \"exact_quote\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\"\n        },\n        {\n          \"evidence_id\": 2,\n          \"evidence_text\": \"VLMs, similarly to large language models (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Introduction\",\n          \"exact_quote\": \"However, VLMs, similarly to large language mod\nels (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\"\n        },\n        {\n          \"evidence_id\": 3,\n          \"evidence_text\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 3. Analysis of hallucinations in VLMs\",\n          \"exact_quote\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\"\n        },\n        {\n          \"evidence_id\": 4,\n          \"evidence_text\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 5,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        },\n        {\n          \"evidence_id\": 6,\n          \"evidence_text\": \"M3ID and M3ID+DPO maintain the fluency and linguistic capabilities of pre-trained VLMs.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 7,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 8,\n          \"evidence_text\": \"M3ID and M3ID+DPO can be paired with Direct Preference Optimization (DPO) to improve the model\u2019s reliance on the prompt image without requiring any labels.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\"\n        },\n        {\n          \"evidence_id\": 9,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28% respectively and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        }\n      ]\n    }\n  ]\n}\n```"
                }
            ],
            "conclusion": {
                "claim_id": 8,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None specified in the abstract",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 9,
            "claim": {
                "text": "M3ID and M3ID+DPO can be paired with Direct Preference Optimization (DPO) to improve the model\u2019s reliance on the prompt image without requiring any labels.",
                "location": "Conclusion",
                "type": "Methodological Claim",
                "exact_quote": "If training is an option, we show that M3ID can be paired with Direct Preference Optimization (DPO) to improve the model\u2019s reliance on the prompt image without requiring any labels."
            },
            "evidence": [
                {
                    "raw": " ```json\n{\n  \"evidence_sets\": [\n    {\n      \"claim_id\": 1,\n      \"evidence\": [\n        {\n          \"evidence_id\": 1,\n          \"evidence_text\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Abstract\",\n          \"exact_quote\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\"\n        },\n        {\n          \"evidence_id\": 2,\n          \"evidence_text\": \"VLMs, similarly to large language models (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Introduction\",\n          \"exact_quote\": \"However, VLMs, similarly to large language mod\nels (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\"\n        },\n        {\n          \"evidence_id\": 3,\n          \"evidence_text\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 3. Analysis of hallucinations in VLMs\",\n          \"exact_quote\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\"\n        },\n        {\n          \"evidence_id\": 4,\n          \"evidence_text\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 5,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        },\n        {\n          \"evidence_id\": 6,\n          \"evidence_text\": \"M3ID and M3ID+DPO maintain the fluency and linguistic capabilities of pre-trained VLMs.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 7,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 8,\n          \"evidence_text\": \"M3ID and M3ID+DPO can be paired with Direct Preference Optimization (DPO) to improve the model\u2019s reliance on the prompt image without requiring any labels.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\"\n        },\n        {\n          \"evidence_id\": 9,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28% respectively and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        }\n      ]\n    }\n  ]\n}\n```"
                }
            ],
            "conclusion": {
                "claim_id": 9,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None specified in the abstract",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 10,
            "claim": {
                "text": "M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28% respectively and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.",
                "location": "Conclusion",
                "type": "Summary Claim",
                "exact_quote": "M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28% respectively and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively."
            },
            "evidence": [
                {
                    "raw": " ```json\n{\n  \"evidence_sets\": [\n    {\n      \"claim_id\": 1,\n      \"evidence\": [\n        {\n          \"evidence_id\": 1,\n          \"evidence_text\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Abstract\",\n          \"exact_quote\": \"Generative Vision-Language Models (VLMs) are prone to generate plausible-sounding textual answers that, however, are not always grounded in the input image.\"\n        },\n        {\n          \"evidence_id\": 2,\n          \"evidence_text\": \"VLMs, similarly to large language models (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Introduction\",\n          \"exact_quote\": \"However, VLMs, similarly to large language mod\nels (LLMs), are prone to 'hallucinations' \u2013 generating plausible-sounding answers without factual basis, leading to potentially ungrounded or fabricated information.\"\n        },\n        {\n          \"evidence_id\": 3,\n          \"evidence_text\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 3. Analysis of hallucinations in VLMs\",\n          \"exact_quote\": \"Our first contribution is to empirically demonstrate that PDM decreases as more tokens are generated making the generations more likely to be hallucinated.\"\n        },\n        {\n          \"evidence_id\": 4,\n          \"evidence_text\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 5,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        },\n        {\n          \"evidence_id\": 6,\n          \"evidence_text\": \"M3ID and M3ID+DPO maintain the fluency and linguistic capabilities of pre-trained VLMs.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 7,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce hallucinations by mitigating visually ungrounded answers.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"Our empirical findings show that our algorithms maintain the fluency and linguistic capabilities of pre-trained VLMs while reducing hallucinations by mitigating visually ungrounded answers.\"\n        },\n        {\n          \"evidence_id\": 8,\n          \"evidence_text\": \"M3ID and M3ID+DPO can be paired with Direct Preference Optimization (DPO) to improve the model\u2019s reliance on the prompt image without requiring any labels.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\",\n          \"location\": \"Section 4. Methods\",\n          \"exact_quote\": \"M3ID can be applied to any off-the-shelf model without additional training or access to model weights, offering a low computational overhead alternative to standard decoding algorithms [13, 24]. Our results show that M3ID enhances the dependence on the visual prompt and reduces the number of hallucinations across various benchmarks while preserving the linguistic fluency of the original model.\"\n        },\n        {\n          \"evidence_id\": 9,\n          \"evidence_text\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28% respectively and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\",\n          \"strength\": \"strong\",\n          \"limitations\": \"None provided in the text\",\n          \"location\": \"Section 5. Experiments\",\n          \"exact_quote\": \"M3ID and M3ID+DPO reduce the percentage of hallucinated objects in captioning tasks by 25% and 28%, respectively, and improve the accuracy on VQA benchmarks such as POPE by 21% and 24% respectively.\"\n        }\n      ]\n    }\n  ]\n}\n```"
                }
            ],
            "conclusion": {
                "claim_id": 10,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "None specified in the abstract",
                "confidence_level": "high"
            }
        }
    ],
    "execution_times": {
        "claims_analysis_time": "106.29 seconds",
        "evidence_analysis_time": "135.82 seconds",
        "conclusions_analysis_time": "55.62 seconds",
        "total_execution_time": "301.13 seconds"
    }
}