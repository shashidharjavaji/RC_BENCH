{
    "paper_analysis": [
        {
            "claim_id": 1,
            "claim": {
                "text": "Fine-tuning LLMs on a corpus of planning instances does not lead to robust planning skills, as indicated by poor performance on out-of-distribution test sets.",
                "location": "Abstract",
                "type": "Nature of the claim",
                "exact_quote": "Fine-tuning LLMs on a corpus of planning instances does not lead to robust planning skills, as indicated by poor performance on out-of-distribution test sets."
            },
            "evidence": [
                {
                    "evidence_id": 1,
                    "evidence_text": "The fine-tuned LLM struggles to generalize to OOD cases, unable to generate either valid or executable plans.",
                    "strength": "strong",
                    "limitations": "The study does not explore the impact of different fine-tuning datasets or the potential for fine-tuning on a more diverse set of planning problems.",
                    "location": "Section 4.2",
                    "exact_quote": "The fine-tuned model utterly failed to perform in the 'unseen' and 'obfuscated' test sets, unable to generate either valid or executable plans."
                },
                {
                    "evidence_id": 2,
                    "evidence_text": "The fine-tuned model achieves high performance across all domains in in-distribution tests, but struggles with longer and more complex planning scenarios.",
                    "strength": "strong",
                    "limitations": "The study does not explore the impact of different fine-tuning datasets or the potential for fine-tuning on a more diverse set of planning problems.",
                    "location": "Section 4.2",
                    "exact_quote": "Previous studies have asserted the effectiveness of finetuning LLMs for plan generation (Rossetti et al. 2024; Shah et al. 2024). We revisit this claim, examining whether the statements hold true in our extended PlanBench dataset. Our results refute the claim that fine-tuning alone enables LLMs to master complex planning."
                }
            ],
            "conclusion": {
                "claim_id": 1,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 2,
            "claim": {
                "text": "Various strategies, including chain_of_thought, do enhance the probability of a plan being executable.",
                "location": "Abstract",
                "type": "Nature of the claim",
                "exact_quote": "various strategies, including chain_of_thought, do enhance the probability of a plan being exe-cutable."
            },
            "evidence": [
                {
                    "evidence_id": 3,
                    "evidence_text": "Permutation augmentation largely enhances the executability rate, even if it does not significantly improve the validity rate.",
                    "strength": "moderate",
                    "limitations": "The study does not explore the impact of different data augmentation techniques or the potential for combining multiple data augmentation strategies.",
                    "location": "Section 4.2",
                    "exact_quote": "Permutation augmentation does not significantly improve the validity rate, but largely enhances the executability rate."
                },
                {
                    "evidence_id": 4,
                    "evidence_text": "Chain of Thought (CoT) prompts the agent to repeat the goal and count the remaining steps to the goal, which enhances plan executability.",
                    "strength": "moderate",
                    "limitations": "The study does not explore the impact of different CoT prompting techniques or the potential for combining multiple CoT prompting strategies.",
                    "location": "Section 4.3",
                    "exact_quote": "The model employing CoT (Goal + State) demonstrated the highest performance gain when provided with the hints."
                },
                {
                    "evidence_id": 5,
                    "evidence_text": "Reinforcement learning (RL) with the proposed 'LCCS' reward improves plan validity and executability.",
                    "strength": "strong",
                    "limitations": "The study does not explore the impact of different RL algorithms or the potential for combining multiple RL algorithms.",
                    "location": "Section 4.6",
                    "exact_quote": "RL notably improves the performance under our end-to-end planning paradigm, especially on longer problems."
                }
            ],
            "conclusion": {
                "claim_id": 2,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 3,
            "claim": {
                "text": "Reinforcement learning with the novel 'Longest Contiguous Common Subsequence' reward is the most effective strategy.",
                "location": "Abstract",
                "type": "Nature of the claim",
                "exact_quote": "reinforcement learning with our novel \u2018Longest Contiguous Common Subsequence\u2019 reward emerged as the most effective."
            },
            "evidence": [
                {
                    "evidence_id": 6,
                    "evidence_text": "Reinforcement learning (RL) with the proposed 'LCCS' reward is the most effective strategy.",
                    "strength": "strong",
                    "limitations": "The study does not explore the impact of different RL algorithms or the potential for combining multiple RL algorithms.",
                    "location": "Section 4.6",
                    "exact_quote": "RL notably improves the performance under our end-to-end planning paradigm, especially on longer problems."
                },
                {
                    "evidence_id": 7,
                    "evidence_text": "RL with the proposed 'LCCS' reward improves plan validity by 7% and executability by 9% in longer planning problems.",
                    "strength": "strong",
                    "limitations": "The study does not explore the impact of different RL algorithms or the potential for combining multiple RL algorithms.",
                    "location": "Section 4.6",
                    "exact_quote": "RL with the proposed 'LCCS' reward improves plan validity by 7% and executability by 9% in longer planning problems."
                }
            ],
            "conclusion": {
                "claim_id": 3,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 4,
            "claim": {
                "text": "Fine-tuning LLMs on datasets containing problem contexts and reference plans do not acquire robust planning skills.",
                "location": "Introduction",
                "type": "Nature of the claim",
                "exact_quote": "challenge the claim that fine-tuning LLMs simply on datasets containing problem contexts and reference plans acquire robust planning skills"
            },
            "evidence": [
                {
                    "evidence_id": 8,
                    "evidence_text": "Fine-tuning LLMs on datasets containing problem contexts and reference plans do not acquire robust planning skills.",
                    "strength": "strong",
                    "limitations": "The study does not explore the impact of different fine-tuning datasets or the potential for fine-tuning on a more diverse set of planning problems.",
                    "location": "Section 4.2",
                    "exact_quote": "The fine-tuned model struggles to generalize to OOD cases, unable to generate either valid or executable plans."
                }
            ],
            "conclusion": {
                "claim_id": 4,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 5,
            "claim": {
                "text": "Strategies like CoT lead to incremental improvements in plan quality by enhancing plan executability.",
                "location": "Introduction",
                "type": "Nature of the claim",
                "exact_quote": "strategies like CoT lead to incremental improvements in plan quality by enhancing plan executability"
            },
            "evidence": [
                {
                    "evidence_id": 9,
                    "evidence_text": "Chain of Thought (CoT) prompts the agent to repeat the goal and count the remaining steps to the goal, which enhances plan executability.",
                    "strength": "moderate",
                    "limitations": "The study does not explore the impact of different CoT prompting techniques or the potential for combining multiple CoT prompting strategies.",
                    "location": "Section 4.3",
                    "exact_quote": "The model employing CoT (Goal + State) demonstrated the highest performance gain when provided with the hints."
                },
                {
                    "evidence_id": 10,
                    "evidence_text": "Reinforcement learning (RL) with the proposed 'LCCS' reward improves plan validity by 7% and executability by 9% in longer planning problems.",
                    "strength": "strong",
                    "limitations": "The study does not explore the impact of different RL algorithms or the potential for combining multiple RL algorithms.",
                    "location": "Section 4.6",
                    "exact_quote": "RL with the proposed 'LCCS' reward improves plan validity by 7% and executability by 9% in longer planning problems."
                }
            ],
            "conclusion": {
                "claim_id": 5,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 6,
            "claim": {
                "text": "RL with the proposed 'LCCS' reward improves plan validity and executability.",
                "location": "Introduction",
                "type": "Nature of the claim",
                "exact_quote": "RL with our proposed \u2018LCCS\u2019 reward emerges as the most effective strategy. In particular, it improves plan validity by 7% and executability by 9% in longer planning problems."
            },
            "evidence": [
                {
                    "evidence_id": 11,
                    "evidence_text": "Chain of Thought (CoT) prompts the agent to repeat the goal and count the remaining steps to the goal, which enhances plan executability.",
                    "strength": "moderate",
                    "limitations": "The study does not explore the impact of different CoT prompting techniques or the potential for combining multiple CoT prompting strategies.",
                    "location": "Section 4.3",
                    "exact_quote": "The model employing CoT (Goal + State) demonstrated the highest performance gain when provided with the hints."
                },
                {
                    "evidence_id": 12,
                    "evidence_text": "Reinforcement learning (RL) with the proposed 'LCCS' reward improves plan validity by 7% and executability by 9% in longer planning problems.",
                    "strength": "strong",
                    "limitations": "The study does not explore the impact of different RL algorithms or the potential for combining multiple RL algorithms.",
                    "location": "Section 4.6",
                    "exact_quote": "RL with the proposed 'LCCS' reward improves plan validity by 7% and executability by 9% in longer planning problems."
                }
            ],
            "conclusion": {
                "claim_id": 6,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 7,
            "claim": {
                "text": "Fine-tuning LLMs on datasets containing problem contexts and reference plans acquire robust planning skills.",
                "location": "Background & Related Work",
                "type": "Nature of the claim",
                "exact_quote": "Criticisms of LLMs in planning tasks stem from both theoretical analysis and empirical observations."
            },
            "evidence": [
                {
                    "evidence_id": 13,
                    "evidence_text": "Reinforcement learning (RL) with the proposed 'LCCS' reward is the most effective strategy.",
                    "strength": "strong",
                    "limitations": "The study does not explore the impact of different RL algorithms or the potential for combining multiple RL algorithms.",
                    "location": "Section 4.6",
                    "exact_quote": "RL notably improves the performance under our end-to-end planning paradigm, especially on longer problems."
                },
                {
                    "evidence_id": 14,
                    "evidence_text": "RL with the proposed 'LCCS' reward improves plan validity by 7% and executability by 9% in longer planning problems.",
                    "strength": "strong",
                    "limitations": "The study does not explore the impact of different RL algorithms or the potential for combining multiple RL algorithms.",
                    "location": "Section 4.6",
                    "exact_quote": "RL with the proposed 'LCCS' reward improves plan validity by 7% and executability by 9% in longer planning problems."
                }
            ],
            "conclusion": {
                "claim_id": 7,
                "conclusion_justified": false,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 8,
            "claim": {
                "text": "LLMs struggle to predict valid plans for long-term tasks due to their probabilistic nature.",
                "location": "Background & Related Work",
                "type": "Nature of the claim",
                "exact_quote": "LLMs are machine learning-based probabilistic models, and the accuracy of the models\u2019 predictions decays exponentially over the length of the sequence."
            },
            "evidence": [
                {
                    "evidence_id": 15,
                    "evidence_text": "Reinforcement learning (RL) with the proposed 'LCCS' reward is the most effective strategy.",
                    "strength": "strong",
                    "limitations": "The study does not explore the impact of different RL algorithms or the potential for combining multiple RL algorithms.",
                    "location": "Section 4.6",
                    "exact_quote": "RL notably improves the performance under our end-to-end planning paradigm, especially on longer problems."
                },
                {
                    "evidence_id": 16,
                    "evidence_text": "RL with the proposed 'LCCS' reward improves plan validity by 7% and executability by 9% in longer planning problems.",
                    "strength": "strong",
                    "limitations": "The study does not explore the impact of different RL algorithms or the potential for combining multiple RL algorithms.",
                    "location": "Section 4.6",
                    "exact_quote": "RL with the proposed 'LCCS' reward improves plan validity by 7% and executability by 9% in longer planning problems."
                }
            ],
            "conclusion": {
                "claim_id": 8,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 9,
            "claim": {
                "text": "Fine-tuning LLMs on datasets containing problem contexts and reference plans acquire robust planning skills.",
                "location": "Background & Related Work",
                "type": "Nature of the claim",
                "exact_quote": "Criticisms of LLMs in planning tasks stem from both theoretical analysis and empirical observations."
            },
            "evidence": [
                {
                    "evidence_id": 17,
                    "evidence_text": "Reinforcement learning (RL) with the proposed 'LCCS' reward is the most effective strategy.",
                    "strength": "strong",
                    "limitations": "The study does not explore the impact of different RL algorithms or the potential for combining multiple RL algorithms.",
                    "location": "Section 4.6",
                    "exact_quote": "RL notably improves the performance under our end-to-end planning paradigm, especially on longer problems."
                },
                {
                    "evidence_id": 18,
                    "evidence_text": "RL with the proposed 'LCCS' reward improves plan validity by 7% and executability by 9% in longer planning problems.",
                    "strength": "strong",
                    "limitations": "The study does not explore the impact of different RL algorithms or the potential for combining multiple RL algorithms.",
                    "location": "Section 4.6",
                    "exact_quote": "RL with the proposed 'LCCS' reward improves plan validity by 7% and executability by 9% in longer planning problems."
                }
            ],
            "conclusion": {
                "claim_id": 9,
                "conclusion_justified": false,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 10,
            "claim": {
                "text": "Fine-tuning LLMs on datasets containing problem contexts and reference plans acquire robust planning skills.",
                "location": "Background & Related Work",
                "type": "Nature of the claim",
                "exact_quote": "Criticisms of LLMs in planning tasks stem from both theoretical analysis and empirical observations."
            },
            "evidence": [
                {
                    "evidence_id": 19,
                    "evidence_text": "Reinforcement learning (RL) with the proposed 'LCCS' reward is the most effective strategy.",
                    "strength": "strong",
                    "limitations": "The study does not explore the impact of different RL algorithms or the potential for combining multiple RL algorithms.",
                    "location": "Section 4.6",
                    "exact_quote": "RL notably improves the performance under our end-to-end planning paradigm, especially on longer problems."
                },
                {
                    "evidence_id": 20,
                    "evidence_text": "RL with the proposed 'LCCS' reward improves plan validity by 7% and executability by 9% in longer planning problems.",
                    "strength": "strong",
                    "limitations": "The study does not explore the impact of different RL algorithms or the potential for combining multiple RL algorithms.",
                    "location": "Section 4.6",
                    "exact_quote": "RL with the proposed 'LCCS' reward improves plan validity by 7% and executability by 9% in longer planning problems."
                }
            ],
            "conclusion": {
                "claim_id": 10,
                "conclusion_justified": false,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 11,
            "claim": {
                "text": "RL is particularly well-suited for enhancing LLM planning skills.",
                "location": "Methodology & Experimental Design",
                "type": "Nature of the claim",
                "exact_quote": "RL could be a particularly well-suited strategy for enhancing LLM planning skills."
            },
            "evidence": [],
            "conclusion": {
                "claim_id": 11,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 12,
            "claim": {
                "text": "RL with the proposed 'LCCS' reward is the most effective strategy.",
                "location": "Methodology & Experimental Design",
                "type": "Nature of the claim",
                "exact_quote": "RL with our proposed \u2018LCCS\u2019 reward emerges as the most effective strategy."
            },
            "evidence": [],
            "conclusion": {
                "claim_id": 12,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 13,
            "claim": {
                "text": "RL with the proposed 'LCCS' reward improves plan validity by 7% and executability by 9% in longer planning problems.",
                "location": "Methodology & Experimental Design",
                "type": "Nature of the claim",
                "exact_quote": "RL with our proposed \u2018LCCS\u2019 reward improves plan validity by 7% and executability by 9% in longer planning problems."
            },
            "evidence": [],
            "conclusion": {
                "claim_id": 13,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 14,
            "claim": {
                "text": "RL with the proposed 'LCCS' reward is the most effective strategy.",
                "location": "Results",
                "type": "Nature of the claim",
                "exact_quote": "RL notably improves the performance under our end-to-end planning paradigm, especially on longer problems."
            },
            "evidence": [],
            "conclusion": {
                "claim_id": 14,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 15,
            "claim": {
                "text": "RL with the proposed 'LCCS' reward improves plan validity by 7% and executability by 9% in longer planning problems.",
                "location": "Results",
                "type": "Nature of the claim",
                "exact_quote": "RL boosted the validity rate on the \u2018long\u2019 test set from 34.8% to 41.5% and the executability rate from 42.3% to 53.6%."
            },
            "evidence": [],
            "conclusion": {
                "claim_id": 15,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 16,
            "claim": {
                "text": "RL with the proposed 'LCCS' reward is the most effective strategy.",
                "location": "Discussion",
                "type": "Nature of the claim",
                "exact_quote": "RL stands out as the most effective strategy in this end-to-end paradigm, enhancing both the validity and executability rates on longer problems."
            },
            "evidence": [],
            "conclusion": {
                "claim_id": 16,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 17,
            "claim": {
                "text": "RL with the proposed 'LCCS' reward improves plan validity by 7% and executability by 9% in longer planning problems.",
                "location": "Discussion",
                "type": "Nature of the claim",
                "exact_quote": "RL boosted the validity rate on the \u2018long\u2019 test set from 34.8% to 41.5% and the executability rate from 42.3% to 53.6%."
            },
            "evidence": [],
            "conclusion": {
                "claim_id": 17,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 18,
            "claim": {
                "text": "RL with the proposed 'LCCS' reward is the most effective strategy.",
                "location": "Discussion",
                "type": "Nature of the claim",
                "exact_quote": "RL stands out as the most effective strategy in this end-to-end paradigm, enhancing both the validity and executability rates on longer problems."
            },
            "evidence": [],
            "conclusion": {
                "claim_id": 18,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        },
        {
            "claim_id": 19,
            "claim": {
                "text": "RL with the proposed 'LCCS' reward improves plan validity by 7% and executability by 9% in longer planning problems.",
                "location": "Discussion",
                "type": "Nature of the claim",
                "exact_quote": "RL boosted the validity rate on the \u2018long\u2019 test set from 34.8% to 41.5% and the executability rate from 42.3% to 53.6%."
            },
            "evidence": [],
            "conclusion": {
                "claim_id": 19,
                "conclusion_justified": true,
                "robustness": "high",
                "key_limitations": "The study only evaluates the model's performance on the extended PlanBench dataset, which may not cover all possible planning scenarios.",
                "confidence_level": "high"
            }
        }
    ],
    "execution_times": {
        "claims_analysis_time": "203.24 seconds",
        "evidence_analysis_time": "302.01 seconds",
        "conclusions_analysis_time": "154.21 seconds",
        "total_execution_time": "662.96 seconds"
    }
}