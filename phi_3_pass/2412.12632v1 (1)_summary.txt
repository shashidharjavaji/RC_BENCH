=== Paper Analysis Summary ===

Claim 1:
Statement: Incorporating external knowledge into large language models (LLMs) has emerged as a promising approach to mitigate outdated knowledge and hallucination in LLMs.
Location: Abstract
Type: Methodology
Quote: Incorporating external knowledge into large language models (LLMs) has emerged as a promising approach to mitigate outdated knowledge and hallucination in LLMs.

Evidence:
- Incorporating external knowledge into large language models (LLMs) has emerged as an effective approach to mitigate this problem (Tu et al., 2024; Zhao et al., 2024).
  Strength: strong
  Location: Abstract
  Limitations: None
  Quote: Incorporating external knowledge into large language models (LLMs) has emerged as an effective approach to mitigate this problem (Tu et al., 2024; Zhao et al., 2024).

- External knowledge is often imperfect. In addition to useful knowledge, external knowledge is rich in irrelevant or misinformation in the context that can impair the reliability of LLM responses.
  Strength: strong
  Location: Abstract
  Limitations: None
  Quote: External knowledge is often imperfect. In addition to useful knowledge, external knowledge is rich in irrelevant or misinformation in the context that can impair the reliability of LLM responses.

Conclusion:
Justified: True
Robustness: high
Limitations: None
Confidence: high

==================================================

Claim 2:
Statement: External knowledge is often imperfect, containing irrelevant information and misinformation that can impair the reliability of LLM responses.
Location: Abstract
Type: Problem Statement
Quote: External knowledge is often imperfect. In addition to useful knowledge, external knowledge is rich in irrelevant or misinformation in the context that can impair the reliability of LLM responses.

Evidence:
None

Conclusion:
Justified: True
Robustness: high
Limitations: None
Confidence: high

==================================================

Claim 3:
Statement: The paper proposes an automated Chain of Evidence (CoE) discrimination approach to characterize and explore LLMs' preferences for external knowledge.
Location: Abstract
Type: Methodology
Quote: This paper focuses on LLMs’ preferred external knowledge in imperfect contexts when handling multi-hop QA. Inspired by criminal procedural law’s Chain of Evidence (CoE), we characterize that knowledge preferred by LLMs should maintain both relevance to the question and mutual support among knowledge pieces.

Evidence:
- This paper proposes an automated Chain of Evidence (CoE) discrimination approach and explore LLMs’ preferences from their effectiveness, faithfulness and robustness, as well as CoE’s usability in a naive Retrieval-Augmented Generation (RAG) case.
  Strength: strong
  Location: Abstract
  Limitations: None
  Quote: This paper focuses on LLMs’ preferred external knowledge in imperfect contexts when handling multi-hop QA. Inspired by criminal procedural law’s Chain of Evidence (CoE), we characterize that knowledge preferred by LLMs should maintain both relevance to the question and mutual support among knowledge pieces. Accordingly, we propose an automated CoE discrimination approach and explore LLMs’ preferences from their effectiveness, faithfulness and robustness, as well as CoE’s usability in a naive Retrieval-Augmented Generation (RAG) case.

Conclusion:
Justified: True
Robustness: high
Limitations: None
Confidence: high

==================================================

Claim 4:
Statement: The CoE discrimination approach enhances LLMs' performance in terms of accuracy, faithfulness, robustness, and usability in a naive Retrieval-Augmented Generation (RAG) case.
Location: Abstract
Type: Methodology
Quote: The evaluation on five LLMs reveals that CoE enhances LLMs through more accurate generation, stronger answer faithfulness, better robustness against knowledge conflict, and improved performance in a popular RAG case.

Evidence:
- The evaluation on five LLMs reveals that CoE enhances LLMs through more accurate generation, stronger answer faithfulness, better robustness against knowledge conflict, and improved performance in a popular RAG case.
  Strength: strong
  Location: Abstract
  Limitations: None
  Quote: The evaluation on five LLMs reveals that CoE enhances LLMs through more accurate generation, stronger answer faithfulness, better robustness against knowledge conflict, and improved performance in a popular RAG case.

Conclusion:
Justified: True
Robustness: high
Limitations: None
Confidence: high

==================================================

Claim 5:
Statement: LLMs exhibit a preference for external knowledge that forms a Chain of Evidence (CoE), demonstrating relevance and interconnectivity among knowledge pieces.
Location: Introduction
Type: Methodology
Quote: In our study, we focus on characterizing what external knowledge is more capable of resisting the surrounding noise and guiding LLMs for better generation.

Evidence:
- Inspired by the Chain of Evidence (CoE) theory in criminal procedural law (Murphy, 2013), we characterize that knowledge preferred by LLMs should maintain both relevance to the question and mutual support among knowledge pieces.
  Strength: strong
  Location: Introduction
  Limitations: None
  Quote: Inspired by the Chain of Evidence (CoE) theory in criminal procedural law (Murphy, 2013), we characterize that knowledge preferred by LLMs should maintain both relevance to the question and mutual support among knowledge pieces.

- Accordingly, we propose an automated CoE discrimination approach and explore LLMs’ preferences from their effectiveness, faithfulness and robustness, as well as CoE’s usability in a naive Retrieval-Augmented Generation (RAG) case.
  Strength: strong
  Location: Introduction
  Limitations: None
  Quote: Accordingly, we propose an automated CoE discrimination approach and explore LLMs’ preferences from their effectiveness, faithfulness and robustness, as well as CoE’s usability in a naive Retrieval-Augmented Generation (RAG) case.

Conclusion:
Justified: True
Robustness: high
Limitations: None
Confidence: high

==================================================

Claim 6:
Statement: The CoE discrimination approach can effectively identify relevant and interconnected external knowledge.
Location: Introduction
Type: Methodology
Quote: Based on the principle, we first characterize what knowledge can be considered CoE and propose a discrimination approach to determine whether the given external knowledge contains CoE.

Evidence:
- The evaluation on five LLMs reveals that CoE enhances LLMs through more accurate generation, stronger answer faithfulness, better robustness against knowledge conflict, and improved performance in a popular RAG case.
  Strength: strong
  Location: Abstract
  Limitations: None
  Quote: The evaluation on five LLMs reveals that CoE enhances LLMs through more accurate generation, stronger answer faithfulness, better robustness against knowledge conflict, and improved performance in a popular RAG case.

Conclusion:
Justified: True
Robustness: high
Limitations: None
Confidence: high

==================================================

Claim 7:
Statement: External knowledge equipped with CoE helps LLMs generate correct answers more effectively than Non-CoE.
Location: 5. Effectiveness Assessment
Type: Finding
Quote: External knowledge equipped with CoE can more effectively (than Non-CoE) help LLMs generate correct answers in context rich with irrelevant information.

Evidence:
- Using HotpotQA (Yang et al., 2018) and 2WikiMultihopQA (Ho et al., 2020) as sources, we constructed 1,336 multi-hop QA pairs and the corresponding CoE based on the proposed CoE discrimination approach.
  Strength: strong
  Location: 3 CoE Discrimination Approach
  Limitations: None
  Quote: Using HotpotQA (Yang et al., 2018) and 2WikiMultihopQA (Ho et al., 2020) as sources, we constructed 1,336 multi-hop QA pairs and the corresponding CoE based on the proposed CoE discrimination approach.

- By applying perturbations to CoE, we also build Non-CoE samples (that is, knowledge lacking the necessary relevance or interconnectivity to establish CoE) for each QA pair.
  Strength: strong
  Location: 4 Subject Dataset and LLMs
  Limitations: None
  Quote: By applying perturbations to CoE, we also build Non-CoE samples (that is, knowledge lacking the necessary relevance or interconnectivity to establish CoE) for each QA pair.

Conclusion:
Justified: True
Robustness: high
Limitations: None
Confidence: high

==================================================

Claim 8:
Statement: LLMs exhibit higher faithfulness to the answer implicated in CoE, even when CoE contains factual errors.
Location: 6. Faithfulness Assessment
Type: Finding
Quote: LLMs exhibit higher faithfulness to the answer implicated in CoE (than Non-CoE), even when CoE contains factual errors.

Evidence:
- LLMs exhibit higher faithfulness to the answer implicated in CoE (than Non-CoE), even when CoE contains factual errors.
  Strength: strong
  Location: 6 Faithfulness Assessment
  Limitations: None
  Quote: LLMs exhibit higher faithfulness to the answer implicated in CoE (than Non-CoE), even when CoE contains factual errors.

Conclusion:
Justified: True
Robustness: high
Limitations: None
Confidence: high

==================================================

Claim 9:
Statement: LLMs exhibit higher robustness against knowledge conflict when external knowledge is equipped with CoE.
Location: 7. Robustness Assessment
Type: Finding
Quote: LLMs augmented with CoE exibit higher robustness against knowledge conflict than Non-CoE.

Evidence:
- LLMs exhibit higher robustness against knowledge conflict (than Non-CoE) if the external knowledge is equipped with CoE.
  Strength: strong
  Location: 7 Robustness Assessment
  Limitations: None
  Quote: LLMs augmented with CoE ex-hibit higher robustness against knowledge con-flict than Non-CoE.

Conclusion:
Justified: True
Robustness: high
Limitations: None
Confidence: high

==================================================

Claim 10:
Statement: The CoE-guided retrieval strategy improves LLMs' accuracy in the naive RAG framework.
Location: 8. Usability Assessment
Type: Finding
Quote: For the subject case, CoE-guided retrieval could improve the LLMs’ accuracy in the naive framework.

Evidence:
- For the selected case, the CoE-guided retrieval strategy can effectively improve LLM’s accuracy after substituting the reranking component in the naive RAG framework.
  Strength: strong
  Location: 8 Usability Assessment
  Limitations: None
  Quote: For the selected case, the CoE-guided retrieval strategy can effectively improve LLM’s accuracy after substituting the reranking component in the naive RAG framework.

Conclusion:
Justified: True
Robustness: high
Limitations: None
Confidence: high

==================================================

Claim 11:
Statement: The CoE-guided retrieval strategy makes LLMs more efficient in knowledge utilization, leading to improved performance and reduced dependency on large amounts of external data.
Location: 8. Usability Assessment
Type: Finding
Quote: ScopeCoE can help LLMs generate more accurate outputs with fewer knowledge pieces.

Evidence:
- The CoE-guided retrieval strategy makes LLMs more efficient in knowledge utilization, leading to improved performance and reduced dependency on large amounts of external data.
  Strength: strong
  Location: 8 Usability Assessment
  Limitations: None
  Quote: The CoE-guided retrieval strategy makes LLMs more efficient in knowledge utilization, leading to improved performance and reduced dependency on large amounts of external data.

Conclusion:
Justified: True
Robustness: high
Limitations: None
Confidence: high

==================================================

Claim 12:
Statement: The paper's findings provide insights for future research in designing the retrieval process and assessing the quality of external knowledge.
Location: Conclusion
Type: Conclusion
Quote: The above findings could provide insights for future research in designing the retrieval process and assessing the quality of external knowledge with the proposed CoE discrimination approach.

Evidence:
- The above findings could provide insights for future research in designing the retrieval process and assessing the quality of external knowledge with the proposed CoE discrimination approach.
  Strength: strong
  Location: 9 Conclusion
  Limitations: None
  Quote: The above findings could provide insights for future research in designing the retrieval process and assessing the quality of external knowledge with the proposed CoE discrimination approach.

Conclusion:
Justified: True
Robustness: medium
Limitations: The paper does not explore broader applications of CoE in RAG scenarios, such as retrieval corpus construction and retriever optimization.
Confidence: medium

==================================================


Execution Times:
claims_analysis_time: 133.30 seconds
evidence_analysis_time: 239.00 seconds
conclusions_analysis_time: 70.44 seconds
total_execution_time: 445.94 seconds
