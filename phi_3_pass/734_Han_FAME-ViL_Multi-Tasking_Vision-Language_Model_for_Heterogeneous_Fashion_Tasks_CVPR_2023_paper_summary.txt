=== Paper Analysis Summary ===

Raw Claims:
 {
    "claims": [
        {
            "claim_id": 1,
            "claim_text": "FAME-ViL can save 61.5% of parameters over alternatives while significantly outperforming the conventional independently trained single-task models.",
            "location": "Abstract",
            "claim_type": "Novel Finding",
            "exact_quote": "It is enabled by two novel components: (1) a task-versatile architecture with cross-attention adapters and task-specific adapters integrated into a unified V+L model, and (2) a stable and effective multi-task training strategy that supports learning from heterogeneous data and prevents negative transfer. Extensive experiments on four fashion tasks show that our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 2,
            "claim_text": "FAME-ViL is the first work to investigate the problem of multi-task learning on heterogeneous fashion tasks, eliminating the parameter redundancy and exploiting the inter-task relatedness.",
            "location": "Introduction",
            "claim_type": "Novel Finding",
            "exact_quote": "For the first time, we investigate the problem of multi-task learning on heterogeneous fashion tasks, eliminating the parameter redundancy and exploiting the inter-task relatedness."
        },
        {
            "claim_id": 3,
            "claim_text": "FAME-ViL outperforms all prior models often by a large margin, validating the performance advantages of our method over alternatives in addition to better parameter efficiency.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL outperforms all prior art fashion models often by a large margin, validating the performance advantages of our method over alternatives in addition to better parameter efficiency."
        },
        {
            "claim_id": 4,
            "claim_text": "FAME-ViL achieves new state-of-the-art performance on all tasks with significantly fewer parameters.",
            "location": "Conclusion",
            "claim_type": "Novel Finding",
            "exact_quote": "We have introduced FAME-ViL for heterogeneous fashion tasks, grounded upon a generic off-the-shelf V+L model. It addresses cross-modal retrieval, text-guided image retrieval, multi-modal classification, and image captioning in a unified architecture. This is made possible by the proposed task-versatile architecture with cross-attention adapters and task-specific adapters, and a scalable multi-task training pipeline with multi-teacher distillation. Extensive experiments showed that our FAME-ViL achieves new state-of-the-art performance on all tasks with significantly fewer parameters."
        },
        {
            "claim_id": 5,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 6,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 7,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 8,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 9,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 10,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 11,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 12,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 13,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 14,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 15,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 16,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 17,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 18,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 19,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 20,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 21,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 22,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 23,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 24,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 25,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 26,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 27,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 28,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 29,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 30,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 31,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 32,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 33,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 34,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 35,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 36,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 37,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 38,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 39,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 40,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 41,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 42,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 43,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 44,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 45,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 46,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 47,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 48,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 49,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 50,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 51,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 52,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 53,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 54,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 55,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 56,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 57,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 58,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 59,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 60,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 61,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 62,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 63,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 64,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
        },
        {
            "claim_id": 65,
            "claim_text": "FAME-ViL's multi-task learning strategy is effective in exploiting the inter-task relatedness.",
            "location": "Experiments",
            "claim_type": "Novel Finding",
            "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperform

Raw Evidence:
 {
    "evidence_sets": [
        {
            "claim_id": 1,
            "evidence": [
                {
                    "evidence_id": 1,
                    "evidence_text": "Extensive experiments on four fashion tasks show that our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 2,
                    "evidence_text": "The results of XMR on FashionGen [61] are reported in Tab. 1. We draw several observations: (1) Our FAME-ViL outperforms all prior art fashion models often by a large margin, validating the performance advantages of our method over alternatives in addition to better parameter efficiency.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL outperforms all prior art fashion models often by a large margin, validating the performance advantages of our method over alternatives in addition to better parameter efficiency."
                },
                {
                    "evidence_id": 3,
                    "evidence_text": "The results of TGIR are shown in Tab. 2. We have similar observations as on XMR. In particular, we note that our single-task variant already achieve a new art performance. With a simple addition-based fusion mechanism, FAME-ViL can even outperform significantly [2] with the same CLIP pre-training and a complex fusion module.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "The results of TGIR are shown in Tab. 2. We have similar observations as on XMR. In particular, we note that our single-task variant already achieve a new art performance. With a simple addition-based fusion mechanism, FAME-ViL can even outperform significantly [2] with the same CLIP pre-training and a complex fusion module."
                },
                {
                    "evidence_id": 4,
                    "evidence_text": "The results of SCR and FIC are shown in the right part of Tab. 3. The original FashionViL [24] has no decoder and cannot support generation tasks. For comparison, we equip it with masked language modelling (MLM) autoregressively [43,93,94] enabling the image captioning. The results of FIC are shown in the right part of Tab. 3, following the common protocol [94]. Our FAME-ViL again achieves state-of-the-art performance with a clear margin.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "The results of SCR and FIC are shown in the right part of Tab. 3. The original FashionViL [24] has no decoder and cannot support generation tasks. For comparison, we equip it with masked language modelling (MLM) autoregressively [43,93,94] enabling the image captioning. The results of FIC are shown in the right part of Tab. 3, following the common protocol [94]. Our FAME-ViL again achieves state-of-the-art performance with a clear margin."
                },
                {
                    "evidence_id": 5,
                    "evidence_text": "We have introduced FAME-ViL for heterogeneous fashion tasks, grounded upon a generic off-the-shelf V+L model. It addresses cross-modal retrieval, text-guided image retrieval, multi-modal classification, and image captioning in a unified architecture. This is made possible by the proposed task-versatile architecture with cross-attention adapters and task-specific adapters, and a scalable multi-task training pipeline with multi-teacher distillation. Extensive experiments showed that our FAME-ViL achieves new state-of-the-art performance on all tasks with significantly fewer parameters.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Conclusion",
                    "exact_quote": "We have introduced FAME-ViL for heterogeneous fashion tasks, grounded upon a generic off-the-shelf V+L model. It addresses cross-modal retrieval, text-guided image retrieval, multi-modal classification, and image captioning in a unified architecture. This is made possible by the proposed task-versatile architecture with cross-attention adapters and task-specific adapters, and a scalable multi-task training pipeline with multi-teacher distillation. Extensive experiments showed that our FAME-ViL achieves new state-of-the-art performance on all tasks with significantly fewer parameters."
                },
                {
                    "evidence_id": 6,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 7,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 8,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 9,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 10,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 11,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 12,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 13,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 14,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 15,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 16,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 17,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 18,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 19,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 20,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 21,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 22,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 23,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 24,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 25,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 26,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 27,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 28,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 29,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 30,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 31,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 32,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 33,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 34,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 35,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 36,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 37,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 38,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 39,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 40,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 41,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 42,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 43,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 44,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 45,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 46,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 47,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 48,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 49,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 50,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 51,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 52,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models."
                },
                {
                    "evidence_id": 53,
                    "evidence_text": "Our FAME-ViL can save 61.5% of parameters over alternatives, while significantly outperforming the conventional independently trained single-task models.",
                    "strength": "strong",
                    "limitations": "None mentioned",
                    "location": "Experiments",
                    "exact_quote": "Our FAME-ViL can save 61.5% of parameters over alternatives

Structured Conclusions:
[
  {
    "claim_id": 1,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 2,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 3,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 4,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 5,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 6,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 7,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 8,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 9,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 10,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 11,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 12,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 13,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 14,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 15,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 16,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 17,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 18,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 19,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 20,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 21,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 22,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 23,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 24,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 25,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 26,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 27,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 28,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 29,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 30,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 31,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 32,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 33,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 34,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 35,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 36,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 37,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 38,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 39,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 40,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 41,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 42,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 43,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 44,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 45,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 46,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 47,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 48,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 49,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 50,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 51,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 52,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 53,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 54,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 55,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 56,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 57,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 58,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 59,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 60,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 61,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 62,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 63,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 64,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  },
  {
    "claim_id": 65,
    "conclusion_justified": true,
    "robustness": "high",
    "key_limitations": "None mentioned",
    "confidence_level": "high"
  }
]


Execution Times:
claims_analysis_time: 735.27 seconds
evidence_analysis_time: 795.49 seconds
conclusions_analysis_time: 425.55 seconds
total_execution_time: 1960.56 seconds
