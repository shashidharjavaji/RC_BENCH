=== Paper Analysis Summary ===

Raw Claims:

```json
{
    "claims": [
        {
            "claim_id": 1,
            "claim_text": "The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines."
        },
        {
            "claim_id": 2,
            "claim_text": "The proposed MGN significantly reduces the number of false positives for audio and visual events.",
            "location": "Section 4.3",
            "claim_type": "Results",
            "exact_quote": "Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494."
        },
        {
            "claim_id": 3,
            "claim_text": "The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV."
        },
        {
            "claim_id": 4,
            "claim_text": "The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.",
            "location": "Section 1",
            "claim_type": "Contribution",
            "exact_quote": "Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB)."
        },
        {
            "claim_id": 5,
            "claim_text": "The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.",
            "location": "Section 3.2",
            "claim_type": "Contribution",
            "exact_quote": "We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics."
        },
        {
            "claim_id": 6,
            "claim_text": "The proposed MGN introduces a modality-aware cross-modal grouping module to match the video-level label.",
            "location": "Section 3.3",
            "claim_type": "Contribution",
            "exact_quote": "We introduce a modality-aware cross-modal grouping module to match the video-level target, although the given target does not indicate modalities."
        },
        {
            "claim_id": 7,
            "claim_text": "The proposed MGN is expected to parse a video into events with different categories and modalities.",
            "location": "Section 5",
            "claim_type": "Conclusion",
            "exact_quote": "The potential future work is to add more grouping stages with learned class-tokens as supervision for each one."
        }
    ]
}
```
```json
{
    "claims": [
        {
            "claim_id": 1,
            "claim_text": "The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines."
        },
        {
            "claim_id": 2,
            "claim_text": "The proposed MGN significantly reduces the number of false positives for audio and visual events.",
            "location": "Section 4.3",
            "claim_type": "Results",
            "exact_quote": "Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494."
        },
        {
            "claim_id": 3,
            "claim_text": "The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV."
        },
        {
            "claim_id": 4,
            "claim_text": "The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.",
            "location": "Section 1",
            "claim_type": "Contribution",
            "exact_quote": "Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB)."
        },
        {
            "claim_id": 5,
            "claim_text": "The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.",
            "location": "Section 3.2",
            "claim_type": "Contribution",
            "exact_quote": "We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics."
        },
        {
            "claim_id": 6,
            "claim_text": "The proposed MGN introduces a modality-aware cross-modal grouping module to match the video-level label.",
            "location": "Section 3.3",
            "claim_type": "Contribution",
            "exact_quote": "We introduce a modality-aware cross-modal grouping module to match the video-level target, although the given target does not indicate modalities."
        },
        {
            "claim_id": 7,
            "claim_text": "The proposed MGN is expected to parse a video into events with different categories and modalities.",
            "location": "Section 5",
            "claim_type": "Conclusion",
            "exact_quote": "The potential future work is to add more grouping stages with learned class-tokens as supervision for each one."
        }
    ]
}
```
```json
{
    "claims": [
        {
            "claim_id": 1,
            "claim_text": "The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines."
        },
        {
            "claim_id": 2,
            "claim_text": "The proposed MGN significantly reduces the number of false positives for audio and visual events.",
            "location": "Section 4.3",
            "claim_type": "Results",
            "exact_quote": "Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494."
        },
        {
            "claim_id": 3,
            "claim_text": "The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV."
        },
        {
            "claim_id": 4,
            "claim_text": "The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.",
            "location": "Section 1",
            "claim_type": "Contribution",
            "exact_quote": "Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB)."
        },
        {
            "claim_id": 5,
            "claim_text": "The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.",
            "location": "Section 3.2",
            "claim_type": "Contribution",
            "exact_quote": "We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics."
        },
        {
            "claim_id": 6,
            "claim_text": "The proposed MGN introduces a modality-aware cross-modal grouping module to match the video-level label.",
            "location": "Section 3.3",
            "claim_type": "Contribution",
            "exact_quote": "We introduce a modality-aware cross-modal grouping module to match the video-level target, although the given target does not indicate modalities."
        },
        {
            "claim_id": 7,
            "claim_text": "The proposed MGN is expected to parse a video into events with different categories and modalities.",
            "location": "Section 5",
            "claim_type": "Conclusion",
            "exact_quote": "The potential future work is to add more grouping stages with learned class-tokens as supervision for each one."
        }
    ]
}
```
```json
{
    "claims": [
        {
            "claim_id": 1,
            "claim_text": "The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines."
        },
        {
            "claim_id": 2,
            "claim_text": "The proposed MGN significantly reduces the number of false positives for audio and visual events.",
            "location": "Section 4.3",
            "claim_type": "Results",
            "exact_quote": "Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494."
        },
        {
            "claim_id": 3,
            "claim_text": "The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV."
        },
        {
            "claim_id": 4,
            "claim_text": "The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.",
            "location": "Section 1",
            "claim_type": "Contribution",
            "exact_quote": "Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB)."
        },
        {
            "claim_id": 5,
            "claim_text": "The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.",
            "location": "Section 3.2",
            "claim_type": "Contribution",
            "exact_quote": "We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics."
        },
        {
            "claim_id": 6,
            "claim_text": "The proposed MGN introduces a modality-aware cross-modal grouping module to match the video-level label.",
            "location": "Section 3.3",
            "claim_type": "Contribution",
            "exact_quote": "We introduce a modality-aware cross-modal grouping module to match the video-level target, although the given target does not indicate modalities."
        },
        {
            "claim_id": 7,
            "claim_text": "The proposed MGN is expected to parse a video into events with different categories and modalities.",
            "location": "Section 5",
            "claim_type": "Conclusion",
            "exact_quote": "The potential future work is to add more grouping stages with learned class-tokens as supervision for each one."
        }
    ]
}
```
```json
{
    "claims": [
        {
            "claim_id": 1,
            "claim_text": "The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines."
        },
        {
            "claim_id": 2,
            "claim_text": "The proposed MGN significantly reduces the number of false positives for audio and visual events.",
            "location": "Section 4.3",
            "claim_type": "Results",
            "exact_quote": "Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494."
        },
        {
            "claim_id": 3,
            "claim_text": "The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV."
        },
        {
            "claim_id": 4,
            "claim_text": "The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.",
            "location": "Section 1",
            "claim_type": "Contribution",
            "exact_quote": "Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB)."
        },
        {
            "claim_id": 5,
            "claim_text": "The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.",
            "location": "Section 3.2",
            "claim_type": "Contribution",
            "exact_quote": "We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics."
        },
        {
            "claim_id": 6,
            "claim_text": "The proposed MGN introduces a modality-aware cross-modal grouping module to match the video-level label.",
            "location": "Section 3.3",
            "claim_type": "Contribution",
            "exact_quote": "We introduce a modality-aware cross-modal grouping module to match the video-level target, although the given target does not indicate modalities."
        },
        {
            "claim_id": 7,
            "claim_text": "The proposed MGN is expected to parse a video into events with different categories and modalities.",
            "location": "Section 5",
            "claim_type": "Conclusion",
            "exact_quote": "The potential future work is to add more grouping stages with learned class-tokens as supervision for each one."
        }
    ]
}
```
```json
{
    "claims": [
        {
            "claim_id": 1,
            "claim_text": "The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines."
        },
        {
            "claim_id": 2,
            "claim_text": "The proposed MGN significantly reduces the number of false positives for audio and visual events.",
            "location": "Section 4.3",
            "claim_type": "Results",
            "exact_quote": "Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494."
        },
        {
            "claim_id": 3,
            "claim_text": "The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV."
        },
        {
            "claim_id": 4,
            "claim_text": "The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.",
            "location": "Section 1",
            "claim_type": "Contribution",
            "exact_quote": "Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB)."
        },
        {
            "claim_id": 5,
            "claim_text": "The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.",
            "location": "Section 3.2",
            "claim_type": "Contribution",
            "exact_quote": "We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics."
        },
        {
            "claim_id": 6,
            "claim_text": "The proposed MGN introduces a modality-aware cross-modal grouping module to match the video-level label.",
            "location": "Section 3.3",
            "claim_type": "Contribution",
            "exact_quote": "We introduce a modality-aware cross-modal grouping module to match the video-level target, although the given target does not indicate modalities."
        },
        {
            "claim_id": 7,
            "claim_text": "The proposed MGN is expected to parse a video into events with different categories and modalities.",
            "location": "Section 5",
            "claim_type": "Conclusion",
            "exact_quote": "The potential future work is to add more grouping stages with learned class-tokens as supervision for each one."
        }
    ]
}
```
```json
{
    "claims": [
        {
            "claim_id": 1,
            "claim_text": "The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines."
        },
        {
            "claim_id": 2,
            "claim_text": "The proposed MGN significantly reduces the number of false positives for audio and visual events.",
            "location": "Section 4.3",
            "claim_type": "Results",
            "exact_quote": "Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494."
        },
        {
            "claim_id": 3,
            "claim_text": "The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV."
        },
        {
            "claim_id": 4,
            "claim_text": "The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.",
            "location": "Section 1",
            "claim_type": "Contribution",
            "exact_quote": "Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB)."
        },
        {
            "claim_id": 5,
            "claim_text": "The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.",
            "location": "Section 3.2",
            "claim_type": "Contribution",
            "exact_quote": "We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics."
        },
        {
            "claim_id": 6,
            "claim_text": "The proposed MGN introduces a modality-aware cross-modal grouping module to match the video-level label.",
            "location": "Section 3.3",
            "claim_type": "Contribution",
            "exact_quote": "We introduce a modality-aware cross-modal grouping module to match the video-level target, although the given target does not indicate modalities."
        },
        {
            "claim_id": 7,
            "claim_text": "The proposed MGN is expected to parse a video into events with different categories and modalities.",
            "location": "Section 5",
            "claim_type": "Conclusion",
            "exact_quote": "The potential future work is to add more grouping stages with learned class-tokens as supervision for each one."
        }
    ]
}
```
```json
{
    "claims": [
        {
            "claim_id": 1,
            "claim_text": "The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines."
        },
        {
            "claim_id": 2,
            "claim_text": "The proposed MGN significantly reduces the number of false positives for audio and visual events.",
            "location": "Section 4.3",
            "claim_type": "Results",
            "exact_quote": "Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494."
        },
        {
            "claim_id": 3,
            "claim_text": "The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV."
        },
        {
            "claim_id": 4,
            "claim_text": "The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.",
            "location": "Section 1",
            "claim_type": "Contribution",
            "exact_quote": "Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB)."
        },
        {
            "claim_id": 5,
            "claim_text": "The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.",
            "location": "Section 3.2",
            "claim_type": "Contribution",
            "exact_quote": "We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics."
        },
        {
            "claim_id": 6,
            "claim_text": "The proposed MGN introduces a modality-aware cross-modal grouping module to match the video-level label.",
            "location": "Section 3.3",
            "claim_type": "Contribution",
            "exact_quote": "We introduce a modality-aware cross-modal grouping module to match the video-level target, although the given target does not indicate modalities."
        },
        {
            "claim_id": 7,
            "claim_text": "The proposed MGN is expected to parse a video into events with different categories and modalities.",
            "location": "Section 5",
            "claim_type": "Conclusion",
            "exact_quote": "The potential future work is to add more grouping stages with learned class-tokens as supervision for each one."
        }
    ]
}
```
```json
{
    "claims": [
        {
            "claim_id": 1,
            "claim_text": "The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines."
        },
        {
            "claim_id": 2,
            "claim_text": "The proposed MGN significantly reduces the number of false positives for audio and visual events.",
            "location": "Section 4.3",
            "claim_type": "Results",
            "exact_quote": "Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494."
        },
        {
            "claim_id": 3,
            "claim_text": "The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV."
        },
        {
            "claim_id": 4,
            "claim_text": "The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.",
            "location": "Section 1",
            "claim_type": "Contribution",
            "exact_quote": "Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB)."
        },
        {
            "claim_id": 5,
            "claim_text": "The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.",
            "location": "Section 3.2",
            "claim_type": "Contribution",
            "exact_quote": "We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics."
        },
        {
            "claim_id": 6,
            "claim_text": "The proposed MGN introduces a modality-aware cross-modal grouping module to match the video-level label.",
            "location": "Section 3.3",
            "claim_type": "Contribution",
            "exact_quote": "We introduce a modality-aware cross-modal grouping module to match the video-level target, although the given target does not indicate modalities."
        },
        {
            "claim_id": 7,
            "claim_text": "The proposed MGN is expected to parse a video into events with different categories and modalities.",
            "location": "Section 5",
            "claim_type": "Conclusion",
            "exact_quote": "The potential future work is to add more grouping stages with learned class-tokens as supervision for each one."
        }
    ]
}
```
```json
{
    "claims": [
        {
            "claim_id": 1,
            "claim_text": "The proposed Multi-modal Grouping Network (MGN) achieves superior results against previous baselines in weakly-supervised audio-visual video parsing.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Experimental results demonstrate the effectiveness and superiority of our MGN against previous network baselines."
        },
        {
            "claim_id": 2,
            "claim_text": "The proposed MGN significantly reduces the number of false positives for audio and visual events.",
            "location": "Section 4.3",
            "claim_type": "Results",
            "exact_quote": "Our MGN with the class-aware unimodal grouping modules decreases the false positives of audio and visual events by large margins, 381 and 494."
        },
        {
            "claim_id": 3,
            "claim_text": "The proposed MGN outperforms baselines by 3.5 Visual, 1.4 Audio-Visual, and 1.6 Tyep@AV for event-level predictions.",
            "location": "Section 4.2",
            "claim_type": "Results",
            "exact_quote": "Our framework with both contrastive learning and label refinement achieves the best segment-level performance in terms of Visual, Audio-Visual, Type@AV, and Event@AV."
        },
        {
            "claim_id": 4,
            "claim_text": "The proposed MGN is much more lightweight, using only 47.2% of the parameters of baselines.",
            "location": "Section 1",
            "claim_type": "Contribution",
            "exact_quote": "Our simple framework achieves improving results against previous baselines on weakly-supervised audiovisual video parsing while using only 47.2% of the parameters of baselines (17 MB vs. 36 MB)."
        },
        {
            "claim_id": 5,
            "claim_text": "The proposed MGN is the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics.",
            "location": "Section 3.2",
            "claim_type": "Contribution",
            "exact_quote": "We are the first to exploit unimodal grouping for learning audio-visual representations with class-aware semantics."
        },
        {
            "claim_id": 6,
            "claim_text": "The proposed MGN introduces a mod

Raw Evidence:

```

Raw Conclusions:


Execution Times:
claims_analysis_time: 704.84 seconds
evidence_analysis_time: 2.11 seconds
conclusions_analysis_time: 765.75 seconds
total_execution_time: 1482.45 seconds
