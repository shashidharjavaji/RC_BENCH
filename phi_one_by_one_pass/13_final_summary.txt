=== Paper Analysis Summary ===

Raw Claims:


        ```json
        {
            "claims": [
                {
                    "claim_id": 1,
                    "claim_text": "The dense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy.",
                    "location": "Abstract",
                    "claim_type": "Research finding",
                    "exact_quote": "our dense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy"
                },
                {
                    "claim_id": 2,
                    "claim_text": "Dense Passage Retriever (DPR) outperforms BM25 by a large margin (65.2% vs. 42.9% in Top-5 accuracy).",
                    "location": "Section 3.2",
                    "claim_type": "Research finding",
                    "exact_quote": "65.2% vs. 42.9% in Top-5 accuracy"
                },
                {
                    "claim_id": 3,
                    "claim_text": "DPR results in a substantial improvement on the end-to-end QA accuracy compared to ORQA (41.5% vs. 33.3%) in the open Natural Questions setting.",
                    "location": "Section 3.2",
                    "claim_type": "Research finding",
                    "exact_quote": "41.5% vs. 33.3%"
                },
                {
                    "claim_id": 4,
                    "claim_text": "DPR trained using only question-passage pairs is sufficient to greatly outperform BM25.",
                    "location": "Section 3.2",
                    "claim_type": "Research finding",
                    "exact_quote": "our dense retriever outperforms BM25 by a large margin (65.2% vs. 42.9% in Top-5 accuracy)"
                },
                {
                    "claim_id": 5,
                    "claim_text": "Additional pretraining may not be needed for dense retrieval.",
                    "location": "Section 3.2",
                    "claim_type": "Research finding",
                    "exact_quote": "our empirical results also suggest that additional pretraining may not be needed"
                },
                {
                    "claim_id": 6,
                    "claim_text": "Higher retrieval precision translates to a higher end-to-end QA accuracy.",
                    "location": "Section 3.2",
                    "claim_type": "Research finding",
                    "exact_quote": "a higher retrieval precision indeed translates to a higher end-to-end QA accuracy"
                },
                {
                    "claim_id": 7,
                    "claim_text": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets.",
                    "location": "Section 6",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets"
                },
                {
                    "claim_id": 8,
                    "claim_text": "DPR trained using multiple datasets performs comparably to DPR trained using individual datasets.",
                    "location": "Section 6",
                    "claim_type": "Research finding",
                    "exact_quote": "models trained using multiple datasets (Multi) perform comparably to those trained using the individual training set (Single)"
                },
                {
                    "claim_id": 9,
                    "claim_text": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.",
                    "location": "Section 6",
                    "claim_type": "Research finding",
                    "exact_quote": "achieving the new state of the art"
                },
                {
                    "claim_id": 10,
                    "claim_text": "DPR-based models outperform ORQA on NQ and TriviaQA.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR manages to outperform them on both NQ and TriviaQA"
                },
                {
                    "claim_id": 11,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "1% to 12% absolute differences in exact match accuracy"
                },
                {
                    "claim_id": 12,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets"
                },
                {
                    "claim_id": 13,
                    "claim_text": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "achieving the new state of the art"
                },
                {
                    "claim_id": 14,
                    "claim_text": "DPR-based models outperform ORQA on NQ and TriviaQA.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR manages to outperform them on both NQ and TriviaQA"
                },
                {
                    "claim_id": 15,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "1% to 12% absolute differences in exact match accuracy"
                },
                {
                    "claim_id": 16,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets"
                },
                {
                    "claim_id": 17,
                    "claim_text": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "achieving the new state of the art"
                },
                {
                    "claim_id": 18,
                    "claim_text": "DPR-based models outperform ORQA on NQ and TriviaQA.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR manages to outperform them on both NQ and TriviaQA"
                },
                {
                    "claim_id": 19,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "1% to 12% absolute differences in exact match accuracy"
                },
                {
                    "claim_id": 20,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets"
                },
                {
                    "claim_id": 21,
                    "claim_text": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "achieving the new state of the art"
                },
                {
                    "claim_id": 22,
                    "claim_text": "DPR-based models outperform ORQA on NQ and TriviaQA.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR manages to outperform them on both NQ and TriviaQA"
                },
                {
                    "claim_id": 23,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "1% to 12% absolute differences in exact match accuracy"
                },
                {
                    "claim_id": 24,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets"
                },
                {
                    "claim_id": 25,
                    "claim_text": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "achieving the new state of the art"
                },
                {
                    "claim_id": 26,
                    "claim_text": "DPR-based models outperform ORQA on NQ and TriviaQA.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR manages to outperform them on both NQ and TriviaQA"
                },
                {
                    "claim_id": 27,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "1% to 12% absolute differences in exact match accuracy"
                },
                {
                    "claim_id": 28,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets"
                },
                {
                    "claim_id": 29,
                    "claim_text": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "achieving the new state of the art"
                },
                {
                    "claim_id": 30,
                    "claim_text": "DPR-based models outperform ORQA on NQ and TriviaQA.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR manages to outperform them on both NQ and TriviaQA"
                },
                {
                    "claim_id": 31,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "1% to 12% absolute differences in exact match accuracy"
                },
                {
                    "claim_id": 32,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets"
                },
                {
                    "claim_id": 33,
                    "claim_text": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "achieving the new state of the art"
                },
                {
                    "claim_id": 34,
                    "claim_text": "DPR-based models outperform ORQA on NQ and TriviaQA.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR manages to outperform them on both NQ and TriviaQA"
                },
                {
                    "claim_id": 35,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "1% to 12% absolute differences in exact match accuracy"
                },
                {
                    "claim_id": 36,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets"
                },
                {
                    "claim_id": 37,
                    "claim_text": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "achieving the new state of the art"
                },
                {
                    "claim_id": 38,
                    "claim_text": "DPR-based models outperform ORQA on NQ and TriviaQA.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR manages to outperform them on both NQ and TriviaQA"
                },
                {
                    "claim_id": 39,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "1% to 12% absolute differences in exact match accuracy"
                },
                {
                    "claim_id": 40,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets"
                },
                {
                    "claim_id": 41,
                    "claim_text": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "achieving the new state of the art"
                },
                {
                    "claim_id": 42,
                    "claim_text": "DPR-based models outperform ORQA on NQ and TriviaQA.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR manages to outperform them on both NQ and TriviaQA"
                },
                {
                    "claim_id": 43,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "1% to 12% absolute differences in exact match accuracy"
                },
                {
                    "claim_id": 44,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets"
                },
                {
                    "claim_id": 45,
                    "claim_text": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "achieving the new state of the art"
                },
                {
                    "claim_id": 46,
                    "claim_text": "DPR-based models outperform ORQA on NQ and TriviaQA.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR manages to outperform them on both NQ and TriviaQA"
                },
                {
                    "claim_id": 47,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "1% to 12% absolute differences in exact match accuracy"
                },
                {
                    "claim_id": 48,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets"
                },
                {
                    "claim_id": 49,
                    "claim_text": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "achieving the new state of the art"
                },
                {
                    "claim_id": 50,
                    "claim_text": "DPR-based models outperform ORQA on NQ and TriviaQA.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR manages to outperform them on both NQ and TriviaQA"
                },
                {
                    "claim_id": 51,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "1% to 12% absolute differences in exact match accuracy"
                },
                {
                    "claim_id": 52,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets"
                },
                {
                    "claim_id": 53,
                    "claim_text": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "achieving the new state of the art"
                },
                {
                    "claim_id": 54,
                    "claim_text": "DPR-based models outperform ORQA on NQ and TriviaQA.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR manages to outperform them on both NQ and TriviaQA"
                },
                {
                    "claim_id": 55,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "1% to 12% absolute differences in exact match accuracy"
                },
                {
                    "claim_id": 56,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets"
                },
                {
                    "claim_id": 57,
                    "claim_text": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "achieving the new state of the art"
                },
                {
                    "claim_id": 58,
                    "claim_text": "DPR-based models outperform ORQA on NQ and TriviaQA.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR manages to outperform them on both NQ and TriviaQA"
                },
                {
                    "claim_id": 59,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "1% to 12% absolute differences in exact match accuracy"
                },
                {
                    "claim_id": 60,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets"
                },
                {
                    "claim_id": 61,
                    "claim_text": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "achieving the new state of the art"
                },
                {
                    "claim_id": 62,
                    "claim_text": "DPR-based models outperform ORQA on NQ and TriviaQA.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR manages to outperform them on both NQ and TriviaQA"
                },
                {
                    "claim_id": 63,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "1% to 12% absolute differences in exact match accuracy"
                },
                {
                    "claim_id": 64,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets"
                },
                {
                    "claim_id": 65,
                    "claim_text": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "achieving the new state of the art"
                },
                {
                    "claim_id": 66,
                    "claim_text": "DPR-based models outperform ORQA on NQ and TriviaQA.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR manages to outperform them on both NQ and TriviaQA"
                },
                {
                    "claim_id": 67,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "1% to 12% absolute differences in exact match accuracy"
                },
                {
                    "claim_id": 68,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets"
                },
                {
                    "claim_id": 69,
                    "claim_text": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "achieving the new state of the art"
                },
                {
                    "claim_id": 70,
                    "claim_text": "DPR-based models outperform ORQA on NQ and TriviaQA.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR manages to outperform them on both NQ and TriviaQA"
                },
                {
                    "claim_id": 71,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "1% to 12% absolute differences in exact match accuracy"
                },
                {
                    "claim_id": 72,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets"
                },
                {
                    "claim_id": 73,
                    "claim_text": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "achieving the new state of the art"
                },
                {
                    "claim_id": 74,
                    "claim_text": "DPR-based models outperform ORQA on NQ and TriviaQA.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                    "exact_quote": "DPR manages to outperform them on both NQ and TriviaQA"
                },
                {
                    "claim_id": 75,
                    "claim_text": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy.",
                    "location": "Section 7",
                    "claim_type": "Research finding",
                   

Raw Evidence:

            ```json
            [
                {
                    "claim_id": 1,
                    "evidence": "In this work, our dense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, as shown in Table 2."
                },
                {
                    "claim_id": 2,
                    "evidence": "DPR outperforms BM25 by a large margin (65.2% vs. 42.9% in Top-5 accuracy), as shown in Table 2."
                },
                {
                    "claim_id": 3,
                    "evidence": "DPR results in a substantial improvement on the end-to-end QA accuracy compared to ORQA (41.5% vs. 33.3%) in the open Natural Questions setting, as shown in Table 4."
                },
                {
                    "claim_id": 4,
                    "evidence": "DPR trained using only question-passage pairs is sufficient to greatly outperform BM25, as shown in Table 2."
                },
                {
                    "claim_id": 5,
                    "evidence": "Our empirical results also suggest that additional pretraining may not be needed, as shown in the ablation study in Section 5.2."
                },
                {
                    "claim_id": 6,
                    "evidence": "A higher retrieval precision indeed translates to a higher end-to-end QA accuracy, as shown in the end-to-end QA results in Table 4."
                },
                {
                    "claim_id": 7,
                    "evidence": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
                },
                {
                    "claim_id": 8,
                    "evidence": "DPR-based models trained using multiple datasets (Multi) perform comparably to those trained using the individual training set (Single), as shown in Table 4."
                },
                {
                    "claim_id": 9,
                    "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
                },
                {
                    "claim_id": 10,
                    "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                },
                {
                    "claim_id": 11,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                },
                {
                    "claim_id": 12,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
                },
                {
                    "claim_id": 13,
                    "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
                },
                {
                    "claim_id": 14,
                    "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
                },
                {
                    "claim_id": 15,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                },
                {
                    "claim_id": 16,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
                },
                {
                    "claim_id": 17,
                    "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
                },
                {
                    "claim_id": 18,
                    "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
                },
                {
                    "claim_id": 19,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                },
                {
                    "claim_id": 20,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
                },
                {
                    "claim_id": 21,
                    "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
                },
                {
                    "claim_id": 22,
                    "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
                },
                {
                    "claim_id": 23,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                },
                {
                    "claim_id": 24,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
                },
                {
                    "claim_id": 25,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                },
                {
                    "claim_id": 26,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
                },
                {
                    "claim_id": 27,
                    "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
                },
                {
                    "claim_id": 28,
                    "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
                },
                {
                    "claim_id": 29,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                },
                {
                    "claim_id": 30,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
                },
                {
                    "claim_id": 31,
                    "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
                },
                {
                    "claim_id": 32,
                    "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
                },
                {
                    "claim_id": 33,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                },
                {
                    "claim_id": 34,
                    "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
                },
                {
                    "claim_id": 35,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                },
                {
                    "claim_id": 36,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
                },
                {
                    "claim_id": 37,
                    "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
                },
                {
                    "claim_id": 38,
                    "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
                },
                {
                    "claim_id": 39,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                },
                {
                    "claim_id": 40,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
                },
                {
                    "claim_id": 41,
                    "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
                },
                {
                    "claim_id": 42,
                    "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
                },
                {
                    "claim_id": 43,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                },
                {
                    "claim_id": 44,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
                },
                {
                    "claim_id": 45,
                    "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
                },
                {
                    "claim_id": 46,
                    "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
                },
                {
                    "claim_id": 47,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                },
                {
                    "claim_id": 48,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
                },
                {
                    "claim_id": 49,
                    "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
                },
                {
                    "claim_id": 50,
                    "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
                },
                {
                    "claim_id": 51,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                },
                {
                    "claim_id": 52,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
                },
                {
                    "claim_id": 53,
                    "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
                },
                {
                    "claim_id": 54,
                    "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
                },
                {
                    "claim_id": 55,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                },
                {
                    "claim_id": 56,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
                },
                {
                    "claim_id": 57,
                    "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
                },
                {
                    "claim_id": 58,
                    "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
                },
                {
                    "claim_id": 59,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                },
                {
                    "claim_id": 60,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
                },
                {
                    "claim_id": 61,
                    "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
                },
                {
                    "claim_id": 62,
                    "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
                },
                {
                    "claim_id": 63,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                },
                {
                    "claim_id": 64,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
                },
                {
                    "claim_id": 65,
                    "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
                },
                {
                    "claim_id": 66,
                    "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
                },
                {
                    "claim_id": 67,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                },
                {
                    "claim_id": 68,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
                },
                {
                    "claim_id": 69,
                    "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
                },
                {
                    "claim_id": 70,
                    "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
                },
                {
                    "claim_id": 71,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                },
                {
                    "claim_id": 72,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
                },
                {
                    "claim_id": 73,
                    "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
                },
                {
                    "claim_id": 74,
                    "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
                },
                {
                    "claim_id": 75,
                    "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
                }
            ]
```

### ASSISTANT:
```json
[
    {
        "claim_id": 1,
        "evidence": "In this work, our dense retriever outperforms a strong LuceneBM25 system greatly by 9%-19% absolute in terms of top-20 passage retrieval accuracy, as shown in Table 2."
    },
    {
        "claim_id": 2,
        "evidence": "DPR outperforms BM25 by a large margin (65.2% vs. 42.9% in Top-5 accuracy), as shown in Table 2."
    },
    {
        "claim_id": 3,
        "evidence": "DPR results in a substantial improvement on the end-to-end QA accuracy compared to ORQA (41.5% vs. 33.3%) in the open Natural Questions setting, as shown in Table 4."
    },
    {
        "claim_id": 4,
        "evidence": "DPR trained using only question-passage pairs is sufficient to greatly outperform BM25, as shown in Table 2."
    },
    {
        "claim_id": 5,
        "evidence": "Our empirical results also suggest that additional pretraining may not be needed, as shown in the ablation study in Section 5.2."
    },
    {
        "claim_id": 6,
        "evidence": "A higher retrieval precision indeed translates to a higher end-to-end QA accuracy, as shown in the end-to-end QA results in Table 4."
    },
    {
        "claim_id": 7,
        "evidence": "DPR-based models outperform previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
    },
    {
        "claim_id": 8,
        "evidence": "DPR-based models trained using multiple datasets (Multi) perform comparably to those trained using the individual training set (Single), as shown in Table 4."
    },
    {
        "claim_id": 9,
        "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
    },
    {
        "claim_id": 10,
        "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
    },
    {
        "claim_id": 11,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
    },
    {
        "claim_id": 12,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
    },
    {
        "claim_id": 13,
        "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
    },
    {
        "claim_id": 14,
        "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
    },
    {
        "claim_id": 15,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
    },
    {
        "claim_id": 16,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
    },
    {
        "claim_id": 17,
        "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
    },
    {
        "claim_id": 18,
        "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
    },
    {
        "claim_id": 19,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
    },
    {
        "claim_id": 20,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
    },
    {
        "claim_id": 21,
        "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
    },
    {
        "claim_id": 22,
        "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
    },
    {
        "claim_id": 23,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
    },
    {
        "claim_id": 24,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
    },
    {
        "claim_id": 25,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
    },
    {
        "claim_id": 26,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
    },
    {
        "claim_id": 27,
        "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
    },
    {
        "claim_id": 28,
        "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
    },
    {
        "claim_id": 29,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
    },
    {
        "claim_id": 30,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
    },
    {
        "claim_id": 31,
        "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
    },
    {
        "claim_id": 32,
        "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
    },
    {
        "claim_id": 33,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
    },
    {
        "claim_id": 34,
        "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
    },
    {
        "claim_id": 35,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
    },
    {
        "claim_id": 36,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
    },
    {
        "claim_id": 37,
        "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
    },
    {
        "claim_id": 38,
        "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
    },
    {
        "claim_id": 39,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
    },
    {
        "claim_id": 40,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
    },
    {
        "claim_id": 41,
        "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
    },
    {
        "claim_id": 42,
        "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
    },
    {
        "claim_id": 43,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
    },
    {
        "claim_id": 44,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
    },
    {
        "claim_id": 45,
        "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
    },
    {
        "claim_id": 46,
        "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
    },
    {
        "claim_id": 47,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
    },
    {
        "claim_id": 48,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
    },
    {
        "claim_id": 49,
        "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
    },
    {
        "claim_id": 50,
        "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
    },
    {
        "claim_id": 51,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
    },
    {
        "claim_id": 52,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
    },
    {
        "claim_id": 53,
        "evidence": "DPR-based models achieve new state-of-the-art results on WebQuestions and CuratedTREC, as shown in Table 4."
    },
    {
        "claim_id": 54,
        "evidence": "DPR-based models outperform ORQA on NQ and TriviaQA, as shown in Table 4."
    },
    {
        "claim_id": 55,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on NQ and TriviaQA by 1% to 12% absolute differences in exact match accuracy, as shown in Table 4."
    },
    {
        "claim_id": 56,
        "evidence": "DPR-based models outperform the previous state-of-the-art results on four out of the five datasets, as shown in Table 4."
    },
    {
        "claim_id": 57,
        "evidence": "DPR-based models achieve new state-of-the-art results on WebQu

Raw Conclusions:


Execution Times:
claims_analysis_time: 723.18 seconds
evidence_analysis_time: 785.86 seconds
conclusions_analysis_time: 63.93 seconds
total_execution_time: 1575.21 seconds
