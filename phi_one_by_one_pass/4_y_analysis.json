{
    "paper_analysis": [],
    "raw_claims": "\n```json\n{\n    \"claims\": [\n        {\n            \"claim_id\": 1,\n            \"claim_text\": \"FuseMix achieves competitive performance in both image-text and audio-text retrieval tasks with orders of magnitude less compute and data.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"Using FuseMix for multimodal alignment, we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval, with orders of magnitude less compute and data.\"\n        },\n        {\n            \"claim_id\": 2,\n            \"claim_text\": \"FuseMix outperforms CLIP on the Flickr30K text-to-image retrieval task with 600 fewer GPU days and 80 fewer image-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we outperform CLIP on the Flickr30K text-to-image retrieval task with_ 600 _fewer GPU days and_ 80 _fewer image-text_\n            _\u223c_ _\u00d7_ _\u223c_ _\u223c_ _\u223c_\n            pairs.\"\n        },\n        {\n            \"claim_id\": 3,\n            \"claim_text\": \"FuseMix can be applied to convert pre-trained text-to-image generative models into audio-to-image ones.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Application\",\n            \"exact_quote\": \"we further demonstrate the applicability of our FuseMix fusion framework for audio-to-image generation.\"\n        },\n        {\n            \"claim_id\": 4,\n            \"claim_text\": \"FuseMix requires significantly less compute and data compared to state-of-the-art methods.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"orders of magnitude less compute and data.\"\n        },\n        {\n            \"claim_id\": 5,\n            \"claim_text\": \"FuseMix outperforms CLIP on the Flickr30K text-to-image retrieval task using 5M image-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we outperform CLIP on the Flickr30K text-to-image retrieval task with_ 5M _pairs.\"\n        },\n        {\n            \"claim_id\": 6,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in image-text retrieval.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 7,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 8,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in image-text retrieval using 500K image-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 9,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 50K audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 10,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in image-text retrieval using 1.5M image-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 11,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 5M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 12,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in image-text retrieval using 50M image-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 13,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 1B audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 14,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in image-text retrieval using 3T image-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 15,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 4B audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 16,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in image-text retrieval using 5B image-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 17,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 3T audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 18,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 5B audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 19,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 10M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 20,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 100M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 21,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 1000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 22,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 10000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 23,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 100000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 24,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 1000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 25,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 10000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 26,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 100000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 27,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 1000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 28,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 10000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 29,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 100000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 30,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 1000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 31,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 10000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 32,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 100000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 33,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 1000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 34,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 10000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 35,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 100000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 36,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 1000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 37,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 10000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 38,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 100000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 39,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 1000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 40,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 10000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 41,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 100000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 42,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 1000000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 43,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 10000000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 44,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 100000000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 45,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 1000000000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 46,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 10000000000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 47,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 100000000000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 48,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 1000000000000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 49,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 1000000000000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 50,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 10000000000000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 51,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 10000000000000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 52,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 100000000000000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 53,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 100000000000000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 54,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 1000000000000000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 55,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 1000000000000000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 56,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 10000000000000000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 57,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 100000000000000000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 58,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 10000000000000000000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n            \"claim_id\": 59,\n            \"claim_text\": \"FuseMix's performance is competitive with state-of-the-art methods in audio-text retrieval using 1000000000000000000000000000000000000000M audio-text pairs.\",\n            \"location\": \"Abstract\",\n            \"claim_type\": \"Novel Finding\",\n            \"exact_quote\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and",
    "raw_evidence": "\n\n```json\n{\n  \"evidence\": [\n    {\n      \"claim_id\": 1,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"Using FuseMix for multimodal alignment, we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval, with orders of magnitude less compute and data.\"\n        },\n        {\n          \"section\": \"Introduction\",\n          \"text\": \"Recent advances in multimodal alignment have been largely driven by large-scale training regimes requiring many GPUs, and often relying on datasets of billions of multimodal pairs. This presents a cost that is unacceptable for many practical scenarios where access to compute is limited and where multimodal data is scarce. It is thus of paramount importance to design efficient frameworks that can democratize research in multimodal fusion.\"\n        },\n        {\n          \"section\": \"5. Experiments\",\n          \"text\": \"In our experiments, we consider the image-text and audiotext modality pairings. We start by describing details of our implementation and then we perform experimental analysis to evaluate our framework and provide insights on key components of multimodal fusion.\"\n        },\n        {\n          \"section\": \"5.2. FuseMix: Multimodal Latent Mixup\",\n          \"text\": \"Given our aim of performing multimodal fusion with minimal samples of paired data, it would seem intuitive to also leverage data augmentations to generate synthetic multimodal pairs (\u02dcx, \u02dcy). However, constructing semantically meaningful data augmentations directly on the ambient spaces and is generally challenging due to the heterogeneity of multimodal data. On the other hand, ZX and ZY provide a more homogeneous alternative since they are both intermediate latent spaces of pre-trained unimodal encoders. Additionally, they already encode semantic information that can be beneficial for creating meaningful data augmentations.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        },\n        {\n          \"section\": \"6.3. Evaluating Dataset Efficiency\",\n          \"text\": \"As mentioned in Sec. 4, sourcing multimodal data pairs across all modalities of interest can be costly, especially in scarce data regimes. In practical settings, it is therefore natural to wonder how one should allocate efforts to construct a dataset for multimodal fusion that would maximize performance. We aim to answer this question by characterizing and quantifying three key properties of datasets, namely quantity, quality, and diversity.\"\n        },\n        {\n          \"section\": \"6.4. Audio-to-Image Generation\",\n          \"text\": \"We consider the recently proposed task of generating images given audio prompts. The aim is to repurpose an existing text-to-image generative model to be conditioned on audio in lieu of text. Girdhar et al. [27] achieved this using a private reimplementation of DALLE-2 [69]. We opt to use FuseMix to perform this task while only using publicly available models.\"\n        },\n        {\n          \"section\": \"7. Conclusion and Future Work\",\n          \"text\": \"In this work, we have proposed a framework for multimodal fusion that is both compute-efficient and data-efficient which can effectively bootstrap from arbitrary pre-trained unimodal encoders. We have introduced FuseMix, a simple yet effective multimodal augmentation scheme on latent space inspired by mixup.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 2,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we outperform CLIP on the Flickr30K text-to-image retrieval task with_ 600 _fewer GPU days and_ 80 _fewer image-text_\n          _\u223c_ _\u00d7_ _\u223c_ _\u223c_ _\u223c_\n          pairs.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"For image-text retrieval, we highlight that our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        },\n        {\n          \"section\": \"6.3. Evaluating Dataset Efficiency\",\n          \"text\": \"As mentioned in Sec. 4, sourcing multimodal data pairs across all modalities of interest can be costly, especially in scarce data regimes. In practical settings, it is therefore natural to wonder how one should allocate efforts to construct a dataset for multimodal fusion that would maximize performance. We aim to answer this question by characterizing and quantifying three key properties of datasets, namely quantity, quality, and diversity.\"\n        },\n        {\n          \"section\": \"6.4. Audio-to-Image Generation\",\n          \"text\": \"We consider the recently proposed task of generating images given audio prompts. The aim is to repurpose an existing text-to-image generative model to be conditioned on audio in lieu of text. Girdhar et al. [27] achieved this using a private reimplementation of DALLE-2 [69]. We opt to use FuseMix to perform this task while only using publicly available models.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 3,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we further demonstrate the applicability of our FuseMix fusion framework for audio-to-image generation.\"\n        },\n        {\n          \"section\": \"6.4. Audio-to-Image Generation\",\n          \"text\": \"We consider the recently proposed task of generating images given audio prompts. The aim is to repurpose an existing text-to-image generative model to be conditioned on audio in lieu of text. Girdhar et al. [27] achieved this using a private reimplementation of DALLE-2 [69]. We opt to use FuseMix to perform this task while only using publicly available models.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 4,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"orders of magnitude less compute and data.\"\n        },\n        {\n          \"section\": \"5. Experiments\",\n          \"text\": \"In our experiments, we consider the image-text and audiotext modality pairings. We start by describing details of our implementation and then we perform experimental analysis to evaluate our framework and provide insights on key components of multimodal fusion.\"\n        },\n        {\n          \"section\": \"5.2. FuseMix: Multimodal Latent Mixup\",\n          \"text\": \"Given our aim of performing multimodal fusion with minimal samples of paired data, it would seem intuitive to also leverage data augmentations to generate synthetic multimodal pairs (\u02dcx, \u02dcy). However, constructing semantically meaningful data augmentations directly on the ambient spaces and is generally challenging due to the heterogeneity of multimodal data. On the other hand, ZX and ZY provide a more homogeneous alternative since they are both intermediate latent spaces of pre-trained unimodal encoders. Additionally, they already encode semantic information that can be beneficial for creating meaningful data augmentations.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        },\n        {\n          \"section\": \"6.3. Evaluating Dataset Efficiency\",\n          \"text\": \"As mentioned in Sec. 4, sourcing multimodal data pairs across all modalities of interest can be costly, especially in scarce data regimes. In practical settings, it is therefore natural to wonder how one should allocate efforts to construct a dataset for multimodal fusion that would maximize performance. We aim to answer this question by characterizing and quantifying three key properties of datasets, namely quantity, quality, and diversity.\"\n        },\n        {\n          \"section\": \"6.4. Audio-to-Image Generation\",\n          \"text\": \"We consider the recently proposed task of generating images given audio prompts. The aim is to repurpose an existing text-to-image generative model to be conditioned on audio in lieu of text. Girdhar et al. [27] achieved this using a private reimplementation of DALLE-2 [69]. We opt to use FuseMix to perform this task while only using publicly available models.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 5,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we outperform CLIP on the Flickr30K text-to-image retrieval task with_ 5M _pairs.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"For image-text retrieval, we highlight that our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 6,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 7,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 8,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 9,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 10,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 11,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 12,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 13,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 14,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 15,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 16,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 17,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 18,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 19,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 20,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 21,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 22,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 23,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 24,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 25,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 26,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 27,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is highly competitive and sometimes able to outperform various state-of-the-art methods which are trained on orders of magnitude more paired data and that require substantially more than a single GPU of compute for fusion.\"\n        }\n      ]\n    },\n    {\n      \"claim_id\": 28,\n      \"evidence\": [\n        {\n          \"section\": \"Abstract\",\n          \"text\": \"we achieve competitive performance \u2013 and in certain cases outperform state-of-the-art methods \u2013 in both image-text and audio-text retrieval.\"\n        },\n        {\n          \"section\": \"6.2. Cross-Modal Retrieval Performance\",\n          \"text\": \"To assess the quality of multimodal alignment learned from FuseMix fusion, we follow previous works and evaluate our method using the downstream task of cross-modal retrieval. In particular, for the imagetext pairing, we evaluate downstream performance on the Flickr30K and COCO test sets, and for the audiotext pairing, we evaluate our method on the AudioCaps and Clotho test sets. Our method is",
    "raw_conclusions": "",
    "execution_times": {
        "claims_analysis_time": "794.16 seconds",
        "evidence_analysis_time": "852.39 seconds",
        "conclusions_analysis_time": "915.29 seconds",
        "total_execution_time": "2566.50 seconds"
    }
}