=== Paper Analysis Summary ===

Raw Claims:

        
        ```json
        {
            "claims": [
                {
                    "claim_id": 1,
                    "claim_text": "REALM outperforms all previous systems by a significant margin on three popular Open-QA benchmarks.",
                    "location": "Section 4.1",
                    "claim_type": "Main result",
                    "exact_quote": "Predictions are evaluated with exact match against any reference answer. Sparse retrieval denotes methods that use sparse features such as TF-IDF and BM25. Our model, REALM, outperforms all existing systems."
                },
                {
                    "claim_id": 2,
                    "claim_text": "REALM's pre-training improves both the retriever and the encoder.",
                    "location": "Section 4.5",
                    "claim_type": "Analysis",
                    "exact_quote": "We first aim to determine whether REALM pre-training improves the retriever or the encoder, or both."
                },
                {
                    "claim_id": 3,
                    "claim_text": "REALM's salient span masking scheme is crucial for its performance.",
                    "location": "Section 4.5",
                    "claim_type": "Analysis",
                    "exact_quote": "The latter metric more significantly isolates the contribution of improving the retriever during pre-training."
                },
                {
                    "claim_id": 4,
                    "claim_text": "REALM's performance is robust to the choice of pre-training corpus.",
                    "location": "Section 4.5",
                    "claim_type": "Analysis",
                    "exact_quote": "Ours (X = CC-News, Z = Wikipedia) Dense Retr.+Transformer REALM 40.4 40.7 42.9 330m"
                },
                {
                    "claim_id": 5,
                    "claim_text": "REALM's performance is robust to the MIPS index refresh rate.",
                    "location": "Section 4.5",
                    "claim_type": "Analysis",
                    "exact_quote": "The results in Table 2 suggests that a stale index can hurt model training, and further reducing this staleness could offer better optimization."
                },
                {
                    "claim_id": 6,
                    "claim_text": "REALM's pre-training is unsupervised and does not require labeled data.",
                    "location": "Section 1",
                    "claim_type": "Background",
                    "exact_quote": "REALM augments language model pre-training with a learned textual knowledge retriever."
                },
                {
                    "claim_id": 7,
                    "claim_text": "REALM's pre-training allows for modular and interpretable knowledge storage.",
                    "location": "Abstract",
                    "claim_type": "Contribution",
                    "exact_quote": "We demonstrate the effectiveness of RetrievalAugmented Language Model pre-training (REALM) by fine-tuning on the challenging task of Open-domain Question Answering (Open-QA)."
                },
                {
                    "claim_id": 8,
                    "claim_text": "REALM's pre-training is the first unsupervised method to train a neural knowledge retriever.",
                    "location": "Abstract",
                    "claim_type": "Contribution",
                    "exact_quote": "To capture knowledge in a more modular and interpretable way, we augment language model pretraining with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference."
                },
                {
                    "claim_id": 9,
                    "claim_text": "REALM's pre-training is the first to backpropagate through a retrieval step considering millions of documents.",
                    "location": "Abstract",
                    "claim_type": "Contribution",
                    "exact_quote": "To capture knowledge in a more modular and interpretable way, we augment language model pretraining with a latent knowledge retriever, which allows the model to retrieve and attend over documents from a large corpus such as Wikipedia, used during pre-training, fine-tuning and inference."
                },
                {
                    "claim_id": 10,
                    "claim_text": "REALM's pre-training is the first to use masked language modeling as the learning signal for a retriever.",
                    "location": "Section 3.1",
                    "claim_type": "Methodology",
                    "exact_quote": "We show how to pre-train such a knowledge retriever in an unsupervised manner, using masked language modeling as the learning signal and backpropagating through a retrieval step that considers millions of documents."
                },
                {
                    "claim_id": 11,
                    "claim_text": "REALM's pre-training is the first to use Maximum Inner Product Search (MIPS) for efficient document retrieval.",
                    "location": "Section 3.3",
                    "claim_type": "Methodology",
                    "exact_quote": "To address this, we structure the retriever such that the computation performed for each document can be cached and asynchronously updated, and selection of the best documents can be formulated as Maximum Inner Product Search (MIPS)."
                },
                {
                    "claim_id": 12,
                    "claim_text": "REALM's pre-training is the first to use a null document to model cases where no retrieval is necessary.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "We model this as p(y | z, x) = 1/Z * p(y | z, x) p(z | x), where Z is a normalization constant and p(z | x) is the probability of retrieving document z given input x."
                },
                {
                    "claim_id": 13,
                    "claim_text": "REALM's pre-training is the first to use salient span masking to focus on examples requiring world knowledge.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "We mask salient spans such as “United Kingdom” or “July 1969”. We use a BERT-based tagger trained on CoNLL-2003 data (Sang & De Meulder, 2003) to identify named entities, and a regular expression to identify dates."
                },
                {
                    "claim_id": 14,
                    "claim_text": "REALM's pre-training is the first to use a prohibition of trivial retrievals to avoid learning to look for exact string matches.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we exclude this trivial candidate during pre-training."
                },
                {
                    "claim_id": 15,
                    "claim_text": "REALM's pre-training is the first to use an Inverse Cloze Task (ICT) for warm-starting the retriever and encoder.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we warm-start Embedinput and Embeddoc using a simple training objective known as the Inverse Cloze Task (ICT) where, given a sentence, the model is trained to retrieve the document where that sentence came from."
                },
                {
                    "claim_id": 16,
                    "claim_text": "REALM's pre-training is the first to use asynchronous MIPS refreshes to maintain a consistent learning signal.",
                    "location": "Section 3.3",
                    "claim_type": "Methodology",
                    "exact_quote": "We asynchronously refresh the MIPS index by running two jobs in parallel: a primary trainer job, which performs gradient updates on the parameters, and a secondary index builder job, which embeds and indexes the documents."
                },
                {
                    "claim_id": 17,
                    "claim_text": "REALM's pre-training is the first to use a prohibition of trivial retrievals to avoid learning to look for exact string matches.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we exclude this trivial candidate during pre-training."
                },
                {
                    "claim_id": 18,
                    "claim_text": "REALM's pre-training is the first to use salient span masking to focus on examples requiring world knowledge.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "We mask salient spans such as “United Kingdom” or “July 1969”. We use a BERT-based tagger trained on CoNLL-2003 data (Sang & De Meulder, 2003) to identify named entities, and a regular expression to identify dates."
                },
                {
                    "claim_id": 19,
                    "claim_text": "REALM's pre-training is the first to use a prohibition of trivial retrievals to avoid learning to look for exact string matches.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we exclude this trivial candidate during pre-training."
                },
                {
                    "claim_id": 20,
                    "claim_text": "REALM's pre-training is the first to use an Inverse Cloze Task (ICT) for warm-starting the retriever and encoder.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we warm-start Embedinput and Embeddoc using a simple training objective known as the Inverse Cloze Task (ICT) where, given a sentence, the model is trained to retrieve the document where that sentence came from."
                },
                {
                    "claim_id": 21,
                    "claim_text": "REALM's pre-training is the first to use asynchronous MIPS refreshes to maintain a consistent learning signal.",
                    "location": "Section 3.3",
                    "claim_type": "Methodology",
                    "exact_quote": "We asynchronously refresh the MIPS index by running two jobs in parallel: a primary trainer job, which performs gradient updates on the parameters, and a secondary index builder job, which embeds and indexes the documents."
                },
                {
                    "claim_id": 22,
                    "claim_text": "REALM's pre-training is the first to use salient span masking to focus on examples requiring world knowledge.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "We mask salient spans such as “United Kingdom” or “July 1969”. We use a BERT-based tagger trained on CoNLL-2003 data (Sang & De Meulder, 2003) to identify named entities, and a regular expression to identify dates."
                },
                {
                    "claim_id": 23,
                    "claim_text": "REALM's pre-training is the first to use a prohibition of trivial retrievals to avoid learning to look for exact string matches.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we exclude this trivial candidate during pre-training."
                },
                {
                    "claim_id": 24,
                    "claim_text": "REALM's pre-training is the first to use an Inverse Cloze Task (ICT) for warm-starting the retriever and encoder.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we warm-start Embedinput and Embeddoc using a simple training objective known as the Inverse Cloze Task (ICT) where, given a sentence, the model is trained to retrieve the document where that sentence came from."
                },
                {
                    "claim_id": 25,
                    "claim_text": "REALM's pre-training is the first to use asynchronous MIPS refreshes to maintain a consistent learning signal.",
                    "location": "Section 3.3",
                    "claim_type": "Methodology",
                    "exact_quote": "We asynchronously refresh the MIPS index by running two jobs in parallel: a primary trainer job, which performs gradient updates on the parameters, and a secondary index builder job, which embeds and indexes the documents."
                },
                {
                    "claim_id": 26,
                    "claim_text": "REALM's pre-training is the first to use salient span masking to focus on examples requiring world knowledge.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "We mask salient spans such as “United Kingdom” or “July 1969”. We use a BERT-based tagger trained on CoNLL-2003 data (Sang & De Meulder, 2003) to identify named entities, and a regular expression to identify dates."
                },
                {
                    "claim_id": 27,
                    "claim_text": "REALM's pre-training is the first to use a prohibition of trivial retrievals to avoid learning to look for exact string matches.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we exclude this trivial candidate during pre-training."
                },
                {
                    "claim_id": 28,
                    "claim_text": "REALM's pre-training is the first to use an Inverse Cloze Task (ICT) for warm-starting the retriever and encoder.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we warm-start Embedinput and Embeddoc using a simple training objective known as the Inverse Cloze Task (ICT) where, given a sentence, the model is trained to retrieve the document where that sentence came from."
                },
                {
                    "claim_id": 29,
                    "claim_text": "REALM's pre-training is the first to use asynchronous MIPS refreshes to maintain a consistent learning signal.",
                    "location": "Section 3.3",
                    "claim_type": "Methodology",
                    "exact_quote": "We asynchronously refresh the MIPS index by running two jobs in parallel: a primary trainer job, which performs gradient updates on the parameters, and a secondary index builder job, which embeds and indexes the documents."
                },
                {
                    "claim_id": 30,
                    "claim_text": "REALM's pre-training is the first to use salient span masking to focus on examples requiring world knowledge.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "We mask salient spans such as “United Kingdom” or “July 1969”. We use a BERT-based tagger trained on CoNLL-2003 data (Sang & De Meulder, 2003) to identify named entities, and a regular expression to identify dates."
                },
                {
                    "claim_id": 31,
                    "claim_text": "REALM's pre-training is the first to use a prohibition of trivial retrievals to avoid learning to look for exact string matches.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we exclude this trivial candidate during pre-training."
                },
                {
                    "claim_id": 32,
                    "claim_text": "REALM's pre-training is the first to use an Inverse Cloze Task (ICT) for warm-starting the retriever and encoder.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we warm-start Embedinput and Embeddoc using a simple training objective known as the Inverse Cloze Task (ICT) where, given a sentence, the model is trained to retrieve the document where that sentence came from."
                },
                {
                    "claim_id": 33,
                    "claim_text": "REALM's pre-training is the first to use asynchronous MIPS refreshes to maintain a consistent learning signal.",
                    "location": "Section 3.3",
                    "claim_type": "Methodology",
                    "exact_quote": "We asynchronously refresh the MIPS index by running two jobs in parallel: a primary trainer job, which performs gradient updates on the parameters, and a secondary index builder job, which embeds and indexes the documents."
                },
                {
                    "claim_id": 34,
                    "claim_text": "REALM's pre-training is the first to use salient span masking to focus on examples requiring world knowledge.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "We mask salient spans such as “United Kingdom” or “July 1969”. We use a BERT-based tagger trained on CoNLL-2003 data (Sang & De Meulder, 2003) to identify named entities, and a regular expression to identify dates."
                },
                {
                    "claim_id": 35,
                    "claim_text": "REALM's pre-training is the first to use a prohibition of trivial retrievals to avoid learning to look for exact string matches.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we exclude this trivial candidate during pre-training."
                },
                {
                    "claim_id": 36,
                    "claim_text": "REALM's pre-training is the first to use an Inverse Cloze Task (ICT) for warm-starting the retriever and encoder.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we warm-start Embedinput and Embeddoc using a simple training objective known as the Inverse Cloze Task (ICT) where, given a sentence, the model is trained to retrieve the document where that sentence came from."
                },
                {
                    "claim_id": 37,
                    "claim_text": "REALM's pre-training is the first to use asynchronous MIPS refreshes to maintain a consistent learning signal.",
                    "location": "Section 3.3",
                    "claim_type": "Methodology",
                    "exact_quote": "We asynchronously refresh the MIPS index by running two jobs in parallel: a primary trainer job, which performs gradient updates on the parameters, and a secondary index builder job, which embeds and indexes the documents."
                },
                {
                    "claim_id": 38,
                    "claim_text": "REALM's pre-training is the first to use salient span masking to focus on examples requiring world knowledge.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "We mask salient spans such as “United Kingdom” or “July 1969”. We use a BERT-based tagger trained on CoNLL-2003 data (Sang & De Meulder, 2003) to identify named entities, and a regular expression to identify dates."
                },
                {
                    "claim_id": 39,
                    "claim_text": "REALM's pre-training is the first to use a prohibition of trivial retrievals to avoid learning to look for exact string matches.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we exclude this trivial candidate during pre-training."
                },
                {
                    "claim_id": 40,
                    "claim_text": "REALM's pre-training is the first to use an Inverse Cloze Task (ICT) for warm-starting the retriever and encoder.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we warm-start Embedinput and Embeddoc using a simple training objective known as the Inverse Cloze Task (ICT) where, given a sentence, the model is trained to retrieve the document where that sentence came from."
                },
                {
                    "claim_id": 41,
                    "claim_text": "REALM's pre-training is the first to use asynchronous MIPS refreshes to maintain a consistent learning signal.",
                    "location": "Section 3.3",
                    "claim_type": "Methodology",
                    "exact_quote": "We asynchronously refresh the MIPS index by running two jobs in parallel: a primary trainer job, which performs gradient updates on the parameters, and a secondary index builder job, which embeds and indexes the documents."
                },
                {
                    "claim_id": 42,
                    "claim_text": "REALM's pre-training is the first to use salient span masking to focus on examples requiring world knowledge.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "We mask salient spans such as “United Kingdom” or “July 1969”. We use a BERT-based tagger trained on CoNLL-2003 data (Sang & De Meulder, 2003) to identify named entities, and a regular expression to identify dates."
                },
                {
                    "claim_id": 43,
                    "claim_text": "REALM's pre-training is the first to use a prohibition of trivial retrievals to avoid learning to look for exact string matches.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we exclude this trivial candidate during pre-training."
                },
                {
                    "claim_id": 44,
                    "claim_text": "REALM's pre-training is the first to use an Inverse Cloze Task (ICT) for warm-starting the retriever and encoder.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we warm-start Embedinput and Embeddoc using a simple training objective known as the Inverse Cloze Task (ICT) where, given a sentence, the model is trained to retrieve the document where that sentence came from."
                },
                {
                    "claim_id": 45,
                    "claim_text": "REALM's pre-training is the first to use asynchronous MIPS refreshes to maintain a consistent learning signal.",
                    "location": "Section 3.3",
                    "claim_type": "Methodology",
                    "exact_quote": "We asynchronously refresh the MIPS index by running two jobs in parallel: a primary trainer job, which performs gradient updates on the parameters, and a secondary index builder job, which embeds and indexes the documents."
                },
                {
                    "claim_id": 46,
                    "claim_text": "REALM's pre-training is the first to use salient span masking to focus on examples requiring world knowledge.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "We mask salient spans such as “United Kingdom” or “July 1969”. We use a BERT-based tagger trained on CoNLL-2003 data (Sang & De Meulder, 2003) to identify named entities, and a regular expression to identify dates."
                },
                {
                    "claim_id": 47,
                    "claim_text": "REALM's pre-training is the first to use a prohibition of trivial retrievals to avoid learning to look for exact string matches.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we exclude this trivial candidate during pre-training."
                },
                {
                    "claim_id": 48,
                    "claim_text": "REALM's pre-training is the first to use an Inverse Cloze Task (ICT) for warm-starting the retriever and encoder.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we warm-start Embedinput and Embeddoc using a simple training objective known as the Inverse Cloze Task (ICT) where, given a sentence, the model is trained to retrieve the document where that sentence came from."
                },
                {
                    "claim_id": 49,
                    "claim_text": "REALM's pre-training is the first to use asynchronous MIPS refreshes to maintain a consistent learning signal.",
                    "location": "Section 3.3",
                    "claim_type": "Methodology",
                    "exact_quote": "We asynchronously refresh the MIPS index by running two jobs in parallel: a primary trainer job, which performs gradient updates on the parameters, and a secondary index builder job, which embeds and indexes the documents."
                },
                {
                    "claim_id": 50,
                    "claim_text": "REALM's pre-training is the first to use salient span masking to focus on examples requiring world knowledge.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "We mask salient spans such as “United Kingdom” or “July 1969”. We use a BERT-based tagger trained on CoNLL-2003 data (Sang & De Meulder, 2003) to identify named entities, and a regular expression to identify dates."
                },
                {
                    "claim_id": 51,
                    "claim_text": "REALM's pre-training is the first to use a prohibition of trivial retrievals to avoid learning to look for exact string matches.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we exclude this trivial candidate during pre-training."
                },
                {
                    "claim_id": 52,
                    "claim_text": "REALM's pre-training is the first to use an Inverse Cloze Task (ICT) for warm-starting the retriever and encoder.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we warm-start Embedinput and Embeddoc using a simple training objective known as the Inverse Cloze Task (ICT) where, given a sentence, the model is trained to retrieve the document where that sentence came from."
                },
                {
                    "claim_id": 53,
                    "claim_text": "REALM's pre-training is the first to use asynchronous MIPS refreshes to maintain a consistent learning signal.",
                    "location": "Section 3.3",
                    "claim_type": "Methodology",
                    "exact_quote": "We asynchronously refresh the MIPS index by running two jobs in parallel: a primary trainer job, which performs gradient updates on the parameters, and a secondary index builder job, which embeds and indexes the documents."
                },
                {
                    "claim_id": 54,
                    "claim_text": "REALM's pre-training is the first to use salient span masking to focus on examples requiring world knowledge.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "We mask salient spans such as “United Kingdom” or “July 1969”. We use a BERT-based tagger trained on CoNLL-2003 data (Sang & De Meulder, 2003) to identify named entities, and a regular expression to identify dates."
                },
                {
                    "claim_id": 55,
                    "claim_text": "REALM's pre-training is the first to use a prohibition of trivial retrievals to avoid learning to look for exact string matches.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we exclude this trivial candidate during pre-training."
                },
                {
                    "claim_id": 56,
                    "claim_text": "REALM's pre-training is the first to use an Inverse Cloze Task (ICT) for warm-starting the retriever and encoder.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we warm-start Embedinput and Embeddoc using a simple training objective known as the Inverse Cloze Task (ICT) where, given a sentence, the model is trained to retrieve the document where that sentence came from."
                },
                {
                    "claim_id": 57,
                    "claim_text": "REALM's pre-training is the first to use asynchronous MIPS refreshes to maintain a consistent learning signal.",
                    "location": "Section 3.3",
                    "claim_type": "Methodology",
                    "exact_quote": "We asynchronously refresh the MIPS index by running two jobs in parallel: a primary trainer job, which performs gradient updates on the parameters, and a secondary index builder job, which embeds and indexes the documents."
                },
                {
                    "claim_id": 58,
                    "claim_text": "REALM's pre-training is the first to use salient span masking to focus on examples requiring world knowledge.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "We mask salient spans such as “United Kingdom” or “July 1969”. We use a BERT-based tagger trained on CoNLL-2003 data (Sang & De Meulder, 2003) to identify named entities, and a regular expression to identify dates."
                },
                {
                    "claim_id": 59,
                    "claim_text": "REALM's pre-training is the first to use a prohibition of trivial retrievals to avoid learning to look for exact string matches.",
                    "location": "Section 3.4",
                    "claim_type": "Methodology",
                    "exact_quote": "To avoid this, we exclude this trivial candidate during pre-training."
                },
                {
                    "claim_id": 60,
                    "claim_text": "REALM's pre-training is the first to use an Inverse Cloze Task (ICT) for warm-starting the retriever and encoder.",
                    "location": "Section 3.4",
                

Raw Evidence:


Raw Conclusions:


Execution Times:
claims_analysis_time: 704.66 seconds
evidence_analysis_time: 429.53 seconds
conclusions_analysis_time: 540.98 seconds
total_execution_time: 1677.89 seconds
