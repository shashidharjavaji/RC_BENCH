Claim 1:
Type: contribution
Statement: Attributed QA is a key first step in the development of attributed LLMs.
Location: Introduction
Exact Quote: We formulate and study Attributed QA as a key first step in the development of attributed LLMs.

Evidence:
- Evidence Text: The paper proposes a reproducible evaluation framework for Attributed QA and benchmarks a broad set of architectures.
  Strength: strong
  Location: Introduction
  Limitations: The paper does not discuss the limitations of the proposed evaluation framework.
  Exact Quote: We propose a reproducible evaluation framework for the task and benchmark a broad set of architectures.

- Evidence Text: The paper demonstrates that a correlated automatic metric (AutoAIS) is suitable for development.
  Strength: strong
  Location: Introduction
  Limitations: The paper does not discuss the limitations of using AutoAIS as a development metric.
  Exact Quote: We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The paper provides a reproducible evaluation framework and demonstrates the suitability of AutoAIS as a development metric.
Key Limitations: The paper does not discuss the limitations of the proposed evaluation framework or AutoAIS.

--------------------------------------------------

Claim 2:
Type: result
Statement: The paper provides concrete answers to three key questions: How to measure attribution?, How well do current state-of-the-art methods perform on attribution?, and How to build LLMs with attribution?
Location: Introduction
Exact Quote: Our experimental work gives concrete answers to two key questions (How to measure attribution?, and How well do current state-of-the-art methods perform on attribution?), and give some hints as to how to address a third (How to build LLMs with attribution?).

Evidence:
- Evidence Text: The paper proposes a reproducible evaluation framework for Attributed QA.
  Strength: strong
  Location: Introduction
  Limitations: The paper does not discuss the limitations of the proposed evaluation framework.
  Exact Quote: We propose a reproducible evaluation framework for the task and benchmark a broad set of architectures.

- Evidence Text: The paper demonstrates that a correlated automatic metric (AutoAIS) is suitable for development.
  Strength: strong
  Location: Introduction
  Limitations: The paper does not discuss the limitations of using AutoAIS as a development metric.
  Exact Quote: We find strong correlation between the two, making AutoAIS a suitable evaluation strategy in development settings.

- Evidence Text: The paper explores different architectures and levels of supervision for Attributed QA.
  Strength: strong
  Location: Approaches to Attributed QA
  Limitations: The paper does not discuss the limitations of the explored architectures and supervision levels.
  Exact Quote: We now describe the different systems investigated in this paper. At a high level, they fall into the following three architecture classes, and may be differentiated in terms of the type and quantity of supervision that is used.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The paper provides concrete answers to three key questions through the proposed evaluation framework, exploration of architectures, and the use of AutoAIS.
Key Limitations: The paper does not discuss the limitations of the proposed evaluation framework, AutoAIS, or the explored architectures and supervision levels.

--------------------------------------------------

Claim 3:
Type: methodology
Statement: The paper's evaluation framework uses human annotations as a gold standard.
Location: Evaluation
Exact Quote: We consider two evaluation metrics for the Attributed QA task: first, human ratings that are the gold-standard, and second, automatic evaluation methods.

Evidence:
- Evidence Text: Human raters are trained using repeated annotations with feedback until reaching high performance on the task.
  Strength: strong
  Location: Evaluation
  Limitations: The paper does not discuss the limitations of the human rating process.
  Exact Quote: Given the cost of human rating, we evaluate on 1000 randomly-chosen questions and estimate standard errors using two-sided bootstrap re-sampling.

- Evidence Text: The paper uses AutoAIS as an automatic evaluation method.
  Strength: strong
  Location: Evaluation
  Limitations: The paper does not discuss the limitations of using AutoAIS as an evaluation method.
  Exact Quote: AutoAIS formulates evaluation as a Natural Language Inference task that asks a model whether the question and answer are entailed by the provided attribution.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The paper uses human annotations as a gold standard and AutoAIS as an automatic evaluation method.
Key Limitations: The paper does not discuss the limitations of the human rating process or AutoAIS.

--------------------------------------------------

Claim 4:
Type: result
Statement: The paper's evaluation framework shows strong correlation between human ratings and AutoAIS.
Location: 5. Correlation between AIS and EM/AutoAIS
Exact Quote: We observe that best AIS performance did not necessarily go hand-in-hand with best EM accuracy. Consistent with this, the Pearson correlation coefficient between the system EM and AIS scores is modest, at 0.45. However, correlation between system AIS and AutoAIS scores is remarkably strong, with a Pearson coefficient of 0.96.

Evidence:
- Evidence Text: The Pearson correlation coefficient between the system EM and AIS scores is modest, at 0.45.
  Strength: strong
  Location: 5. Correlation between AIS and EM/AutoAIS
  Limitations: The correlation between EM and AIS scores does not necessarily indicate a strong relationship between the two metrics.
  Exact Quote: We observe that best AIS performance did not necessarily go hand-in-hand with best EM accuracy. Consistent with this, the Pearson correlation coefficient between the system EM and AIS scores is modest, at 0.45.

- Evidence Text: The Pearson correlation coefficient between system AIS and AutoAIS scores is remarkably strong, with a Pearson coefficient of 0.96.
  Strength: strong
  Location: 5. Correlation between AIS and EM/AutoAIS
  Limitations: The strong correlation between system AIS and AutoAIS scores does not necessarily indicate a strong relationship between the two metrics.
  Exact Quote: However, correlation between system AIS and AutoAIS scores is remarkably strong, with a Pearson coefficient of 0.96.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The paper demonstrates a strong correlation between human ratings and AutoAIS.
Key Limitations: The paper does not discuss the limitations of the correlation between human ratings and AutoAIS.

--------------------------------------------------

Claim 5:
Type: result
Statement: The paper's evaluation framework shows that AutoAIS is fit-for-purpose as a development metric.
Location: 5. Correlation between AIS and EM/AutoAIS
Exact Quote: AutoAIS is fit-for-purpose as a development metric, but had shortcomings including only moderate correlation with human ratings at the instance-level.

Evidence:
- Evidence Text: The paper demonstrates a strong correlation between human ratings and AutoAIS at the system level.
  Strength: strong
  Location: 5. Correlation between AIS and EM/AutoAIS
  Limitations: The correlation between human ratings and AutoAIS at the instance-level is only moderate.
  Exact Quote: AutoAIS is fit-for-purpose as a development metric, but had shortcomings including only moderate correlation with human ratings at the instance-level.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The paper demonstrates that AutoAIS is fit-for-purpose as a development metric.
Key Limitations: The paper does not discuss the limitations of using AutoAIS as a development metric.

--------------------------------------------------

Claim 6:
Type: result
Statement: The paper's evaluation framework shows that retrieve-then-read approaches achieve the strongest performance on Attributed QA.
Location: 5. Experiments
Exact Quote: Retrieve-then-read approaches achieve the strongest performance on our evaluation, but require full use of a traditional training set.

Evidence:
- Evidence Text: The best RTR system achieves the highest performance on AIS.
  Strength: strong
  Location: 5. Experiments
  Limitations: The best RTR system requires full use of a traditional training set.
  Exact Quote: The best RTR achieves the highest performance (p 10[−][5], t = 4.55, in comparison with the _≪_ best non-RTR system)

- Evidence Text: The best RTR system uses LLMs with relatively small numbers of parameters.
  Strength: moderate
  Location: 5. Experiments
  Limitations: The best RTR system uses LLMs with relatively small numbers of parameters, which may limit its performance.
  Exact Quote: However, RTR approaches have the shortcoming that they require relatively large amounts of explicit supervision, for example in the form of NQ examples.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The paper demonstrates that retrieve-then-read approaches achieve the strongest performance on Attributed QA.
Key Limitations: Retrieve-then-read approaches require full use of a traditional training set and may be resource-intensive.

--------------------------------------------------

Claim 7:
Type: result
Statement: The paper's evaluation framework shows that post-hoc attribution is a viable architecture for future work.
Location: 5. Experiments
Exact Quote: Post-hoc attribution appears to be a viable architecture for future work, but remains challenging.

Evidence:
- Evidence Text: The best post-hoc retrieval system achieves relatively high EM.
  Strength: moderate
  Location: 5. Experiments
  Limitations: The best post-hoc retrieval system requires minimal amounts of supervision for answer generation.
  Exact Quote: However, these models generally require LLMs with large numbers of parameters.

- Evidence Text: The best LLM-as-retriever system is competitive with low-resource post-hoc attribution.
  Strength: moderate
  Location: 5. Experiments
  Limitations: The best LLM-as-retriever system requires LLMs with large numbers of parameters.
  Exact Quote: End-to-end models have the potential benefit of not requiring retrieval at all.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The paper demonstrates that post-hoc attribution is a viable architecture for future work.
Key Limitations: Post-hoc attribution remains challenging and requires LLMs with large numbers of parameters.

--------------------------------------------------

Claim 8:
Type: result
Statement: The paper's evaluation framework shows that AutoAIS is fit-for-purpose as a development metric but has shortcomings.
Location: 5. Correlation between AIS and EM/AutoAIS
Exact Quote: AutoAIS is fit-for-purpose as a development metric, but had shortcomings including only moderate correlation with human ratings at the instance-level.

Evidence:
- Evidence Text: The Pearson correlation coefficient between system AIS and AutoAIS scores is remarkably strong, with a Pearson coefficient of 0.96.
  Strength: strong
  Location: 5. Correlation between AIS and EM/AutoAIS
  Limitations: The correlation between human ratings and AutoAIS at the instance-level is only moderate.
  Exact Quote: AutoAIS is fit-for-purpose as a development metric, but had shortcomings including only moderate correlation with human ratings at the instance-level.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The paper demonstrates that AutoAIS is fit-for-purpose as a development metric but has shortcomings.
Key Limitations: AutoAIS has shortcomings including only moderate correlation with human ratings at the instance-level.

--------------------------------------------------

Claim 9:
Type: result
Statement: The paper's evaluation framework shows that retrieve-then-read approaches require full use of a traditional training set.
Location: 5. Experiments
Exact Quote: Retrieve-then-read approaches require full use of a traditional training set.

Evidence:
- Evidence Text: The best RTR system uses LLMs with relatively small numbers of parameters.
  Strength: moderate
  Location: 5. Experiments
  Limitations: The best RTR system uses LLMs with relatively small numbers of parameters, which may limit its performance.
  Exact Quote: However, RTR approaches have the shortcoming that they require relatively large amounts of explicit supervision, for example in the form of NQ examples.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The paper demonstrates that retrieve-then-read approaches require full use of a traditional training set.
Key Limitations: Retrieve-then-read approaches require full use of a traditional training set and may be resource-intensive.

--------------------------------------------------

Claim 10:
Type: result
Statement: The paper's evaluation framework shows that post-hoc attribution remains challenging.
Location: 5. Experiments
Exact Quote: Post-hoc attribution appears to be a viable architecture for future work, but remains challenging.

Evidence:
- Evidence Text: The best post-hoc retrieval system requires minimal amounts of supervision for answer generation.
  Strength: moderate
  Location: 5. Experiments
  Limitations: The best post-hoc retrieval system requires minimal amounts of supervision for answer generation.
  Exact Quote: However, these models generally require LLMs with large numbers of parameters.

- Evidence Text: The best LLM-as-retriever system is competitive with low-resource post-hoc attribution.
  Strength: moderate
  Location: 5. Experiments
  Limitations: The best LLM-as-retriever system requires LLMs with large numbers of parameters.
  Exact Quote: End-to-end models have the potential benefit of not requiring retrieval at all.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The paper demonstrates that post-hoc attribution remains challenging.
Key Limitations: Post-hoc attribution remains challenging and requires LLMs with large numbers of parameters.

--------------------------------------------------

