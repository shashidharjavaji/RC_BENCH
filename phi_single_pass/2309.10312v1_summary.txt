Claim 1:
Type: methodology
Statement: The observational mode evaluates whether a neuron activates on all and only input strings that refer to a concept picked out by the proposed explanation.
Location: 2. Observation-Based Evaluation
Exact Quote: In the observational mode, we evaluate whether the neuron a activates on all and only input strings that refer to a concept picked out by the proposed explanation E.

Evidence:
- Evidence Text: The authors construct two sets of test sentences to probe for Type I and Type II errors.
  Strength: moderate
  Location: 3. Observation-Based Evaluation
  Limitations: The approach relies on the assumption that the explanation can be approximated by a finite set of strings.
  Exact Quote: We construct two sets of test sentences. One set probes for Type I errors by evaluating the claim 'a activates on q E', and the other set probes for Type II errors by evaluating the claim 'a only activates on q E'. We search for token sequences that the neuron a activates on over a large corpus, record the sentence context of the token sequence, and prompt GPT-3.5 to determine whether the token sequence is in E.'

- Evidence Text: The authors report low F1 scores in the observational mode.
  Strength: strong
  Location: 3. Observation-Based Evaluation
  Limitations: The low F1 scores suggest that the observational mode may not be reliable for evaluating the faithfulness of natural language explanations.
  Exact Quote: Our experimental results show that the Bills et al. 2023 explanations are not well aligned with neuron activations; with an F1 score around 0.6 across 300 of the top-scoring explanations, it seems as though it would be risky to depend on these explanations for downstream tasks.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence supports the claim that the observational mode has limitations in evaluating the faithfulness of natural language explanations.
Key Limitations: The approach may not capture the full range of neuron activations and may be sensitive to the choice of test sentences.

--------------------------------------------------

Claim 2:
Type: methodology
Statement: The intervention mode assesses whether the neuron is a causal mediator of the concept denoted by the explanation.
Location: 4. Intervention-Based Evaluation
Exact Quote: The goal of intervention-based evaluation is to assess the claim that a neuron a is a causal mediator of the concept denoted by E.

Evidence:
- Evidence Text: The authors identify tasks that depend on the concept and intervene on the neuron a to study whether the neuron is a causal mediator of the concept.
  Strength: moderate
  Location: 4. Intervention-Based Evaluation
  Limitations: The approach relies on the assumption that the model performs the task perfectly.
  Exact Quote: To conduct these analyses, we first identify a task that takes any string q E as part of the input and has an output behavior that depends on E.

- Evidence Text: The authors report little or no evidence for causal effects in the intervention mode.
  Strength: strong
  Location: 4. Intervention-Based Evaluation
  Limitations: The lack of evidence for causal effects suggests that individual neurons may not be the best unit of analysis for understanding model behavior.
  Exact Quote: we are unable to find evidence that neurons are causal mediators of the concepts denoted by the explanations.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence supports the claim that individual neurons may not be the best unit of analysis for understanding model behavior.
Key Limitations: The approach may not capture the full range of model behaviors and may be sensitive to the choice of tasks.

--------------------------------------------------

Claim 3:
Type: contribution
Statement: Natural language explanations of neurons may not be the best choice for explaining large language models.
Location: 5. General Discussion
Exact Quote: We are more optimistic about approaches to model explanation that are grounded in structured formalisms (e.g., programming languages) and seek to explain how groups of neurons act in concert to represent examples and shape input–output behaviors.

Evidence:
- Evidence Text: The authors report low F1 scores in the observational mode and little or no evidence for causal effects in the intervention mode.
  Strength: moderate
  Location: 5. General Discussion
  Limitations: The evidence is limited to the GPT-2 XL model and may not generalize to other models.
  Exact Quote: Our work contributes to improving the faithfulness of neuron interpretability methods that use natural language as a medium. Faithful explanation could provide the basis for safety assessments, bias detection efforts, model editing, and many other downstream applications.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence supports the claim that natural language explanations may not be the best choice for explaining large language models, but the evidence is limited to the GPT-2 XL model and may not generalize to other models.
Key Limitations: The approach may not capture the full range of model behaviors and may be sensitive to the choice of tasks.

--------------------------------------------------

Claim 4:
Type: result
Statement: Individual neurons may not be the best unit of analysis for understanding model behavior.
Location: 4. Intervention-Based Evaluation
Exact Quote: MLP layer neurons, when evaluated as a whole, have strong causal effects on model behavior, especially in the first layer.

Evidence:
- Evidence Text: The authors report high interchange intervention accuracy (IIA) for MLP layer neurons.
  Strength: moderate
  Location: 4. Intervention-Based Evaluation
  Limitations: The evidence is limited to the GPT-2 XL model and may not generalize to other models.
  Exact Quote: MLP layer neurons, when evaluated as a whole, have strong causal effects on model behavior, especially in the first layer.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence supports the claim that individual neurons may not be the best unit of analysis for understanding model behavior.
Key Limitations: The evidence is limited to the GPT-2 XL model and may not generalize to other models.

--------------------------------------------------

Claim 5:
Type: contribution
Statement: Structured formalisms (e.g., programming languages) may be better for explaining large language models.
Location: 5. General Discussion
Exact Quote: We are more optimistic about approaches to model explanation that are grounded in structured formalisms (e.g., programming languages) and seek to explain how groups of neurons act in concert to represent examples and shape input–output behaviors.

Evidence:
- Evidence Text: The authors report low F1 scores in the observational mode and little or no evidence for causal effects in the intervention mode.
  Strength: moderate
  Location: 5. General Discussion
  Limitations: The evidence is limited to the GPT-2 XL model and may not generalize to other models.
  Exact Quote: Our work contributes to improving the faithfulness of neuron interpretability methods that use natural language as a medium. Faithful explanation could provide the basis for safety assessments, bias detection efforts, model editing, and many other downstream applications.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence supports the claim that structured formalisms may be better for explaining large language models, but the evidence is limited to the GPT-2 XL model and may not generalize to other models.
Key Limitations: The approach may not capture the full range of model behaviors and may be sensitive to the choice of tasks.

--------------------------------------------------

