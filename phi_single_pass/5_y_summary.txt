Claim 1:
Type: contribution
Statement: A simple ResNet-like architecture is an effective baseline for tabular DL, which was overlooked by existing literature.
Location: Introduction
Exact Quote: we demonstrate that a simple ResNet-like architecture is an effective baseline for tabular DL, which was overlooked by existing literature.

Evidence:
- Evidence Text: ResNet outperforms other models on most tasks and becomes a new powerful solution for the field.
  Strength: strong
  Location: Results
  Limitations: none mentioned
  Exact Quote: ResNet turns out to be an effective baseline that none of the competitors can consistently outperform.

- Evidence Text: ResNet outperforms MLP on tasks where deeper representations can be helpful.
  Strength: moderate
  Location: Results
  Limitations: none mentioned
  Exact Quote: Overall, we expect this architecture to outperform MLP on tasks where deeper representations can be helpful.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence shows that ResNet outperforms other models and MLP on tasks where deeper representations are beneficial.
Key Limitations: none mentioned

--------------------------------------------------

Claim 2:
Type: result
Statement: FT-Transformer is a simple adaptation of the Transformer architecture that outperforms other DL solutions on most tasks.
Location: Introduction
Exact Quote: we introduce FT-Transformer — a simple adaptation of the Transformer architecture for tabular data that becomes a new powerful solution for the field.

Evidence:
- Evidence Text: FT-Transformer outperforms other models on most tasks.
  Strength: strong
  Location: Results
  Limitations: none mentioned
  Exact Quote: FT-Transformer performs best on most tasks and becomes a new powerful solution for the field.

- Evidence Text: FT-Transformer performs on par with ResNet on the remaining problems.
  Strength: moderate
  Location: Results
  Limitations: none mentioned
  Exact Quote: FT-Transformer performs on par with ResNet on the remaining problems.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence shows that FT-Transformer outperforms other models on most tasks and performs on par with ResNet on the remaining problems.
Key Limitations: none mentioned

--------------------------------------------------

Claim 3:
Type: result
Statement: There is still no universally superior solution among GBDT and deep models.
Location: Conclusion
Exact Quote: we reveal that there is still no universally superior solution among GBDT and deep models.

Evidence:
- Evidence Text: GBDT dominates on some tasks.
  Strength: strong
  Location: Results
  Limitations: none mentioned
  Exact Quote: GBDT still dominates on some tasks.

- Evidence Text: DL models outperform GBDT on most of the tasks.
  Strength: strong
  Location: Results
  Limitations: none mentioned
  Exact Quote: DL models outperform GBDT on most of the tasks.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence shows that GBDT and DL models have their strengths and weaknesses, and no single model is universally superior.
Key Limitations: none mentioned

--------------------------------------------------

Claim 4:
Type: result
Statement: FT-Transformer provides competitive performance across the whole range of tasks.
Location: Analysis
Exact Quote: FT-Transformer yields competitive performance across the whole range of tasks.

Evidence:
- Evidence Text: FT-Transformer outperforms ResNet on tasks where GBDT is superior.
  Strength: strong
  Location: Analysis
  Limitations: none mentioned
  Exact Quote: FT-Transformer delivers most of its advantage over the 'conventional' DL model in the form of ResNet exactly on those problems where GBDT is superior.

- Evidence Text: FT-Transformer performs on par with ResNet on the remaining problems.
  Strength: moderate
  Location: Analysis
  Limitations: none mentioned
  Exact Quote: FT-Transformer performs on par with ResNet on the remaining problems.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence shows that FT-Transformer outperforms ResNet on tasks where GBDT is superior and performs on par with ResNet on the remaining problems.
Key Limitations: none mentioned

--------------------------------------------------

Claim 5:
Type: methodology
Statement: Feature biases in Feature Tokenizer are essential for good performance.
Location: Ablation study
Exact Quote: we test some design choices of FT-Transformer.

Evidence:
- Evidence Text: FT-Transformer without feature biases performs worse than with feature biases.
  Strength: strong
  Location: Ablation study
  Limitations: none mentioned
  Exact Quote: FT-Transformer (w/o feature biases) 0.470 0.381 0.724 0.727 0.958 8.843 0.964 0.751

- Evidence Text: FT-Transformer with feature biases performs better than without feature biases.
  Strength: strong
  Location: Ablation study
  Limitations: none mentioned
  Exact Quote: FT-Transformer **0.459 0.391 0.732 0.729 0.960 8.855 0.970 0.746**

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence shows that FT-Transformer with feature biases performs better than without feature biases.
Key Limitations: none mentioned

--------------------------------------------------

Claim 6:
Type: methodology
Statement: Attention maps can be a good choice in terms of cost-effectiveness for obtaining feature importances.
Location: Obtaining feature importances from attention maps
Exact Quote: the simple averaging of attention maps can be a good choice in terms of cost-effectiveness.

Evidence:
- Evidence Text: Attention maps yield reasonable feature importances.
  Strength: moderate
  Location: Obtaining feature importances from attention maps
  Limitations: none mentioned
  Exact Quote: The proposed method yields reasonable feature importances.

- Evidence Text: Attention maps perform similarly to Integrated Gradients.
  Strength: moderate
  Location: Obtaining feature importances from attention maps
  Limitations: none mentioned
  Exact Quote: The proposed method performs similarly to IG (note that this does not imply similarity to IG’s feature importances).

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence shows that attention maps yield reasonable feature importances and perform similarly to Integrated Gradients.
Key Limitations: none mentioned

--------------------------------------------------

