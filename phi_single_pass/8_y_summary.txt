Claim 1:
Type: contribution
Statement: MultimodalSum is the first work on self-supervised multimodal opinion summarization.
Location: Section 3
Exact Quote: This study is the first work on self-supervised multimodal opinion summarization;

Evidence:
- Evidence Text: The study is the first work on self-supervised multimodal opinion summarization.
  Strength: strong
  Location: Section 3
  Limitations: None
  Exact Quote: This study is the first work on self-supervised multimodal opinion summarization;

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The claim is explicitly stated as a contribution of the study.
Key Limitations: None

--------------------------------------------------

Claim 2:
Type: methodology
Statement: A multimodal training pipeline is proposed to resolve the heterogeneity between input modalities.
Location: Section 4
Exact Quote: To address this challenge, we propose a multimodal training pipeline.

Evidence:
- Evidence Text: The pipeline regards the text modality as a pivot modality.
  Strength: moderate
  Location: Section 5
  Limitations: The approach assumes that text modality can be used as a pivot for other modalities.
  Exact Quote: The pipeline regards the text modality as a pivot modality.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the description of the training pipeline.
Key Limitations: The approach assumes that text modality can be used as a pivot for other modalities.

--------------------------------------------------

Claim 3:
Type: result
Statement: The effectiveness of the model framework and training pipeline is verified through experiments on Yelp and Amazon datasets.
Location: Section 6
Exact Quote: We verified the effectiveness of our multimodal framework and training pipeline with various experiments on real review datasets.

Evidence:
- Evidence Text: The results for opinion summarization on two datasets are shown in Table 2.
  Strength: strong
  Location: Section 6
  Limitations: The results are limited to the Yelp and Amazon datasets.
  Exact Quote: The results for opinion summarization on two datasets are shown in Table 2.

- Evidence Text: MultimodalSum showed superior results compared with extractive and abstractive baselines for both token-level and document-level measures.
  Strength: strong
  Location: Section 7.1
  Limitations: The comparison is limited to the selected baseline models.
  Exact Quote: MultimodalSum showed superior results compared with extractive and abstractive baselines for both token-level and document-level measures.

- Evidence Text: MultimodalSum achieved state-of-the-art results on the Amazon dataset and outperformed the comparable model by a large margin in the R-L representing the ROUGE scores on the Yelp dataset.
  Strength: strong
  Location: Section 7.1
  Limitations: The comparison is limited to the selected baseline models.
  Exact Quote: MultimodalSum achieved state-of-the-art results on the Amazon dataset and outperformed the comparable model by a large margin in the R-L representing the ROUGE scores on the Yelp dataset.

- Evidence Text: MultimodalSum outperformed gold summaries for two criteria in human evaluation.
  Strength: moderate
  Location: Section 7.1.3
  Limitations: The comparison is limited to gold summaries and the evaluation is based on a small sample size.
  Exact Quote: MultimodalSum outperformed gold summaries for two criteria in human evaluation.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the results presented in the paper.
Key Limitations: The comparison is limited to gold summaries and the evaluation is based on a small sample size.

--------------------------------------------------

Claim 4:
Type: result
Statement: The utility of the table modality was higher than that of the image modality.
Location: Section 7.2
Exact Quote: The image modality contains detailed information not revealed in the table modality.

Evidence:
- Evidence Text: The aggregated value of the table was relatively high for generating 'Red Lobster', which is the name of the restaurants.
  Strength: moderate
  Location: Section 7.2
  Limitations: The result is based on a single example and may not generalize to other cases.
  Exact Quote: The aggregated value of the table was relatively high for generating 'Red Lobster', which is the name of the restaurants.

- Evidence Text: The aggregated values of the table were higher than those of the image in the entire test data.
  Strength: moderate
  Location: Section 7.2
  Limitations: The result is based on the entire test data and may not generalize to other cases.
  Exact Quote: The aggregated values of the table were higher than those of the image in the entire test data.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the results presented in the paper.
Key Limitations: The result is based on a single example and may not generalize to other cases.

--------------------------------------------------

Claim 5:
Type: result
Statement: The use of rating deviation improves the quality of summarization.
Location: Section 7.1.2
Exact Quote: Using only BART achieved comparable or better results than many extractive and abstractive baselines.

Evidence:
- Evidence Text: BART with further pretraining generated more diverse words and rich expressions from the review corpus.
  Strength: moderate
  Location: Section 7.1.2
  Limitations: The result is based on the Yelp and Amazon datasets and may not generalize to other datasets.
  Exact Quote: BART with further pretraining generated more diverse words and rich expressions from the review corpus.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the results presented in the paper.
Key Limitations: The result is based on the Yelp and Amazon datasets and may not generalize to other datasets.

--------------------------------------------------

Claim 6:
Type: result
Statement: The use of other modalities improves the summarization quality.
Location: Section 7.2
Exact Quote: Both modalities improved the summarization quality compared with UnimodalSum, and they brought additional improvements when used altogether.

Evidence:
- Evidence Text: Both modalities improved the summarization quality compared with UnimodalSum.
  Strength: moderate
  Location: Section 7.2
  Limitations: The comparison is limited to the selected baseline models.
  Exact Quote: Both modalities improved the summarization quality compared with UnimodalSum.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the results presented in the paper.
Key Limitations: The comparison is limited to the selected baseline models.

--------------------------------------------------

Claim 7:
Type: result
Statement: The use of the text modality and other modalities pretraining helps the training of the multimodal framework.
Location: Section 7.2
Exact Quote: MultimodalSum without text modality pretraining showed stable performance from the beginning, but the performance did not improve significantly.

Evidence:
- Evidence Text: MultimodalSum without text modality pretraining showed stable performance from the beginning, but the performance did not improve significantly.
  Strength: moderate
  Location: Section 7.2
  Limitations: The comparison is limited to the selected baseline models.
  Exact Quote: MultimodalSum without text modality pretraining showed stable performance from the beginning, but the performance did not improve significantly.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The claim is supported by the results presented in the paper.
Key Limitations: The comparison is limited to the selected baseline models.

--------------------------------------------------

