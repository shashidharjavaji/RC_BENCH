Claim 1:
Type: result
Statement: GopherCite is able to produce answers with high-quality supporting evidence 80% of the time on the Natural Questions subset and 67% of the time on the ELI5 subset.
Location: Results section
Exact Quote: The modelâ€™s response is found to be high-quality 80% of the time on this Natural Questions subset, and 67% of the time on the ELI5 subset.

Evidence:
- Evidence Text: Human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets.
  Strength: strong
  Location: Results section
  Limitations: Human evaluation may be subject to bias and does not guarantee objective truthfulness.
  Exact Quote: We measure the per-formance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets.

- Evidence Text: Abstaining from the third of questions for which it is most unsure improves performance to 90% and 80% respectively.
  Strength: moderate
  Location: Results section
  Limitations: Performance improvement is based on human evaluation and may not reflect objective truthfulness.
  Exact Quote: Abstaining from the third of questions for which it is most unsure improves performance to 90% and 80% respectively.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: high
Justification: The evidence is based on human evaluation, which is a standard method for assessing language model performance, though it may not fully capture objective truthfulness.
Key Limitations: Human evaluation is subjective and may not always align with objective truthfulness.

--------------------------------------------------

Claim 2:
Type: result
Statement: GopherCite's performance on the TruthfulQA dataset shows a misalignment between 'Supported' and 'True'.
Location: Discussion section
Exact Quote: When evaluated on the TruthfulQA benchmark Lin et al. (2021), GopherCite achieves high Supported&Plausible results but does not score well in the Truthful&Informative objective.

Evidence:
- Evidence Text: Qualitative examples in Table 5 illustrate situations in which the claim is Supported&Plausible, although it is not True in the sense of dataset definition.
  Strength: moderate
  Location: Discussion section
  Limitations: The evaluation method may not fully capture the nuances of truthfulness and informativeness.
  Exact Quote: Qualitative examples in Table 5 illustrate the misalignment between these metrics.

- Evidence Text: The SQA format is adversarial to such examples: if there is no document found by Search which states that Red Bull cannot cause wings to grow, it is very difficult to produce a true response with supporting evidence.
  Strength: moderate
  Location: Discussion section
  Limitations: The claim's truthfulness depends on the availability of supporting documents, which may not always exist.
  Exact Quote: The SQA setting is adversarial to such examples: if there is no document found by Search which states that Red Bull cannot cause wings to grow, it is very difficult to produce a true response with supporting evidence.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence is based on qualitative examples and discussion of the SQA format's limitations.
Key Limitations: The evaluation is based on qualitative examples and may not fully capture the complexity of truthfulness and informativeness.

--------------------------------------------------

Claim 3:
Type: result
Statement: Reward model training using human preferences shows dramatic improvement when used for reranking responses and as a target for reinforcement learning.
Location: Results section
Exact Quote: Reward modelling using these human ratings shows dramatic improvement when used for reranking responses and as a target for reinforcement learning.

Evidence:
- Evidence Text: Reward model training using human preferences shows dramatic improvement when used for reranking responses and as a target for reinforcement learning.
  Strength: strong
  Location: Results section
  Limitations: The improvement is based on human evaluation and may not fully capture objective truthfulness.
  Exact Quote: Reward modelling using these human ratings shows dramatic improvement when used for reranking responses and as a target for reinforcement learning.

- Evidence Text: The benefit is less clear when combining the generator models with reranking.
  Strength: moderate
  Location: Results section
  Limitations: The improvement is less clear when combining the generator models with reranking.
  Exact Quote: The benefit is less clear when combining the generator models with reranking.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence is based on human evaluation and shows consistent improvement in performance.
Key Limitations: The improvement is based on human evaluation and may not fully capture objective truthfulness.

--------------------------------------------------

Claim 4:
Type: result
Statement: Reward modeling provides a natural mechanism to abstain from answering when we lack confidence in an answer.
Location: Results section
Exact Quote: Reward modeling provides a natural mechanism to abstain from answering when we lack confidence in an answer.

Evidence:
- Evidence Text: Abstaining from the third of questions for which it is most unsure improves performance to 90% and 80% respectively.
  Strength: moderate
  Location: Results section
  Limitations: Performance improvement is based on human evaluation and may not reflect objective truthfulness.
  Exact Quote: Abstaining from the third of questions for which it is most unsure improves performance to 90% and 80% respectively.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: high
Justification: The evidence is based on human evaluation and shows consistent improvement in performance.
Key Limitations: Human evaluation may not fully capture objective truthfulness.

--------------------------------------------------

Claim 5:
Type: result
Statement: GopherCite's performance improves with model scale and the number of candidates used for reranking.
Location: Results section
Exact Quote: Model performance varies with the number of model parameters, and the number of candidates used for reranking.

Evidence:
- Evidence Text: Scaling the Supervised Fine-tuning generator brings clear improvements in both the Supported&Plausible scores as well as the Preference judgements.
  Strength: strong
  Location: Results section
  Limitations: The improvement is based on human evaluation and may not fully capture objective truthfulness.
  Exact Quote: Model performance varies with the number of model parameters, and the number of candidates used for reranking.

- Evidence Text: RL is outperformed by SFT consistently for all numbers of candidates.
  Strength: moderate
  Location: Results section
  Limitations: The comparison is based on human evaluation and may not fully capture objective truthfulness.
  Exact Quote: RL is outperformed by SFT consistently for all numbers of candidates.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence is based on human evaluation and shows consistent improvement in performance.
Key Limitations: Human evaluation may not fully capture objective truthfulness.

--------------------------------------------------

