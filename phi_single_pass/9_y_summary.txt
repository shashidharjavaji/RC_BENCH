Claim 1:
Type: result
Statement: Image prompts cast into the transformer embedding space do not encode interpretable semantics.
Location: Section 3.1
Exact Quote: A KolmogorovSmirnov test [19, 37] shows no significant difference (D =.037, p >.5) between CLIPScore distributions comparing real decoded prompts and random embeddings to images.

Evidence:
- Evidence Text: KolmogorovSmirnov test results showing no significant difference between CLIPScore distributions of real and random embeddings.
  Strength: strong
  Location: Section 3.1
  Limitations: The test does not account for the possibility of subtle semantic differences not captured by CLIPScore.
  Exact Quote: A KolmogorovSmirnov test [19, 37] shows no significant difference (D =.037, p >.5) between CLIPScore distributions comparing real decoded prompts and random embeddings to images.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The statistical test provides strong evidence that image prompts do not encode interpretable semantics.
Key Limitations: The test may not capture all forms of semantic encoding.

--------------------------------------------------

Claim 2:
Type: result
Statement: Multimodal neurons can be found within the transformer, and they are active in response to particular image semantics.
Location: Section 2.2
Exact Quote: We calculate gk,c for uk across all layers with respect to the first noun c in the generated caption.

Evidence:
- Evidence Text: Calculation of gk,c for neurons across all layers.
  Strength: moderate
  Location: Section 2.2
  Limitations: The method assumes that the first noun is the most relevant semantic concept, which may not always be the case.
  Exact Quote: We calculate gk,c for uk across all layers with respect to the first noun c in the generated caption.

- Evidence Text: Heatmaps showing consistent selectivity for image regions translated into related text.
  Strength: moderate
  Location: Section 4.2
  Limitations: Heatmaps may not capture all relevant semantic concepts.
  Exact Quote: Figure 4. Top-activating COCO images for two multimodal neurons. Heatmaps (0.95 percentile of activations) illustrate consistent selectivity for image regions translated into related text.

Evaluation:
Conclusion Justified: Yes
Robustness: medium
Confidence Level: medium
Justification: The evidence supports the claim that multimodal neurons are active in response to specific image semantics.
Key Limitations: The method may not capture all relevant semantic concepts.

--------------------------------------------------

Claim 3:
Type: result
Statement: Multimodal neurons causally affect output: modulating them can remove concepts from image captions.
Location: Section 3.3
Exact Quote: Ablating the same number of top-scoring units decreases token probability by 80% on average.

Evidence:
- Evidence Text: Ablation of top-scoring units leads to a significant decrease in token probability.
  Strength: strong
  Location: Section 3.3
  Limitations: The effect of ablation may vary depending on the specific units and images.
  Exact Quote: Ablating the same number of top-scoring units decreases token probability by 80% on average.

- Evidence Text: Ablation of multimodal neurons leads to significant changes in the semantics of GPT-generated captions.
  Strength: moderate
  Location: Section 3.3
  Limitations: The changes in semantics may not be consistent across all images and concepts.
  Exact Quote: Ablating multimodal neurons also leads to significant changes in the semantics of GPT-generated captions.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence shows that modulating multimodal neurons can significantly affect the output of the model.
Key Limitations: The effect of modulation may vary depending on the specific units and images.

--------------------------------------------------

Claim 4:
Type: result
Statement: Decoded tokens from multimodal neurons perform competitively with GPT image captions on CLIPScore, and outperform a baseline on BERTScore.
Location: Section 3.2
Exact Quote: Table 1 shows that tokens decoded from multimodal neurons perform competitively with GPT image captions on CLIPScore, and outperform a baseline on BERTScore.

Evidence:
- Evidence Text: Comparison of CLIPScore and BERTScore for decoded tokens and GPT captions.
  Strength: strong
  Location: Section 3.2
  Limitations: The comparison is limited to a subset of the validation set and may not generalize to all images and concepts.
  Exact Quote: Table 1 shows that tokens decoded from multimodal neurons perform competitively with GPT image captions on CLIPScore, and outperform a baseline on BERTScore.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The evidence shows that decoded tokens from multimodal neurons perform competitively with GPT image captions.
Key Limitations: The comparison is limited to a subset of the validation set and may not generalize to all images and concepts.

--------------------------------------------------

