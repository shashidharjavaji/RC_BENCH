Claim 1:
Type: methodology
Statement: Integrated Gradients satisfy Sensitivity(a) and Implementation Invariance.
Location: Section 3
Exact Quote: Integrated gradients are the unique path method that is symmetry-preserving.

Evidence:
- Evidence Text: Theorem 1 proves that Integrated gradients satisfy Sensitivity(a) and Implementation Invariance.
  Strength: strong
  Location: Section 4.2
  Limitations: The proof assumes a non-straightline path and may not generalize to all possible paths.
  Exact Quote: Proof. Consider a non-straightline path γ : [0, 1]_ R[n] → from baseline to input. W.l.o.g., there exists t0 ∈ [0, 1] such that for two dimensions i, j, γi(t0) > γj(t0).

- Evidence Text: Integrated gradients are the unique path method that is symmetry-preserving.
  Strength: strong
  Location: Section 4.2
  Limitations: The proof assumes a non-straightline path and may not generalize to all possible paths.
  Exact Quote: Theorem 1 proves that Integrated gradients satisfy Sensitivity(a) and Implementation Invariance.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The proof provided in the paper demonstrates that Integrated gradients satisfy Sensitivity(a) and Implementation Invariance under the assumption of a non-straightline path.
Key Limitations: The proof assumes a non-straightline path and may not generalize to all possible paths.

--------------------------------------------------

Claim 2:
Type: methodology
Statement: Integrated Gradients can be implemented using a few calls to the gradient operator.
Location: Section 3
Exact Quote: Integrated gradients can be implemented using a few calls to the gradients operator, can be applied to a variety of deep networks, and has a strong theoretical justification.

Evidence:
- Evidence Text: The method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator.
  Strength: strong
  Location: Section 3
  Limitations: The implementation simplicity may depend on the complexity of the network and the baseline.
  Exact Quote: Integrated gradients can be implemented using a few calls to the gradient operator, can be applied to a variety of deep networks, and has a strong theoretical justification.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The paper provides a clear explanation of how Integrated gradients can be implemented using a few calls to the gradient operator.
Key Limitations: The implementation simplicity may depend on the complexity of the network and the baseline.

--------------------------------------------------

Claim 3:
Type: methodology
Statement: Integrated Gradients satisfy Completeness.
Location: Section 3
Exact Quote: Integrated gradients satisfy an axiom called completeness that the attributions add up to the difference between the output of F at the input x and the baseline x[′].

Evidence:
- Evidence Text: Proposition 1 states that Integrated gradients satisfy Completeness.
  Strength: strong
  Location: Section 3
  Limitations: The proposition assumes that the function F is differentiable almost everywhere.
  Exact Quote: Proposition 1. If F : R[n] → R is differentiable almost everywhere then Σ[n]i=1[IntegratedGrads]i[(][x][) =][ F] [(][x][)][ −] _[F]_ [(][x][′][)]

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The proposition provides a mathematical proof that Integrated gradients satisfy Completeness.
Key Limitations: The proposition assumes that the function F is differentiable almost everywhere.

--------------------------------------------------

Claim 4:
Type: methodology
Statement: Integrated Gradients can be applied to a variety of deep networks.
Location: Section 6
Exact Quote: We apply this method to a couple of image models, a couple of text models and a chemistry model.

Evidence:
- Evidence Text: The method is applied to image models, text models, and a chemistry model.
  Strength: strong
  Location: Section 6
  Limitations: The paper does not provide a detailed comparison of the performance of Integrated gradients across different types of networks.
  Exact Quote: We apply this method to a couple of image models, a couple of text models and a chemistry model.

Evaluation:
Conclusion Justified: Yes
Robustness: high
Confidence Level: high
Justification: The paper demonstrates the application of Integrated gradients to different types of deep networks.
Key Limitations: The paper does not provide a detailed comparison of the performance of Integrated gradients across different types of networks.

--------------------------------------------------

Claim 5:
Type: result
Statement: Integrated Gradients can be used to debug networks, extract rules from a network, and enable users to engage with models better.
Location: Section 6
Exact Quote: These applications demonstrate the use of our technique in either improving our understanding of the network, performing debugging, performing rule extraction, or aiding an end user in understanding the network’s prediction.

Evidence:
- Evidence Text: The method is applied to image models, text models, and a chemistry model.
  Strength: moderate
  Location: Section 6
  Limitations: The paper does not provide a detailed comparison of the performance of Integrated gradients across different types of networks.
  Exact Quote: We apply this method to a couple of image models, a couple of text models and a chemistry model.

Evaluation:
Conclusion Justified: Yes
Robustness: moderate
Confidence Level: medium
Justification: The paper demonstrates the application of Integrated gradients to different types of deep networks, but does not provide a detailed comparison of the performance of Integrated gradients across different types of networks.
Key Limitations: The paper does not provide a detailed comparison of the performance of Integrated gradients across different types of networks.

--------------------------------------------------

